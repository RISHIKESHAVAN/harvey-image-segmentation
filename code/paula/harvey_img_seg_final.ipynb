{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S3Z9RpTzMru"
   },
   "source": [
    "# IMAGE SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO5JSO_XoMHP"
   },
   "source": [
    "0.60939 (early stopping)\n",
    "\n",
    "test dice loss 0,68\n",
    "\n",
    "0.60600 normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrOVaxkAzMrz"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1qeXTk22P5G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import datetime\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "%matplotlib inline\n",
    "#!pip install -U tensorflow-addons\n",
    "#!pip install livelossplot\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from livelossplot import PlotLossesKeras\n",
    "#from tensorflow_addons.layers import GroupNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RadkHpETzMr1",
    "outputId": "d8170906-8ef5-4f2b-8de2-d8846b861ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZnNUG36zMr2"
   },
   "source": [
    "Checking the available CPU and GPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1_iOX3GzMr3",
    "outputId": "f611d3df-0d44-45be-eddf-7bea936b14ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUMFGSRrzMr4"
   },
   "source": [
    "## Initial Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3h2E27PzMr4"
   },
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcCzFmCFzMr5"
   },
   "source": [
    "Setting the path to train images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIYBs8DgjK_6",
    "outputId": "31763f65-f6ae-499a-c9ce-d7ae6c774629"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3UlFrQCxtyko"
   },
   "outputs": [],
   "source": [
    "train_x_loc = \"../../data/train_images/\"\n",
    "train_y_loc = \"../../data/train_masks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iowZOLHzMr6"
   },
   "source": [
    "Extract the names of the image files and sort and store them in a list. This will later be iterated over to read and store the image and mask data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ahdPNGncxMTj"
   },
   "outputs": [],
   "source": [
    "img_names = [s[:-4] for s in os.listdir(train_x_loc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tecctf2RzMr7"
   },
   "source": [
    "In order to prevent clogging up the RAM, we will create batches of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zLXFdphuzMr7"
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnX7IxWdzMr8",
    "outputId": "480d2cd4-e778-4dfc-f2a4-ef18f9becb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images were split into batches of size 87.\n",
      "Number of batches =  3\n"
     ]
    }
   ],
   "source": [
    "img_names_batches = [img_names[i:i + TRAIN_BATCH_SIZE] for i in range(0, len(img_names), TRAIN_BATCH_SIZE)]\n",
    "print(\"Train images were split into batches of size {}.\".format(TRAIN_BATCH_SIZE))\n",
    "print(\"Number of batches = \",len(img_names_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jawz78kazMr8"
   },
   "source": [
    "### Image Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QntnYbjhzMr8"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 512\n",
    "N_CHANNEL = 3\n",
    "N_CLASSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjHfal1DzMr9"
   },
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgTnNVo9zMr9"
   },
   "source": [
    "### UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9ngCTNyYzMr9"
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((INPUT_SIZE, INPUT_SIZE, N_CHANNEL))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.2)(pool1) # Dropout(0.2)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.2)(pool2) #Dropout(0.2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.2)(pool3) \n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.2)(pool4) # Dropout(0.2)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(N_CLASSES, (1, 1), activation=\"softmax\")(conv9) #\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model\n",
    "    \n",
    "def get_unet2(): #testing groupNormalization\n",
    "    inputs = Input((INPUT_SIZE, INPUT_SIZE, N_CHANNEL))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    #conv1 = GroupNormalization(groups=4,axis=2)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.2)(pool1) # Dropout(0.2)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    #conv2 = GroupNormalization(groups=4)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.2)(pool2) #Dropout(0.2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    #conv3 = GroupNormalization(groups=4)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.2)(pool3) \n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    #conv4 = GroupNormalization(groups=4)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.2)(pool4) # Dropout(0.2)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(drop4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(N_CLASSES, (1, 1), activation=\"softmax\")(conv9) #\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01svdM8IzMr-"
   },
   "source": [
    "In semantic segmentation, you need as many masks as you have object classes. \n",
    "In our dataset, each pixel in every mask has been assigned a single integer probability that it belongs to a \n",
    "certain class - 0 to 26. The correct class is the one with the highest probability. \n",
    "\n",
    "Sparse categorical crossentropy is more efficient than other loss functions when you're dealing with \n",
    "lots of classes and to perform pixel-wise multiclass prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNqMXmH6zMr_"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT1GfWrGzMr_"
   },
   "source": [
    "### Creating CallBacks\n",
    "\n",
    "A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9AdBOqXCzMr_"
   },
   "outputs": [],
   "source": [
    "# TensorBoard visuluaziations\n",
    "# To visualize, execute the following commands in a new jupyter notebook:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /tmp/tboard_logs8\n",
    "tensorboard = TensorBoard(log_dir='/tmp/tboard_logs8', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def on_train_batch_begin(self, batch, logs=None):\n",
    "    print('\\nTraining: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    print('\\nTraining: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "  def on_test_batch_begin(self, batch, logs=None):\n",
    "    print('\\nEvaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "  def on_test_batch_end(self, batch, logs=None):\n",
    "    print('\\nEvaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "#Other callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"accuracy\",patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjaKkHj70sD"
   },
   "source": [
    "Defining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SmR6CUDK70CP"
   },
   "outputs": [],
   "source": [
    "import keras.losses \n",
    "from keras.metrics import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)= sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "# def mean_iou(y_true, y_pred):\n",
    "#     prec = []\n",
    "#     for t in np.arange(0.5, 1.0, 0.05):\n",
    "#         y_pred_ = tf.cast(y_pred > t, tf.int32) #tf.to_int32(y_pred > t)\n",
    "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "#         K.get_session().run(tf.local_variables_initializer())\n",
    "#         with tf.control_dependencies([up_opt]):\n",
    "#             score = tf.identity(score)\n",
    "#         prec.append(score)\n",
    "#     return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "keras.losses.custom_objects = bce_dice_loss\n",
    "# keras.api._v2.keras.metrics.custom_objects = mean_iou\n",
    "\n",
    "# tf.keras.utils.get_custom_objects()['bce_dice_loss'] = bce_dice_loss\n",
    "# tf.keras.utils.get_custom_objects()['mean_iou'] = mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OKNz6GAzMr_"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NtP4MlDhzMsA"
   },
   "outputs": [],
   "source": [
    "# img_names_batches = [img_names_batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGbASac8zMsA",
    "outputId": "01cafc4e-b627-42c7-f92f-6626b445baaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 87\n",
      "1 87\n",
      "2 87\n"
     ]
    }
   ],
   "source": [
    "for i, imgnames in enumerate(img_names_batches):\n",
    "    print(i, len(imgnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8jgjerXzMsA",
    "outputId": "3645bf46-c5f1-49b5-e123-11438cf97808",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model set. Creating new model.\n",
      "UNet model created from scratch.\n",
      "Reading train data\n",
      "x train ----  120\n",
      "x val ----  54\n",
      "Train and Validation data created\n",
      "Epoch 1/25\n",
      "\n",
      "Training: batch 0 begins at 21:53:57.940887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 21:53:58.398946: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 0 ends at 21:53:59.798439\n",
      " 1/60 [..............................] - ETA: 1:52 - loss: 1.6486 - bce_dice_loss: 1.6486\n",
      "Training: batch 1 begins at 21:53:59.854557\n",
      "\n",
      "Training: batch 1 ends at 21:54:00.609359\n",
      " 2/60 [>.............................] - ETA: 43s - loss: 1.6251 - bce_dice_loss: 1.6251 \n",
      "Training: batch 2 begins at 21:54:00.613745\n",
      "\n",
      "Training: batch 2 ends at 21:54:01.385731\n",
      " 3/60 [>.............................] - ETA: 43s - loss: 1.5971 - bce_dice_loss: 1.5971\n",
      "Training: batch 3 begins at 21:54:01.389053\n",
      "\n",
      "Training: batch 3 ends at 21:54:02.171995\n",
      " 4/60 [=>............................] - ETA: 43s - loss: 1.5740 - bce_dice_loss: 1.5740\n",
      "Training: batch 4 begins at 21:54:02.176609\n",
      "\n",
      "Training: batch 4 ends at 21:54:02.925377\n",
      " 5/60 [=>............................] - ETA: 42s - loss: 1.5774 - bce_dice_loss: 1.5774\n",
      "Training: batch 5 begins at 21:54:02.929720\n",
      "\n",
      "Training: batch 5 ends at 21:54:03.673125\n",
      " 6/60 [==>...........................] - ETA: 41s - loss: 1.5496 - bce_dice_loss: 1.5496WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1266s vs `on_train_batch_end` time: 0.6115s). Check your callbacks.\n",
      "\n",
      "Training: batch 6 begins at 21:54:03.676977\n",
      "\n",
      "Training: batch 6 ends at 21:54:04.428651\n",
      " 7/60 [==>...........................] - ETA: 40s - loss: 1.5370 - bce_dice_loss: 1.5370\n",
      "Training: batch 7 begins at 21:54:04.433003\n",
      "\n",
      "Training: batch 7 ends at 21:54:05.212514\n",
      " 8/60 [===>..........................] - ETA: 39s - loss: 1.5221 - bce_dice_loss: 1.5221\n",
      "Training: batch 8 begins at 21:54:05.216950\n",
      "\n",
      "Training: batch 8 ends at 21:54:06.011640\n",
      " 9/60 [===>..........................] - ETA: 39s - loss: 1.5070 - bce_dice_loss: 1.5070\n",
      "Training: batch 9 begins at 21:54:06.015028\n",
      "\n",
      "Training: batch 9 ends at 21:54:06.833118\n",
      "10/60 [====>.........................] - ETA: 38s - loss: 1.4919 - bce_dice_loss: 1.4919\n",
      "Training: batch 10 begins at 21:54:06.838793\n",
      "\n",
      "Training: batch 10 ends at 21:54:07.638626\n",
      "11/60 [====>.........................] - ETA: 38s - loss: 1.4745 - bce_dice_loss: 1.4745\n",
      "Training: batch 11 begins at 21:54:07.643127\n",
      "\n",
      "Training: batch 11 ends at 21:54:08.427493\n",
      "12/60 [=====>........................] - ETA: 37s - loss: 1.4672 - bce_dice_loss: 1.4672\n",
      "Training: batch 12 begins at 21:54:08.429986\n",
      "\n",
      "Training: batch 12 ends at 21:54:09.209893\n",
      "13/60 [=====>........................] - ETA: 36s - loss: 1.4564 - bce_dice_loss: 1.4564\n",
      "Training: batch 13 begins at 21:54:09.214444\n",
      "\n",
      "Training: batch 13 ends at 21:54:09.965668\n",
      "14/60 [======>.......................] - ETA: 35s - loss: 1.4445 - bce_dice_loss: 1.4445\n",
      "Training: batch 14 begins at 21:54:09.970283\n",
      "\n",
      "Training: batch 14 ends at 21:54:10.715084\n",
      "15/60 [======>.......................] - ETA: 34s - loss: 1.4373 - bce_dice_loss: 1.4373\n",
      "Training: batch 15 begins at 21:54:10.721021\n",
      "\n",
      "Training: batch 15 ends at 21:54:11.476227\n",
      "16/60 [=======>......................] - ETA: 34s - loss: 1.4256 - bce_dice_loss: 1.4256\n",
      "Training: batch 16 begins at 21:54:11.480469\n",
      "\n",
      "Training: batch 16 ends at 21:54:12.258096\n",
      "17/60 [=======>......................] - ETA: 33s - loss: 1.4143 - bce_dice_loss: 1.4143\n",
      "Training: batch 17 begins at 21:54:12.262705\n",
      "\n",
      "Training: batch 17 ends at 21:54:13.004631\n",
      "18/60 [========>.....................] - ETA: 32s - loss: 1.4006 - bce_dice_loss: 1.4006\n",
      "Training: batch 18 begins at 21:54:13.008989\n",
      "\n",
      "Training: batch 18 ends at 21:54:13.753737\n",
      "19/60 [========>.....................] - ETA: 31s - loss: 1.3912 - bce_dice_loss: 1.3912\n",
      "Training: batch 19 begins at 21:54:13.758118\n",
      "\n",
      "Training: batch 19 ends at 21:54:14.504741\n",
      "20/60 [=========>....................] - ETA: 30s - loss: 1.3825 - bce_dice_loss: 1.3825\n",
      "Training: batch 20 begins at 21:54:14.509115\n",
      "\n",
      "Training: batch 20 ends at 21:54:15.254544\n",
      "21/60 [=========>....................] - ETA: 30s - loss: 1.3710 - bce_dice_loss: 1.3710\n",
      "Training: batch 21 begins at 21:54:15.258710\n",
      "\n",
      "Training: batch 21 ends at 21:54:16.005996\n",
      "22/60 [==========>...................] - ETA: 29s - loss: 1.3624 - bce_dice_loss: 1.3624\n",
      "Training: batch 22 begins at 21:54:16.010640\n",
      "\n",
      "Training: batch 22 ends at 21:54:16.756375\n",
      "23/60 [==========>...................] - ETA: 28s - loss: 1.3503 - bce_dice_loss: 1.3503\n",
      "Training: batch 23 begins at 21:54:16.759240\n",
      "\n",
      "Training: batch 23 ends at 21:54:17.503638\n",
      "24/60 [===========>..................] - ETA: 27s - loss: 1.3420 - bce_dice_loss: 1.3420\n",
      "Training: batch 24 begins at 21:54:17.507784\n",
      "\n",
      "Training: batch 24 ends at 21:54:18.302910\n",
      "25/60 [===========>..................] - ETA: 26s - loss: 1.3347 - bce_dice_loss: 1.3347\n",
      "Training: batch 25 begins at 21:54:18.308261\n",
      "\n",
      "Training: batch 25 ends at 21:54:19.098087\n",
      "26/60 [============>.................] - ETA: 26s - loss: 1.3220 - bce_dice_loss: 1.3220\n",
      "Training: batch 26 begins at 21:54:19.103322\n",
      "\n",
      "Training: batch 26 ends at 21:54:19.916896\n",
      "27/60 [============>.................] - ETA: 25s - loss: 1.3195 - bce_dice_loss: 1.3195\n",
      "Training: batch 27 begins at 21:54:19.922556\n",
      "\n",
      "Training: batch 27 ends at 21:54:20.712123\n",
      "28/60 [=============>................] - ETA: 24s - loss: 1.3122 - bce_dice_loss: 1.3122\n",
      "Training: batch 28 begins at 21:54:20.716873\n",
      "\n",
      "Training: batch 28 ends at 21:54:21.471416\n",
      "29/60 [=============>................] - ETA: 23s - loss: 1.2979 - bce_dice_loss: 1.2979\n",
      "Training: batch 29 begins at 21:54:21.475762\n",
      "\n",
      "Training: batch 29 ends at 21:54:22.222785\n",
      "30/60 [==============>...............] - ETA: 23s - loss: 1.2881 - bce_dice_loss: 1.2881\n",
      "Training: batch 30 begins at 21:54:22.227141\n",
      "\n",
      "Training: batch 30 ends at 21:54:22.973728\n",
      "31/60 [==============>...............] - ETA: 22s - loss: 1.2834 - bce_dice_loss: 1.2834\n",
      "Training: batch 31 begins at 21:54:22.978092\n",
      "\n",
      "Training: batch 31 ends at 21:54:23.751317\n",
      "32/60 [===============>..............] - ETA: 21s - loss: 1.2761 - bce_dice_loss: 1.2761\n",
      "Training: batch 32 begins at 21:54:23.756529\n",
      "\n",
      "Training: batch 32 ends at 21:54:24.503298\n",
      "33/60 [===============>..............] - ETA: 20s - loss: 1.2676 - bce_dice_loss: 1.2676\n",
      "Training: batch 33 begins at 21:54:24.507937\n",
      "\n",
      "Training: batch 33 ends at 21:54:25.254184\n",
      "34/60 [================>.............] - ETA: 20s - loss: 1.2591 - bce_dice_loss: 1.2591\n",
      "Training: batch 34 begins at 21:54:25.258468\n",
      "\n",
      "Training: batch 34 ends at 21:54:26.032640\n",
      "35/60 [================>.............] - ETA: 19s - loss: 1.2523 - bce_dice_loss: 1.2523\n",
      "Training: batch 35 begins at 21:54:26.037758\n",
      "\n",
      "Training: batch 35 ends at 21:54:26.795947\n",
      "36/60 [=================>............] - ETA: 18s - loss: 1.2435 - bce_dice_loss: 1.2435\n",
      "Training: batch 36 begins at 21:54:26.800042\n",
      "\n",
      "Training: batch 36 ends at 21:54:27.567533\n",
      "37/60 [=================>............] - ETA: 17s - loss: 1.2336 - bce_dice_loss: 1.2336\n",
      "Training: batch 37 begins at 21:54:27.572401\n",
      "\n",
      "Training: batch 37 ends at 21:54:28.319253\n",
      "38/60 [==================>...........] - ETA: 16s - loss: 1.2328 - bce_dice_loss: 1.2328\n",
      "Training: batch 38 begins at 21:54:28.323479\n",
      "\n",
      "Training: batch 38 ends at 21:54:29.074494\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 1.2268 - bce_dice_loss: 1.2268\n",
      "Training: batch 39 begins at 21:54:29.078839\n",
      "\n",
      "Training: batch 39 ends at 21:54:29.823362\n",
      "40/60 [===================>..........] - ETA: 15s - loss: 1.2246 - bce_dice_loss: 1.2246\n",
      "Training: batch 40 begins at 21:54:29.827444\n",
      "\n",
      "Training: batch 40 ends at 21:54:30.608624\n",
      "41/60 [===================>..........] - ETA: 14s - loss: 1.2222 - bce_dice_loss: 1.2222\n",
      "Training: batch 41 begins at 21:54:30.612718\n",
      "\n",
      "Training: batch 41 ends at 21:54:31.368542\n",
      "42/60 [====================>.........] - ETA: 13s - loss: 1.2103 - bce_dice_loss: 1.2103\n",
      "Training: batch 42 begins at 21:54:31.371819\n",
      "\n",
      "Training: batch 42 ends at 21:54:32.163282\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 1.2062 - bce_dice_loss: 1.2062\n",
      "Training: batch 43 begins at 21:54:32.167058\n",
      "\n",
      "Training: batch 43 ends at 21:54:32.921376\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 1.2030 - bce_dice_loss: 1.2030\n",
      "Training: batch 44 begins at 21:54:32.925745\n",
      "\n",
      "Training: batch 44 ends at 21:54:33.776128\n",
      "45/60 [=====================>........] - ETA: 11s - loss: 1.1972 - bce_dice_loss: 1.1972\n",
      "Training: batch 45 begins at 21:54:33.778367\n",
      "\n",
      "Training: batch 45 ends at 21:54:34.706070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/60 [======================>.......] - ETA: 10s - loss: 1.1962 - bce_dice_loss: 1.1962\n",
      "Training: batch 46 begins at 21:54:34.708902\n",
      "\n",
      "Training: batch 46 ends at 21:54:35.522516\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 1.1892 - bce_dice_loss: 1.1892\n",
      "Training: batch 47 begins at 21:54:35.526920\n",
      "\n",
      "Training: batch 47 ends at 21:54:36.341304\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 1.1817 - bce_dice_loss: 1.1817 \n",
      "Training: batch 48 begins at 21:54:36.345492\n",
      "\n",
      "Training: batch 48 ends at 21:54:37.226805\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 1.1796 - bce_dice_loss: 1.1796\n",
      "Training: batch 49 begins at 21:54:37.229774\n",
      "\n",
      "Training: batch 49 ends at 21:54:38.203129\n",
      "50/60 [========================>.....] - ETA: 7s - loss: 1.1693 - bce_dice_loss: 1.1693\n",
      "Training: batch 50 begins at 21:54:38.205991\n",
      "\n",
      "Training: batch 50 ends at 21:54:39.173869\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 1.1668 - bce_dice_loss: 1.1668\n",
      "Training: batch 51 begins at 21:54:39.176313\n",
      "\n",
      "Training: batch 51 ends at 21:54:40.014011\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 1.1648 - bce_dice_loss: 1.1648\n",
      "Training: batch 52 begins at 21:54:40.018688\n",
      "\n",
      "Training: batch 52 ends at 21:54:40.855787\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 1.1598 - bce_dice_loss: 1.1598\n",
      "Training: batch 53 begins at 21:54:40.858716\n",
      "\n",
      "Training: batch 53 ends at 21:54:41.629280\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 1.1560 - bce_dice_loss: 1.1560\n",
      "Training: batch 54 begins at 21:54:41.633545\n",
      "\n",
      "Training: batch 54 ends at 21:54:42.473430\n",
      "55/60 [==========================>...] - ETA: 3s - loss: 1.1570 - bce_dice_loss: 1.1570\n",
      "Training: batch 55 begins at 21:54:42.475969\n",
      "\n",
      "Training: batch 55 ends at 21:54:43.281539\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 1.1547 - bce_dice_loss: 1.1547\n",
      "Training: batch 56 begins at 21:54:43.286441\n",
      "\n",
      "Training: batch 56 ends at 21:54:44.094513\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 1.1494 - bce_dice_loss: 1.1494\n",
      "Training: batch 57 begins at 21:54:44.097410\n",
      "\n",
      "Training: batch 57 ends at 21:54:44.907595\n",
      "58/60 [============================>.] - ETA: 1s - loss: 1.1444 - bce_dice_loss: 1.1444\n",
      "Training: batch 58 begins at 21:54:44.910123\n",
      "\n",
      "Training: batch 58 ends at 21:54:45.688561\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1432 - bce_dice_loss: 1.1432\n",
      "Training: batch 59 begins at 21:54:45.692689\n",
      "\n",
      "Training: batch 59 ends at 21:54:46.498933\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1398 - bce_dice_loss: 1.1398\n",
      "Evaluating: batch 0 begins at 21:54:47.456349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 21:54:47.555312: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 0 ends at 21:54:47.919986\n",
      "\n",
      "Evaluating: batch 1 begins at 21:54:47.920968\n",
      "\n",
      "Evaluating: batch 1 ends at 21:54:48.138740\n",
      "\n",
      "Evaluating: batch 2 begins at 21:54:48.140114\n",
      "\n",
      "Evaluating: batch 2 ends at 21:54:48.367054\n",
      "\n",
      "Evaluating: batch 3 begins at 21:54:48.369097\n",
      "\n",
      "Evaluating: batch 3 ends at 21:54:48.629392\n",
      "\n",
      "Evaluating: batch 4 begins at 21:54:48.631733\n",
      "\n",
      "Evaluating: batch 4 ends at 21:54:48.877126\n",
      "\n",
      "Evaluating: batch 5 begins at 21:54:48.879664\n",
      "\n",
      "Evaluating: batch 5 ends at 21:54:49.112787\n",
      "\n",
      "Evaluating: batch 6 begins at 21:54:49.115401\n",
      "\n",
      "Evaluating: batch 6 ends at 21:54:49.349168\n",
      "\n",
      "Evaluating: batch 7 begins at 21:54:49.351233\n",
      "\n",
      "Evaluating: batch 7 ends at 21:54:49.583497\n",
      "\n",
      "Evaluating: batch 8 begins at 21:54:49.584920\n",
      "\n",
      "Evaluating: batch 8 ends at 21:54:49.813062\n",
      "\n",
      "Evaluating: batch 9 begins at 21:54:49.815283\n",
      "\n",
      "Evaluating: batch 9 ends at 21:54:50.089052\n",
      "\n",
      "Evaluating: batch 10 begins at 21:54:50.090637\n",
      "\n",
      "Evaluating: batch 10 ends at 21:54:50.351119\n",
      "\n",
      "Evaluating: batch 11 begins at 21:54:50.353133\n",
      "\n",
      "Evaluating: batch 11 ends at 21:54:50.616246\n",
      "\n",
      "Evaluating: batch 12 begins at 21:54:50.618214\n",
      "\n",
      "Evaluating: batch 12 ends at 21:54:50.867160\n",
      "\n",
      "Evaluating: batch 13 begins at 21:54:50.869068\n",
      "\n",
      "Evaluating: batch 13 ends at 21:54:51.097087\n",
      "\n",
      "Evaluating: batch 14 begins at 21:54:51.099243\n",
      "\n",
      "Evaluating: batch 14 ends at 21:54:51.349428\n",
      "\n",
      "Evaluating: batch 15 begins at 21:54:51.350943\n",
      "\n",
      "Evaluating: batch 15 ends at 21:54:51.621622\n",
      "\n",
      "Evaluating: batch 16 begins at 21:54:51.624361\n",
      "\n",
      "Evaluating: batch 16 ends at 21:54:51.854969\n",
      "\n",
      "Evaluating: batch 17 begins at 21:54:51.856212\n",
      "\n",
      "Evaluating: batch 17 ends at 21:54:52.082784\n",
      "\n",
      "Evaluating: batch 18 begins at 21:54:52.085262\n",
      "\n",
      "Evaluating: batch 18 ends at 21:54:52.308067\n",
      "\n",
      "Evaluating: batch 19 begins at 21:54:52.309715\n",
      "\n",
      "Evaluating: batch 19 ends at 21:54:52.534655\n",
      "\n",
      "Evaluating: batch 20 begins at 21:54:52.536331\n",
      "\n",
      "Evaluating: batch 20 ends at 21:54:52.762409\n",
      "\n",
      "Evaluating: batch 21 begins at 21:54:52.763638\n",
      "\n",
      "Evaluating: batch 21 ends at 21:54:52.999044\n",
      "\n",
      "Evaluating: batch 22 begins at 21:54:53.000509\n",
      "\n",
      "Evaluating: batch 22 ends at 21:54:53.228541\n",
      "\n",
      "Evaluating: batch 23 begins at 21:54:53.231322\n",
      "\n",
      "Evaluating: batch 23 ends at 21:54:53.464437\n",
      "\n",
      "Evaluating: batch 24 begins at 21:54:53.466159\n",
      "\n",
      "Evaluating: batch 24 ends at 21:54:53.688760\n",
      "\n",
      "Evaluating: batch 25 begins at 21:54:53.690172\n",
      "\n",
      "Evaluating: batch 25 ends at 21:54:53.917407\n",
      "\n",
      "Evaluating: batch 26 begins at 21:54:53.920085\n",
      "\n",
      "Evaluating: batch 26 ends at 21:54:54.142714\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95232, saving model to ./keras.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 21:54:54.708354: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 58s 950ms/step - loss: 1.1398 - bce_dice_loss: 1.1398 - val_loss: 0.9523 - val_bce_dice_loss: 0.9523\n",
      "Epoch 2/25\n",
      "\n",
      "Training: batch 0 begins at 21:54:55.920360\n",
      "\n",
      "Training: batch 0 ends at 21:54:56.747254\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.9196 - bce_dice_loss: 0.9196\n",
      "Training: batch 1 begins at 21:54:56.750998\n",
      "\n",
      "Training: batch 1 ends at 21:54:57.677420\n",
      " 2/60 [>.............................] - ETA: 53s - loss: 1.0432 - bce_dice_loss: 1.0432\n",
      "Training: batch 2 begins at 21:54:57.681910\n",
      "\n",
      "Training: batch 2 ends at 21:54:58.601989\n",
      " 3/60 [>.............................] - ETA: 52s - loss: 1.0262 - bce_dice_loss: 1.0262\n",
      "Training: batch 3 begins at 21:54:58.606618\n",
      "\n",
      "Training: batch 3 ends at 21:54:59.507277\n",
      " 4/60 [=>............................] - ETA: 51s - loss: 0.9737 - bce_dice_loss: 0.9737\n",
      "Training: batch 4 begins at 21:54:59.510007\n",
      "\n",
      "Training: batch 4 ends at 21:55:00.411878\n",
      " 5/60 [=>............................] - ETA: 50s - loss: 0.9684 - bce_dice_loss: 0.9684\n",
      "Training: batch 5 begins at 21:55:00.416883\n",
      "\n",
      "Training: batch 5 ends at 21:55:01.290489\n",
      " 6/60 [==>...........................] - ETA: 49s - loss: 0.9620 - bce_dice_loss: 0.9620\n",
      "Training: batch 6 begins at 21:55:01.295143\n",
      "\n",
      "Training: batch 6 ends at 21:55:02.098330\n",
      " 7/60 [==>...........................] - ETA: 47s - loss: 0.9551 - bce_dice_loss: 0.9551\n",
      "Training: batch 7 begins at 21:55:02.101023\n",
      "\n",
      "Training: batch 7 ends at 21:55:02.906417\n",
      " 8/60 [===>..........................] - ETA: 45s - loss: 0.9533 - bce_dice_loss: 0.9533\n",
      "Training: batch 8 begins at 21:55:02.909444\n",
      "\n",
      "Training: batch 8 ends at 21:55:03.703908\n",
      " 9/60 [===>..........................] - ETA: 44s - loss: 0.9321 - bce_dice_loss: 0.9321\n",
      "Training: batch 9 begins at 21:55:03.707160\n",
      "\n",
      "Training: batch 9 ends at 21:55:04.538075\n",
      "10/60 [====>.........................] - ETA: 43s - loss: 0.9296 - bce_dice_loss: 0.9296\n",
      "Training: batch 10 begins at 21:55:04.545121\n",
      "\n",
      "Training: batch 10 ends at 21:55:05.414171\n",
      "11/60 [====>.........................] - ETA: 42s - loss: 0.9291 - bce_dice_loss: 0.9291\n",
      "Training: batch 11 begins at 21:55:05.419335\n",
      "\n",
      "Training: batch 11 ends at 21:55:06.222259\n",
      "12/60 [=====>........................] - ETA: 41s - loss: 0.9263 - bce_dice_loss: 0.9263\n",
      "Training: batch 12 begins at 21:55:06.226054\n",
      "\n",
      "Training: batch 12 ends at 21:55:07.031642\n",
      "13/60 [=====>........................] - ETA: 40s - loss: 0.9248 - bce_dice_loss: 0.9248\n",
      "Training: batch 13 begins at 21:55:07.036144\n",
      "\n",
      "Training: batch 13 ends at 21:55:07.916051\n",
      "14/60 [======>.......................] - ETA: 39s - loss: 0.9070 - bce_dice_loss: 0.9070\n",
      "Training: batch 14 begins at 21:55:07.919437\n",
      "\n",
      "Training: batch 14 ends at 21:55:08.728368\n",
      "15/60 [======>.......................] - ETA: 38s - loss: 0.9185 - bce_dice_loss: 0.9185\n",
      "Training: batch 15 begins at 21:55:08.731809\n",
      "\n",
      "Training: batch 15 ends at 21:55:09.550866\n",
      "16/60 [=======>......................] - ETA: 37s - loss: 0.9237 - bce_dice_loss: 0.9237\n",
      "Training: batch 16 begins at 21:55:09.554644\n",
      "\n",
      "Training: batch 16 ends at 21:55:10.534279\n",
      "17/60 [=======>......................] - ETA: 37s - loss: 0.9245 - bce_dice_loss: 0.9245\n",
      "Training: batch 17 begins at 21:55:10.539174\n",
      "\n",
      "Training: batch 17 ends at 21:55:11.393708\n",
      "18/60 [========>.....................] - ETA: 36s - loss: 0.9196 - bce_dice_loss: 0.9196\n",
      "Training: batch 18 begins at 21:55:11.396313\n",
      "\n",
      "Training: batch 18 ends at 21:55:12.256550\n",
      "19/60 [========>.....................] - ETA: 35s - loss: 0.9114 - bce_dice_loss: 0.9114\n",
      "Training: batch 19 begins at 21:55:12.259156\n",
      "\n",
      "Training: batch 19 ends at 21:55:13.126134\n",
      "20/60 [=========>....................] - ETA: 34s - loss: 0.9130 - bce_dice_loss: 0.9130\n",
      "Training: batch 20 begins at 21:55:13.129807\n",
      "\n",
      "Training: batch 20 ends at 21:55:13.946573\n",
      "21/60 [=========>....................] - ETA: 33s - loss: 0.9063 - bce_dice_loss: 0.9063\n",
      "Training: batch 21 begins at 21:55:13.949961\n",
      "\n",
      "Training: batch 21 ends at 21:55:14.824347\n",
      "22/60 [==========>...................] - ETA: 32s - loss: 0.9126 - bce_dice_loss: 0.9126\n",
      "Training: batch 22 begins at 21:55:14.827849\n",
      "\n",
      "Training: batch 22 ends at 21:55:15.670982\n",
      "23/60 [==========>...................] - ETA: 31s - loss: 0.9152 - bce_dice_loss: 0.9152\n",
      "Training: batch 23 begins at 21:55:15.675145\n",
      "\n",
      "Training: batch 23 ends at 21:55:16.480935\n",
      "24/60 [===========>..................] - ETA: 30s - loss: 0.9233 - bce_dice_loss: 0.9233\n",
      "Training: batch 24 begins at 21:55:16.485031\n",
      "\n",
      "Training: batch 24 ends at 21:55:17.353861\n",
      "25/60 [===========>..................] - ETA: 30s - loss: 0.9138 - bce_dice_loss: 0.9138\n",
      "Training: batch 25 begins at 21:55:17.357092\n",
      "\n",
      "Training: batch 25 ends at 21:55:18.251703\n",
      "26/60 [============>.................] - ETA: 29s - loss: 0.9164 - bce_dice_loss: 0.9164\n",
      "Training: batch 26 begins at 21:55:18.255762\n",
      "\n",
      "Training: batch 26 ends at 21:55:19.092142\n",
      "27/60 [============>.................] - ETA: 28s - loss: 0.9102 - bce_dice_loss: 0.9102\n",
      "Training: batch 27 begins at 21:55:19.096812\n",
      "\n",
      "Training: batch 27 ends at 21:55:19.959537\n",
      "28/60 [=============>................] - ETA: 27s - loss: 0.9103 - bce_dice_loss: 0.9103\n",
      "Training: batch 28 begins at 21:55:19.962497\n",
      "\n",
      "Training: batch 28 ends at 21:55:20.857894\n",
      "29/60 [=============>................] - ETA: 26s - loss: 0.9094 - bce_dice_loss: 0.9094\n",
      "Training: batch 29 begins at 21:55:20.860873\n",
      "\n",
      "Training: batch 29 ends at 21:55:21.721586\n",
      "30/60 [==============>...............] - ETA: 25s - loss: 0.9098 - bce_dice_loss: 0.9098\n",
      "Training: batch 30 begins at 21:55:21.723940\n",
      "\n",
      "Training: batch 30 ends at 21:55:22.647806\n",
      "31/60 [==============>...............] - ETA: 25s - loss: 0.9029 - bce_dice_loss: 0.9029\n",
      "Training: batch 31 begins at 21:55:22.650172\n",
      "\n",
      "Training: batch 31 ends at 21:55:23.723968\n",
      "32/60 [===============>..............] - ETA: 24s - loss: 0.9056 - bce_dice_loss: 0.9056\n",
      "Training: batch 32 begins at 21:55:23.727683\n",
      "\n",
      "Training: batch 32 ends at 21:55:24.628766\n",
      "33/60 [===============>..............] - ETA: 23s - loss: 0.9078 - bce_dice_loss: 0.9078\n",
      "Training: batch 33 begins at 21:55:24.631121\n",
      "\n",
      "Training: batch 33 ends at 21:55:25.547004\n",
      "34/60 [================>.............] - ETA: 22s - loss: 0.9087 - bce_dice_loss: 0.9087\n",
      "Training: batch 34 begins at 21:55:25.551412\n",
      "\n",
      "Training: batch 34 ends at 21:55:26.408768\n",
      "35/60 [================>.............] - ETA: 21s - loss: 0.9038 - bce_dice_loss: 0.9038\n",
      "Training: batch 35 begins at 21:55:26.411825\n",
      "\n",
      "Training: batch 35 ends at 21:55:27.430607\n",
      "36/60 [=================>............] - ETA: 21s - loss: 0.9004 - bce_dice_loss: 0.9004\n",
      "Training: batch 36 begins at 21:55:27.434062\n",
      "\n",
      "Training: batch 36 ends at 21:55:28.340025\n",
      "37/60 [=================>............] - ETA: 20s - loss: 0.9010 - bce_dice_loss: 0.9010\n",
      "Training: batch 37 begins at 21:55:28.342369\n",
      "\n",
      "Training: batch 37 ends at 21:55:29.198920\n",
      "38/60 [==================>...........] - ETA: 19s - loss: 0.9004 - bce_dice_loss: 0.9004\n",
      "Training: batch 38 begins at 21:55:29.201356\n",
      "\n",
      "Training: batch 38 ends at 21:55:30.034649\n",
      "39/60 [==================>...........] - ETA: 18s - loss: 0.8969 - bce_dice_loss: 0.8969\n",
      "Training: batch 39 begins at 21:55:30.038344\n",
      "\n",
      "Training: batch 39 ends at 21:55:30.925974\n",
      "40/60 [===================>..........] - ETA: 17s - loss: 0.8964 - bce_dice_loss: 0.8964\n",
      "Training: batch 40 begins at 21:55:30.929392\n",
      "\n",
      "Training: batch 40 ends at 21:55:31.769369\n",
      "41/60 [===================>..........] - ETA: 16s - loss: 0.8872 - bce_dice_loss: 0.8872\n",
      "Training: batch 41 begins at 21:55:31.772590\n",
      "\n",
      "Training: batch 41 ends at 21:55:32.634587\n",
      "42/60 [====================>.........] - ETA: 15s - loss: 0.8847 - bce_dice_loss: 0.8847\n",
      "Training: batch 42 begins at 21:55:32.637949\n",
      "\n",
      "Training: batch 42 ends at 21:55:33.496269\n",
      "43/60 [====================>.........] - ETA: 14s - loss: 0.8820 - bce_dice_loss: 0.8820\n",
      "Training: batch 43 begins at 21:55:33.498291\n",
      "\n",
      "Training: batch 43 ends at 21:55:34.379827\n",
      "44/60 [=====================>........] - ETA: 14s - loss: 0.8745 - bce_dice_loss: 0.8745\n",
      "Training: batch 44 begins at 21:55:34.382152\n",
      "\n",
      "Training: batch 44 ends at 21:55:35.305575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/60 [=====================>........] - ETA: 13s - loss: 0.8692 - bce_dice_loss: 0.8692\n",
      "Training: batch 45 begins at 21:55:35.308013\n",
      "\n",
      "Training: batch 45 ends at 21:55:36.100863\n",
      "46/60 [======================>.......] - ETA: 12s - loss: 0.8714 - bce_dice_loss: 0.8714\n",
      "Training: batch 46 begins at 21:55:36.105456\n",
      "\n",
      "Training: batch 46 ends at 21:55:36.901115\n",
      "47/60 [======================>.......] - ETA: 11s - loss: 0.8675 - bce_dice_loss: 0.8675\n",
      "Training: batch 47 begins at 21:55:36.903669\n",
      "\n",
      "Training: batch 47 ends at 21:55:37.705031\n",
      "48/60 [=======================>......] - ETA: 10s - loss: 0.8638 - bce_dice_loss: 0.8638\n",
      "Training: batch 48 begins at 21:55:37.707849\n",
      "\n",
      "Training: batch 48 ends at 21:55:38.500201\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.8648 - bce_dice_loss: 0.8648 \n",
      "Training: batch 49 begins at 21:55:38.504984\n",
      "\n",
      "Training: batch 49 ends at 21:55:39.299137\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.8654 - bce_dice_loss: 0.8654\n",
      "Training: batch 50 begins at 21:55:39.303356\n",
      "\n",
      "Training: batch 50 ends at 21:55:40.089583\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.8646 - bce_dice_loss: 0.8646\n",
      "Training: batch 51 begins at 21:55:40.094045\n",
      "\n",
      "Training: batch 51 ends at 21:55:40.905735\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.8641 - bce_dice_loss: 0.8641\n",
      "Training: batch 52 begins at 21:55:40.910261\n",
      "\n",
      "Training: batch 52 ends at 21:55:41.697815\n",
      "53/60 [=========================>....] - ETA: 6s - loss: 0.8615 - bce_dice_loss: 0.8615\n",
      "Training: batch 53 begins at 21:55:41.702484\n",
      "\n",
      "Training: batch 53 ends at 21:55:42.504382\n",
      "54/60 [==========================>...] - ETA: 5s - loss: 0.8636 - bce_dice_loss: 0.8636\n",
      "Training: batch 54 begins at 21:55:42.509331\n",
      "\n",
      "Training: batch 54 ends at 21:55:43.296477\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.8638 - bce_dice_loss: 0.8638\n",
      "Training: batch 55 begins at 21:55:43.300866\n",
      "\n",
      "Training: batch 55 ends at 21:55:44.093069\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.8586 - bce_dice_loss: 0.8586\n",
      "Training: batch 56 begins at 21:55:44.097695\n",
      "\n",
      "Training: batch 56 ends at 21:55:44.911099\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.8571 - bce_dice_loss: 0.8571\n",
      "Training: batch 57 begins at 21:55:44.915221\n",
      "\n",
      "Training: batch 57 ends at 21:55:45.701556\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.8570 - bce_dice_loss: 0.8570\n",
      "Training: batch 58 begins at 21:55:45.704937\n",
      "\n",
      "Training: batch 58 ends at 21:55:46.492791\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8568 - bce_dice_loss: 0.8568\n",
      "Training: batch 59 begins at 21:55:46.497240\n",
      "\n",
      "Training: batch 59 ends at 21:55:47.314778\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8531 - bce_dice_loss: 0.8531\n",
      "Evaluating: batch 0 begins at 21:55:47.350364\n",
      "\n",
      "Evaluating: batch 0 ends at 21:55:47.618386\n",
      "\n",
      "Evaluating: batch 1 begins at 21:55:47.619518\n",
      "\n",
      "Evaluating: batch 1 ends at 21:55:47.836535\n",
      "\n",
      "Evaluating: batch 2 begins at 21:55:47.838625\n",
      "\n",
      "Evaluating: batch 2 ends at 21:55:48.056382\n",
      "\n",
      "Evaluating: batch 3 begins at 21:55:48.058707\n",
      "\n",
      "Evaluating: batch 3 ends at 21:55:48.278981\n",
      "\n",
      "Evaluating: batch 4 begins at 21:55:48.281680\n",
      "\n",
      "Evaluating: batch 4 ends at 21:55:48.501020\n",
      "\n",
      "Evaluating: batch 5 begins at 21:55:48.503690\n",
      "\n",
      "Evaluating: batch 5 ends at 21:55:48.724185\n",
      "\n",
      "Evaluating: batch 6 begins at 21:55:48.725764\n",
      "\n",
      "Evaluating: batch 6 ends at 21:55:48.941181\n",
      "\n",
      "Evaluating: batch 7 begins at 21:55:48.943307\n",
      "\n",
      "Evaluating: batch 7 ends at 21:55:49.165840\n",
      "\n",
      "Evaluating: batch 8 begins at 21:55:49.168726\n",
      "\n",
      "Evaluating: batch 8 ends at 21:55:49.389479\n",
      "\n",
      "Evaluating: batch 9 begins at 21:55:49.392083\n",
      "\n",
      "Evaluating: batch 9 ends at 21:55:49.612578\n",
      "\n",
      "Evaluating: batch 10 begins at 21:55:49.615350\n",
      "\n",
      "Evaluating: batch 10 ends at 21:55:49.836508\n",
      "\n",
      "Evaluating: batch 11 begins at 21:55:49.838928\n",
      "\n",
      "Evaluating: batch 11 ends at 21:55:50.055989\n",
      "\n",
      "Evaluating: batch 12 begins at 21:55:50.057275\n",
      "\n",
      "Evaluating: batch 12 ends at 21:55:50.275638\n",
      "\n",
      "Evaluating: batch 13 begins at 21:55:50.278390\n",
      "\n",
      "Evaluating: batch 13 ends at 21:55:50.500135\n",
      "\n",
      "Evaluating: batch 14 begins at 21:55:50.502637\n",
      "\n",
      "Evaluating: batch 14 ends at 21:55:50.723539\n",
      "\n",
      "Evaluating: batch 15 begins at 21:55:50.724819\n",
      "\n",
      "Evaluating: batch 15 ends at 21:55:50.946358\n",
      "\n",
      "Evaluating: batch 16 begins at 21:55:50.949084\n",
      "\n",
      "Evaluating: batch 16 ends at 21:55:51.171436\n",
      "\n",
      "Evaluating: batch 17 begins at 21:55:51.174146\n",
      "\n",
      "Evaluating: batch 17 ends at 21:55:51.396262\n",
      "\n",
      "Evaluating: batch 18 begins at 21:55:51.398769\n",
      "\n",
      "Evaluating: batch 18 ends at 21:55:51.620341\n",
      "\n",
      "Evaluating: batch 19 begins at 21:55:51.622856\n",
      "\n",
      "Evaluating: batch 19 ends at 21:55:51.845789\n",
      "\n",
      "Evaluating: batch 20 begins at 21:55:51.848059\n",
      "\n",
      "Evaluating: batch 20 ends at 21:55:52.067627\n",
      "\n",
      "Evaluating: batch 21 begins at 21:55:52.069200\n",
      "\n",
      "Evaluating: batch 21 ends at 21:55:52.287615\n",
      "\n",
      "Evaluating: batch 22 begins at 21:55:52.288994\n",
      "\n",
      "Evaluating: batch 22 ends at 21:55:52.509876\n",
      "\n",
      "Evaluating: batch 23 begins at 21:55:52.511858\n",
      "\n",
      "Evaluating: batch 23 ends at 21:55:52.731130\n",
      "\n",
      "Evaluating: batch 24 begins at 21:55:52.733725\n",
      "\n",
      "Evaluating: batch 24 ends at 21:55:52.952735\n",
      "\n",
      "Evaluating: batch 25 begins at 21:55:52.955305\n",
      "\n",
      "Evaluating: batch 25 ends at 21:55:53.175657\n",
      "\n",
      "Evaluating: batch 26 begins at 21:55:53.178204\n",
      "\n",
      "Evaluating: batch 26 ends at 21:55:53.396849\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95232 to 0.76264, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 59s 986ms/step - loss: 0.8531 - bce_dice_loss: 0.8531 - val_loss: 0.7626 - val_bce_dice_loss: 0.7626\n",
      "Epoch 3/25\n",
      "\n",
      "Training: batch 0 begins at 21:55:54.915494\n",
      "\n",
      "Training: batch 0 ends at 21:55:55.730304\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.8334 - bce_dice_loss: 0.8334\n",
      "Training: batch 1 begins at 21:55:55.734137\n",
      "\n",
      "Training: batch 1 ends at 21:55:56.550885\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.8143 - bce_dice_loss: 0.8143\n",
      "Training: batch 2 begins at 21:55:56.556086\n",
      "\n",
      "Training: batch 2 ends at 21:55:57.393568\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.7912 - bce_dice_loss: 0.7912\n",
      "Training: batch 3 begins at 21:55:57.397836\n",
      "\n",
      "Training: batch 3 ends at 21:55:58.200947\n",
      " 4/60 [=>............................] - ETA: 46s - loss: 0.7868 - bce_dice_loss: 0.7868\n",
      "Training: batch 4 begins at 21:55:58.204309\n",
      "\n",
      "Training: batch 4 ends at 21:55:58.995909\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.7663 - bce_dice_loss: 0.7663\n",
      "Training: batch 5 begins at 21:55:59.000272\n",
      "\n",
      "Training: batch 5 ends at 21:55:59.786532\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.7732 - bce_dice_loss: 0.7732\n",
      "Training: batch 6 begins at 21:55:59.791002\n",
      "\n",
      "Training: batch 6 ends at 21:56:00.608391\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.7837 - bce_dice_loss: 0.7837\n",
      "Training: batch 7 begins at 21:56:00.613064\n",
      "\n",
      "Training: batch 7 ends at 21:56:01.392185\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.7935 - bce_dice_loss: 0.7935\n",
      "Training: batch 8 begins at 21:56:01.397126\n",
      "\n",
      "Training: batch 8 ends at 21:56:02.192059\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.7834 - bce_dice_loss: 0.7834\n",
      "Training: batch 9 begins at 21:56:02.196710\n",
      "\n",
      "Training: batch 9 ends at 21:56:02.988918\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.7775 - bce_dice_loss: 0.7775\n",
      "Training: batch 10 begins at 21:56:02.993055\n",
      "\n",
      "Training: batch 10 ends at 21:56:03.809436\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.7882 - bce_dice_loss: 0.7882\n",
      "Training: batch 11 begins at 21:56:03.813931\n",
      "\n",
      "Training: batch 11 ends at 21:56:04.604055\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.7950 - bce_dice_loss: 0.7950\n",
      "Training: batch 12 begins at 21:56:04.608510\n",
      "\n",
      "Training: batch 12 ends at 21:56:05.398231\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.7894 - bce_dice_loss: 0.7894\n",
      "Training: batch 13 begins at 21:56:05.402942\n",
      "\n",
      "Training: batch 13 ends at 21:56:06.217637\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.7990 - bce_dice_loss: 0.7990\n",
      "Training: batch 14 begins at 21:56:06.222149\n",
      "\n",
      "Training: batch 14 ends at 21:56:07.024714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/60 [======>.......................] - ETA: 36s - loss: 0.7912 - bce_dice_loss: 0.7912\n",
      "Training: batch 15 begins at 21:56:07.027528\n",
      "\n",
      "Training: batch 15 ends at 21:56:07.822462\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.7905 - bce_dice_loss: 0.7905\n",
      "Training: batch 16 begins at 21:56:07.826811\n",
      "\n",
      "Training: batch 16 ends at 21:56:08.617078\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.7937 - bce_dice_loss: 0.7937\n",
      "Training: batch 17 begins at 21:56:08.621427\n",
      "\n",
      "Training: batch 17 ends at 21:56:09.414059\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.7923 - bce_dice_loss: 0.7923\n",
      "Training: batch 18 begins at 21:56:09.419926\n",
      "\n",
      "Training: batch 18 ends at 21:56:10.234264\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.7843 - bce_dice_loss: 0.7843\n",
      "Training: batch 19 begins at 21:56:10.238740\n",
      "\n",
      "Training: batch 19 ends at 21:56:11.029797\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.7866 - bce_dice_loss: 0.7866\n",
      "Training: batch 20 begins at 21:56:11.033109\n",
      "\n",
      "Training: batch 20 ends at 21:56:11.856422\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.7808 - bce_dice_loss: 0.7808\n",
      "Training: batch 21 begins at 21:56:11.860639\n",
      "\n",
      "Training: batch 21 ends at 21:56:12.654006\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.7750 - bce_dice_loss: 0.7750\n",
      "Training: batch 22 begins at 21:56:12.659350\n",
      "\n",
      "Training: batch 22 ends at 21:56:13.473664\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.7780 - bce_dice_loss: 0.7780\n",
      "Training: batch 23 begins at 21:56:13.480100\n",
      "\n",
      "Training: batch 23 ends at 21:56:14.266756\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.7802 - bce_dice_loss: 0.7802\n",
      "Training: batch 24 begins at 21:56:14.271706\n",
      "\n",
      "Training: batch 24 ends at 21:56:15.070138\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.7811 - bce_dice_loss: 0.7811\n",
      "Training: batch 25 begins at 21:56:15.075134\n",
      "\n",
      "Training: batch 25 ends at 21:56:15.858891\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.7857 - bce_dice_loss: 0.7857\n",
      "Training: batch 26 begins at 21:56:15.863554\n",
      "\n",
      "Training: batch 26 ends at 21:56:16.664043\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.7889 - bce_dice_loss: 0.7889\n",
      "Training: batch 27 begins at 21:56:16.668540\n",
      "\n",
      "Training: batch 27 ends at 21:56:17.478636\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.7905 - bce_dice_loss: 0.7905\n",
      "Training: batch 28 begins at 21:56:17.483528\n",
      "\n",
      "Training: batch 28 ends at 21:56:18.279625\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.7954 - bce_dice_loss: 0.7954\n",
      "Training: batch 29 begins at 21:56:18.283698\n",
      "\n",
      "Training: batch 29 ends at 21:56:19.080815\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.7997 - bce_dice_loss: 0.7997\n",
      "Training: batch 30 begins at 21:56:19.085213\n",
      "\n",
      "Training: batch 30 ends at 21:56:19.877081\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.7973 - bce_dice_loss: 0.7973\n",
      "Training: batch 31 begins at 21:56:19.881942\n",
      "\n",
      "Training: batch 31 ends at 21:56:20.696629\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.7882 - bce_dice_loss: 0.7882\n",
      "Training: batch 32 begins at 21:56:20.701203\n",
      "\n",
      "Training: batch 32 ends at 21:56:21.482521\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.7886 - bce_dice_loss: 0.7886\n",
      "Training: batch 33 begins at 21:56:21.487168\n",
      "\n",
      "Training: batch 33 ends at 21:56:22.278670\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.7832 - bce_dice_loss: 0.7832\n",
      "Training: batch 34 begins at 21:56:22.283445\n",
      "\n",
      "Training: batch 34 ends at 21:56:23.095563\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.7857 - bce_dice_loss: 0.7857\n",
      "Training: batch 35 begins at 21:56:23.098401\n",
      "\n",
      "Training: batch 35 ends at 21:56:23.882902\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.7821 - bce_dice_loss: 0.7821\n",
      "Training: batch 36 begins at 21:56:23.886011\n",
      "\n",
      "Training: batch 36 ends at 21:56:24.665727\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.7785 - bce_dice_loss: 0.7785\n",
      "Training: batch 37 begins at 21:56:24.670267\n",
      "\n",
      "Training: batch 37 ends at 21:56:25.457622\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.7788 - bce_dice_loss: 0.7788\n",
      "Training: batch 38 begins at 21:56:25.462541\n",
      "\n",
      "Training: batch 38 ends at 21:56:26.252948\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.7755 - bce_dice_loss: 0.7755\n",
      "Training: batch 39 begins at 21:56:26.257439\n",
      "\n",
      "Training: batch 39 ends at 21:56:27.079657\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.7724 - bce_dice_loss: 0.7724\n",
      "Training: batch 40 begins at 21:56:27.084368\n",
      "\n",
      "Training: batch 40 ends at 21:56:27.884504\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.7719 - bce_dice_loss: 0.7719\n",
      "Training: batch 41 begins at 21:56:27.888968\n",
      "\n",
      "Training: batch 41 ends at 21:56:28.707713\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.7743 - bce_dice_loss: 0.7743\n",
      "Training: batch 42 begins at 21:56:28.710288\n",
      "\n",
      "Training: batch 42 ends at 21:56:29.497391\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.7767 - bce_dice_loss: 0.7767\n",
      "Training: batch 43 begins at 21:56:29.500322\n",
      "\n",
      "Training: batch 43 ends at 21:56:30.290293\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.7763 - bce_dice_loss: 0.7763\n",
      "Training: batch 44 begins at 21:56:30.294452\n",
      "\n",
      "Training: batch 44 ends at 21:56:31.081532\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.7819 - bce_dice_loss: 0.7819\n",
      "Training: batch 45 begins at 21:56:31.085823\n",
      "\n",
      "Training: batch 45 ends at 21:56:31.873224\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.7837 - bce_dice_loss: 0.7837\n",
      "Training: batch 46 begins at 21:56:31.877631\n",
      "\n",
      "Training: batch 46 ends at 21:56:32.668612\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.7854 - bce_dice_loss: 0.7854\n",
      "Training: batch 47 begins at 21:56:32.673162\n",
      "\n",
      "Training: batch 47 ends at 21:56:33.456061\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.7819 - bce_dice_loss: 0.7819 \n",
      "Training: batch 48 begins at 21:56:33.460653\n",
      "\n",
      "Training: batch 48 ends at 21:56:34.240507\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.7790 - bce_dice_loss: 0.7790\n",
      "Training: batch 49 begins at 21:56:34.244820\n",
      "\n",
      "Training: batch 49 ends at 21:56:35.034521\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.7795 - bce_dice_loss: 0.7795\n",
      "Training: batch 50 begins at 21:56:35.037516\n",
      "\n",
      "Training: batch 50 ends at 21:56:35.827340\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.7723 - bce_dice_loss: 0.7723\n",
      "Training: batch 51 begins at 21:56:35.832606\n",
      "\n",
      "Training: batch 51 ends at 21:56:36.647006\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.7706 - bce_dice_loss: 0.7706\n",
      "Training: batch 52 begins at 21:56:36.651344\n",
      "\n",
      "Training: batch 52 ends at 21:56:37.456805\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.7667 - bce_dice_loss: 0.7667\n",
      "Training: batch 53 begins at 21:56:37.461544\n",
      "\n",
      "Training: batch 53 ends at 21:56:38.255665\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.7714 - bce_dice_loss: 0.7714\n",
      "Training: batch 54 begins at 21:56:38.260213\n",
      "\n",
      "Training: batch 54 ends at 21:56:39.072390\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.7696 - bce_dice_loss: 0.7696\n",
      "Training: batch 55 begins at 21:56:39.076795\n",
      "\n",
      "Training: batch 55 ends at 21:56:39.857143\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.7702 - bce_dice_loss: 0.7702\n",
      "Training: batch 56 begins at 21:56:39.861894\n",
      "\n",
      "Training: batch 56 ends at 21:56:40.678042\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.7698 - bce_dice_loss: 0.7698\n",
      "Training: batch 57 begins at 21:56:40.681625\n",
      "\n",
      "Training: batch 57 ends at 21:56:41.469193\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.7749 - bce_dice_loss: 0.7749\n",
      "Training: batch 58 begins at 21:56:41.473895\n",
      "\n",
      "Training: batch 58 ends at 21:56:42.261781\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7766 - bce_dice_loss: 0.7766\n",
      "Training: batch 59 begins at 21:56:42.266068\n",
      "\n",
      "Training: batch 59 ends at 21:56:43.068092\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7769 - bce_dice_loss: 0.7769\n",
      "Evaluating: batch 0 begins at 21:56:43.102402\n",
      "\n",
      "Evaluating: batch 0 ends at 21:56:43.368873\n",
      "\n",
      "Evaluating: batch 1 begins at 21:56:43.370066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 1 ends at 21:56:43.587026\n",
      "\n",
      "Evaluating: batch 2 begins at 21:56:43.588576\n",
      "\n",
      "Evaluating: batch 2 ends at 21:56:43.805854\n",
      "\n",
      "Evaluating: batch 3 begins at 21:56:43.809240\n",
      "\n",
      "Evaluating: batch 3 ends at 21:56:44.029641\n",
      "\n",
      "Evaluating: batch 4 begins at 21:56:44.032172\n",
      "\n",
      "Evaluating: batch 4 ends at 21:56:44.251644\n",
      "\n",
      "Evaluating: batch 5 begins at 21:56:44.254246\n",
      "\n",
      "Evaluating: batch 5 ends at 21:56:44.475504\n",
      "\n",
      "Evaluating: batch 6 begins at 21:56:44.477872\n",
      "\n",
      "Evaluating: batch 6 ends at 21:56:44.697720\n",
      "\n",
      "Evaluating: batch 7 begins at 21:56:44.700132\n",
      "\n",
      "Evaluating: batch 7 ends at 21:56:44.920388\n",
      "\n",
      "Evaluating: batch 8 begins at 21:56:44.922870\n",
      "\n",
      "Evaluating: batch 8 ends at 21:56:45.142204\n",
      "\n",
      "Evaluating: batch 9 begins at 21:56:45.144801\n",
      "\n",
      "Evaluating: batch 9 ends at 21:56:45.366059\n",
      "\n",
      "Evaluating: batch 10 begins at 21:56:45.368583\n",
      "\n",
      "Evaluating: batch 10 ends at 21:56:45.589685\n",
      "\n",
      "Evaluating: batch 11 begins at 21:56:45.592207\n",
      "\n",
      "Evaluating: batch 11 ends at 21:56:45.812624\n",
      "\n",
      "Evaluating: batch 12 begins at 21:56:45.815063\n",
      "\n",
      "Evaluating: batch 12 ends at 21:56:46.036165\n",
      "\n",
      "Evaluating: batch 13 begins at 21:56:46.038317\n",
      "\n",
      "Evaluating: batch 13 ends at 21:56:46.259858\n",
      "\n",
      "Evaluating: batch 14 begins at 21:56:46.262436\n",
      "\n",
      "Evaluating: batch 14 ends at 21:56:46.483864\n",
      "\n",
      "Evaluating: batch 15 begins at 21:56:46.486413\n",
      "\n",
      "Evaluating: batch 15 ends at 21:56:46.707904\n",
      "\n",
      "Evaluating: batch 16 begins at 21:56:46.710628\n",
      "\n",
      "Evaluating: batch 16 ends at 21:56:46.938972\n",
      "\n",
      "Evaluating: batch 17 begins at 21:56:46.940893\n",
      "\n",
      "Evaluating: batch 17 ends at 21:56:47.160380\n",
      "\n",
      "Evaluating: batch 18 begins at 21:56:47.162859\n",
      "\n",
      "Evaluating: batch 18 ends at 21:56:47.385231\n",
      "\n",
      "Evaluating: batch 19 begins at 21:56:47.387791\n",
      "\n",
      "Evaluating: batch 19 ends at 21:56:47.607876\n",
      "\n",
      "Evaluating: batch 20 begins at 21:56:47.610424\n",
      "\n",
      "Evaluating: batch 20 ends at 21:56:47.834557\n",
      "\n",
      "Evaluating: batch 21 begins at 21:56:47.836986\n",
      "\n",
      "Evaluating: batch 21 ends at 21:56:48.056695\n",
      "\n",
      "Evaluating: batch 22 begins at 21:56:48.059329\n",
      "\n",
      "Evaluating: batch 22 ends at 21:56:48.279249\n",
      "\n",
      "Evaluating: batch 23 begins at 21:56:48.281731\n",
      "\n",
      "Evaluating: batch 23 ends at 21:56:48.501139\n",
      "\n",
      "Evaluating: batch 24 begins at 21:56:48.503674\n",
      "\n",
      "Evaluating: batch 24 ends at 21:56:48.722442\n",
      "\n",
      "Evaluating: batch 25 begins at 21:56:48.724143\n",
      "\n",
      "Evaluating: batch 25 ends at 21:56:48.942627\n",
      "\n",
      "Evaluating: batch 26 begins at 21:56:48.944717\n",
      "\n",
      "Evaluating: batch 26 ends at 21:56:49.165376\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.76264\n",
      "60/60 [==============================] - 54s 906ms/step - loss: 0.7769 - bce_dice_loss: 0.7769 - val_loss: 0.7866 - val_bce_dice_loss: 0.7866\n",
      "Epoch 4/25\n",
      "\n",
      "Training: batch 0 begins at 21:56:49.195052\n",
      "\n",
      "Training: batch 0 ends at 21:56:49.990857\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.7005 - bce_dice_loss: 0.7005\n",
      "Training: batch 1 begins at 21:56:49.995631\n",
      "\n",
      "Training: batch 1 ends at 21:56:50.794213\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.7283 - bce_dice_loss: 0.7283\n",
      "Training: batch 2 begins at 21:56:50.798593\n",
      "\n",
      "Training: batch 2 ends at 21:56:51.587460\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.7449 - bce_dice_loss: 0.7449\n",
      "Training: batch 3 begins at 21:56:51.591783\n",
      "\n",
      "Training: batch 3 ends at 21:56:52.391198\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.8223 - bce_dice_loss: 0.8223\n",
      "Training: batch 4 begins at 21:56:52.397589\n",
      "\n",
      "Training: batch 4 ends at 21:56:53.205508\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.7599 - bce_dice_loss: 0.7599\n",
      "Training: batch 5 begins at 21:56:53.210516\n",
      "\n",
      "Training: batch 5 ends at 21:56:54.027884\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.7648 - bce_dice_loss: 0.7648\n",
      "Training: batch 6 begins at 21:56:54.032241\n",
      "\n",
      "Training: batch 6 ends at 21:56:54.815845\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.7786 - bce_dice_loss: 0.7786\n",
      "Training: batch 7 begins at 21:56:54.819301\n",
      "\n",
      "Training: batch 7 ends at 21:56:55.634503\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.7805 - bce_dice_loss: 0.7805\n",
      "Training: batch 8 begins at 21:56:55.639274\n",
      "\n",
      "Training: batch 8 ends at 21:56:56.418572\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.7970 - bce_dice_loss: 0.7970\n",
      "Training: batch 9 begins at 21:56:56.422857\n",
      "\n",
      "Training: batch 9 ends at 21:56:57.222129\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.7935 - bce_dice_loss: 0.7935\n",
      "Training: batch 10 begins at 21:56:57.226435\n",
      "\n",
      "Training: batch 10 ends at 21:56:58.037965\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.7979 - bce_dice_loss: 0.7979\n",
      "Training: batch 11 begins at 21:56:58.042500\n",
      "\n",
      "Training: batch 11 ends at 21:56:58.833875\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.7989 - bce_dice_loss: 0.7989\n",
      "Training: batch 12 begins at 21:56:58.838227\n",
      "\n",
      "Training: batch 12 ends at 21:56:59.648261\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.8077 - bce_dice_loss: 0.8077\n",
      "Training: batch 13 begins at 21:56:59.652480\n",
      "\n",
      "Training: batch 13 ends at 21:57:00.448506\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.8104 - bce_dice_loss: 0.8104\n",
      "Training: batch 14 begins at 21:57:00.452990\n",
      "\n",
      "Training: batch 14 ends at 21:57:01.246106\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.8054 - bce_dice_loss: 0.8054\n",
      "Training: batch 15 begins at 21:57:01.250415\n",
      "\n",
      "Training: batch 15 ends at 21:57:02.040907\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.7957 - bce_dice_loss: 0.7957\n",
      "Training: batch 16 begins at 21:57:02.045306\n",
      "\n",
      "Training: batch 16 ends at 21:57:02.835115\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.7897 - bce_dice_loss: 0.7897\n",
      "Training: batch 17 begins at 21:57:02.839391\n",
      "\n",
      "Training: batch 17 ends at 21:57:03.652263\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.7854 - bce_dice_loss: 0.7854\n",
      "Training: batch 18 begins at 21:57:03.656649\n",
      "\n",
      "Training: batch 18 ends at 21:57:04.434290\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.7800 - bce_dice_loss: 0.7800\n",
      "Training: batch 19 begins at 21:57:04.438089\n",
      "\n",
      "Training: batch 19 ends at 21:57:05.231154\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.7842 - bce_dice_loss: 0.7842\n",
      "Training: batch 20 begins at 21:57:05.235772\n",
      "\n",
      "Training: batch 20 ends at 21:57:06.024717\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.7875 - bce_dice_loss: 0.7875\n",
      "Training: batch 21 begins at 21:57:06.029510\n",
      "\n",
      "Training: batch 21 ends at 21:57:06.826226\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.7789 - bce_dice_loss: 0.7789\n",
      "Training: batch 22 begins at 21:57:06.829449\n",
      "\n",
      "Training: batch 22 ends at 21:57:07.626789\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.7726 - bce_dice_loss: 0.7726\n",
      "Training: batch 23 begins at 21:57:07.631301\n",
      "\n",
      "Training: batch 23 ends at 21:57:08.444081\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.7759 - bce_dice_loss: 0.7759\n",
      "Training: batch 24 begins at 21:57:08.449206\n",
      "\n",
      "Training: batch 24 ends at 21:57:09.240507\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.7763 - bce_dice_loss: 0.7763\n",
      "Training: batch 25 begins at 21:57:09.244286\n",
      "\n",
      "Training: batch 25 ends at 21:57:10.031761\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.7792 - bce_dice_loss: 0.7792\n",
      "Training: batch 26 begins at 21:57:10.036012\n",
      "\n",
      "Training: batch 26 ends at 21:57:10.824435\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.7803 - bce_dice_loss: 0.7803\n",
      "Training: batch 27 begins at 21:57:10.827861\n",
      "\n",
      "Training: batch 27 ends at 21:57:11.621718\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.7753 - bce_dice_loss: 0.7753\n",
      "Training: batch 28 begins at 21:57:11.626335\n",
      "\n",
      "Training: batch 28 ends at 21:57:12.403242\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.7783 - bce_dice_loss: 0.7783\n",
      "Training: batch 29 begins at 21:57:12.407639\n",
      "\n",
      "Training: batch 29 ends at 21:57:13.222488\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.7787 - bce_dice_loss: 0.7787\n",
      "Training: batch 30 begins at 21:57:13.228272\n",
      "\n",
      "Training: batch 30 ends at 21:57:14.017487\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.7716 - bce_dice_loss: 0.7716\n",
      "Training: batch 31 begins at 21:57:14.021147\n",
      "\n",
      "Training: batch 31 ends at 21:57:14.809270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/60 [===============>..............] - ETA: 22s - loss: 0.7727 - bce_dice_loss: 0.7727\n",
      "Training: batch 32 begins at 21:57:14.814435\n",
      "\n",
      "Training: batch 32 ends at 21:57:15.615803\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.7750 - bce_dice_loss: 0.7750\n",
      "Training: batch 33 begins at 21:57:15.619676\n",
      "\n",
      "Training: batch 33 ends at 21:57:16.400689\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.7720 - bce_dice_loss: 0.7720\n",
      "Training: batch 34 begins at 21:57:16.405675\n",
      "\n",
      "Training: batch 34 ends at 21:57:17.186743\n",
      "35/60 [================>.............] - ETA: 19s - loss: 0.7728 - bce_dice_loss: 0.7728\n",
      "Training: batch 35 begins at 21:57:17.191034\n",
      "\n",
      "Training: batch 35 ends at 21:57:18.018824\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.7722 - bce_dice_loss: 0.7722\n",
      "Training: batch 36 begins at 21:57:18.025749\n",
      "\n",
      "Training: batch 36 ends at 21:57:18.810428\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.7742 - bce_dice_loss: 0.7742\n",
      "Training: batch 37 begins at 21:57:18.815602\n",
      "\n",
      "Training: batch 37 ends at 21:57:19.601269\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.7700 - bce_dice_loss: 0.7700\n",
      "Training: batch 38 begins at 21:57:19.606373\n",
      "\n",
      "Training: batch 38 ends at 21:57:20.393255\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.7691 - bce_dice_loss: 0.7691\n",
      "Training: batch 39 begins at 21:57:20.397689\n",
      "\n",
      "Training: batch 39 ends at 21:57:21.184972\n",
      "40/60 [===================>..........] - ETA: 15s - loss: 0.7666 - bce_dice_loss: 0.7666\n",
      "Training: batch 40 begins at 21:57:21.189331\n",
      "\n",
      "Training: batch 40 ends at 21:57:21.977444\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.7658 - bce_dice_loss: 0.7658\n",
      "Training: batch 41 begins at 21:57:21.982316\n",
      "\n",
      "Training: batch 41 ends at 21:57:22.771775\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.7644 - bce_dice_loss: 0.7644\n",
      "Training: batch 42 begins at 21:57:22.776275\n",
      "\n",
      "Training: batch 42 ends at 21:57:23.586438\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.7604 - bce_dice_loss: 0.7604\n",
      "Training: batch 43 begins at 21:57:23.590840\n",
      "\n",
      "Training: batch 43 ends at 21:57:24.379017\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.7615 - bce_dice_loss: 0.7615\n",
      "Training: batch 44 begins at 21:57:24.383939\n",
      "\n",
      "Training: batch 44 ends at 21:57:25.170219\n",
      "45/60 [=====================>........] - ETA: 11s - loss: 0.7547 - bce_dice_loss: 0.7547\n",
      "Training: batch 45 begins at 21:57:25.175023\n",
      "\n",
      "Training: batch 45 ends at 21:57:25.960086\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.7489 - bce_dice_loss: 0.7489\n",
      "Training: batch 46 begins at 21:57:25.964826\n",
      "\n",
      "Training: batch 46 ends at 21:57:26.775366\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.7510 - bce_dice_loss: 0.7510\n",
      "Training: batch 47 begins at 21:57:26.779446\n",
      "\n",
      "Training: batch 47 ends at 21:57:27.590156\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.7523 - bce_dice_loss: 0.7523 \n",
      "Training: batch 48 begins at 21:57:27.594366\n",
      "\n",
      "Training: batch 48 ends at 21:57:28.386634\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.7486 - bce_dice_loss: 0.7486\n",
      "Training: batch 49 begins at 21:57:28.391317\n",
      "\n",
      "Training: batch 49 ends at 21:57:29.175990\n",
      "50/60 [========================>.....] - ETA: 7s - loss: 0.7491 - bce_dice_loss: 0.7491\n",
      "Training: batch 50 begins at 21:57:29.180176\n",
      "\n",
      "Training: batch 50 ends at 21:57:29.965977\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.7547 - bce_dice_loss: 0.7547\n",
      "Training: batch 51 begins at 21:57:29.970388\n",
      "\n",
      "Training: batch 51 ends at 21:57:30.761871\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.7554 - bce_dice_loss: 0.7554\n",
      "Training: batch 52 begins at 21:57:30.765209\n",
      "\n",
      "Training: batch 52 ends at 21:57:31.554880\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.7563 - bce_dice_loss: 0.7563\n",
      "Training: batch 53 begins at 21:57:31.559160\n",
      "\n",
      "Training: batch 53 ends at 21:57:32.343610\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.7580 - bce_dice_loss: 0.7580\n",
      "Training: batch 54 begins at 21:57:32.348203\n",
      "\n",
      "Training: batch 54 ends at 21:57:33.156055\n",
      "55/60 [==========================>...] - ETA: 3s - loss: 0.7563 - bce_dice_loss: 0.7563\n",
      "Training: batch 55 begins at 21:57:33.160467\n",
      "\n",
      "Training: batch 55 ends at 21:57:33.945666\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.7596 - bce_dice_loss: 0.7596\n",
      "Training: batch 56 begins at 21:57:33.950222\n",
      "\n",
      "Training: batch 56 ends at 21:57:34.734096\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.7610 - bce_dice_loss: 0.7610\n",
      "Training: batch 57 begins at 21:57:34.738597\n",
      "\n",
      "Training: batch 57 ends at 21:57:35.524451\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.7619 - bce_dice_loss: 0.7619\n",
      "Training: batch 58 begins at 21:57:35.528926\n",
      "\n",
      "Training: batch 58 ends at 21:57:36.325794\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7619 - bce_dice_loss: 0.7619\n",
      "Training: batch 59 begins at 21:57:36.330588\n",
      "\n",
      "Training: batch 59 ends at 21:57:37.124228\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7609 - bce_dice_loss: 0.7609\n",
      "Evaluating: batch 0 begins at 21:57:37.158808\n",
      "\n",
      "Evaluating: batch 0 ends at 21:57:37.424791\n",
      "\n",
      "Evaluating: batch 1 begins at 21:57:37.425932\n",
      "\n",
      "Evaluating: batch 1 ends at 21:57:37.638081\n",
      "\n",
      "Evaluating: batch 2 begins at 21:57:37.640055\n",
      "\n",
      "Evaluating: batch 2 ends at 21:57:37.858664\n",
      "\n",
      "Evaluating: batch 3 begins at 21:57:37.861222\n",
      "\n",
      "Evaluating: batch 3 ends at 21:57:38.082646\n",
      "\n",
      "Evaluating: batch 4 begins at 21:57:38.085243\n",
      "\n",
      "Evaluating: batch 4 ends at 21:57:38.307460\n",
      "\n",
      "Evaluating: batch 5 begins at 21:57:38.309501\n",
      "\n",
      "Evaluating: batch 5 ends at 21:57:38.539094\n",
      "\n",
      "Evaluating: batch 6 begins at 21:57:38.541758\n",
      "\n",
      "Evaluating: batch 6 ends at 21:57:38.765881\n",
      "\n",
      "Evaluating: batch 7 begins at 21:57:38.768427\n",
      "\n",
      "Evaluating: batch 7 ends at 21:57:38.990331\n",
      "\n",
      "Evaluating: batch 8 begins at 21:57:38.992594\n",
      "\n",
      "Evaluating: batch 8 ends at 21:57:39.216428\n",
      "\n",
      "Evaluating: batch 9 begins at 21:57:39.219117\n",
      "\n",
      "Evaluating: batch 9 ends at 21:57:39.441201\n",
      "\n",
      "Evaluating: batch 10 begins at 21:57:39.443818\n",
      "\n",
      "Evaluating: batch 10 ends at 21:57:39.664872\n",
      "\n",
      "Evaluating: batch 11 begins at 21:57:39.667292\n",
      "\n",
      "Evaluating: batch 11 ends at 21:57:39.889835\n",
      "\n",
      "Evaluating: batch 12 begins at 21:57:39.892773\n",
      "\n",
      "Evaluating: batch 12 ends at 21:57:40.114243\n",
      "\n",
      "Evaluating: batch 13 begins at 21:57:40.116968\n",
      "\n",
      "Evaluating: batch 13 ends at 21:57:40.337630\n",
      "\n",
      "Evaluating: batch 14 begins at 21:57:40.340247\n",
      "\n",
      "Evaluating: batch 14 ends at 21:57:40.562348\n",
      "\n",
      "Evaluating: batch 15 begins at 21:57:40.564724\n",
      "\n",
      "Evaluating: batch 15 ends at 21:57:40.785303\n",
      "\n",
      "Evaluating: batch 16 begins at 21:57:40.787805\n",
      "\n",
      "Evaluating: batch 16 ends at 21:57:41.010318\n",
      "\n",
      "Evaluating: batch 17 begins at 21:57:41.012846\n",
      "\n",
      "Evaluating: batch 17 ends at 21:57:41.233887\n",
      "\n",
      "Evaluating: batch 18 begins at 21:57:41.236555\n",
      "\n",
      "Evaluating: batch 18 ends at 21:57:41.458623\n",
      "\n",
      "Evaluating: batch 19 begins at 21:57:41.461252\n",
      "\n",
      "Evaluating: batch 19 ends at 21:57:41.681267\n",
      "\n",
      "Evaluating: batch 20 begins at 21:57:41.684030\n",
      "\n",
      "Evaluating: batch 20 ends at 21:57:41.903828\n",
      "\n",
      "Evaluating: batch 21 begins at 21:57:41.906295\n",
      "\n",
      "Evaluating: batch 21 ends at 21:57:42.121874\n",
      "\n",
      "Evaluating: batch 22 begins at 21:57:42.124648\n",
      "\n",
      "Evaluating: batch 22 ends at 21:57:42.360463\n",
      "\n",
      "Evaluating: batch 23 begins at 21:57:42.361733\n",
      "\n",
      "Evaluating: batch 23 ends at 21:57:42.577656\n",
      "\n",
      "Evaluating: batch 24 begins at 21:57:42.580189\n",
      "\n",
      "Evaluating: batch 24 ends at 21:57:42.804573\n",
      "\n",
      "Evaluating: batch 25 begins at 21:57:42.805793\n",
      "\n",
      "Evaluating: batch 25 ends at 21:57:43.035779\n",
      "\n",
      "Evaluating: batch 26 begins at 21:57:43.037259\n",
      "\n",
      "Evaluating: batch 26 ends at 21:57:43.263143\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.76264 to 0.72900, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 933ms/step - loss: 0.7609 - bce_dice_loss: 0.7609 - val_loss: 0.7290 - val_bce_dice_loss: 0.7290\n",
      "Epoch 5/25\n",
      "\n",
      "Training: batch 0 begins at 21:57:45.016895\n",
      "\n",
      "Training: batch 0 ends at 21:57:45.859982\n",
      " 1/60 [..............................] - ETA: 49s - loss: 0.5744 - bce_dice_loss: 0.5744\n",
      "Training: batch 1 begins at 21:57:45.862909\n",
      "\n",
      "Training: batch 1 ends at 21:57:46.806368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/60 [>.............................] - ETA: 54s - loss: 0.6721 - bce_dice_loss: 0.6721\n",
      "Training: batch 2 begins at 21:57:46.809051\n",
      "\n",
      "Training: batch 2 ends at 21:57:47.707592\n",
      " 3/60 [>.............................] - ETA: 52s - loss: 0.7366 - bce_dice_loss: 0.7366\n",
      "Training: batch 3 begins at 21:57:47.712104\n",
      "\n",
      "Training: batch 3 ends at 21:57:48.608115\n",
      " 4/60 [=>............................] - ETA: 51s - loss: 0.7105 - bce_dice_loss: 0.7105\n",
      "Training: batch 4 begins at 21:57:48.611286\n",
      "\n",
      "Training: batch 4 ends at 21:57:49.444111\n",
      " 5/60 [=>............................] - ETA: 49s - loss: 0.7451 - bce_dice_loss: 0.7451\n",
      "Training: batch 5 begins at 21:57:49.448101\n",
      "\n",
      "Training: batch 5 ends at 21:57:50.290663\n",
      " 6/60 [==>...........................] - ETA: 47s - loss: 0.7268 - bce_dice_loss: 0.7268\n",
      "Training: batch 6 begins at 21:57:50.293098\n",
      "\n",
      "Training: batch 6 ends at 21:57:51.348644\n",
      " 7/60 [==>...........................] - ETA: 48s - loss: 0.7247 - bce_dice_loss: 0.7247\n",
      "Training: batch 7 begins at 21:57:51.350946\n",
      "\n",
      "Training: batch 7 ends at 21:57:52.419914\n",
      " 8/60 [===>..........................] - ETA: 48s - loss: 0.6861 - bce_dice_loss: 0.6861\n",
      "Training: batch 8 begins at 21:57:52.422045\n",
      "\n",
      "Training: batch 8 ends at 21:57:53.471733\n",
      " 9/60 [===>..........................] - ETA: 48s - loss: 0.6984 - bce_dice_loss: 0.6984\n",
      "Training: batch 9 begins at 21:57:53.475665\n",
      "\n",
      "Training: batch 9 ends at 21:57:54.395965\n",
      "10/60 [====>.........................] - ETA: 47s - loss: 0.7173 - bce_dice_loss: 0.7173\n",
      "Training: batch 10 begins at 21:57:54.400339\n",
      "\n",
      "Training: batch 10 ends at 21:57:55.261784\n",
      "11/60 [====>.........................] - ETA: 46s - loss: 0.7263 - bce_dice_loss: 0.7263\n",
      "Training: batch 11 begins at 21:57:55.265883\n",
      "\n",
      "Training: batch 11 ends at 21:57:56.047413\n",
      "12/60 [=====>........................] - ETA: 44s - loss: 0.7219 - bce_dice_loss: 0.7219\n",
      "Training: batch 12 begins at 21:57:56.050472\n",
      "\n",
      "Training: batch 12 ends at 21:57:56.869687\n",
      "13/60 [=====>........................] - ETA: 43s - loss: 0.7327 - bce_dice_loss: 0.7327\n",
      "Training: batch 13 begins at 21:57:56.872560\n",
      "\n",
      "Training: batch 13 ends at 21:57:57.676037\n",
      "14/60 [======>.......................] - ETA: 41s - loss: 0.7230 - bce_dice_loss: 0.7230\n",
      "Training: batch 14 begins at 21:57:57.680349\n",
      "\n",
      "Training: batch 14 ends at 21:57:58.486225\n",
      "15/60 [======>.......................] - ETA: 40s - loss: 0.7282 - bce_dice_loss: 0.7282\n",
      "Training: batch 15 begins at 21:57:58.489314\n",
      "\n",
      "Training: batch 15 ends at 21:57:59.323762\n",
      "16/60 [=======>......................] - ETA: 39s - loss: 0.7331 - bce_dice_loss: 0.7331\n",
      "Training: batch 16 begins at 21:57:59.327352\n",
      "\n",
      "Training: batch 16 ends at 21:58:00.205320\n",
      "17/60 [=======>......................] - ETA: 38s - loss: 0.7384 - bce_dice_loss: 0.7384\n",
      "Training: batch 17 begins at 21:58:00.208637\n",
      "\n",
      "Training: batch 17 ends at 21:58:01.013437\n",
      "18/60 [========>.....................] - ETA: 37s - loss: 0.7454 - bce_dice_loss: 0.7454\n",
      "Training: batch 18 begins at 21:58:01.016477\n",
      "\n",
      "Training: batch 18 ends at 21:58:01.879429\n",
      "19/60 [========>.....................] - ETA: 36s - loss: 0.7457 - bce_dice_loss: 0.7457\n",
      "Training: batch 19 begins at 21:58:01.883733\n",
      "\n",
      "Training: batch 19 ends at 21:58:02.701411\n",
      "20/60 [=========>....................] - ETA: 35s - loss: 0.7464 - bce_dice_loss: 0.7464\n",
      "Training: batch 20 begins at 21:58:02.704504\n",
      "\n",
      "Training: batch 20 ends at 21:58:03.485199\n",
      "21/60 [=========>....................] - ETA: 34s - loss: 0.7405 - bce_dice_loss: 0.7405\n",
      "Training: batch 21 begins at 21:58:03.489322\n",
      "\n",
      "Training: batch 21 ends at 21:58:04.278569\n",
      "22/60 [==========>...................] - ETA: 33s - loss: 0.7329 - bce_dice_loss: 0.7329\n",
      "Training: batch 22 begins at 21:58:04.282259\n",
      "\n",
      "Training: batch 22 ends at 21:58:05.071925\n",
      "23/60 [==========>...................] - ETA: 32s - loss: 0.7223 - bce_dice_loss: 0.7223\n",
      "Training: batch 23 begins at 21:58:05.076347\n",
      "\n",
      "Training: batch 23 ends at 21:58:05.874455\n",
      "24/60 [===========>..................] - ETA: 31s - loss: 0.7234 - bce_dice_loss: 0.7234\n",
      "Training: batch 24 begins at 21:58:05.878760\n",
      "\n",
      "Training: batch 24 ends at 21:58:06.748176\n",
      "25/60 [===========>..................] - ETA: 30s - loss: 0.7289 - bce_dice_loss: 0.7289\n",
      "Training: batch 25 begins at 21:58:06.753315\n",
      "\n",
      "Training: batch 25 ends at 21:58:07.619312\n",
      "26/60 [============>.................] - ETA: 29s - loss: 0.7208 - bce_dice_loss: 0.7208\n",
      "Training: batch 26 begins at 21:58:07.623460\n",
      "\n",
      "Training: batch 26 ends at 21:58:08.484635\n",
      "27/60 [============>.................] - ETA: 28s - loss: 0.7166 - bce_dice_loss: 0.7166\n",
      "Training: batch 27 begins at 21:58:08.489218\n",
      "\n",
      "Training: batch 27 ends at 21:58:09.285467\n",
      "28/60 [=============>................] - ETA: 27s - loss: 0.7219 - bce_dice_loss: 0.7219\n",
      "Training: batch 28 begins at 21:58:09.289709\n",
      "\n",
      "Training: batch 28 ends at 21:58:10.078874\n",
      "29/60 [=============>................] - ETA: 26s - loss: 0.7211 - bce_dice_loss: 0.7211\n",
      "Training: batch 29 begins at 21:58:10.083184\n",
      "\n",
      "Training: batch 29 ends at 21:58:10.869657\n",
      "30/60 [==============>...............] - ETA: 25s - loss: 0.7180 - bce_dice_loss: 0.7180\n",
      "Training: batch 30 begins at 21:58:10.874091\n",
      "\n",
      "Training: batch 30 ends at 21:58:11.664340\n",
      "31/60 [==============>...............] - ETA: 24s - loss: 0.7221 - bce_dice_loss: 0.7221\n",
      "Training: batch 31 begins at 21:58:11.668534\n",
      "\n",
      "Training: batch 31 ends at 21:58:12.456600\n",
      "32/60 [===============>..............] - ETA: 24s - loss: 0.7259 - bce_dice_loss: 0.7259\n",
      "Training: batch 32 begins at 21:58:12.460751\n",
      "\n",
      "Training: batch 32 ends at 21:58:13.241879\n",
      "33/60 [===============>..............] - ETA: 23s - loss: 0.7281 - bce_dice_loss: 0.7281\n",
      "Training: batch 33 begins at 21:58:13.246292\n",
      "\n",
      "Training: batch 33 ends at 21:58:14.026698\n",
      "34/60 [================>.............] - ETA: 22s - loss: 0.7300 - bce_dice_loss: 0.7300\n",
      "Training: batch 34 begins at 21:58:14.030074\n",
      "\n",
      "Training: batch 34 ends at 21:58:14.816919\n",
      "35/60 [================>.............] - ETA: 21s - loss: 0.7298 - bce_dice_loss: 0.7298\n",
      "Training: batch 35 begins at 21:58:14.819813\n",
      "\n",
      "Training: batch 35 ends at 21:58:15.605503\n",
      "36/60 [=================>............] - ETA: 20s - loss: 0.7323 - bce_dice_loss: 0.7323\n",
      "Training: batch 36 begins at 21:58:15.609555\n",
      "\n",
      "Training: batch 36 ends at 21:58:16.398252\n",
      "37/60 [=================>............] - ETA: 19s - loss: 0.7289 - bce_dice_loss: 0.7289\n",
      "Training: batch 37 begins at 21:58:16.402767\n",
      "\n",
      "Training: batch 37 ends at 21:58:17.223316\n",
      "38/60 [==================>...........] - ETA: 18s - loss: 0.7310 - bce_dice_loss: 0.7310\n",
      "Training: batch 38 begins at 21:58:17.227574\n",
      "\n",
      "Training: batch 38 ends at 21:58:18.022782\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.7238 - bce_dice_loss: 0.7238\n",
      "Training: batch 39 begins at 21:58:18.025311\n",
      "\n",
      "Training: batch 39 ends at 21:58:18.811929\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.7252 - bce_dice_loss: 0.7252\n",
      "Training: batch 40 begins at 21:58:18.815335\n",
      "\n",
      "Training: batch 40 ends at 21:58:19.605553\n",
      "41/60 [===================>..........] - ETA: 16s - loss: 0.7268 - bce_dice_loss: 0.7268\n",
      "Training: batch 41 begins at 21:58:19.609968\n",
      "\n",
      "Training: batch 41 ends at 21:58:20.400992\n",
      "42/60 [====================>.........] - ETA: 15s - loss: 0.7280 - bce_dice_loss: 0.7280\n",
      "Training: batch 42 begins at 21:58:20.404292\n",
      "\n",
      "Training: batch 42 ends at 21:58:21.193623\n",
      "43/60 [====================>.........] - ETA: 14s - loss: 0.7317 - bce_dice_loss: 0.7317\n",
      "Training: batch 43 begins at 21:58:21.197963\n",
      "\n",
      "Training: batch 43 ends at 21:58:22.009781\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.7319 - bce_dice_loss: 0.7319\n",
      "Training: batch 44 begins at 21:58:22.015226\n",
      "\n",
      "Training: batch 44 ends at 21:58:22.809605\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.7342 - bce_dice_loss: 0.7342\n",
      "Training: batch 45 begins at 21:58:22.813792\n",
      "\n",
      "Training: batch 45 ends at 21:58:23.595880\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.7323 - bce_dice_loss: 0.7323\n",
      "Training: batch 46 begins at 21:58:23.599190\n",
      "\n",
      "Training: batch 46 ends at 21:58:24.392173\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.7338 - bce_dice_loss: 0.7338\n",
      "Training: batch 47 begins at 21:58:24.396365\n",
      "\n",
      "Training: batch 47 ends at 21:58:25.212723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/60 [=======================>......] - ETA: 10s - loss: 0.7308 - bce_dice_loss: 0.7308\n",
      "Training: batch 48 begins at 21:58:25.217205\n",
      "\n",
      "Training: batch 48 ends at 21:58:26.003476\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.7291 - bce_dice_loss: 0.7291 \n",
      "Training: batch 49 begins at 21:58:26.007961\n",
      "\n",
      "Training: batch 49 ends at 21:58:26.821171\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.7277 - bce_dice_loss: 0.7277\n",
      "Training: batch 50 begins at 21:58:26.825540\n",
      "\n",
      "Training: batch 50 ends at 21:58:27.622170\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.7298 - bce_dice_loss: 0.7298\n",
      "Training: batch 51 begins at 21:58:27.626785\n",
      "\n",
      "Training: batch 51 ends at 21:58:28.423551\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.7291 - bce_dice_loss: 0.7291\n",
      "Training: batch 52 begins at 21:58:28.427451\n",
      "\n",
      "Training: batch 52 ends at 21:58:29.217285\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.7304 - bce_dice_loss: 0.7304\n",
      "Training: batch 53 begins at 21:58:29.221662\n",
      "\n",
      "Training: batch 53 ends at 21:58:30.031944\n",
      "54/60 [==========================>...] - ETA: 5s - loss: 0.7308 - bce_dice_loss: 0.7308\n",
      "Training: batch 54 begins at 21:58:30.036325\n",
      "\n",
      "Training: batch 54 ends at 21:58:30.819739\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.7323 - bce_dice_loss: 0.7323\n",
      "Training: batch 55 begins at 21:58:30.822288\n",
      "\n",
      "Training: batch 55 ends at 21:58:31.610405\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.7307 - bce_dice_loss: 0.7307\n",
      "Training: batch 56 begins at 21:58:31.614954\n",
      "\n",
      "Training: batch 56 ends at 21:58:32.406967\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.7266 - bce_dice_loss: 0.7266\n",
      "Training: batch 57 begins at 21:58:32.411209\n",
      "\n",
      "Training: batch 57 ends at 21:58:33.200556\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.7235 - bce_dice_loss: 0.7235\n",
      "Training: batch 58 begins at 21:58:33.205613\n",
      "\n",
      "Training: batch 58 ends at 21:58:33.993807\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7290 - bce_dice_loss: 0.7290\n",
      "Training: batch 59 begins at 21:58:33.998052\n",
      "\n",
      "Training: batch 59 ends at 21:58:34.785876\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7241 - bce_dice_loss: 0.7241\n",
      "Evaluating: batch 0 begins at 21:58:34.819251\n",
      "\n",
      "Evaluating: batch 0 ends at 21:58:35.095569\n",
      "\n",
      "Evaluating: batch 1 begins at 21:58:35.097554\n",
      "\n",
      "Evaluating: batch 1 ends at 21:58:35.309350\n",
      "\n",
      "Evaluating: batch 2 begins at 21:58:35.311199\n",
      "\n",
      "Evaluating: batch 2 ends at 21:58:35.529544\n",
      "\n",
      "Evaluating: batch 3 begins at 21:58:35.532221\n",
      "\n",
      "Evaluating: batch 3 ends at 21:58:35.748983\n",
      "\n",
      "Evaluating: batch 4 begins at 21:58:35.750814\n",
      "\n",
      "Evaluating: batch 4 ends at 21:58:35.969694\n",
      "\n",
      "Evaluating: batch 5 begins at 21:58:35.972055\n",
      "\n",
      "Evaluating: batch 5 ends at 21:58:36.192482\n",
      "\n",
      "Evaluating: batch 6 begins at 21:58:36.194953\n",
      "\n",
      "Evaluating: batch 6 ends at 21:58:36.414519\n",
      "\n",
      "Evaluating: batch 7 begins at 21:58:36.417071\n",
      "\n",
      "Evaluating: batch 7 ends at 21:58:36.637945\n",
      "\n",
      "Evaluating: batch 8 begins at 21:58:36.640304\n",
      "\n",
      "Evaluating: batch 8 ends at 21:58:36.860985\n",
      "\n",
      "Evaluating: batch 9 begins at 21:58:36.862493\n",
      "\n",
      "Evaluating: batch 9 ends at 21:58:37.079515\n",
      "\n",
      "Evaluating: batch 10 begins at 21:58:37.082109\n",
      "\n",
      "Evaluating: batch 10 ends at 21:58:37.311510\n",
      "\n",
      "Evaluating: batch 11 begins at 21:58:37.315109\n",
      "\n",
      "Evaluating: batch 11 ends at 21:58:37.537498\n",
      "\n",
      "Evaluating: batch 12 begins at 21:58:37.540064\n",
      "\n",
      "Evaluating: batch 12 ends at 21:58:37.762600\n",
      "\n",
      "Evaluating: batch 13 begins at 21:58:37.765255\n",
      "\n",
      "Evaluating: batch 13 ends at 21:58:37.987979\n",
      "\n",
      "Evaluating: batch 14 begins at 21:58:37.990688\n",
      "\n",
      "Evaluating: batch 14 ends at 21:58:38.213250\n",
      "\n",
      "Evaluating: batch 15 begins at 21:58:38.214708\n",
      "\n",
      "Evaluating: batch 15 ends at 21:58:38.436387\n",
      "\n",
      "Evaluating: batch 16 begins at 21:58:38.438167\n",
      "\n",
      "Evaluating: batch 16 ends at 21:58:38.660141\n",
      "\n",
      "Evaluating: batch 17 begins at 21:58:38.661827\n",
      "\n",
      "Evaluating: batch 17 ends at 21:58:38.896686\n",
      "\n",
      "Evaluating: batch 18 begins at 21:58:38.898550\n",
      "\n",
      "Evaluating: batch 18 ends at 21:58:39.123261\n",
      "\n",
      "Evaluating: batch 19 begins at 21:58:39.124559\n",
      "\n",
      "Evaluating: batch 19 ends at 21:58:39.355166\n",
      "\n",
      "Evaluating: batch 20 begins at 21:58:39.356288\n",
      "\n",
      "Evaluating: batch 20 ends at 21:58:39.576686\n",
      "\n",
      "Evaluating: batch 21 begins at 21:58:39.579154\n",
      "\n",
      "Evaluating: batch 21 ends at 21:58:39.802150\n",
      "\n",
      "Evaluating: batch 22 begins at 21:58:39.803533\n",
      "\n",
      "Evaluating: batch 22 ends at 21:58:40.076183\n",
      "\n",
      "Evaluating: batch 23 begins at 21:58:40.077377\n",
      "\n",
      "Evaluating: batch 23 ends at 21:58:40.326419\n",
      "\n",
      "Evaluating: batch 24 begins at 21:58:40.327620\n",
      "\n",
      "Evaluating: batch 24 ends at 21:58:40.645238\n",
      "\n",
      "Evaluating: batch 25 begins at 21:58:40.646675\n",
      "\n",
      "Evaluating: batch 25 ends at 21:58:40.912615\n",
      "\n",
      "Evaluating: batch 26 begins at 21:58:40.914147\n",
      "\n",
      "Evaluating: batch 26 ends at 21:58:41.177747\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.72900 to 0.66358, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 58s 970ms/step - loss: 0.7241 - bce_dice_loss: 0.7241 - val_loss: 0.6636 - val_bce_dice_loss: 0.6636\n",
      "Epoch 6/25\n",
      "\n",
      "Training: batch 0 begins at 21:58:43.078524\n",
      "\n",
      "Training: batch 0 ends at 21:58:44.041662\n",
      " 1/60 [..............................] - ETA: 56s - loss: 0.5241 - bce_dice_loss: 0.5241\n",
      "Training: batch 1 begins at 21:58:44.046018\n",
      "\n",
      "Training: batch 1 ends at 21:58:44.909129\n",
      " 2/60 [>.............................] - ETA: 50s - loss: 0.5666 - bce_dice_loss: 0.5666\n",
      "Training: batch 2 begins at 21:58:44.913187\n",
      "\n",
      "Training: batch 2 ends at 21:58:45.707052\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.6579 - bce_dice_loss: 0.6579\n",
      "Training: batch 3 begins at 21:58:45.709854\n",
      "\n",
      "Training: batch 3 ends at 21:58:46.607241\n",
      " 4/60 [=>............................] - ETA: 47s - loss: 0.6083 - bce_dice_loss: 0.6083\n",
      "Training: batch 4 begins at 21:58:46.609889\n",
      "\n",
      "Training: batch 4 ends at 21:58:47.436116\n",
      " 5/60 [=>............................] - ETA: 46s - loss: 0.5922 - bce_dice_loss: 0.5922\n",
      "Training: batch 5 begins at 21:58:47.440497\n",
      "\n",
      "Training: batch 5 ends at 21:58:48.235508\n",
      " 6/60 [==>...........................] - ETA: 45s - loss: 0.6106 - bce_dice_loss: 0.6106\n",
      "Training: batch 6 begins at 21:58:48.239089\n",
      "\n",
      "Training: batch 6 ends at 21:58:49.037585\n",
      " 7/60 [==>...........................] - ETA: 44s - loss: 0.6483 - bce_dice_loss: 0.6483\n",
      "Training: batch 7 begins at 21:58:49.042649\n",
      "\n",
      "Training: batch 7 ends at 21:58:49.845728\n",
      " 8/60 [===>..........................] - ETA: 43s - loss: 0.6461 - bce_dice_loss: 0.6461\n",
      "Training: batch 8 begins at 21:58:49.849753\n",
      "\n",
      "Training: batch 8 ends at 21:58:50.642490\n",
      " 9/60 [===>..........................] - ETA: 42s - loss: 0.6201 - bce_dice_loss: 0.6201\n",
      "Training: batch 9 begins at 21:58:50.644966\n",
      "\n",
      "Training: batch 9 ends at 21:58:51.446915\n",
      "10/60 [====>.........................] - ETA: 41s - loss: 0.6366 - bce_dice_loss: 0.6366\n",
      "Training: batch 10 begins at 21:58:51.451342\n",
      "\n",
      "Training: batch 10 ends at 21:58:52.249053\n",
      "11/60 [====>.........................] - ETA: 40s - loss: 0.6322 - bce_dice_loss: 0.6322\n",
      "Training: batch 11 begins at 21:58:52.253158\n",
      "\n",
      "Training: batch 11 ends at 21:58:53.046888\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.6490 - bce_dice_loss: 0.6490\n",
      "Training: batch 12 begins at 21:58:53.051122\n",
      "\n",
      "Training: batch 12 ends at 21:58:53.851773\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.6663 - bce_dice_loss: 0.6663\n",
      "Training: batch 13 begins at 21:58:53.856229\n",
      "\n",
      "Training: batch 13 ends at 21:58:54.655019\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.6688 - bce_dice_loss: 0.6688\n",
      "Training: batch 14 begins at 21:58:54.657736\n",
      "\n",
      "Training: batch 14 ends at 21:58:55.456377\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.6656 - bce_dice_loss: 0.6656\n",
      "Training: batch 15 begins at 21:58:55.460742\n",
      "\n",
      "Training: batch 15 ends at 21:58:56.255840\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6617 - bce_dice_loss: 0.6617\n",
      "Training: batch 16 begins at 21:58:56.260533\n",
      "\n",
      "Training: batch 16 ends at 21:58:57.054702\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6732 - bce_dice_loss: 0.6732\n",
      "Training: batch 17 begins at 21:58:57.059055\n",
      "\n",
      "Training: batch 17 ends at 21:58:57.865981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/60 [========>.....................] - ETA: 34s - loss: 0.6784 - bce_dice_loss: 0.6784\n",
      "Training: batch 18 begins at 21:58:57.870539\n",
      "\n",
      "Training: batch 18 ends at 21:58:58.688532\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.6834 - bce_dice_loss: 0.6834\n",
      "Training: batch 19 begins at 21:58:58.692897\n",
      "\n",
      "Training: batch 19 ends at 21:58:59.495995\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.6846 - bce_dice_loss: 0.6846\n",
      "Training: batch 20 begins at 21:58:59.500256\n",
      "\n",
      "Training: batch 20 ends at 21:59:00.303552\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6932 - bce_dice_loss: 0.6932\n",
      "Training: batch 21 begins at 21:59:00.309226\n",
      "\n",
      "Training: batch 21 ends at 21:59:01.119792\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6860 - bce_dice_loss: 0.6860\n",
      "Training: batch 22 begins at 21:59:01.124274\n",
      "\n",
      "Training: batch 22 ends at 21:59:01.917055\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.6749 - bce_dice_loss: 0.6749\n",
      "Training: batch 23 begins at 21:59:01.921331\n",
      "\n",
      "Training: batch 23 ends at 21:59:02.739357\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.6785 - bce_dice_loss: 0.6785\n",
      "Training: batch 24 begins at 21:59:02.741918\n",
      "\n",
      "Training: batch 24 ends at 21:59:03.545386\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.6834 - bce_dice_loss: 0.6834\n",
      "Training: batch 25 begins at 21:59:03.550058\n",
      "\n",
      "Training: batch 25 ends at 21:59:04.359493\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.6982 - bce_dice_loss: 0.6982\n",
      "Training: batch 26 begins at 21:59:04.364877\n",
      "\n",
      "Training: batch 26 ends at 21:59:05.149848\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6979 - bce_dice_loss: 0.6979\n",
      "Training: batch 27 begins at 21:59:05.153581\n",
      "\n",
      "Training: batch 27 ends at 21:59:05.958383\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6968 - bce_dice_loss: 0.6968\n",
      "Training: batch 28 begins at 21:59:05.961085\n",
      "\n",
      "Training: batch 28 ends at 21:59:06.779945\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.6998 - bce_dice_loss: 0.6998\n",
      "Training: batch 29 begins at 21:59:06.785464\n",
      "\n",
      "Training: batch 29 ends at 21:59:07.572670\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.7016 - bce_dice_loss: 0.7016\n",
      "Training: batch 30 begins at 21:59:07.575488\n",
      "\n",
      "Training: batch 30 ends at 21:59:08.396389\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.7005 - bce_dice_loss: 0.7005\n",
      "Training: batch 31 begins at 21:59:08.400806\n",
      "\n",
      "Training: batch 31 ends at 21:59:09.234262\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6976 - bce_dice_loss: 0.6976\n",
      "Training: batch 32 begins at 21:59:09.237355\n",
      "\n",
      "Training: batch 32 ends at 21:59:10.029165\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6968 - bce_dice_loss: 0.6968\n",
      "Training: batch 33 begins at 21:59:10.032113\n",
      "\n",
      "Training: batch 33 ends at 21:59:10.838678\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.6883 - bce_dice_loss: 0.6883\n",
      "Training: batch 34 begins at 21:59:10.843050\n",
      "\n",
      "Training: batch 34 ends at 21:59:11.634896\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.6902 - bce_dice_loss: 0.6902\n",
      "Training: batch 35 begins at 21:59:11.639189\n",
      "\n",
      "Training: batch 35 ends at 21:59:12.470694\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6918 - bce_dice_loss: 0.6918\n",
      "Training: batch 36 begins at 21:59:12.476412\n",
      "\n",
      "Training: batch 36 ends at 21:59:13.290487\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6858 - bce_dice_loss: 0.6858\n",
      "Training: batch 37 begins at 21:59:13.293492\n",
      "\n",
      "Training: batch 37 ends at 21:59:14.114414\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6873 - bce_dice_loss: 0.6873\n",
      "Training: batch 38 begins at 21:59:14.118439\n",
      "\n",
      "Training: batch 38 ends at 21:59:14.990431\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.6895 - bce_dice_loss: 0.6895\n",
      "Training: batch 39 begins at 21:59:14.993058\n",
      "\n",
      "Training: batch 39 ends at 21:59:15.902396\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6917 - bce_dice_loss: 0.6917\n",
      "Training: batch 40 begins at 21:59:15.905849\n",
      "\n",
      "Training: batch 40 ends at 21:59:16.747895\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6910 - bce_dice_loss: 0.6910\n",
      "Training: batch 41 begins at 21:59:16.750484\n",
      "\n",
      "Training: batch 41 ends at 21:59:17.560985\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6883 - bce_dice_loss: 0.6883\n",
      "Training: batch 42 begins at 21:59:17.563408\n",
      "\n",
      "Training: batch 42 ends at 21:59:18.478434\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6891 - bce_dice_loss: 0.6891\n",
      "Training: batch 43 begins at 21:59:18.483077\n",
      "\n",
      "Training: batch 43 ends at 21:59:19.312237\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.6897 - bce_dice_loss: 0.6897\n",
      "Training: batch 44 begins at 21:59:19.315268\n",
      "\n",
      "Training: batch 44 ends at 21:59:20.176679\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.6911 - bce_dice_loss: 0.6911\n",
      "Training: batch 45 begins at 21:59:20.180851\n",
      "\n",
      "Training: batch 45 ends at 21:59:21.063956\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6869 - bce_dice_loss: 0.6869\n",
      "Training: batch 46 begins at 21:59:21.067699\n",
      "\n",
      "Training: batch 46 ends at 21:59:21.870809\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6897 - bce_dice_loss: 0.6897\n",
      "Training: batch 47 begins at 21:59:21.875756\n",
      "\n",
      "Training: batch 47 ends at 21:59:22.662626\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6922 - bce_dice_loss: 0.6922 \n",
      "Training: batch 48 begins at 21:59:22.666811\n",
      "\n",
      "Training: batch 48 ends at 21:59:23.483116\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.6908 - bce_dice_loss: 0.6908\n",
      "Training: batch 49 begins at 21:59:23.487336\n",
      "\n",
      "Training: batch 49 ends at 21:59:24.275476\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.6945 - bce_dice_loss: 0.6945\n",
      "Training: batch 50 begins at 21:59:24.278035\n",
      "\n",
      "Training: batch 50 ends at 21:59:25.189739\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6901 - bce_dice_loss: 0.6901\n",
      "Training: batch 51 begins at 21:59:25.192479\n",
      "\n",
      "Training: batch 51 ends at 21:59:26.072170\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6905 - bce_dice_loss: 0.6905\n",
      "Training: batch 52 begins at 21:59:26.075566\n",
      "\n",
      "Training: batch 52 ends at 21:59:27.015779\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6937 - bce_dice_loss: 0.6937\n",
      "Training: batch 53 begins at 21:59:27.022392\n",
      "\n",
      "Training: batch 53 ends at 21:59:27.937031\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6929 - bce_dice_loss: 0.6929\n",
      "Training: batch 54 begins at 21:59:27.940892\n",
      "\n",
      "Training: batch 54 ends at 21:59:28.849292\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.6934 - bce_dice_loss: 0.6934\n",
      "Training: batch 55 begins at 21:59:28.852687\n",
      "\n",
      "Training: batch 55 ends at 21:59:29.661006\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6960 - bce_dice_loss: 0.6960\n",
      "Training: batch 56 begins at 21:59:29.665752\n",
      "\n",
      "Training: batch 56 ends at 21:59:30.499973\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6931 - bce_dice_loss: 0.6931\n",
      "Training: batch 57 begins at 21:59:30.503369\n",
      "\n",
      "Training: batch 57 ends at 21:59:31.370358\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6947 - bce_dice_loss: 0.6947\n",
      "Training: batch 58 begins at 21:59:31.373249\n",
      "\n",
      "Training: batch 58 ends at 21:59:32.213714\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6981 - bce_dice_loss: 0.6981\n",
      "Training: batch 59 begins at 21:59:32.218043\n",
      "\n",
      "Training: batch 59 ends at 21:59:33.035607\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6986 - bce_dice_loss: 0.6986\n",
      "Evaluating: batch 0 begins at 21:59:33.069978\n",
      "\n",
      "Evaluating: batch 0 ends at 21:59:33.359940\n",
      "\n",
      "Evaluating: batch 1 begins at 21:59:33.361219\n",
      "\n",
      "Evaluating: batch 1 ends at 21:59:33.584006\n",
      "\n",
      "Evaluating: batch 2 begins at 21:59:33.586516\n",
      "\n",
      "Evaluating: batch 2 ends at 21:59:33.808798\n",
      "\n",
      "Evaluating: batch 3 begins at 21:59:33.814519\n",
      "\n",
      "Evaluating: batch 3 ends at 21:59:34.033195\n",
      "\n",
      "Evaluating: batch 4 begins at 21:59:34.034453\n",
      "\n",
      "Evaluating: batch 4 ends at 21:59:34.256620\n",
      "\n",
      "Evaluating: batch 5 begins at 21:59:34.257929\n",
      "\n",
      "Evaluating: batch 5 ends at 21:59:34.481010\n",
      "\n",
      "Evaluating: batch 6 begins at 21:59:34.482356\n",
      "\n",
      "Evaluating: batch 6 ends at 21:59:34.701619\n",
      "\n",
      "Evaluating: batch 7 begins at 21:59:34.702919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 7 ends at 21:59:34.922680\n",
      "\n",
      "Evaluating: batch 8 begins at 21:59:34.923963\n",
      "\n",
      "Evaluating: batch 8 ends at 21:59:35.143459\n",
      "\n",
      "Evaluating: batch 9 begins at 21:59:35.145096\n",
      "\n",
      "Evaluating: batch 9 ends at 21:59:35.365223\n",
      "\n",
      "Evaluating: batch 10 begins at 21:59:35.366796\n",
      "\n",
      "Evaluating: batch 10 ends at 21:59:35.592380\n",
      "\n",
      "Evaluating: batch 11 begins at 21:59:35.593681\n",
      "\n",
      "Evaluating: batch 11 ends at 21:59:35.819898\n",
      "\n",
      "Evaluating: batch 12 begins at 21:59:35.822563\n",
      "\n",
      "Evaluating: batch 12 ends at 21:59:36.044241\n",
      "\n",
      "Evaluating: batch 13 begins at 21:59:36.045596\n",
      "\n",
      "Evaluating: batch 13 ends at 21:59:36.272061\n",
      "\n",
      "Evaluating: batch 14 begins at 21:59:36.274194\n",
      "\n",
      "Evaluating: batch 14 ends at 21:59:36.506224\n",
      "\n",
      "Evaluating: batch 15 begins at 21:59:36.507361\n",
      "\n",
      "Evaluating: batch 15 ends at 21:59:36.728829\n",
      "\n",
      "Evaluating: batch 16 begins at 21:59:36.730080\n",
      "\n",
      "Evaluating: batch 16 ends at 21:59:37.028487\n",
      "\n",
      "Evaluating: batch 17 begins at 21:59:37.029534\n",
      "\n",
      "Evaluating: batch 17 ends at 21:59:37.245321\n",
      "\n",
      "Evaluating: batch 18 begins at 21:59:37.246528\n",
      "\n",
      "Evaluating: batch 18 ends at 21:59:37.476698\n",
      "\n",
      "Evaluating: batch 19 begins at 21:59:37.477946\n",
      "\n",
      "Evaluating: batch 19 ends at 21:59:37.714566\n",
      "\n",
      "Evaluating: batch 20 begins at 21:59:37.716905\n",
      "\n",
      "Evaluating: batch 20 ends at 21:59:37.933570\n",
      "\n",
      "Evaluating: batch 21 begins at 21:59:37.934832\n",
      "\n",
      "Evaluating: batch 21 ends at 21:59:38.157198\n",
      "\n",
      "Evaluating: batch 22 begins at 21:59:38.158592\n",
      "\n",
      "Evaluating: batch 22 ends at 21:59:38.381844\n",
      "\n",
      "Evaluating: batch 23 begins at 21:59:38.383148\n",
      "\n",
      "Evaluating: batch 23 ends at 21:59:38.599776\n",
      "\n",
      "Evaluating: batch 24 begins at 21:59:38.601021\n",
      "\n",
      "Evaluating: batch 24 ends at 21:59:38.817690\n",
      "\n",
      "Evaluating: batch 25 begins at 21:59:38.819190\n",
      "\n",
      "Evaluating: batch 25 ends at 21:59:39.044490\n",
      "\n",
      "Evaluating: batch 26 begins at 21:59:39.045874\n",
      "\n",
      "Evaluating: batch 26 ends at 21:59:39.260354\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66358\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 0.6986 - bce_dice_loss: 0.6986 - val_loss: 0.7156 - val_bce_dice_loss: 0.7156\n",
      "Epoch 7/25\n",
      "\n",
      "Training: batch 0 begins at 21:59:39.286391\n",
      "\n",
      "Training: batch 0 ends at 21:59:40.090633\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.6217 - bce_dice_loss: 0.6217\n",
      "Training: batch 1 begins at 21:59:40.093523\n",
      "\n",
      "Training: batch 1 ends at 21:59:40.899618\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.7014 - bce_dice_loss: 0.7014\n",
      "Training: batch 2 begins at 21:59:40.904560\n",
      "\n",
      "Training: batch 2 ends at 21:59:41.701031\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.7213 - bce_dice_loss: 0.7213\n",
      "Training: batch 3 begins at 21:59:41.705944\n",
      "\n",
      "Training: batch 3 ends at 21:59:42.519796\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.7515 - bce_dice_loss: 0.7515\n",
      "Training: batch 4 begins at 21:59:42.523932\n",
      "\n",
      "Training: batch 4 ends at 21:59:43.318664\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.7202 - bce_dice_loss: 0.7202\n",
      "Training: batch 5 begins at 21:59:43.323085\n",
      "\n",
      "Training: batch 5 ends at 21:59:44.149162\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6992 - bce_dice_loss: 0.6992\n",
      "Training: batch 6 begins at 21:59:44.153216\n",
      "\n",
      "Training: batch 6 ends at 21:59:44.964071\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.6948 - bce_dice_loss: 0.6948\n",
      "Training: batch 7 begins at 21:59:44.968371\n",
      "\n",
      "Training: batch 7 ends at 21:59:45.773138\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.6919 - bce_dice_loss: 0.6919\n",
      "Training: batch 8 begins at 21:59:45.778226\n",
      "\n",
      "Training: batch 8 ends at 21:59:46.574586\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.6751 - bce_dice_loss: 0.6751\n",
      "Training: batch 9 begins at 21:59:46.580705\n",
      "\n",
      "Training: batch 9 ends at 21:59:47.431964\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.6806 - bce_dice_loss: 0.6806\n",
      "Training: batch 10 begins at 21:59:47.434410\n",
      "\n",
      "Training: batch 10 ends at 21:59:48.261894\n",
      "11/60 [====>.........................] - ETA: 40s - loss: 0.7112 - bce_dice_loss: 0.7112\n",
      "Training: batch 11 begins at 21:59:48.266469\n",
      "\n",
      "Training: batch 11 ends at 21:59:49.059319\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.6930 - bce_dice_loss: 0.6930\n",
      "Training: batch 12 begins at 21:59:49.063365\n",
      "\n",
      "Training: batch 12 ends at 21:59:49.856032\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.6952 - bce_dice_loss: 0.6952\n",
      "Training: batch 13 begins at 21:59:49.860223\n",
      "\n",
      "Training: batch 13 ends at 21:59:50.673160\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.7007 - bce_dice_loss: 0.7007\n",
      "Training: batch 14 begins at 21:59:50.675680\n",
      "\n",
      "Training: batch 14 ends at 21:59:51.473776\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.7066 - bce_dice_loss: 0.7066\n",
      "Training: batch 15 begins at 21:59:51.477893\n",
      "\n",
      "Training: batch 15 ends at 21:59:52.321128\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.7058 - bce_dice_loss: 0.7058\n",
      "Training: batch 16 begins at 21:59:52.323696\n",
      "\n",
      "Training: batch 16 ends at 21:59:53.134127\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.7193 - bce_dice_loss: 0.7193\n",
      "Training: batch 17 begins at 21:59:53.137779\n",
      "\n",
      "Training: batch 17 ends at 21:59:54.013862\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.7286 - bce_dice_loss: 0.7286\n",
      "Training: batch 18 begins at 21:59:54.017138\n",
      "\n",
      "Training: batch 18 ends at 21:59:54.864178\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.7299 - bce_dice_loss: 0.7299\n",
      "Training: batch 19 begins at 21:59:54.867014\n",
      "\n",
      "Training: batch 19 ends at 21:59:55.720382\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.7324 - bce_dice_loss: 0.7324\n",
      "Training: batch 20 begins at 21:59:55.722794\n",
      "\n",
      "Training: batch 20 ends at 21:59:56.549293\n",
      "21/60 [=========>....................] - ETA: 32s - loss: 0.7376 - bce_dice_loss: 0.7376\n",
      "Training: batch 21 begins at 21:59:56.552058\n",
      "\n",
      "Training: batch 21 ends at 21:59:57.363466\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.7352 - bce_dice_loss: 0.7352\n",
      "Training: batch 22 begins at 21:59:57.366070\n",
      "\n",
      "Training: batch 22 ends at 21:59:58.174571\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.7374 - bce_dice_loss: 0.7374\n",
      "Training: batch 23 begins at 21:59:58.177298\n",
      "\n",
      "Training: batch 23 ends at 21:59:58.986462\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.7424 - bce_dice_loss: 0.7424\n",
      "Training: batch 24 begins at 21:59:58.990690\n",
      "\n",
      "Training: batch 24 ends at 21:59:59.790162\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.7451 - bce_dice_loss: 0.7451\n",
      "Training: batch 25 begins at 21:59:59.793810\n",
      "\n",
      "Training: batch 25 ends at 22:00:00.584757\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.7429 - bce_dice_loss: 0.7429\n",
      "Training: batch 26 begins at 22:00:00.588973\n",
      "\n",
      "Training: batch 26 ends at 22:00:01.386306\n",
      "27/60 [============>.................] - ETA: 27s - loss: 0.7372 - bce_dice_loss: 0.7372\n",
      "Training: batch 27 begins at 22:00:01.390525\n",
      "\n",
      "Training: batch 27 ends at 22:00:02.208592\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.7377 - bce_dice_loss: 0.7377\n",
      "Training: batch 28 begins at 22:00:02.212788\n",
      "\n",
      "Training: batch 28 ends at 22:00:03.021286\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.7366 - bce_dice_loss: 0.7366\n",
      "Training: batch 29 begins at 22:00:03.025564\n",
      "\n",
      "Training: batch 29 ends at 22:00:03.844995\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.7317 - bce_dice_loss: 0.7317\n",
      "Training: batch 30 begins at 22:00:03.849064\n",
      "\n",
      "Training: batch 30 ends at 22:00:04.641172\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.7257 - bce_dice_loss: 0.7257\n",
      "Training: batch 31 begins at 22:00:04.645044\n",
      "\n",
      "Training: batch 31 ends at 22:00:05.441382\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.7167 - bce_dice_loss: 0.7167\n",
      "Training: batch 32 begins at 22:00:05.445594\n",
      "\n",
      "Training: batch 32 ends at 22:00:06.246810\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.7163 - bce_dice_loss: 0.7163\n",
      "Training: batch 33 begins at 22:00:06.251084\n",
      "\n",
      "Training: batch 33 ends at 22:00:07.053090\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.7121 - bce_dice_loss: 0.7121\n",
      "Training: batch 34 begins at 22:00:07.056654\n",
      "\n",
      "Training: batch 34 ends at 22:00:07.860449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/60 [================>.............] - ETA: 20s - loss: 0.7171 - bce_dice_loss: 0.7171\n",
      "Training: batch 35 begins at 22:00:07.864765\n",
      "\n",
      "Training: batch 35 ends at 22:00:08.661193\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.7199 - bce_dice_loss: 0.7199\n",
      "Training: batch 36 begins at 22:00:08.665517\n",
      "\n",
      "Training: batch 36 ends at 22:00:09.516921\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.7214 - bce_dice_loss: 0.7214\n",
      "Training: batch 37 begins at 22:00:09.521164\n",
      "\n",
      "Training: batch 37 ends at 22:00:10.321786\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.7165 - bce_dice_loss: 0.7165\n",
      "Training: batch 38 begins at 22:00:10.324948\n",
      "\n",
      "Training: batch 38 ends at 22:00:11.133901\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.7153 - bce_dice_loss: 0.7153\n",
      "Training: batch 39 begins at 22:00:11.136779\n",
      "\n",
      "Training: batch 39 ends at 22:00:12.010562\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.7127 - bce_dice_loss: 0.7127\n",
      "Training: batch 40 begins at 22:00:12.012846\n",
      "\n",
      "Training: batch 40 ends at 22:00:12.876741\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.7150 - bce_dice_loss: 0.7150\n",
      "Training: batch 41 begins at 22:00:12.879098\n",
      "\n",
      "Training: batch 41 ends at 22:00:13.749997\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.7154 - bce_dice_loss: 0.7154\n",
      "Training: batch 42 begins at 22:00:13.752690\n",
      "\n",
      "Training: batch 42 ends at 22:00:14.559643\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.7154 - bce_dice_loss: 0.7154\n",
      "Training: batch 43 begins at 22:00:14.563686\n",
      "\n",
      "Training: batch 43 ends at 22:00:15.391679\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.7146 - bce_dice_loss: 0.7146\n",
      "Training: batch 44 begins at 22:00:15.395807\n",
      "\n",
      "Training: batch 44 ends at 22:00:16.205394\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.7107 - bce_dice_loss: 0.7107\n",
      "Training: batch 45 begins at 22:00:16.208783\n",
      "\n",
      "Training: batch 45 ends at 22:00:17.011050\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.7176 - bce_dice_loss: 0.7176\n",
      "Training: batch 46 begins at 22:00:17.013570\n",
      "\n",
      "Training: batch 46 ends at 22:00:17.813484\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.7157 - bce_dice_loss: 0.7157\n",
      "Training: batch 47 begins at 22:00:17.816093\n",
      "\n",
      "Training: batch 47 ends at 22:00:18.611588\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.7128 - bce_dice_loss: 0.7128 \n",
      "Training: batch 48 begins at 22:00:18.615315\n",
      "\n",
      "Training: batch 48 ends at 22:00:19.415063\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.7121 - bce_dice_loss: 0.7121\n",
      "Training: batch 49 begins at 22:00:19.419705\n",
      "\n",
      "Training: batch 49 ends at 22:00:20.211639\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.7065 - bce_dice_loss: 0.7065\n",
      "Training: batch 50 begins at 22:00:20.216032\n",
      "\n",
      "Training: batch 50 ends at 22:00:21.013637\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.7051 - bce_dice_loss: 0.7051\n",
      "Training: batch 51 begins at 22:00:21.017153\n",
      "\n",
      "Training: batch 51 ends at 22:00:21.809587\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.7050 - bce_dice_loss: 0.7050\n",
      "Training: batch 52 begins at 22:00:21.812984\n",
      "\n",
      "Training: batch 52 ends at 22:00:22.598496\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.7034 - bce_dice_loss: 0.7034\n",
      "Training: batch 53 begins at 22:00:22.602891\n",
      "\n",
      "Training: batch 53 ends at 22:00:23.399427\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.7020 - bce_dice_loss: 0.7020\n",
      "Training: batch 54 begins at 22:00:23.404170\n",
      "\n",
      "Training: batch 54 ends at 22:00:24.201576\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.7016 - bce_dice_loss: 0.7016\n",
      "Training: batch 55 begins at 22:00:24.206015\n",
      "\n",
      "Training: batch 55 ends at 22:00:25.013354\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.7039 - bce_dice_loss: 0.7039\n",
      "Training: batch 56 begins at 22:00:25.019076\n",
      "\n",
      "Training: batch 56 ends at 22:00:25.808360\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.7045 - bce_dice_loss: 0.7045\n",
      "Training: batch 57 begins at 22:00:25.811757\n",
      "\n",
      "Training: batch 57 ends at 22:00:26.605722\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.7017 - bce_dice_loss: 0.7017\n",
      "Training: batch 58 begins at 22:00:26.608866\n",
      "\n",
      "Training: batch 58 ends at 22:00:27.402049\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7040 - bce_dice_loss: 0.7040\n",
      "Training: batch 59 begins at 22:00:27.406277\n",
      "\n",
      "Training: batch 59 ends at 22:00:28.189585\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7019 - bce_dice_loss: 0.7019\n",
      "Evaluating: batch 0 begins at 22:00:28.220270\n",
      "\n",
      "Evaluating: batch 0 ends at 22:00:28.492898\n",
      "\n",
      "Evaluating: batch 1 begins at 22:00:28.494211\n",
      "\n",
      "Evaluating: batch 1 ends at 22:00:28.710132\n",
      "\n",
      "Evaluating: batch 2 begins at 22:00:28.711668\n",
      "\n",
      "Evaluating: batch 2 ends at 22:00:28.932315\n",
      "\n",
      "Evaluating: batch 3 begins at 22:00:28.934172\n",
      "\n",
      "Evaluating: batch 3 ends at 22:00:29.148269\n",
      "\n",
      "Evaluating: batch 4 begins at 22:00:29.149514\n",
      "\n",
      "Evaluating: batch 4 ends at 22:00:29.368967\n",
      "\n",
      "Evaluating: batch 5 begins at 22:00:29.370314\n",
      "\n",
      "Evaluating: batch 5 ends at 22:00:29.589486\n",
      "\n",
      "Evaluating: batch 6 begins at 22:00:29.590781\n",
      "\n",
      "Evaluating: batch 6 ends at 22:00:29.810512\n",
      "\n",
      "Evaluating: batch 7 begins at 22:00:29.812405\n",
      "\n",
      "Evaluating: batch 7 ends at 22:00:30.032173\n",
      "\n",
      "Evaluating: batch 8 begins at 22:00:30.033607\n",
      "\n",
      "Evaluating: batch 8 ends at 22:00:30.252563\n",
      "\n",
      "Evaluating: batch 9 begins at 22:00:30.254137\n",
      "\n",
      "Evaluating: batch 9 ends at 22:00:30.472883\n",
      "\n",
      "Evaluating: batch 10 begins at 22:00:30.474379\n",
      "\n",
      "Evaluating: batch 10 ends at 22:00:30.691979\n",
      "\n",
      "Evaluating: batch 11 begins at 22:00:30.693205\n",
      "\n",
      "Evaluating: batch 11 ends at 22:00:30.914096\n",
      "\n",
      "Evaluating: batch 12 begins at 22:00:30.915439\n",
      "\n",
      "Evaluating: batch 12 ends at 22:00:31.135049\n",
      "\n",
      "Evaluating: batch 13 begins at 22:00:31.136912\n",
      "\n",
      "Evaluating: batch 13 ends at 22:00:31.357323\n",
      "\n",
      "Evaluating: batch 14 begins at 22:00:31.358747\n",
      "\n",
      "Evaluating: batch 14 ends at 22:00:31.576988\n",
      "\n",
      "Evaluating: batch 15 begins at 22:00:31.578848\n",
      "\n",
      "Evaluating: batch 15 ends at 22:00:31.800491\n",
      "\n",
      "Evaluating: batch 16 begins at 22:00:31.802038\n",
      "\n",
      "Evaluating: batch 16 ends at 22:00:32.019838\n",
      "\n",
      "Evaluating: batch 17 begins at 22:00:32.021536\n",
      "\n",
      "Evaluating: batch 17 ends at 22:00:32.241187\n",
      "\n",
      "Evaluating: batch 18 begins at 22:00:32.243063\n",
      "\n",
      "Evaluating: batch 18 ends at 22:00:32.464495\n",
      "\n",
      "Evaluating: batch 19 begins at 22:00:32.466012\n",
      "\n",
      "Evaluating: batch 19 ends at 22:00:32.681562\n",
      "\n",
      "Evaluating: batch 20 begins at 22:00:32.682883\n",
      "\n",
      "Evaluating: batch 20 ends at 22:00:32.902179\n",
      "\n",
      "Evaluating: batch 21 begins at 22:00:32.903896\n",
      "\n",
      "Evaluating: batch 21 ends at 22:00:33.120019\n",
      "\n",
      "Evaluating: batch 22 begins at 22:00:33.121293\n",
      "\n",
      "Evaluating: batch 22 ends at 22:00:33.337483\n",
      "\n",
      "Evaluating: batch 23 begins at 22:00:33.338818\n",
      "\n",
      "Evaluating: batch 23 ends at 22:00:33.553632\n",
      "\n",
      "Evaluating: batch 24 begins at 22:00:33.554942\n",
      "\n",
      "Evaluating: batch 24 ends at 22:00:33.772903\n",
      "\n",
      "Evaluating: batch 25 begins at 22:00:33.774242\n",
      "\n",
      "Evaluating: batch 25 ends at 22:00:33.991184\n",
      "\n",
      "Evaluating: batch 26 begins at 22:00:33.992474\n",
      "\n",
      "Evaluating: batch 26 ends at 22:00:34.209674\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.66358 to 0.63915, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 57s 945ms/step - loss: 0.7019 - bce_dice_loss: 0.7019 - val_loss: 0.6391 - val_bce_dice_loss: 0.6391\n",
      "Epoch 8/25\n",
      "\n",
      "Training: batch 0 begins at 22:00:35.859054\n",
      "\n",
      "Training: batch 0 ends at 22:00:36.678861\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.7122 - bce_dice_loss: 0.7122\n",
      "Training: batch 1 begins at 22:00:36.682644\n",
      "\n",
      "Training: batch 1 ends at 22:00:37.513717\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.6269 - bce_dice_loss: 0.6269\n",
      "Training: batch 2 begins at 22:00:37.518337\n",
      "\n",
      "Training: batch 2 ends at 22:00:38.324971\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.5729 - bce_dice_loss: 0.5729\n",
      "Training: batch 3 begins at 22:00:38.328582\n",
      "\n",
      "Training: batch 3 ends at 22:00:39.112528\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.6206 - bce_dice_loss: 0.6206\n",
      "Training: batch 4 begins at 22:00:39.114991\n",
      "\n",
      "Training: batch 4 ends at 22:00:39.905995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/60 [=>............................] - ETA: 44s - loss: 0.5953 - bce_dice_loss: 0.5953\n",
      "Training: batch 5 begins at 22:00:39.909547\n",
      "\n",
      "Training: batch 5 ends at 22:00:40.705945\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6406 - bce_dice_loss: 0.6406\n",
      "Training: batch 6 begins at 22:00:40.709526\n",
      "\n",
      "Training: batch 6 ends at 22:00:41.513501\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6693 - bce_dice_loss: 0.6693\n",
      "Training: batch 7 begins at 22:00:41.517849\n",
      "\n",
      "Training: batch 7 ends at 22:00:42.325217\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.6982 - bce_dice_loss: 0.6982\n",
      "Training: batch 8 begins at 22:00:42.330172\n",
      "\n",
      "Training: batch 8 ends at 22:00:43.124838\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.7102 - bce_dice_loss: 0.7102\n",
      "Training: batch 9 begins at 22:00:43.128490\n",
      "\n",
      "Training: batch 9 ends at 22:00:43.950733\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.6963 - bce_dice_loss: 0.6963\n",
      "Training: batch 10 begins at 22:00:43.954573\n",
      "\n",
      "Training: batch 10 ends at 22:00:44.748852\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.6931 - bce_dice_loss: 0.6931\n",
      "Training: batch 11 begins at 22:00:44.753589\n",
      "\n",
      "Training: batch 11 ends at 22:00:45.536857\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.7223 - bce_dice_loss: 0.7223\n",
      "Training: batch 12 begins at 22:00:45.540209\n",
      "\n",
      "Training: batch 12 ends at 22:00:46.332844\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.7067 - bce_dice_loss: 0.7067\n",
      "Training: batch 13 begins at 22:00:46.337220\n",
      "\n",
      "Training: batch 13 ends at 22:00:47.130869\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.7017 - bce_dice_loss: 0.7017\n",
      "Training: batch 14 begins at 22:00:47.133311\n",
      "\n",
      "Training: batch 14 ends at 22:00:47.951262\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.6865 - bce_dice_loss: 0.6865\n",
      "Training: batch 15 begins at 22:00:47.955593\n",
      "\n",
      "Training: batch 15 ends at 22:00:48.752755\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6894 - bce_dice_loss: 0.6894\n",
      "Training: batch 16 begins at 22:00:48.755851\n",
      "\n",
      "Training: batch 16 ends at 22:00:49.546302\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6914 - bce_dice_loss: 0.6914\n",
      "Training: batch 17 begins at 22:00:49.550733\n",
      "\n",
      "Training: batch 17 ends at 22:00:50.370109\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.6871 - bce_dice_loss: 0.6871\n",
      "Training: batch 18 begins at 22:00:50.373703\n",
      "\n",
      "Training: batch 18 ends at 22:00:51.176672\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.6892 - bce_dice_loss: 0.6892\n",
      "Training: batch 19 begins at 22:00:51.180772\n",
      "\n",
      "Training: batch 19 ends at 22:00:51.992667\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.6866 - bce_dice_loss: 0.6866\n",
      "Training: batch 20 begins at 22:00:51.996667\n",
      "\n",
      "Training: batch 20 ends at 22:00:52.790244\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6984 - bce_dice_loss: 0.6984\n",
      "Training: batch 21 begins at 22:00:52.793920\n",
      "\n",
      "Training: batch 21 ends at 22:00:53.585117\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6928 - bce_dice_loss: 0.6928\n",
      "Training: batch 22 begins at 22:00:53.588694\n",
      "\n",
      "Training: batch 22 ends at 22:00:54.383614\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.6866 - bce_dice_loss: 0.6866\n",
      "Training: batch 23 begins at 22:00:54.387983\n",
      "\n",
      "Training: batch 23 ends at 22:00:55.183123\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.6819 - bce_dice_loss: 0.6819\n",
      "Training: batch 24 begins at 22:00:55.187544\n",
      "\n",
      "Training: batch 24 ends at 22:00:55.982483\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.6821 - bce_dice_loss: 0.6821\n",
      "Training: batch 25 begins at 22:00:55.986405\n",
      "\n",
      "Training: batch 25 ends at 22:00:56.779496\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.6807 - bce_dice_loss: 0.6807\n",
      "Training: batch 26 begins at 22:00:56.783877\n",
      "\n",
      "Training: batch 26 ends at 22:00:57.575441\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6882 - bce_dice_loss: 0.6882\n",
      "Training: batch 27 begins at 22:00:57.579366\n",
      "\n",
      "Training: batch 27 ends at 22:00:58.395880\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6898 - bce_dice_loss: 0.6898\n",
      "Training: batch 28 begins at 22:00:58.403336\n",
      "\n",
      "Training: batch 28 ends at 22:00:59.201042\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.6897 - bce_dice_loss: 0.6897\n",
      "Training: batch 29 begins at 22:00:59.204026\n",
      "\n",
      "Training: batch 29 ends at 22:00:59.992417\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.6857 - bce_dice_loss: 0.6857\n",
      "Training: batch 30 begins at 22:00:59.995877\n",
      "\n",
      "Training: batch 30 ends at 22:01:00.789037\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6805 - bce_dice_loss: 0.6805\n",
      "Training: batch 31 begins at 22:01:00.792607\n",
      "\n",
      "Training: batch 31 ends at 22:01:01.618088\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6709 - bce_dice_loss: 0.6709\n",
      "Training: batch 32 begins at 22:01:01.622429\n",
      "\n",
      "Training: batch 32 ends at 22:01:02.420300\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6692 - bce_dice_loss: 0.6692\n",
      "Training: batch 33 begins at 22:01:02.423101\n",
      "\n",
      "Training: batch 33 ends at 22:01:03.248394\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6691 - bce_dice_loss: 0.6691\n",
      "Training: batch 34 begins at 22:01:03.252855\n",
      "\n",
      "Training: batch 34 ends at 22:01:04.035230\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.6716 - bce_dice_loss: 0.6716\n",
      "Training: batch 35 begins at 22:01:04.039786\n",
      "\n",
      "Training: batch 35 ends at 22:01:04.825459\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6698 - bce_dice_loss: 0.6698\n",
      "Training: batch 36 begins at 22:01:04.829131\n",
      "\n",
      "Training: batch 36 ends at 22:01:05.625496\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6630 - bce_dice_loss: 0.6630\n",
      "Training: batch 37 begins at 22:01:05.629673\n",
      "\n",
      "Training: batch 37 ends at 22:01:06.428784\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6622 - bce_dice_loss: 0.6622\n",
      "Training: batch 38 begins at 22:01:06.433015\n",
      "\n",
      "Training: batch 38 ends at 22:01:07.242473\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.6625 - bce_dice_loss: 0.6625\n",
      "Training: batch 39 begins at 22:01:07.246760\n",
      "\n",
      "Training: batch 39 ends at 22:01:08.047908\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6663 - bce_dice_loss: 0.6663\n",
      "Training: batch 40 begins at 22:01:08.052551\n",
      "\n",
      "Training: batch 40 ends at 22:01:08.841487\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6648 - bce_dice_loss: 0.6648\n",
      "Training: batch 41 begins at 22:01:08.845739\n",
      "\n",
      "Training: batch 41 ends at 22:01:09.638187\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6704 - bce_dice_loss: 0.6704\n",
      "Training: batch 42 begins at 22:01:09.642285\n",
      "\n",
      "Training: batch 42 ends at 22:01:10.433857\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6689 - bce_dice_loss: 0.6689\n",
      "Training: batch 43 begins at 22:01:10.437574\n",
      "\n",
      "Training: batch 43 ends at 22:01:11.231219\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.6688 - bce_dice_loss: 0.6688\n",
      "Training: batch 44 begins at 22:01:11.233697\n",
      "\n",
      "Training: batch 44 ends at 22:01:12.038896\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.6642 - bce_dice_loss: 0.6642\n",
      "Training: batch 45 begins at 22:01:12.041557\n",
      "\n",
      "Training: batch 45 ends at 22:01:12.833163\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6672 - bce_dice_loss: 0.6672\n",
      "Training: batch 46 begins at 22:01:12.836654\n",
      "\n",
      "Training: batch 46 ends at 22:01:13.627930\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6648 - bce_dice_loss: 0.6648\n",
      "Training: batch 47 begins at 22:01:13.631931\n",
      "\n",
      "Training: batch 47 ends at 22:01:14.438931\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6632 - bce_dice_loss: 0.6632 \n",
      "Training: batch 48 begins at 22:01:14.442202\n",
      "\n",
      "Training: batch 48 ends at 22:01:15.237606\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.6616 - bce_dice_loss: 0.6616\n",
      "Training: batch 49 begins at 22:01:15.241524\n",
      "\n",
      "Training: batch 49 ends at 22:01:16.036253\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.6618 - bce_dice_loss: 0.6618\n",
      "Training: batch 50 begins at 22:01:16.039248\n",
      "\n",
      "Training: batch 50 ends at 22:01:16.835154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6613 - bce_dice_loss: 0.6613\n",
      "Training: batch 51 begins at 22:01:16.838837\n",
      "\n",
      "Training: batch 51 ends at 22:01:17.635236\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6620 - bce_dice_loss: 0.6620\n",
      "Training: batch 52 begins at 22:01:17.638945\n",
      "\n",
      "Training: batch 52 ends at 22:01:18.436892\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6608 - bce_dice_loss: 0.6608\n",
      "Training: batch 53 begins at 22:01:18.440476\n",
      "\n",
      "Training: batch 53 ends at 22:01:19.249942\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6616 - bce_dice_loss: 0.6616\n",
      "Training: batch 54 begins at 22:01:19.254769\n",
      "\n",
      "Training: batch 54 ends at 22:01:20.048638\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.6590 - bce_dice_loss: 0.6590\n",
      "Training: batch 55 begins at 22:01:20.052888\n",
      "\n",
      "Training: batch 55 ends at 22:01:20.841295\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6579 - bce_dice_loss: 0.6579\n",
      "Training: batch 56 begins at 22:01:20.844255\n",
      "\n",
      "Training: batch 56 ends at 22:01:21.634156\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6593 - bce_dice_loss: 0.6593\n",
      "Training: batch 57 begins at 22:01:21.638001\n",
      "\n",
      "Training: batch 57 ends at 22:01:22.434384\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6591 - bce_dice_loss: 0.6591\n",
      "Training: batch 58 begins at 22:01:22.438651\n",
      "\n",
      "Training: batch 58 ends at 22:01:23.231505\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6595 - bce_dice_loss: 0.6595\n",
      "Training: batch 59 begins at 22:01:23.236134\n",
      "\n",
      "Training: batch 59 ends at 22:01:24.058306\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6558 - bce_dice_loss: 0.6558\n",
      "Evaluating: batch 0 begins at 22:01:24.086956\n",
      "\n",
      "Evaluating: batch 0 ends at 22:01:24.355013\n",
      "\n",
      "Evaluating: batch 1 begins at 22:01:24.356174\n",
      "\n",
      "Evaluating: batch 1 ends at 22:01:24.576435\n",
      "\n",
      "Evaluating: batch 2 begins at 22:01:24.577701\n",
      "\n",
      "Evaluating: batch 2 ends at 22:01:24.796108\n",
      "\n",
      "Evaluating: batch 3 begins at 22:01:24.797503\n",
      "\n",
      "Evaluating: batch 3 ends at 22:01:25.015129\n",
      "\n",
      "Evaluating: batch 4 begins at 22:01:25.016485\n",
      "\n",
      "Evaluating: batch 4 ends at 22:01:25.234551\n",
      "\n",
      "Evaluating: batch 5 begins at 22:01:25.236024\n",
      "\n",
      "Evaluating: batch 5 ends at 22:01:25.458096\n",
      "\n",
      "Evaluating: batch 6 begins at 22:01:25.459478\n",
      "\n",
      "Evaluating: batch 6 ends at 22:01:25.684304\n",
      "\n",
      "Evaluating: batch 7 begins at 22:01:25.686833\n",
      "\n",
      "Evaluating: batch 7 ends at 22:01:25.908682\n",
      "\n",
      "Evaluating: batch 8 begins at 22:01:25.909951\n",
      "\n",
      "Evaluating: batch 8 ends at 22:01:26.132675\n",
      "\n",
      "Evaluating: batch 9 begins at 22:01:26.133915\n",
      "\n",
      "Evaluating: batch 9 ends at 22:01:26.351874\n",
      "\n",
      "Evaluating: batch 10 begins at 22:01:26.353441\n",
      "\n",
      "Evaluating: batch 10 ends at 22:01:26.571329\n",
      "\n",
      "Evaluating: batch 11 begins at 22:01:26.572858\n",
      "\n",
      "Evaluating: batch 11 ends at 22:01:26.794915\n",
      "\n",
      "Evaluating: batch 12 begins at 22:01:26.796168\n",
      "\n",
      "Evaluating: batch 12 ends at 22:01:27.014740\n",
      "\n",
      "Evaluating: batch 13 begins at 22:01:27.016198\n",
      "\n",
      "Evaluating: batch 13 ends at 22:01:27.243008\n",
      "\n",
      "Evaluating: batch 14 begins at 22:01:27.244344\n",
      "\n",
      "Evaluating: batch 14 ends at 22:01:27.467919\n",
      "\n",
      "Evaluating: batch 15 begins at 22:01:27.469989\n",
      "\n",
      "Evaluating: batch 15 ends at 22:01:27.696338\n",
      "\n",
      "Evaluating: batch 16 begins at 22:01:27.697751\n",
      "\n",
      "Evaluating: batch 16 ends at 22:01:27.918399\n",
      "\n",
      "Evaluating: batch 17 begins at 22:01:27.919741\n",
      "\n",
      "Evaluating: batch 17 ends at 22:01:28.140075\n",
      "\n",
      "Evaluating: batch 18 begins at 22:01:28.141483\n",
      "\n",
      "Evaluating: batch 18 ends at 22:01:28.365335\n",
      "\n",
      "Evaluating: batch 19 begins at 22:01:28.366687\n",
      "\n",
      "Evaluating: batch 19 ends at 22:01:28.584117\n",
      "\n",
      "Evaluating: batch 20 begins at 22:01:28.585375\n",
      "\n",
      "Evaluating: batch 20 ends at 22:01:28.802200\n",
      "\n",
      "Evaluating: batch 21 begins at 22:01:28.803563\n",
      "\n",
      "Evaluating: batch 21 ends at 22:01:29.021097\n",
      "\n",
      "Evaluating: batch 22 begins at 22:01:29.022472\n",
      "\n",
      "Evaluating: batch 22 ends at 22:01:29.245463\n",
      "\n",
      "Evaluating: batch 23 begins at 22:01:29.246957\n",
      "\n",
      "Evaluating: batch 23 ends at 22:01:29.465070\n",
      "\n",
      "Evaluating: batch 24 begins at 22:01:29.466657\n",
      "\n",
      "Evaluating: batch 24 ends at 22:01:29.683875\n",
      "\n",
      "Evaluating: batch 25 begins at 22:01:29.685371\n",
      "\n",
      "Evaluating: batch 25 ends at 22:01:29.900390\n",
      "\n",
      "Evaluating: batch 26 begins at 22:01:29.901825\n",
      "\n",
      "Evaluating: batch 26 ends at 22:01:30.118280\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.63915 to 0.60325, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 934ms/step - loss: 0.6558 - bce_dice_loss: 0.6558 - val_loss: 0.6032 - val_bce_dice_loss: 0.6032\n",
      "Epoch 9/25\n",
      "\n",
      "Training: batch 0 begins at 22:01:31.779173\n",
      "\n",
      "Training: batch 0 ends at 22:01:32.602482\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.7031 - bce_dice_loss: 0.7031\n",
      "Training: batch 1 begins at 22:01:32.605287\n",
      "\n",
      "Training: batch 1 ends at 22:01:33.415830\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.6394 - bce_dice_loss: 0.6394\n",
      "Training: batch 2 begins at 22:01:33.419961\n",
      "\n",
      "Training: batch 2 ends at 22:01:34.238575\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.6673 - bce_dice_loss: 0.6673\n",
      "Training: batch 3 begins at 22:01:34.242918\n",
      "\n",
      "Training: batch 3 ends at 22:01:35.035624\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.6026 - bce_dice_loss: 0.6026\n",
      "Training: batch 4 begins at 22:01:35.039147\n",
      "\n",
      "Training: batch 4 ends at 22:01:35.835432\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.6160 - bce_dice_loss: 0.6160\n",
      "Training: batch 5 begins at 22:01:35.838197\n",
      "\n",
      "Training: batch 5 ends at 22:01:36.637078\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6265 - bce_dice_loss: 0.6265\n",
      "Training: batch 6 begins at 22:01:36.640572\n",
      "\n",
      "Training: batch 6 ends at 22:01:37.435159\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6121 - bce_dice_loss: 0.6121\n",
      "Training: batch 7 begins at 22:01:37.439321\n",
      "\n",
      "Training: batch 7 ends at 22:01:38.247514\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.6266 - bce_dice_loss: 0.6266\n",
      "Training: batch 8 begins at 22:01:38.252168\n",
      "\n",
      "Training: batch 8 ends at 22:01:39.071749\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.6288 - bce_dice_loss: 0.6288\n",
      "Training: batch 9 begins at 22:01:39.075198\n",
      "\n",
      "Training: batch 9 ends at 22:01:39.878573\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.6219 - bce_dice_loss: 0.6219\n",
      "Training: batch 10 begins at 22:01:39.881679\n",
      "\n",
      "Training: batch 10 ends at 22:01:40.675298\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.6368 - bce_dice_loss: 0.6368\n",
      "Training: batch 11 begins at 22:01:40.678841\n",
      "\n",
      "Training: batch 11 ends at 22:01:41.470063\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.6287 - bce_dice_loss: 0.6287\n",
      "Training: batch 12 begins at 22:01:41.472905\n",
      "\n",
      "Training: batch 12 ends at 22:01:42.256956\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.6230 - bce_dice_loss: 0.6230\n",
      "Training: batch 13 begins at 22:01:42.261358\n",
      "\n",
      "Training: batch 13 ends at 22:01:43.052617\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.6205 - bce_dice_loss: 0.6205\n",
      "Training: batch 14 begins at 22:01:43.056745\n",
      "\n",
      "Training: batch 14 ends at 22:01:43.847610\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.6112 - bce_dice_loss: 0.6112\n",
      "Training: batch 15 begins at 22:01:43.852786\n",
      "\n",
      "Training: batch 15 ends at 22:01:44.643513\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6079 - bce_dice_loss: 0.6079\n",
      "Training: batch 16 begins at 22:01:44.647980\n",
      "\n",
      "Training: batch 16 ends at 22:01:45.469752\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6220 - bce_dice_loss: 0.6220\n",
      "Training: batch 17 begins at 22:01:45.473099\n",
      "\n",
      "Training: batch 17 ends at 22:01:46.266368\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.6286 - bce_dice_loss: 0.6286\n",
      "Training: batch 18 begins at 22:01:46.270923\n",
      "\n",
      "Training: batch 18 ends at 22:01:47.079509\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.6307 - bce_dice_loss: 0.6307\n",
      "Training: batch 19 begins at 22:01:47.084170\n",
      "\n",
      "Training: batch 19 ends at 22:01:47.921522\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.6375 - bce_dice_loss: 0.6375\n",
      "Training: batch 20 begins at 22:01:47.924010\n",
      "\n",
      "Training: batch 20 ends at 22:01:48.755848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6392 - bce_dice_loss: 0.6392\n",
      "Training: batch 21 begins at 22:01:48.759020\n",
      "\n",
      "Training: batch 21 ends at 22:01:49.552608\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6407 - bce_dice_loss: 0.6407\n",
      "Training: batch 22 begins at 22:01:49.555253\n",
      "\n",
      "Training: batch 22 ends at 22:01:50.355544\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.6379 - bce_dice_loss: 0.6379\n",
      "Training: batch 23 begins at 22:01:50.359695\n",
      "\n",
      "Training: batch 23 ends at 22:01:51.155445\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.6382 - bce_dice_loss: 0.6382\n",
      "Training: batch 24 begins at 22:01:51.159657\n",
      "\n",
      "Training: batch 24 ends at 22:01:51.962342\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.6354 - bce_dice_loss: 0.6354\n",
      "Training: batch 25 begins at 22:01:51.966759\n",
      "\n",
      "Training: batch 25 ends at 22:01:52.760715\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.6462 - bce_dice_loss: 0.6462\n",
      "Training: batch 26 begins at 22:01:52.766487\n",
      "\n",
      "Training: batch 26 ends at 22:01:53.557875\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6406 - bce_dice_loss: 0.6406\n",
      "Training: batch 27 begins at 22:01:53.562155\n",
      "\n",
      "Training: batch 27 ends at 22:01:54.384619\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6381 - bce_dice_loss: 0.6381\n",
      "Training: batch 28 begins at 22:01:54.389903\n",
      "\n",
      "Training: batch 28 ends at 22:01:55.197154\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.6431 - bce_dice_loss: 0.6431\n",
      "Training: batch 29 begins at 22:01:55.201804\n",
      "\n",
      "Training: batch 29 ends at 22:01:55.997965\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.6353 - bce_dice_loss: 0.6353\n",
      "Training: batch 30 begins at 22:01:56.000516\n",
      "\n",
      "Training: batch 30 ends at 22:01:56.803272\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6386 - bce_dice_loss: 0.6386\n",
      "Training: batch 31 begins at 22:01:56.807506\n",
      "\n",
      "Training: batch 31 ends at 22:01:57.605712\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6347 - bce_dice_loss: 0.6347\n",
      "Training: batch 32 begins at 22:01:57.609505\n",
      "\n",
      "Training: batch 32 ends at 22:01:58.413829\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6379 - bce_dice_loss: 0.6379\n",
      "Training: batch 33 begins at 22:01:58.417957\n",
      "\n",
      "Training: batch 33 ends at 22:01:59.227833\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6412 - bce_dice_loss: 0.6412\n",
      "Training: batch 34 begins at 22:01:59.232081\n",
      "\n",
      "Training: batch 34 ends at 22:02:00.050836\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.6429 - bce_dice_loss: 0.6429\n",
      "Training: batch 35 begins at 22:02:00.055335\n",
      "\n",
      "Training: batch 35 ends at 22:02:00.856585\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6429 - bce_dice_loss: 0.6429\n",
      "Training: batch 36 begins at 22:02:00.859049\n",
      "\n",
      "Training: batch 36 ends at 22:02:01.673956\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6418 - bce_dice_loss: 0.6418\n",
      "Training: batch 37 begins at 22:02:01.677435\n",
      "\n",
      "Training: batch 37 ends at 22:02:02.465996\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6416 - bce_dice_loss: 0.6416\n",
      "Training: batch 38 begins at 22:02:02.471131\n",
      "\n",
      "Training: batch 38 ends at 22:02:03.253672\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.6421 - bce_dice_loss: 0.6421\n",
      "Training: batch 39 begins at 22:02:03.257268\n",
      "\n",
      "Training: batch 39 ends at 22:02:04.075544\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6404 - bce_dice_loss: 0.6404\n",
      "Training: batch 40 begins at 22:02:04.079094\n",
      "\n",
      "Training: batch 40 ends at 22:02:04.897831\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6414 - bce_dice_loss: 0.6414\n",
      "Training: batch 41 begins at 22:02:04.902510\n",
      "\n",
      "Training: batch 41 ends at 22:02:05.705688\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6478 - bce_dice_loss: 0.6478\n",
      "Training: batch 42 begins at 22:02:05.709409\n",
      "\n",
      "Training: batch 42 ends at 22:02:06.504697\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6471 - bce_dice_loss: 0.6471\n",
      "Training: batch 43 begins at 22:02:06.507284\n",
      "\n",
      "Training: batch 43 ends at 22:02:07.300314\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.6455 - bce_dice_loss: 0.6455\n",
      "Training: batch 44 begins at 22:02:07.305606\n",
      "\n",
      "Training: batch 44 ends at 22:02:08.123419\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.6513 - bce_dice_loss: 0.6513\n",
      "Training: batch 45 begins at 22:02:08.127729\n",
      "\n",
      "Training: batch 45 ends at 22:02:08.926828\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6510 - bce_dice_loss: 0.6510\n",
      "Training: batch 46 begins at 22:02:08.930627\n",
      "\n",
      "Training: batch 46 ends at 22:02:09.722283\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6550 - bce_dice_loss: 0.6550\n",
      "Training: batch 47 begins at 22:02:09.725563\n",
      "\n",
      "Training: batch 47 ends at 22:02:10.539496\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6546 - bce_dice_loss: 0.6546 \n",
      "Training: batch 48 begins at 22:02:10.543663\n",
      "\n",
      "Training: batch 48 ends at 22:02:11.337763\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.6545 - bce_dice_loss: 0.6545\n",
      "Training: batch 49 begins at 22:02:11.340598\n",
      "\n",
      "Training: batch 49 ends at 22:02:12.152391\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.6584 - bce_dice_loss: 0.6584\n",
      "Training: batch 50 begins at 22:02:12.156719\n",
      "\n",
      "Training: batch 50 ends at 22:02:12.952035\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6564 - bce_dice_loss: 0.6564\n",
      "Training: batch 51 begins at 22:02:12.956858\n",
      "\n",
      "Training: batch 51 ends at 22:02:13.755541\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6590 - bce_dice_loss: 0.6590\n",
      "Training: batch 52 begins at 22:02:13.760182\n",
      "\n",
      "Training: batch 52 ends at 22:02:14.555682\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6599 - bce_dice_loss: 0.6599\n",
      "Training: batch 53 begins at 22:02:14.559090\n",
      "\n",
      "Training: batch 53 ends at 22:02:15.350869\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6587 - bce_dice_loss: 0.6587\n",
      "Training: batch 54 begins at 22:02:15.355226\n",
      "\n",
      "Training: batch 54 ends at 22:02:16.160560\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.6609 - bce_dice_loss: 0.6609\n",
      "Training: batch 55 begins at 22:02:16.165093\n",
      "\n",
      "Training: batch 55 ends at 22:02:16.956485\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6618 - bce_dice_loss: 0.6618\n",
      "Training: batch 56 begins at 22:02:16.960469\n",
      "\n",
      "Training: batch 56 ends at 22:02:17.748105\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6630 - bce_dice_loss: 0.6630\n",
      "Training: batch 57 begins at 22:02:17.750912\n",
      "\n",
      "Training: batch 57 ends at 22:02:18.557248\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6615 - bce_dice_loss: 0.6615\n",
      "Training: batch 58 begins at 22:02:18.561547\n",
      "\n",
      "Training: batch 58 ends at 22:02:19.353989\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6620 - bce_dice_loss: 0.6620\n",
      "Training: batch 59 begins at 22:02:19.358254\n",
      "\n",
      "Training: batch 59 ends at 22:02:20.180307\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6633 - bce_dice_loss: 0.6633\n",
      "Evaluating: batch 0 begins at 22:02:20.214261\n",
      "\n",
      "Evaluating: batch 0 ends at 22:02:20.484405\n",
      "\n",
      "Evaluating: batch 1 begins at 22:02:20.485601\n",
      "\n",
      "Evaluating: batch 1 ends at 22:02:20.698469\n",
      "\n",
      "Evaluating: batch 2 begins at 22:02:20.699873\n",
      "\n",
      "Evaluating: batch 2 ends at 22:02:20.923792\n",
      "\n",
      "Evaluating: batch 3 begins at 22:02:20.925377\n",
      "\n",
      "Evaluating: batch 3 ends at 22:02:21.147738\n",
      "\n",
      "Evaluating: batch 4 begins at 22:02:21.149003\n",
      "\n",
      "Evaluating: batch 4 ends at 22:02:21.367507\n",
      "\n",
      "Evaluating: batch 5 begins at 22:02:21.369668\n",
      "\n",
      "Evaluating: batch 5 ends at 22:02:21.588153\n",
      "\n",
      "Evaluating: batch 6 begins at 22:02:21.589699\n",
      "\n",
      "Evaluating: batch 6 ends at 22:02:21.806730\n",
      "\n",
      "Evaluating: batch 7 begins at 22:02:21.808021\n",
      "\n",
      "Evaluating: batch 7 ends at 22:02:22.028226\n",
      "\n",
      "Evaluating: batch 8 begins at 22:02:22.029855\n",
      "\n",
      "Evaluating: batch 8 ends at 22:02:22.249305\n",
      "\n",
      "Evaluating: batch 9 begins at 22:02:22.251685\n",
      "\n",
      "Evaluating: batch 9 ends at 22:02:22.470253\n",
      "\n",
      "Evaluating: batch 10 begins at 22:02:22.472610\n",
      "\n",
      "Evaluating: batch 10 ends at 22:02:22.691274\n",
      "\n",
      "Evaluating: batch 11 begins at 22:02:22.693990\n",
      "\n",
      "Evaluating: batch 11 ends at 22:02:22.920098\n",
      "\n",
      "Evaluating: batch 12 begins at 22:02:22.922880\n",
      "\n",
      "Evaluating: batch 12 ends at 22:02:23.142583\n",
      "\n",
      "Evaluating: batch 13 begins at 22:02:23.144436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 13 ends at 22:02:23.358732\n",
      "\n",
      "Evaluating: batch 14 begins at 22:02:23.359942\n",
      "\n",
      "Evaluating: batch 14 ends at 22:02:23.580341\n",
      "\n",
      "Evaluating: batch 15 begins at 22:02:23.581870\n",
      "\n",
      "Evaluating: batch 15 ends at 22:02:23.800806\n",
      "\n",
      "Evaluating: batch 16 begins at 22:02:23.802416\n",
      "\n",
      "Evaluating: batch 16 ends at 22:02:24.022113\n",
      "\n",
      "Evaluating: batch 17 begins at 22:02:24.023718\n",
      "\n",
      "Evaluating: batch 17 ends at 22:02:24.242892\n",
      "\n",
      "Evaluating: batch 18 begins at 22:02:24.244092\n",
      "\n",
      "Evaluating: batch 18 ends at 22:02:24.467712\n",
      "\n",
      "Evaluating: batch 19 begins at 22:02:24.470681\n",
      "\n",
      "Evaluating: batch 19 ends at 22:02:24.688893\n",
      "\n",
      "Evaluating: batch 20 begins at 22:02:24.690471\n",
      "\n",
      "Evaluating: batch 20 ends at 22:02:24.909002\n",
      "\n",
      "Evaluating: batch 21 begins at 22:02:24.910286\n",
      "\n",
      "Evaluating: batch 21 ends at 22:02:25.131532\n",
      "\n",
      "Evaluating: batch 22 begins at 22:02:25.132908\n",
      "\n",
      "Evaluating: batch 22 ends at 22:02:25.349185\n",
      "\n",
      "Evaluating: batch 23 begins at 22:02:25.350428\n",
      "\n",
      "Evaluating: batch 23 ends at 22:02:25.565101\n",
      "\n",
      "Evaluating: batch 24 begins at 22:02:25.566402\n",
      "\n",
      "Evaluating: batch 24 ends at 22:02:25.782595\n",
      "\n",
      "Evaluating: batch 25 begins at 22:02:25.783751\n",
      "\n",
      "Evaluating: batch 25 ends at 22:02:26.000504\n",
      "\n",
      "Evaluating: batch 26 begins at 22:02:26.001872\n",
      "\n",
      "Evaluating: batch 26 ends at 22:02:26.219057\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.60325\n",
      "60/60 [==============================] - 54s 909ms/step - loss: 0.6633 - bce_dice_loss: 0.6633 - val_loss: 0.6563 - val_bce_dice_loss: 0.6563\n",
      "Epoch 10/25\n",
      "\n",
      "Training: batch 0 begins at 22:02:26.249567\n",
      "\n",
      "Training: batch 0 ends at 22:02:27.065576\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.5791 - bce_dice_loss: 0.5791\n",
      "Training: batch 1 begins at 22:02:27.068066\n",
      "\n",
      "Training: batch 1 ends at 22:02:27.869704\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.6748 - bce_dice_loss: 0.6748\n",
      "Training: batch 2 begins at 22:02:27.872679\n",
      "\n",
      "Training: batch 2 ends at 22:02:28.672937\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.6638 - bce_dice_loss: 0.6638\n",
      "Training: batch 3 begins at 22:02:28.678229\n",
      "\n",
      "Training: batch 3 ends at 22:02:29.472907\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.6869 - bce_dice_loss: 0.6869\n",
      "Training: batch 4 begins at 22:02:29.475857\n",
      "\n",
      "Training: batch 4 ends at 22:02:30.271337\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.6917 - bce_dice_loss: 0.6917\n",
      "Training: batch 5 begins at 22:02:30.274359\n",
      "\n",
      "Training: batch 5 ends at 22:02:31.064784\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6647 - bce_dice_loss: 0.6647\n",
      "Training: batch 6 begins at 22:02:31.069758\n",
      "\n",
      "Training: batch 6 ends at 22:02:31.864867\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6823 - bce_dice_loss: 0.6823\n",
      "Training: batch 7 begins at 22:02:31.869484\n",
      "\n",
      "Training: batch 7 ends at 22:02:32.660515\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.6771 - bce_dice_loss: 0.6771\n",
      "Training: batch 8 begins at 22:02:32.664583\n",
      "\n",
      "Training: batch 8 ends at 22:02:33.456068\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.7007 - bce_dice_loss: 0.7007\n",
      "Training: batch 9 begins at 22:02:33.459881\n",
      "\n",
      "Training: batch 9 ends at 22:02:34.253139\n",
      "10/60 [====>.........................] - ETA: 39s - loss: 0.7061 - bce_dice_loss: 0.7061\n",
      "Training: batch 10 begins at 22:02:34.255586\n",
      "\n",
      "Training: batch 10 ends at 22:02:35.046114\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.7023 - bce_dice_loss: 0.7023\n",
      "Training: batch 11 begins at 22:02:35.050340\n",
      "\n",
      "Training: batch 11 ends at 22:02:35.844489\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.7040 - bce_dice_loss: 0.7040\n",
      "Training: batch 12 begins at 22:02:35.848953\n",
      "\n",
      "Training: batch 12 ends at 22:02:36.637788\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.7018 - bce_dice_loss: 0.7018\n",
      "Training: batch 13 begins at 22:02:36.641991\n",
      "\n",
      "Training: batch 13 ends at 22:02:37.439259\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.6973 - bce_dice_loss: 0.6973\n",
      "Training: batch 14 begins at 22:02:37.443565\n",
      "\n",
      "Training: batch 14 ends at 22:02:38.234809\n",
      "15/60 [======>.......................] - ETA: 35s - loss: 0.6914 - bce_dice_loss: 0.6914\n",
      "Training: batch 15 begins at 22:02:38.239571\n",
      "\n",
      "Training: batch 15 ends at 22:02:39.031047\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6874 - bce_dice_loss: 0.6874\n",
      "Training: batch 16 begins at 22:02:39.036278\n",
      "\n",
      "Training: batch 16 ends at 22:02:39.834031\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6836 - bce_dice_loss: 0.6836\n",
      "Training: batch 17 begins at 22:02:39.838543\n",
      "\n",
      "Training: batch 17 ends at 22:02:40.632205\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.6742 - bce_dice_loss: 0.6742\n",
      "Training: batch 18 begins at 22:02:40.636204\n",
      "\n",
      "Training: batch 18 ends at 22:02:41.425918\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.6687 - bce_dice_loss: 0.6687\n",
      "Training: batch 19 begins at 22:02:41.430122\n",
      "\n",
      "Training: batch 19 ends at 22:02:42.222601\n",
      "20/60 [=========>....................] - ETA: 31s - loss: 0.6615 - bce_dice_loss: 0.6615\n",
      "Training: batch 20 begins at 22:02:42.227311\n",
      "\n",
      "Training: batch 20 ends at 22:02:43.049883\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6528 - bce_dice_loss: 0.6528\n",
      "Training: batch 21 begins at 22:02:43.055437\n",
      "\n",
      "Training: batch 21 ends at 22:02:43.853481\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6489 - bce_dice_loss: 0.6489\n",
      "Training: batch 22 begins at 22:02:43.858049\n",
      "\n",
      "Training: batch 22 ends at 22:02:44.651330\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.6374 - bce_dice_loss: 0.6374\n",
      "Training: batch 23 begins at 22:02:44.655508\n",
      "\n",
      "Training: batch 23 ends at 22:02:45.451953\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.6370 - bce_dice_loss: 0.6370\n",
      "Training: batch 24 begins at 22:02:45.456149\n",
      "\n",
      "Training: batch 24 ends at 22:02:46.254191\n",
      "25/60 [===========>..................] - ETA: 27s - loss: 0.6311 - bce_dice_loss: 0.6311\n",
      "Training: batch 25 begins at 22:02:46.257626\n",
      "\n",
      "Training: batch 25 ends at 22:02:47.079403\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.6334 - bce_dice_loss: 0.6334\n",
      "Training: batch 26 begins at 22:02:47.084025\n",
      "\n",
      "Training: batch 26 ends at 22:02:47.884021\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6380 - bce_dice_loss: 0.6380\n",
      "Training: batch 27 begins at 22:02:47.887622\n",
      "\n",
      "Training: batch 27 ends at 22:02:48.670924\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6459 - bce_dice_loss: 0.6459\n",
      "Training: batch 28 begins at 22:02:48.674866\n",
      "\n",
      "Training: batch 28 ends at 22:02:49.493758\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.6372 - bce_dice_loss: 0.6372\n",
      "Training: batch 29 begins at 22:02:49.498148\n",
      "\n",
      "Training: batch 29 ends at 22:02:50.294254\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.6353 - bce_dice_loss: 0.6353\n",
      "Training: batch 30 begins at 22:02:50.298450\n",
      "\n",
      "Training: batch 30 ends at 22:02:51.091786\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6436 - bce_dice_loss: 0.6436\n",
      "Training: batch 31 begins at 22:02:51.095742\n",
      "\n",
      "Training: batch 31 ends at 22:02:51.894463\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6451 - bce_dice_loss: 0.6451\n",
      "Training: batch 32 begins at 22:02:51.899072\n",
      "\n",
      "Training: batch 32 ends at 22:02:52.690918\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6472 - bce_dice_loss: 0.6472\n",
      "Training: batch 33 begins at 22:02:52.694577\n",
      "\n",
      "Training: batch 33 ends at 22:02:53.485020\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6429 - bce_dice_loss: 0.6429\n",
      "Training: batch 34 begins at 22:02:53.487698\n",
      "\n",
      "Training: batch 34 ends at 22:02:54.272413\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.6395 - bce_dice_loss: 0.6395\n",
      "Training: batch 35 begins at 22:02:54.275776\n",
      "\n",
      "Training: batch 35 ends at 22:02:55.069348\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6359 - bce_dice_loss: 0.6359\n",
      "Training: batch 36 begins at 22:02:55.073389\n",
      "\n",
      "Training: batch 36 ends at 22:02:55.868671\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6418 - bce_dice_loss: 0.6418\n",
      "Training: batch 37 begins at 22:02:55.872691\n",
      "\n",
      "Training: batch 37 ends at 22:02:56.662260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6491 - bce_dice_loss: 0.6491\n",
      "Training: batch 38 begins at 22:02:56.666562\n",
      "\n",
      "Training: batch 38 ends at 22:02:57.450029\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.6464 - bce_dice_loss: 0.6464\n",
      "Training: batch 39 begins at 22:02:57.454497\n",
      "\n",
      "Training: batch 39 ends at 22:02:58.272584\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6465 - bce_dice_loss: 0.6465\n",
      "Training: batch 40 begins at 22:02:58.275093\n",
      "\n",
      "Training: batch 40 ends at 22:02:59.069752\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6488 - bce_dice_loss: 0.6488\n",
      "Training: batch 41 begins at 22:02:59.074185\n",
      "\n",
      "Training: batch 41 ends at 22:02:59.878793\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6512 - bce_dice_loss: 0.6512\n",
      "Training: batch 42 begins at 22:02:59.883085\n",
      "\n",
      "Training: batch 42 ends at 22:03:00.688225\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6485 - bce_dice_loss: 0.6485\n",
      "Training: batch 43 begins at 22:03:00.691209\n",
      "\n",
      "Training: batch 43 ends at 22:03:01.483006\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.6429 - bce_dice_loss: 0.6429\n",
      "Training: batch 44 begins at 22:03:01.487468\n",
      "\n",
      "Training: batch 44 ends at 22:03:02.305077\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.6406 - bce_dice_loss: 0.6406\n",
      "Training: batch 45 begins at 22:03:02.308359\n",
      "\n",
      "Training: batch 45 ends at 22:03:03.103589\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6408 - bce_dice_loss: 0.6408\n",
      "Training: batch 46 begins at 22:03:03.106992\n",
      "\n",
      "Training: batch 46 ends at 22:03:03.896455\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6422 - bce_dice_loss: 0.6422\n",
      "Training: batch 47 begins at 22:03:03.901373\n",
      "\n",
      "Training: batch 47 ends at 22:03:04.701144\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6398 - bce_dice_loss: 0.6398 \n",
      "Training: batch 48 begins at 22:03:04.705417\n",
      "\n",
      "Training: batch 48 ends at 22:03:05.490711\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.6372 - bce_dice_loss: 0.6372\n",
      "Training: batch 49 begins at 22:03:05.495061\n",
      "\n",
      "Training: batch 49 ends at 22:03:06.291606\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.6357 - bce_dice_loss: 0.6357\n",
      "Training: batch 50 begins at 22:03:06.295621\n",
      "\n",
      "Training: batch 50 ends at 22:03:07.088542\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6369 - bce_dice_loss: 0.6369\n",
      "Training: batch 51 begins at 22:03:07.092698\n",
      "\n",
      "Training: batch 51 ends at 22:03:07.881597\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6414 - bce_dice_loss: 0.6414\n",
      "Training: batch 52 begins at 22:03:07.885960\n",
      "\n",
      "Training: batch 52 ends at 22:03:08.712514\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6364 - bce_dice_loss: 0.6364\n",
      "Training: batch 53 begins at 22:03:08.716105\n",
      "\n",
      "Training: batch 53 ends at 22:03:09.511584\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6356 - bce_dice_loss: 0.6356\n",
      "Training: batch 54 begins at 22:03:09.515811\n",
      "\n",
      "Training: batch 54 ends at 22:03:10.322225\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.6348 - bce_dice_loss: 0.6348\n",
      "Training: batch 55 begins at 22:03:10.326182\n",
      "\n",
      "Training: batch 55 ends at 22:03:11.114907\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6320 - bce_dice_loss: 0.6320\n",
      "Training: batch 56 begins at 22:03:11.119739\n",
      "\n",
      "Training: batch 56 ends at 22:03:11.923374\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6326 - bce_dice_loss: 0.6326\n",
      "Training: batch 57 begins at 22:03:11.926961\n",
      "\n",
      "Training: batch 57 ends at 22:03:12.720799\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6351 - bce_dice_loss: 0.6351\n",
      "Training: batch 58 begins at 22:03:12.723583\n",
      "\n",
      "Training: batch 58 ends at 22:03:13.513701\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6348 - bce_dice_loss: 0.6348\n",
      "Training: batch 59 begins at 22:03:13.517943\n",
      "\n",
      "Training: batch 59 ends at 22:03:14.331338\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6348 - bce_dice_loss: 0.6348\n",
      "Evaluating: batch 0 begins at 22:03:14.364337\n",
      "\n",
      "Evaluating: batch 0 ends at 22:03:14.628852\n",
      "\n",
      "Evaluating: batch 1 begins at 22:03:14.630092\n",
      "\n",
      "Evaluating: batch 1 ends at 22:03:14.842712\n",
      "\n",
      "Evaluating: batch 2 begins at 22:03:14.843949\n",
      "\n",
      "Evaluating: batch 2 ends at 22:03:15.063133\n",
      "\n",
      "Evaluating: batch 3 begins at 22:03:15.065047\n",
      "\n",
      "Evaluating: batch 3 ends at 22:03:15.285522\n",
      "\n",
      "Evaluating: batch 4 begins at 22:03:15.287279\n",
      "\n",
      "Evaluating: batch 4 ends at 22:03:15.509493\n",
      "\n",
      "Evaluating: batch 5 begins at 22:03:15.510852\n",
      "\n",
      "Evaluating: batch 5 ends at 22:03:15.731931\n",
      "\n",
      "Evaluating: batch 6 begins at 22:03:15.733338\n",
      "\n",
      "Evaluating: batch 6 ends at 22:03:15.955666\n",
      "\n",
      "Evaluating: batch 7 begins at 22:03:15.957251\n",
      "\n",
      "Evaluating: batch 7 ends at 22:03:16.179009\n",
      "\n",
      "Evaluating: batch 8 begins at 22:03:16.180401\n",
      "\n",
      "Evaluating: batch 8 ends at 22:03:16.399071\n",
      "\n",
      "Evaluating: batch 9 begins at 22:03:16.400411\n",
      "\n",
      "Evaluating: batch 9 ends at 22:03:16.622182\n",
      "\n",
      "Evaluating: batch 10 begins at 22:03:16.623310\n",
      "\n",
      "Evaluating: batch 10 ends at 22:03:16.844475\n",
      "\n",
      "Evaluating: batch 11 begins at 22:03:16.846101\n",
      "\n",
      "Evaluating: batch 11 ends at 22:03:17.063988\n",
      "\n",
      "Evaluating: batch 12 begins at 22:03:17.065250\n",
      "\n",
      "Evaluating: batch 12 ends at 22:03:17.284450\n",
      "\n",
      "Evaluating: batch 13 begins at 22:03:17.286070\n",
      "\n",
      "Evaluating: batch 13 ends at 22:03:17.506868\n",
      "\n",
      "Evaluating: batch 14 begins at 22:03:17.508140\n",
      "\n",
      "Evaluating: batch 14 ends at 22:03:17.729727\n",
      "\n",
      "Evaluating: batch 15 begins at 22:03:17.731054\n",
      "\n",
      "Evaluating: batch 15 ends at 22:03:17.953562\n",
      "\n",
      "Evaluating: batch 16 begins at 22:03:17.954856\n",
      "\n",
      "Evaluating: batch 16 ends at 22:03:18.178263\n",
      "\n",
      "Evaluating: batch 17 begins at 22:03:18.180190\n",
      "\n",
      "Evaluating: batch 17 ends at 22:03:18.401250\n",
      "\n",
      "Evaluating: batch 18 begins at 22:03:18.403014\n",
      "\n",
      "Evaluating: batch 18 ends at 22:03:18.623055\n",
      "\n",
      "Evaluating: batch 19 begins at 22:03:18.625511\n",
      "\n",
      "Evaluating: batch 19 ends at 22:03:18.841487\n",
      "\n",
      "Evaluating: batch 20 begins at 22:03:18.842780\n",
      "\n",
      "Evaluating: batch 20 ends at 22:03:19.059363\n",
      "\n",
      "Evaluating: batch 21 begins at 22:03:19.060518\n",
      "\n",
      "Evaluating: batch 21 ends at 22:03:19.278193\n",
      "\n",
      "Evaluating: batch 22 begins at 22:03:19.279472\n",
      "\n",
      "Evaluating: batch 22 ends at 22:03:19.496891\n",
      "\n",
      "Evaluating: batch 23 begins at 22:03:19.498123\n",
      "\n",
      "Evaluating: batch 23 ends at 22:03:19.714167\n",
      "\n",
      "Evaluating: batch 24 begins at 22:03:19.715336\n",
      "\n",
      "Evaluating: batch 24 ends at 22:03:19.931391\n",
      "\n",
      "Evaluating: batch 25 begins at 22:03:19.932617\n",
      "\n",
      "Evaluating: batch 25 ends at 22:03:20.152009\n",
      "\n",
      "Evaluating: batch 26 begins at 22:03:20.153775\n",
      "\n",
      "Evaluating: batch 26 ends at 22:03:20.372120\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.60325\n",
      "60/60 [==============================] - 54s 904ms/step - loss: 0.6348 - bce_dice_loss: 0.6348 - val_loss: 0.6110 - val_bce_dice_loss: 0.6110\n",
      "Epoch 11/25\n",
      "\n",
      "Training: batch 0 begins at 22:03:20.399551\n",
      "\n",
      "Training: batch 0 ends at 22:03:21.192842\n",
      " 1/60 [..............................] - ETA: 46s - loss: 0.5571 - bce_dice_loss: 0.5571\n",
      "Training: batch 1 begins at 22:03:21.197191\n",
      "\n",
      "Training: batch 1 ends at 22:03:21.997728\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5525 - bce_dice_loss: 0.5525\n",
      "Training: batch 2 begins at 22:03:22.002938\n",
      "\n",
      "Training: batch 2 ends at 22:03:22.793434\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.5849 - bce_dice_loss: 0.5849\n",
      "Training: batch 3 begins at 22:03:22.797895\n",
      "\n",
      "Training: batch 3 ends at 22:03:23.587024\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.5825 - bce_dice_loss: 0.5825\n",
      "Training: batch 4 begins at 22:03:23.590606\n",
      "\n",
      "Training: batch 4 ends at 22:03:24.381917\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.5846 - bce_dice_loss: 0.5846\n",
      "Training: batch 5 begins at 22:03:24.386421\n",
      "\n",
      "Training: batch 5 ends at 22:03:25.183214\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6380 - bce_dice_loss: 0.6380\n",
      "Training: batch 6 begins at 22:03:25.187375\n",
      "\n",
      "Training: batch 6 ends at 22:03:25.977495\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6544 - bce_dice_loss: 0.6544\n",
      "Training: batch 7 begins at 22:03:25.980230\n",
      "\n",
      "Training: batch 7 ends at 22:03:26.795017\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.6519 - bce_dice_loss: 0.6519\n",
      "Training: batch 8 begins at 22:03:26.799137\n",
      "\n",
      "Training: batch 8 ends at 22:03:27.596847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.6364 - bce_dice_loss: 0.6364\n",
      "Training: batch 9 begins at 22:03:27.601683\n",
      "\n",
      "Training: batch 9 ends at 22:03:28.405171\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.6359 - bce_dice_loss: 0.6359\n",
      "Training: batch 10 begins at 22:03:28.408001\n",
      "\n",
      "Training: batch 10 ends at 22:03:29.204333\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.6307 - bce_dice_loss: 0.6307\n",
      "Training: batch 11 begins at 22:03:29.207827\n",
      "\n",
      "Training: batch 11 ends at 22:03:30.005883\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.6385 - bce_dice_loss: 0.6385\n",
      "Training: batch 12 begins at 22:03:30.009297\n",
      "\n",
      "Training: batch 12 ends at 22:03:30.800602\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.6296 - bce_dice_loss: 0.6296\n",
      "Training: batch 13 begins at 22:03:30.803730\n",
      "\n",
      "Training: batch 13 ends at 22:03:31.593059\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.6359 - bce_dice_loss: 0.6359\n",
      "Training: batch 14 begins at 22:03:31.597106\n",
      "\n",
      "Training: batch 14 ends at 22:03:32.386831\n",
      "15/60 [======>.......................] - ETA: 35s - loss: 0.6225 - bce_dice_loss: 0.6225\n",
      "Training: batch 15 begins at 22:03:32.389941\n",
      "\n",
      "Training: batch 15 ends at 22:03:33.181253\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6198 - bce_dice_loss: 0.6198\n",
      "Training: batch 16 begins at 22:03:33.185813\n",
      "\n",
      "Training: batch 16 ends at 22:03:33.979696\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6322 - bce_dice_loss: 0.6322\n",
      "Training: batch 17 begins at 22:03:33.984602\n",
      "\n",
      "Training: batch 17 ends at 22:03:34.774352\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.6291 - bce_dice_loss: 0.6291\n",
      "Training: batch 18 begins at 22:03:34.778016\n",
      "\n",
      "Training: batch 18 ends at 22:03:35.569491\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.6400 - bce_dice_loss: 0.6400\n",
      "Training: batch 19 begins at 22:03:35.573695\n",
      "\n",
      "Training: batch 19 ends at 22:03:36.371725\n",
      "20/60 [=========>....................] - ETA: 31s - loss: 0.6332 - bce_dice_loss: 0.6332\n",
      "Training: batch 20 begins at 22:03:36.374339\n",
      "\n",
      "Training: batch 20 ends at 22:03:37.196038\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6336 - bce_dice_loss: 0.6336\n",
      "Training: batch 21 begins at 22:03:37.200582\n",
      "\n",
      "Training: batch 21 ends at 22:03:37.993197\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6244 - bce_dice_loss: 0.6244\n",
      "Training: batch 22 begins at 22:03:37.995643\n",
      "\n",
      "Training: batch 22 ends at 22:03:38.792516\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.6120 - bce_dice_loss: 0.6120\n",
      "Training: batch 23 begins at 22:03:38.795793\n",
      "\n",
      "Training: batch 23 ends at 22:03:39.601431\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.6131 - bce_dice_loss: 0.6131\n",
      "Training: batch 24 begins at 22:03:39.605163\n",
      "\n",
      "Training: batch 24 ends at 22:03:40.395685\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.6175 - bce_dice_loss: 0.6175\n",
      "Training: batch 25 begins at 22:03:40.400013\n",
      "\n",
      "Training: batch 25 ends at 22:03:41.178274\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.6068 - bce_dice_loss: 0.6068\n",
      "Training: batch 26 begins at 22:03:41.182754\n",
      "\n",
      "Training: batch 26 ends at 22:03:41.991017\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6101 - bce_dice_loss: 0.6101\n",
      "Training: batch 27 begins at 22:03:41.994347\n",
      "\n",
      "Training: batch 27 ends at 22:03:42.786675\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6070 - bce_dice_loss: 0.6070\n",
      "Training: batch 28 begins at 22:03:42.789821\n",
      "\n",
      "Training: batch 28 ends at 22:03:43.590991\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.6066 - bce_dice_loss: 0.6066\n",
      "Training: batch 29 begins at 22:03:43.598004\n",
      "\n",
      "Training: batch 29 ends at 22:03:44.390351\n",
      "30/60 [==============>...............] - ETA: 23s - loss: 0.6138 - bce_dice_loss: 0.6138\n",
      "Training: batch 30 begins at 22:03:44.395621\n",
      "\n",
      "Training: batch 30 ends at 22:03:45.199942\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6083 - bce_dice_loss: 0.6083\n",
      "Training: batch 31 begins at 22:03:45.204334\n",
      "\n",
      "Training: batch 31 ends at 22:03:46.000819\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6157 - bce_dice_loss: 0.6157\n",
      "Training: batch 32 begins at 22:03:46.004716\n",
      "\n",
      "Training: batch 32 ends at 22:03:46.817788\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6113 - bce_dice_loss: 0.6113\n",
      "Training: batch 33 begins at 22:03:46.821618\n",
      "\n",
      "Training: batch 33 ends at 22:03:47.618861\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6126 - bce_dice_loss: 0.6126\n",
      "Training: batch 34 begins at 22:03:47.623228\n",
      "\n",
      "Training: batch 34 ends at 22:03:48.426032\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.6126 - bce_dice_loss: 0.6126\n",
      "Training: batch 35 begins at 22:03:48.437177\n",
      "\n",
      "Training: batch 35 ends at 22:03:49.247191\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6185 - bce_dice_loss: 0.6185\n",
      "Training: batch 36 begins at 22:03:49.251189\n",
      "\n",
      "Training: batch 36 ends at 22:03:50.133110\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6171 - bce_dice_loss: 0.6171\n",
      "Training: batch 37 begins at 22:03:50.137228\n",
      "\n",
      "Training: batch 37 ends at 22:03:51.053932\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6133 - bce_dice_loss: 0.6133\n",
      "Training: batch 38 begins at 22:03:51.058687\n",
      "\n",
      "Training: batch 38 ends at 22:03:52.031321\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.6165 - bce_dice_loss: 0.6165\n",
      "Training: batch 39 begins at 22:03:52.035927\n",
      "\n",
      "Training: batch 39 ends at 22:03:52.985079\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6134 - bce_dice_loss: 0.6134\n",
      "Training: batch 40 begins at 22:03:52.988614\n",
      "\n",
      "Training: batch 40 ends at 22:03:53.840168\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6135 - bce_dice_loss: 0.6135\n",
      "Training: batch 41 begins at 22:03:53.845536\n",
      "\n",
      "Training: batch 41 ends at 22:03:54.863635\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6150 - bce_dice_loss: 0.6150\n",
      "Training: batch 42 begins at 22:03:54.869100\n",
      "\n",
      "Training: batch 42 ends at 22:03:55.717811\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6182 - bce_dice_loss: 0.6182\n",
      "Training: batch 43 begins at 22:03:55.722669\n",
      "\n",
      "Training: batch 43 ends at 22:03:56.520508\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.6299 - bce_dice_loss: 0.6299\n",
      "Training: batch 44 begins at 22:03:56.523886\n",
      "\n",
      "Training: batch 44 ends at 22:03:57.341863\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.6291 - bce_dice_loss: 0.6291\n",
      "Training: batch 45 begins at 22:03:57.346307\n",
      "\n",
      "Training: batch 45 ends at 22:03:58.148034\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6238 - bce_dice_loss: 0.6238\n",
      "Training: batch 46 begins at 22:03:58.153412\n",
      "\n",
      "Training: batch 46 ends at 22:03:58.952890\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6251 - bce_dice_loss: 0.6251\n",
      "Training: batch 47 begins at 22:03:58.956182\n",
      "\n",
      "Training: batch 47 ends at 22:03:59.744651\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6263 - bce_dice_loss: 0.6263 \n",
      "Training: batch 48 begins at 22:03:59.748507\n",
      "\n",
      "Training: batch 48 ends at 22:04:00.571156\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.6203 - bce_dice_loss: 0.6203\n",
      "Training: batch 49 begins at 22:04:00.574000\n",
      "\n",
      "Training: batch 49 ends at 22:04:01.364186\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.6210 - bce_dice_loss: 0.6210\n",
      "Training: batch 50 begins at 22:04:01.369367\n",
      "\n",
      "Training: batch 50 ends at 22:04:02.159315\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6217 - bce_dice_loss: 0.6217\n",
      "Training: batch 51 begins at 22:04:02.163782\n",
      "\n",
      "Training: batch 51 ends at 22:04:02.953705\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6209 - bce_dice_loss: 0.6209\n",
      "Training: batch 52 begins at 22:04:02.957943\n",
      "\n",
      "Training: batch 52 ends at 22:04:03.778062\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6190 - bce_dice_loss: 0.6190\n",
      "Training: batch 53 begins at 22:04:03.782060\n",
      "\n",
      "Training: batch 53 ends at 22:04:04.570348\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6180 - bce_dice_loss: 0.6180\n",
      "Training: batch 54 begins at 22:04:04.574187\n",
      "\n",
      "Training: batch 54 ends at 22:04:05.362555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/60 [==========================>...] - ETA: 4s - loss: 0.6160 - bce_dice_loss: 0.6160\n",
      "Training: batch 55 begins at 22:04:05.367369\n",
      "\n",
      "Training: batch 55 ends at 22:04:06.149532\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6157 - bce_dice_loss: 0.6157\n",
      "Training: batch 56 begins at 22:04:06.153737\n",
      "\n",
      "Training: batch 56 ends at 22:04:06.943209\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6152 - bce_dice_loss: 0.6152\n",
      "Training: batch 57 begins at 22:04:06.947332\n",
      "\n",
      "Training: batch 57 ends at 22:04:07.736405\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6138 - bce_dice_loss: 0.6138\n",
      "Training: batch 58 begins at 22:04:07.739627\n",
      "\n",
      "Training: batch 58 ends at 22:04:08.531951\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6148 - bce_dice_loss: 0.6148\n",
      "Training: batch 59 begins at 22:04:08.536122\n",
      "\n",
      "Training: batch 59 ends at 22:04:09.358264\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6137 - bce_dice_loss: 0.6137\n",
      "Evaluating: batch 0 begins at 22:04:09.393327\n",
      "\n",
      "Evaluating: batch 0 ends at 22:04:09.664025\n",
      "\n",
      "Evaluating: batch 1 begins at 22:04:09.665338\n",
      "\n",
      "Evaluating: batch 1 ends at 22:04:09.878133\n",
      "\n",
      "Evaluating: batch 2 begins at 22:04:09.879386\n",
      "\n",
      "Evaluating: batch 2 ends at 22:04:10.098456\n",
      "\n",
      "Evaluating: batch 3 begins at 22:04:10.100244\n",
      "\n",
      "Evaluating: batch 3 ends at 22:04:10.318088\n",
      "\n",
      "Evaluating: batch 4 begins at 22:04:10.319316\n",
      "\n",
      "Evaluating: batch 4 ends at 22:04:10.539967\n",
      "\n",
      "Evaluating: batch 5 begins at 22:04:10.541338\n",
      "\n",
      "Evaluating: batch 5 ends at 22:04:10.763662\n",
      "\n",
      "Evaluating: batch 6 begins at 22:04:10.764987\n",
      "\n",
      "Evaluating: batch 6 ends at 22:04:10.981391\n",
      "\n",
      "Evaluating: batch 7 begins at 22:04:10.982665\n",
      "\n",
      "Evaluating: batch 7 ends at 22:04:11.202579\n",
      "\n",
      "Evaluating: batch 8 begins at 22:04:11.203953\n",
      "\n",
      "Evaluating: batch 8 ends at 22:04:11.423182\n",
      "\n",
      "Evaluating: batch 9 begins at 22:04:11.424195\n",
      "\n",
      "Evaluating: batch 9 ends at 22:04:11.645136\n",
      "\n",
      "Evaluating: batch 10 begins at 22:04:11.646570\n",
      "\n",
      "Evaluating: batch 10 ends at 22:04:11.865904\n",
      "\n",
      "Evaluating: batch 11 begins at 22:04:11.867369\n",
      "\n",
      "Evaluating: batch 11 ends at 22:04:12.087197\n",
      "\n",
      "Evaluating: batch 12 begins at 22:04:12.089254\n",
      "\n",
      "Evaluating: batch 12 ends at 22:04:12.307535\n",
      "\n",
      "Evaluating: batch 13 begins at 22:04:12.308915\n",
      "\n",
      "Evaluating: batch 13 ends at 22:04:12.531752\n",
      "\n",
      "Evaluating: batch 14 begins at 22:04:12.533944\n",
      "\n",
      "Evaluating: batch 14 ends at 22:04:12.756194\n",
      "\n",
      "Evaluating: batch 15 begins at 22:04:12.757913\n",
      "\n",
      "Evaluating: batch 15 ends at 22:04:12.980588\n",
      "\n",
      "Evaluating: batch 16 begins at 22:04:12.981883\n",
      "\n",
      "Evaluating: batch 16 ends at 22:04:13.200622\n",
      "\n",
      "Evaluating: batch 17 begins at 22:04:13.203184\n",
      "\n",
      "Evaluating: batch 17 ends at 22:04:13.423621\n",
      "\n",
      "Evaluating: batch 18 begins at 22:04:13.424998\n",
      "\n",
      "Evaluating: batch 18 ends at 22:04:13.646037\n",
      "\n",
      "Evaluating: batch 19 begins at 22:04:13.647471\n",
      "\n",
      "Evaluating: batch 19 ends at 22:04:13.866046\n",
      "\n",
      "Evaluating: batch 20 begins at 22:04:13.867752\n",
      "\n",
      "Evaluating: batch 20 ends at 22:04:14.088354\n",
      "\n",
      "Evaluating: batch 21 begins at 22:04:14.089717\n",
      "\n",
      "Evaluating: batch 21 ends at 22:04:14.305634\n",
      "\n",
      "Evaluating: batch 22 begins at 22:04:14.307200\n",
      "\n",
      "Evaluating: batch 22 ends at 22:04:14.521136\n",
      "\n",
      "Evaluating: batch 23 begins at 22:04:14.522578\n",
      "\n",
      "Evaluating: batch 23 ends at 22:04:14.738960\n",
      "\n",
      "Evaluating: batch 24 begins at 22:04:14.740452\n",
      "\n",
      "Evaluating: batch 24 ends at 22:04:14.960304\n",
      "\n",
      "Evaluating: batch 25 begins at 22:04:14.961851\n",
      "\n",
      "Evaluating: batch 25 ends at 22:04:15.179574\n",
      "\n",
      "Evaluating: batch 26 begins at 22:04:15.180845\n",
      "\n",
      "Evaluating: batch 26 ends at 22:04:15.397056\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60325\n",
      "60/60 [==============================] - 55s 919ms/step - loss: 0.6137 - bce_dice_loss: 0.6137 - val_loss: 0.6214 - val_bce_dice_loss: 0.6214\n",
      "Epoch 12/25\n",
      "\n",
      "Training: batch 0 begins at 22:04:15.425123\n",
      "\n",
      "Training: batch 0 ends at 22:04:16.221799\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3737 - bce_dice_loss: 0.3737\n",
      "Training: batch 1 begins at 22:04:16.226143\n",
      "\n",
      "Training: batch 1 ends at 22:04:17.022121\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.4456 - bce_dice_loss: 0.4456\n",
      "Training: batch 2 begins at 22:04:17.025854\n",
      "\n",
      "Training: batch 2 ends at 22:04:17.818105\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.5086 - bce_dice_loss: 0.5086\n",
      "Training: batch 3 begins at 22:04:17.821929\n",
      "\n",
      "Training: batch 3 ends at 22:04:18.632711\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5272 - bce_dice_loss: 0.5272\n",
      "Training: batch 4 begins at 22:04:18.636811\n",
      "\n",
      "Training: batch 4 ends at 22:04:19.439818\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.5741 - bce_dice_loss: 0.5741\n",
      "Training: batch 5 begins at 22:04:19.443706\n",
      "\n",
      "Training: batch 5 ends at 22:04:20.237850\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5662 - bce_dice_loss: 0.5662\n",
      "Training: batch 6 begins at 22:04:20.241514\n",
      "\n",
      "Training: batch 6 ends at 22:04:21.024600\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6056 - bce_dice_loss: 0.6056\n",
      "Training: batch 7 begins at 22:04:21.028926\n",
      "\n",
      "Training: batch 7 ends at 22:04:21.824114\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5989 - bce_dice_loss: 0.5989\n",
      "Training: batch 8 begins at 22:04:21.827701\n",
      "\n",
      "Training: batch 8 ends at 22:04:22.620023\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.6158 - bce_dice_loss: 0.6158\n",
      "Training: batch 9 begins at 22:04:22.624225\n",
      "\n",
      "Training: batch 9 ends at 22:04:23.414137\n",
      "10/60 [====>.........................] - ETA: 39s - loss: 0.5936 - bce_dice_loss: 0.5936\n",
      "Training: batch 10 begins at 22:04:23.418613\n",
      "\n",
      "Training: batch 10 ends at 22:04:24.215617\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.6006 - bce_dice_loss: 0.6006\n",
      "Training: batch 11 begins at 22:04:24.219510\n",
      "\n",
      "Training: batch 11 ends at 22:04:25.008998\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.6256 - bce_dice_loss: 0.6256\n",
      "Training: batch 12 begins at 22:04:25.013369\n",
      "\n",
      "Training: batch 12 ends at 22:04:25.803555\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.6311 - bce_dice_loss: 0.6311\n",
      "Training: batch 13 begins at 22:04:25.807060\n",
      "\n",
      "Training: batch 13 ends at 22:04:26.605433\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.6428 - bce_dice_loss: 0.6428\n",
      "Training: batch 14 begins at 22:04:26.609624\n",
      "\n",
      "Training: batch 14 ends at 22:04:27.417122\n",
      "15/60 [======>.......................] - ETA: 35s - loss: 0.6485 - bce_dice_loss: 0.6485\n",
      "Training: batch 15 begins at 22:04:27.422671\n",
      "\n",
      "Training: batch 15 ends at 22:04:28.225096\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.6473 - bce_dice_loss: 0.6473\n",
      "Training: batch 16 begins at 22:04:28.229491\n",
      "\n",
      "Training: batch 16 ends at 22:04:29.023989\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.6511 - bce_dice_loss: 0.6511\n",
      "Training: batch 17 begins at 22:04:29.027973\n",
      "\n",
      "Training: batch 17 ends at 22:04:29.825495\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.6476 - bce_dice_loss: 0.6476\n",
      "Training: batch 18 begins at 22:04:29.829638\n",
      "\n",
      "Training: batch 18 ends at 22:04:30.617437\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.6449 - bce_dice_loss: 0.6449\n",
      "Training: batch 19 begins at 22:04:30.621213\n",
      "\n",
      "Training: batch 19 ends at 22:04:31.414718\n",
      "20/60 [=========>....................] - ETA: 31s - loss: 0.6483 - bce_dice_loss: 0.6483\n",
      "Training: batch 20 begins at 22:04:31.418506\n",
      "\n",
      "Training: batch 20 ends at 22:04:32.207206\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.6457 - bce_dice_loss: 0.6457\n",
      "Training: batch 21 begins at 22:04:32.211669\n",
      "\n",
      "Training: batch 21 ends at 22:04:33.002489\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.6376 - bce_dice_loss: 0.6376\n",
      "Training: batch 22 begins at 22:04:33.006005\n",
      "\n",
      "Training: batch 22 ends at 22:04:33.798351\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.6352 - bce_dice_loss: 0.6352\n",
      "Training: batch 23 begins at 22:04:33.802734\n",
      "\n",
      "Training: batch 23 ends at 22:04:34.592170\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.6305 - bce_dice_loss: 0.6305\n",
      "Training: batch 24 begins at 22:04:34.596097\n",
      "\n",
      "Training: batch 24 ends at 22:04:35.393922\n",
      "25/60 [===========>..................] - ETA: 27s - loss: 0.6402 - bce_dice_loss: 0.6402\n",
      "Training: batch 25 begins at 22:04:35.398098\n",
      "\n",
      "Training: batch 25 ends at 22:04:36.189381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/60 [============>.................] - ETA: 27s - loss: 0.6379 - bce_dice_loss: 0.6379\n",
      "Training: batch 26 begins at 22:04:36.192885\n",
      "\n",
      "Training: batch 26 ends at 22:04:36.991374\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.6289 - bce_dice_loss: 0.6289\n",
      "Training: batch 27 begins at 22:04:36.995443\n",
      "\n",
      "Training: batch 27 ends at 22:04:37.786469\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.6310 - bce_dice_loss: 0.6310\n",
      "Training: batch 28 begins at 22:04:37.789882\n",
      "\n",
      "Training: batch 28 ends at 22:04:38.612649\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.6271 - bce_dice_loss: 0.6271\n",
      "Training: batch 29 begins at 22:04:38.619592\n",
      "\n",
      "Training: batch 29 ends at 22:04:39.410901\n",
      "30/60 [==============>...............] - ETA: 23s - loss: 0.6278 - bce_dice_loss: 0.6278\n",
      "Training: batch 30 begins at 22:04:39.415733\n",
      "\n",
      "Training: batch 30 ends at 22:04:40.205152\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6272 - bce_dice_loss: 0.6272\n",
      "Training: batch 31 begins at 22:04:40.209307\n",
      "\n",
      "Training: batch 31 ends at 22:04:41.026050\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6269 - bce_dice_loss: 0.6269\n",
      "Training: batch 32 begins at 22:04:41.032050\n",
      "\n",
      "Training: batch 32 ends at 22:04:41.822718\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6261 - bce_dice_loss: 0.6261\n",
      "Training: batch 33 begins at 22:04:41.826814\n",
      "\n",
      "Training: batch 33 ends at 22:04:42.611891\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6272 - bce_dice_loss: 0.6272\n",
      "Training: batch 34 begins at 22:04:42.616047\n",
      "\n",
      "Training: batch 34 ends at 22:04:43.403676\n",
      "35/60 [================>.............] - ETA: 19s - loss: 0.6275 - bce_dice_loss: 0.6275\n",
      "Training: batch 35 begins at 22:04:43.408140\n",
      "\n",
      "Training: batch 35 ends at 22:04:44.213722\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.6282 - bce_dice_loss: 0.6282\n",
      "Training: batch 36 begins at 22:04:44.218516\n",
      "\n",
      "Training: batch 36 ends at 22:04:45.013911\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.6248 - bce_dice_loss: 0.6248\n",
      "Training: batch 37 begins at 22:04:45.020012\n",
      "\n",
      "Training: batch 37 ends at 22:04:45.819543\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.6237 - bce_dice_loss: 0.6237\n",
      "Training: batch 38 begins at 22:04:45.822800\n",
      "\n",
      "Training: batch 38 ends at 22:04:46.608008\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.6298 - bce_dice_loss: 0.6298\n",
      "Training: batch 39 begins at 22:04:46.611656\n",
      "\n",
      "Training: batch 39 ends at 22:04:47.404482\n",
      "40/60 [===================>..........] - ETA: 15s - loss: 0.6301 - bce_dice_loss: 0.6301\n",
      "Training: batch 40 begins at 22:04:47.407373\n",
      "\n",
      "Training: batch 40 ends at 22:04:48.212038\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.6254 - bce_dice_loss: 0.6254\n",
      "Training: batch 41 begins at 22:04:48.216572\n",
      "\n",
      "Training: batch 41 ends at 22:04:49.008671\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.6161 - bce_dice_loss: 0.6161\n",
      "Training: batch 42 begins at 22:04:49.013087\n",
      "\n",
      "Training: batch 42 ends at 22:04:49.811641\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6262 - bce_dice_loss: 0.6262\n",
      "Training: batch 43 begins at 22:04:49.816882\n",
      "\n",
      "Training: batch 43 ends at 22:04:50.624510\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.6238 - bce_dice_loss: 0.6238\n",
      "Training: batch 44 begins at 22:04:50.629074\n",
      "\n",
      "Training: batch 44 ends at 22:04:51.417192\n",
      "45/60 [=====================>........] - ETA: 11s - loss: 0.6326 - bce_dice_loss: 0.6326\n",
      "Training: batch 45 begins at 22:04:51.420674\n",
      "\n",
      "Training: batch 45 ends at 22:04:52.212697\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.6324 - bce_dice_loss: 0.6324\n",
      "Training: batch 46 begins at 22:04:52.217382\n",
      "\n",
      "Training: batch 46 ends at 22:04:53.034827\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.6353 - bce_dice_loss: 0.6353\n",
      "Training: batch 47 begins at 22:04:53.038477\n",
      "\n",
      "Training: batch 47 ends at 22:04:53.830382\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.6315 - bce_dice_loss: 0.6315 \n",
      "Training: batch 48 begins at 22:04:53.834985\n",
      "\n",
      "Training: batch 48 ends at 22:04:54.624536\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.6240 - bce_dice_loss: 0.6240\n",
      "Training: batch 49 begins at 22:04:54.628981\n",
      "\n",
      "Training: batch 49 ends at 22:04:55.419500\n",
      "50/60 [========================>.....] - ETA: 7s - loss: 0.6223 - bce_dice_loss: 0.6223\n",
      "Training: batch 50 begins at 22:04:55.423738\n",
      "\n",
      "Training: batch 50 ends at 22:04:56.233211\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.6238 - bce_dice_loss: 0.6238\n",
      "Training: batch 51 begins at 22:04:56.236754\n",
      "\n",
      "Training: batch 51 ends at 22:04:57.024637\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.6242 - bce_dice_loss: 0.6242\n",
      "Training: batch 52 begins at 22:04:57.028711\n",
      "\n",
      "Training: batch 52 ends at 22:04:57.818322\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.6246 - bce_dice_loss: 0.6246\n",
      "Training: batch 53 begins at 22:04:57.821483\n",
      "\n",
      "Training: batch 53 ends at 22:04:58.618477\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.6244 - bce_dice_loss: 0.6244\n",
      "Training: batch 54 begins at 22:04:58.621195\n",
      "\n",
      "Training: batch 54 ends at 22:04:59.419379\n",
      "55/60 [==========================>...] - ETA: 3s - loss: 0.6252 - bce_dice_loss: 0.6252\n",
      "Training: batch 55 begins at 22:04:59.423726\n",
      "\n",
      "Training: batch 55 ends at 22:05:00.214483\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.6228 - bce_dice_loss: 0.6228\n",
      "Training: batch 56 begins at 22:05:00.218870\n",
      "\n",
      "Training: batch 56 ends at 22:05:01.025520\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.6219 - bce_dice_loss: 0.6219\n",
      "Training: batch 57 begins at 22:05:01.029814\n",
      "\n",
      "Training: batch 57 ends at 22:05:01.826108\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.6261 - bce_dice_loss: 0.6261\n",
      "Training: batch 58 begins at 22:05:01.830520\n",
      "\n",
      "Training: batch 58 ends at 22:05:02.618738\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6261 - bce_dice_loss: 0.6261\n",
      "Training: batch 59 begins at 22:05:02.622316\n",
      "\n",
      "Training: batch 59 ends at 22:05:03.410139\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6292 - bce_dice_loss: 0.6292\n",
      "Evaluating: batch 0 begins at 22:05:03.442040\n",
      "\n",
      "Evaluating: batch 0 ends at 22:05:03.705647\n",
      "\n",
      "Evaluating: batch 1 begins at 22:05:03.707003\n",
      "\n",
      "Evaluating: batch 1 ends at 22:05:03.920139\n",
      "\n",
      "Evaluating: batch 2 begins at 22:05:03.921374\n",
      "\n",
      "Evaluating: batch 2 ends at 22:05:04.136586\n",
      "\n",
      "Evaluating: batch 3 begins at 22:05:04.137835\n",
      "\n",
      "Evaluating: batch 3 ends at 22:05:04.354814\n",
      "\n",
      "Evaluating: batch 4 begins at 22:05:04.356483\n",
      "\n",
      "Evaluating: batch 4 ends at 22:05:04.577612\n",
      "\n",
      "Evaluating: batch 5 begins at 22:05:04.579296\n",
      "\n",
      "Evaluating: batch 5 ends at 22:05:04.801418\n",
      "\n",
      "Evaluating: batch 6 begins at 22:05:04.802934\n",
      "\n",
      "Evaluating: batch 6 ends at 22:05:05.023269\n",
      "\n",
      "Evaluating: batch 7 begins at 22:05:05.024545\n",
      "\n",
      "Evaluating: batch 7 ends at 22:05:05.244071\n",
      "\n",
      "Evaluating: batch 8 begins at 22:05:05.245810\n",
      "\n",
      "Evaluating: batch 8 ends at 22:05:05.461787\n",
      "\n",
      "Evaluating: batch 9 begins at 22:05:05.463264\n",
      "\n",
      "Evaluating: batch 9 ends at 22:05:05.682642\n",
      "\n",
      "Evaluating: batch 10 begins at 22:05:05.684106\n",
      "\n",
      "Evaluating: batch 10 ends at 22:05:05.906539\n",
      "\n",
      "Evaluating: batch 11 begins at 22:05:05.908495\n",
      "\n",
      "Evaluating: batch 11 ends at 22:05:06.126233\n",
      "\n",
      "Evaluating: batch 12 begins at 22:05:06.127479\n",
      "\n",
      "Evaluating: batch 12 ends at 22:05:06.345061\n",
      "\n",
      "Evaluating: batch 13 begins at 22:05:06.346292\n",
      "\n",
      "Evaluating: batch 13 ends at 22:05:06.567625\n",
      "\n",
      "Evaluating: batch 14 begins at 22:05:06.569114\n",
      "\n",
      "Evaluating: batch 14 ends at 22:05:06.789101\n",
      "\n",
      "Evaluating: batch 15 begins at 22:05:06.790520\n",
      "\n",
      "Evaluating: batch 15 ends at 22:05:07.010689\n",
      "\n",
      "Evaluating: batch 16 begins at 22:05:07.012724\n",
      "\n",
      "Evaluating: batch 16 ends at 22:05:07.228074\n",
      "\n",
      "Evaluating: batch 17 begins at 22:05:07.229407\n",
      "\n",
      "Evaluating: batch 17 ends at 22:05:07.448594\n",
      "\n",
      "Evaluating: batch 18 begins at 22:05:07.449840\n",
      "\n",
      "Evaluating: batch 18 ends at 22:05:07.668598\n",
      "\n",
      "Evaluating: batch 19 begins at 22:05:07.669919\n",
      "\n",
      "Evaluating: batch 19 ends at 22:05:07.890845\n",
      "\n",
      "Evaluating: batch 20 begins at 22:05:07.892801\n",
      "\n",
      "Evaluating: batch 20 ends at 22:05:08.108978\n",
      "\n",
      "Evaluating: batch 21 begins at 22:05:08.110309\n",
      "\n",
      "Evaluating: batch 21 ends at 22:05:08.333191\n",
      "\n",
      "Evaluating: batch 22 begins at 22:05:08.335205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 22 ends at 22:05:08.551829\n",
      "\n",
      "Evaluating: batch 23 begins at 22:05:08.553087\n",
      "\n",
      "Evaluating: batch 23 ends at 22:05:08.770750\n",
      "\n",
      "Evaluating: batch 24 begins at 22:05:08.772109\n",
      "\n",
      "Evaluating: batch 24 ends at 22:05:08.992388\n",
      "\n",
      "Evaluating: batch 25 begins at 22:05:08.994336\n",
      "\n",
      "Evaluating: batch 25 ends at 22:05:09.211062\n",
      "\n",
      "Evaluating: batch 26 begins at 22:05:09.212368\n",
      "\n",
      "Evaluating: batch 26 ends at 22:05:09.431209\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.60325\n",
      "60/60 [==============================] - 54s 902ms/step - loss: 0.6292 - bce_dice_loss: 0.6292 - val_loss: 0.6989 - val_bce_dice_loss: 0.6989\n",
      "Epoch 13/25\n",
      "\n",
      "Training: batch 0 begins at 22:05:09.462079\n",
      "\n",
      "Training: batch 0 ends at 22:05:10.260506\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.6151 - bce_dice_loss: 0.6151\n",
      "Training: batch 1 begins at 22:05:10.266111\n",
      "\n",
      "Training: batch 1 ends at 22:05:11.085869\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.5357 - bce_dice_loss: 0.5357\n",
      "Training: batch 2 begins at 22:05:11.089512\n",
      "\n",
      "Training: batch 2 ends at 22:05:11.878988\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.5239 - bce_dice_loss: 0.5239\n",
      "Training: batch 3 begins at 22:05:11.883933\n",
      "\n",
      "Training: batch 3 ends at 22:05:12.670606\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.4881 - bce_dice_loss: 0.4881\n",
      "Training: batch 4 begins at 22:05:12.674051\n",
      "\n",
      "Training: batch 4 ends at 22:05:13.492843\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4686 - bce_dice_loss: 0.4686\n",
      "Training: batch 5 begins at 22:05:13.497127\n",
      "\n",
      "Training: batch 5 ends at 22:05:14.285578\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4617 - bce_dice_loss: 0.4617\n",
      "Training: batch 6 begins at 22:05:14.288992\n",
      "\n",
      "Training: batch 6 ends at 22:05:15.083759\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4784 - bce_dice_loss: 0.4784\n",
      "Training: batch 7 begins at 22:05:15.087167\n",
      "\n",
      "Training: batch 7 ends at 22:05:15.879381\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4926 - bce_dice_loss: 0.4926\n",
      "Training: batch 8 begins at 22:05:15.883850\n",
      "\n",
      "Training: batch 8 ends at 22:05:16.673502\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.4848 - bce_dice_loss: 0.4848\n",
      "Training: batch 9 begins at 22:05:16.678274\n",
      "\n",
      "Training: batch 9 ends at 22:05:17.493725\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4899 - bce_dice_loss: 0.4899\n",
      "Training: batch 10 begins at 22:05:17.498454\n",
      "\n",
      "Training: batch 10 ends at 22:05:18.300759\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4950 - bce_dice_loss: 0.4950\n",
      "Training: batch 11 begins at 22:05:18.304908\n",
      "\n",
      "Training: batch 11 ends at 22:05:19.098878\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5105 - bce_dice_loss: 0.5105\n",
      "Training: batch 12 begins at 22:05:19.103284\n",
      "\n",
      "Training: batch 12 ends at 22:05:19.907611\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5337 - bce_dice_loss: 0.5337\n",
      "Training: batch 13 begins at 22:05:19.911193\n",
      "\n",
      "Training: batch 13 ends at 22:05:20.705480\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5397 - bce_dice_loss: 0.5397\n",
      "Training: batch 14 begins at 22:05:20.709882\n",
      "\n",
      "Training: batch 14 ends at 22:05:21.500847\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5407 - bce_dice_loss: 0.5407\n",
      "Training: batch 15 begins at 22:05:21.505352\n",
      "\n",
      "Training: batch 15 ends at 22:05:22.323829\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5493 - bce_dice_loss: 0.5493\n",
      "Training: batch 16 begins at 22:05:22.327909\n",
      "\n",
      "Training: batch 16 ends at 22:05:23.124305\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5578 - bce_dice_loss: 0.5578\n",
      "Training: batch 17 begins at 22:05:23.128281\n",
      "\n",
      "Training: batch 17 ends at 22:05:23.924309\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5681 - bce_dice_loss: 0.5681\n",
      "Training: batch 18 begins at 22:05:23.928700\n",
      "\n",
      "Training: batch 18 ends at 22:05:24.720391\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5855 - bce_dice_loss: 0.5855\n",
      "Training: batch 19 begins at 22:05:24.724497\n",
      "\n",
      "Training: batch 19 ends at 22:05:25.527256\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5838 - bce_dice_loss: 0.5838\n",
      "Training: batch 20 begins at 22:05:25.531960\n",
      "\n",
      "Training: batch 20 ends at 22:05:26.326263\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5770 - bce_dice_loss: 0.5770\n",
      "Training: batch 21 begins at 22:05:26.331874\n",
      "\n",
      "Training: batch 21 ends at 22:05:27.120632\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5888 - bce_dice_loss: 0.5888\n",
      "Training: batch 22 begins at 22:05:27.124092\n",
      "\n",
      "Training: batch 22 ends at 22:05:27.920159\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5789 - bce_dice_loss: 0.5789\n",
      "Training: batch 23 begins at 22:05:27.923772\n",
      "\n",
      "Training: batch 23 ends at 22:05:28.731157\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5777 - bce_dice_loss: 0.5777\n",
      "Training: batch 24 begins at 22:05:28.734407\n",
      "\n",
      "Training: batch 24 ends at 22:05:29.526449\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5806 - bce_dice_loss: 0.5806\n",
      "Training: batch 25 begins at 22:05:29.530722\n",
      "\n",
      "Training: batch 25 ends at 22:05:30.323590\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5892 - bce_dice_loss: 0.5892\n",
      "Training: batch 26 begins at 22:05:30.327284\n",
      "\n",
      "Training: batch 26 ends at 22:05:31.118708\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5927 - bce_dice_loss: 0.5927\n",
      "Training: batch 27 begins at 22:05:31.121883\n",
      "\n",
      "Training: batch 27 ends at 22:05:31.913654\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5953 - bce_dice_loss: 0.5953\n",
      "Training: batch 28 begins at 22:05:31.917765\n",
      "\n",
      "Training: batch 28 ends at 22:05:32.735773\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5953 - bce_dice_loss: 0.5953\n",
      "Training: batch 29 begins at 22:05:32.739359\n",
      "\n",
      "Training: batch 29 ends at 22:05:33.531000\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5975 - bce_dice_loss: 0.5975\n",
      "Training: batch 30 begins at 22:05:33.534795\n",
      "\n",
      "Training: batch 30 ends at 22:05:34.326006\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.6051 - bce_dice_loss: 0.6051\n",
      "Training: batch 31 begins at 22:05:34.330405\n",
      "\n",
      "Training: batch 31 ends at 22:05:35.119498\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.6027 - bce_dice_loss: 0.6027\n",
      "Training: batch 32 begins at 22:05:35.123274\n",
      "\n",
      "Training: batch 32 ends at 22:05:35.920368\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.6020 - bce_dice_loss: 0.6020\n",
      "Training: batch 33 begins at 22:05:35.925212\n",
      "\n",
      "Training: batch 33 ends at 22:05:36.716879\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.6011 - bce_dice_loss: 0.6011\n",
      "Training: batch 34 begins at 22:05:36.721333\n",
      "\n",
      "Training: batch 34 ends at 22:05:37.516181\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5999 - bce_dice_loss: 0.5999\n",
      "Training: batch 35 begins at 22:05:37.519734\n",
      "\n",
      "Training: batch 35 ends at 22:05:38.333826\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5971 - bce_dice_loss: 0.5971\n",
      "Training: batch 36 begins at 22:05:38.337052\n",
      "\n",
      "Training: batch 36 ends at 22:05:39.133276\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5946 - bce_dice_loss: 0.5946\n",
      "Training: batch 37 begins at 22:05:39.137393\n",
      "\n",
      "Training: batch 37 ends at 22:05:39.934946\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5966 - bce_dice_loss: 0.5966\n",
      "Training: batch 38 begins at 22:05:39.938354\n",
      "\n",
      "Training: batch 38 ends at 22:05:40.728148\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5977 - bce_dice_loss: 0.5977\n",
      "Training: batch 39 begins at 22:05:40.731946\n",
      "\n",
      "Training: batch 39 ends at 22:05:41.544627\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.6005 - bce_dice_loss: 0.6005\n",
      "Training: batch 40 begins at 22:05:41.550583\n",
      "\n",
      "Training: batch 40 ends at 22:05:42.344845\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5989 - bce_dice_loss: 0.5989\n",
      "Training: batch 41 begins at 22:05:42.349557\n",
      "\n",
      "Training: batch 41 ends at 22:05:43.134929\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5989 - bce_dice_loss: 0.5989\n",
      "Training: batch 42 begins at 22:05:43.139113\n",
      "\n",
      "Training: batch 42 ends at 22:05:43.921627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/60 [====================>.........] - ETA: 13s - loss: 0.6010 - bce_dice_loss: 0.6010\n",
      "Training: batch 43 begins at 22:05:43.926270\n",
      "\n",
      "Training: batch 43 ends at 22:05:44.713994\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5982 - bce_dice_loss: 0.5982\n",
      "Training: batch 44 begins at 22:05:44.717444\n",
      "\n",
      "Training: batch 44 ends at 22:05:45.534078\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5956 - bce_dice_loss: 0.5956\n",
      "Training: batch 45 begins at 22:05:45.537416\n",
      "\n",
      "Training: batch 45 ends at 22:05:46.328247\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5952 - bce_dice_loss: 0.5952\n",
      "Training: batch 46 begins at 22:05:46.333569\n",
      "\n",
      "Training: batch 46 ends at 22:05:47.123061\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5963 - bce_dice_loss: 0.5963\n",
      "Training: batch 47 begins at 22:05:47.127230\n",
      "\n",
      "Training: batch 47 ends at 22:05:47.920531\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5950 - bce_dice_loss: 0.5950 \n",
      "Training: batch 48 begins at 22:05:47.924325\n",
      "\n",
      "Training: batch 48 ends at 22:05:48.742586\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5940 - bce_dice_loss: 0.5940\n",
      "Training: batch 49 begins at 22:05:48.746067\n",
      "\n",
      "Training: batch 49 ends at 22:05:49.537703\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5976 - bce_dice_loss: 0.5976\n",
      "Training: batch 50 begins at 22:05:49.541228\n",
      "\n",
      "Training: batch 50 ends at 22:05:50.350835\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5959 - bce_dice_loss: 0.5959\n",
      "Training: batch 51 begins at 22:05:50.353687\n",
      "\n",
      "Training: batch 51 ends at 22:05:51.147142\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5989 - bce_dice_loss: 0.5989\n",
      "Training: batch 52 begins at 22:05:51.151585\n",
      "\n",
      "Training: batch 52 ends at 22:05:51.944404\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5996 - bce_dice_loss: 0.5996\n",
      "Training: batch 53 begins at 22:05:51.949249\n",
      "\n",
      "Training: batch 53 ends at 22:05:52.765348\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5994 - bce_dice_loss: 0.5994\n",
      "Training: batch 54 begins at 22:05:52.768721\n",
      "\n",
      "Training: batch 54 ends at 22:05:53.556522\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5929 - bce_dice_loss: 0.5929\n",
      "Training: batch 55 begins at 22:05:53.560308\n",
      "\n",
      "Training: batch 55 ends at 22:05:54.346454\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5896 - bce_dice_loss: 0.5896\n",
      "Training: batch 56 begins at 22:05:54.351135\n",
      "\n",
      "Training: batch 56 ends at 22:05:55.162427\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5916 - bce_dice_loss: 0.5916\n",
      "Training: batch 57 begins at 22:05:55.167573\n",
      "\n",
      "Training: batch 57 ends at 22:05:55.970822\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5937 - bce_dice_loss: 0.5937\n",
      "Training: batch 58 begins at 22:05:55.973986\n",
      "\n",
      "Training: batch 58 ends at 22:05:56.784705\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5961 - bce_dice_loss: 0.5961\n",
      "Training: batch 59 begins at 22:05:56.788000\n",
      "\n",
      "Training: batch 59 ends at 22:05:57.580628\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5940 - bce_dice_loss: 0.5940\n",
      "Evaluating: batch 0 begins at 22:05:57.614218\n",
      "\n",
      "Evaluating: batch 0 ends at 22:05:57.886875\n",
      "\n",
      "Evaluating: batch 1 begins at 22:05:57.888166\n",
      "\n",
      "Evaluating: batch 1 ends at 22:05:58.101474\n",
      "\n",
      "Evaluating: batch 2 begins at 22:05:58.102729\n",
      "\n",
      "Evaluating: batch 2 ends at 22:05:58.323510\n",
      "\n",
      "Evaluating: batch 3 begins at 22:05:58.324891\n",
      "\n",
      "Evaluating: batch 3 ends at 22:05:58.551706\n",
      "\n",
      "Evaluating: batch 4 begins at 22:05:58.553188\n",
      "\n",
      "Evaluating: batch 4 ends at 22:05:58.778392\n",
      "\n",
      "Evaluating: batch 5 begins at 22:05:58.779831\n",
      "\n",
      "Evaluating: batch 5 ends at 22:05:59.000897\n",
      "\n",
      "Evaluating: batch 6 begins at 22:05:59.002322\n",
      "\n",
      "Evaluating: batch 6 ends at 22:05:59.223113\n",
      "\n",
      "Evaluating: batch 7 begins at 22:05:59.224921\n",
      "\n",
      "Evaluating: batch 7 ends at 22:05:59.441648\n",
      "\n",
      "Evaluating: batch 8 begins at 22:05:59.442867\n",
      "\n",
      "Evaluating: batch 8 ends at 22:05:59.666095\n",
      "\n",
      "Evaluating: batch 9 begins at 22:05:59.667485\n",
      "\n",
      "Evaluating: batch 9 ends at 22:05:59.889691\n",
      "\n",
      "Evaluating: batch 10 begins at 22:05:59.890940\n",
      "\n",
      "Evaluating: batch 10 ends at 22:06:00.115535\n",
      "\n",
      "Evaluating: batch 11 begins at 22:06:00.117330\n",
      "\n",
      "Evaluating: batch 11 ends at 22:06:00.336205\n",
      "\n",
      "Evaluating: batch 12 begins at 22:06:00.338535\n",
      "\n",
      "Evaluating: batch 12 ends at 22:06:00.553953\n",
      "\n",
      "Evaluating: batch 13 begins at 22:06:00.555210\n",
      "\n",
      "Evaluating: batch 13 ends at 22:06:00.774831\n",
      "\n",
      "Evaluating: batch 14 begins at 22:06:00.776593\n",
      "\n",
      "Evaluating: batch 14 ends at 22:06:00.996912\n",
      "\n",
      "Evaluating: batch 15 begins at 22:06:00.998393\n",
      "\n",
      "Evaluating: batch 15 ends at 22:06:01.219239\n",
      "\n",
      "Evaluating: batch 16 begins at 22:06:01.220795\n",
      "\n",
      "Evaluating: batch 16 ends at 22:06:01.442267\n",
      "\n",
      "Evaluating: batch 17 begins at 22:06:01.444157\n",
      "\n",
      "Evaluating: batch 17 ends at 22:06:01.666965\n",
      "\n",
      "Evaluating: batch 18 begins at 22:06:01.668506\n",
      "\n",
      "Evaluating: batch 18 ends at 22:06:01.892595\n",
      "\n",
      "Evaluating: batch 19 begins at 22:06:01.894144\n",
      "\n",
      "Evaluating: batch 19 ends at 22:06:02.110745\n",
      "\n",
      "Evaluating: batch 20 begins at 22:06:02.111991\n",
      "\n",
      "Evaluating: batch 20 ends at 22:06:02.326566\n",
      "\n",
      "Evaluating: batch 21 begins at 22:06:02.327794\n",
      "\n",
      "Evaluating: batch 21 ends at 22:06:02.550026\n",
      "\n",
      "Evaluating: batch 22 begins at 22:06:02.551626\n",
      "\n",
      "Evaluating: batch 22 ends at 22:06:02.768988\n",
      "\n",
      "Evaluating: batch 23 begins at 22:06:02.770308\n",
      "\n",
      "Evaluating: batch 23 ends at 22:06:02.987345\n",
      "\n",
      "Evaluating: batch 24 begins at 22:06:02.988601\n",
      "\n",
      "Evaluating: batch 24 ends at 22:06:03.206345\n",
      "\n",
      "Evaluating: batch 25 begins at 22:06:03.207752\n",
      "\n",
      "Evaluating: batch 25 ends at 22:06:03.425637\n",
      "\n",
      "Evaluating: batch 26 begins at 22:06:03.427670\n",
      "\n",
      "Evaluating: batch 26 ends at 22:06:03.643399\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.60325 to 0.55298, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 933ms/step - loss: 0.5940 - bce_dice_loss: 0.5940 - val_loss: 0.5530 - val_bce_dice_loss: 0.5530\n",
      "Epoch 14/25\n",
      "\n",
      "Training: batch 0 begins at 22:06:05.293244\n",
      "\n",
      "Training: batch 0 ends at 22:06:06.105286\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3608 - bce_dice_loss: 0.3608\n",
      "Training: batch 1 begins at 22:06:06.111889\n",
      "\n",
      "Training: batch 1 ends at 22:06:06.903032\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.4586 - bce_dice_loss: 0.4586\n",
      "Training: batch 2 begins at 22:06:06.906576\n",
      "\n",
      "Training: batch 2 ends at 22:06:07.699440\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.4406 - bce_dice_loss: 0.4406\n",
      "Training: batch 3 begins at 22:06:07.704350\n",
      "\n",
      "Training: batch 3 ends at 22:06:08.506767\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.4574 - bce_dice_loss: 0.4574\n",
      "Training: batch 4 begins at 22:06:08.510697\n",
      "\n",
      "Training: batch 4 ends at 22:06:09.319089\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4108 - bce_dice_loss: 0.4108\n",
      "Training: batch 5 begins at 22:06:09.323526\n",
      "\n",
      "Training: batch 5 ends at 22:06:10.121219\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4484 - bce_dice_loss: 0.4484\n",
      "Training: batch 6 begins at 22:06:10.125628\n",
      "\n",
      "Training: batch 6 ends at 22:06:10.916365\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4753 - bce_dice_loss: 0.4753\n",
      "Training: batch 7 begins at 22:06:10.919956\n",
      "\n",
      "Training: batch 7 ends at 22:06:11.713540\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5153 - bce_dice_loss: 0.5153\n",
      "Training: batch 8 begins at 22:06:11.717191\n",
      "\n",
      "Training: batch 8 ends at 22:06:12.510561\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5122 - bce_dice_loss: 0.5122\n",
      "Training: batch 9 begins at 22:06:12.513376\n",
      "\n",
      "Training: batch 9 ends at 22:06:13.330066\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4935 - bce_dice_loss: 0.4935\n",
      "Training: batch 10 begins at 22:06:13.334258\n",
      "\n",
      "Training: batch 10 ends at 22:06:14.122397\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4995 - bce_dice_loss: 0.4995\n",
      "Training: batch 11 begins at 22:06:14.125507\n",
      "\n",
      "Training: batch 11 ends at 22:06:14.917346\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5238 - bce_dice_loss: 0.5238\n",
      "Training: batch 12 begins at 22:06:14.921704\n",
      "\n",
      "Training: batch 12 ends at 22:06:15.740184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5262 - bce_dice_loss: 0.5262\n",
      "Training: batch 13 begins at 22:06:15.743611\n",
      "\n",
      "Training: batch 13 ends at 22:06:16.539201\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5495 - bce_dice_loss: 0.5495\n",
      "Training: batch 14 begins at 22:06:16.543371\n",
      "\n",
      "Training: batch 14 ends at 22:06:17.357300\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5545 - bce_dice_loss: 0.5545\n",
      "Training: batch 15 begins at 22:06:17.360587\n",
      "\n",
      "Training: batch 15 ends at 22:06:18.180118\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5540 - bce_dice_loss: 0.5540\n",
      "Training: batch 16 begins at 22:06:18.182589\n",
      "\n",
      "Training: batch 16 ends at 22:06:19.020854\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5519 - bce_dice_loss: 0.5519\n",
      "Training: batch 17 begins at 22:06:19.024870\n",
      "\n",
      "Training: batch 17 ends at 22:06:19.819631\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5542 - bce_dice_loss: 0.5542\n",
      "Training: batch 18 begins at 22:06:19.823400\n",
      "\n",
      "Training: batch 18 ends at 22:06:20.639762\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5633 - bce_dice_loss: 0.5633\n",
      "Training: batch 19 begins at 22:06:20.643027\n",
      "\n",
      "Training: batch 19 ends at 22:06:21.440226\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5676 - bce_dice_loss: 0.5676\n",
      "Training: batch 20 begins at 22:06:21.445655\n",
      "\n",
      "Training: batch 20 ends at 22:06:22.244301\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5590 - bce_dice_loss: 0.5590\n",
      "Training: batch 21 begins at 22:06:22.248149\n",
      "\n",
      "Training: batch 21 ends at 22:06:23.044044\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5753 - bce_dice_loss: 0.5753\n",
      "Training: batch 22 begins at 22:06:23.048288\n",
      "\n",
      "Training: batch 22 ends at 22:06:23.838098\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5731 - bce_dice_loss: 0.5731\n",
      "Training: batch 23 begins at 22:06:23.841905\n",
      "\n",
      "Training: batch 23 ends at 22:06:24.630994\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5725 - bce_dice_loss: 0.5725\n",
      "Training: batch 24 begins at 22:06:24.634825\n",
      "\n",
      "Training: batch 24 ends at 22:06:25.417822\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5703 - bce_dice_loss: 0.5703\n",
      "Training: batch 25 begins at 22:06:25.423602\n",
      "\n",
      "Training: batch 25 ends at 22:06:26.241821\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5683 - bce_dice_loss: 0.5683\n",
      "Training: batch 26 begins at 22:06:26.247729\n",
      "\n",
      "Training: batch 26 ends at 22:06:27.044566\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5724 - bce_dice_loss: 0.5724\n",
      "Training: batch 27 begins at 22:06:27.048031\n",
      "\n",
      "Training: batch 27 ends at 22:06:27.840329\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5763 - bce_dice_loss: 0.5763\n",
      "Training: batch 28 begins at 22:06:27.843353\n",
      "\n",
      "Training: batch 28 ends at 22:06:28.643584\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5823 - bce_dice_loss: 0.5823\n",
      "Training: batch 29 begins at 22:06:28.647543\n",
      "\n",
      "Training: batch 29 ends at 22:06:29.470057\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5843 - bce_dice_loss: 0.5843\n",
      "Training: batch 30 begins at 22:06:29.476287\n",
      "\n",
      "Training: batch 30 ends at 22:06:30.267910\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5831 - bce_dice_loss: 0.5831\n",
      "Training: batch 31 begins at 22:06:30.272167\n",
      "\n",
      "Training: batch 31 ends at 22:06:31.069834\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5870 - bce_dice_loss: 0.5870\n",
      "Training: batch 32 begins at 22:06:31.073523\n",
      "\n",
      "Training: batch 32 ends at 22:06:31.879683\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5868 - bce_dice_loss: 0.5868\n",
      "Training: batch 33 begins at 22:06:31.884183\n",
      "\n",
      "Training: batch 33 ends at 22:06:32.678400\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5873 - bce_dice_loss: 0.5873\n",
      "Training: batch 34 begins at 22:06:32.682732\n",
      "\n",
      "Training: batch 34 ends at 22:06:33.464902\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5816 - bce_dice_loss: 0.5816\n",
      "Training: batch 35 begins at 22:06:33.469295\n",
      "\n",
      "Training: batch 35 ends at 22:06:34.259954\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5775 - bce_dice_loss: 0.5775\n",
      "Training: batch 36 begins at 22:06:34.263014\n",
      "\n",
      "Training: batch 36 ends at 22:06:35.057034\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5785 - bce_dice_loss: 0.5785\n",
      "Training: batch 37 begins at 22:06:35.060362\n",
      "\n",
      "Training: batch 37 ends at 22:06:35.856022\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5772 - bce_dice_loss: 0.5772\n",
      "Training: batch 38 begins at 22:06:35.859553\n",
      "\n",
      "Training: batch 38 ends at 22:06:36.672302\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5765 - bce_dice_loss: 0.5765\n",
      "Training: batch 39 begins at 22:06:36.675808\n",
      "\n",
      "Training: batch 39 ends at 22:06:37.467516\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5769 - bce_dice_loss: 0.5769\n",
      "Training: batch 40 begins at 22:06:37.471213\n",
      "\n",
      "Training: batch 40 ends at 22:06:38.266540\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5762 - bce_dice_loss: 0.5762\n",
      "Training: batch 41 begins at 22:06:38.269169\n",
      "\n",
      "Training: batch 41 ends at 22:06:39.096516\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5749 - bce_dice_loss: 0.5749\n",
      "Training: batch 42 begins at 22:06:39.100994\n",
      "\n",
      "Training: batch 42 ends at 22:06:39.895343\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5688 - bce_dice_loss: 0.5688\n",
      "Training: batch 43 begins at 22:06:39.899861\n",
      "\n",
      "Training: batch 43 ends at 22:06:40.704936\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5676 - bce_dice_loss: 0.5676\n",
      "Training: batch 44 begins at 22:06:40.708725\n",
      "\n",
      "Training: batch 44 ends at 22:06:41.509738\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5689 - bce_dice_loss: 0.5689\n",
      "Training: batch 45 begins at 22:06:41.512112\n",
      "\n",
      "Training: batch 45 ends at 22:06:42.305901\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5679 - bce_dice_loss: 0.5679\n",
      "Training: batch 46 begins at 22:06:42.309240\n",
      "\n",
      "Training: batch 46 ends at 22:06:43.130744\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5693 - bce_dice_loss: 0.5693\n",
      "Training: batch 47 begins at 22:06:43.135262\n",
      "\n",
      "Training: batch 47 ends at 22:06:43.931471\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5767 - bce_dice_loss: 0.5767 \n",
      "Training: batch 48 begins at 22:06:43.936374\n",
      "\n",
      "Training: batch 48 ends at 22:06:44.743944\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5754 - bce_dice_loss: 0.5754\n",
      "Training: batch 49 begins at 22:06:44.747161\n",
      "\n",
      "Training: batch 49 ends at 22:06:45.532593\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5763 - bce_dice_loss: 0.5763\n",
      "Training: batch 50 begins at 22:06:45.536975\n",
      "\n",
      "Training: batch 50 ends at 22:06:46.325156\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5719 - bce_dice_loss: 0.5719\n",
      "Training: batch 51 begins at 22:06:46.329594\n",
      "\n",
      "Training: batch 51 ends at 22:06:47.113930\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5691 - bce_dice_loss: 0.5691\n",
      "Training: batch 52 begins at 22:06:47.116912\n",
      "\n",
      "Training: batch 52 ends at 22:06:47.922004\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5695 - bce_dice_loss: 0.5695\n",
      "Training: batch 53 begins at 22:06:47.925397\n",
      "\n",
      "Training: batch 53 ends at 22:06:48.724136\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5680 - bce_dice_loss: 0.5680\n",
      "Training: batch 54 begins at 22:06:48.728984\n",
      "\n",
      "Training: batch 54 ends at 22:06:49.521292\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5676 - bce_dice_loss: 0.5676\n",
      "Training: batch 55 begins at 22:06:49.524497\n",
      "\n",
      "Training: batch 55 ends at 22:06:50.343717\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5723 - bce_dice_loss: 0.5723\n",
      "Training: batch 56 begins at 22:06:50.348096\n",
      "\n",
      "Training: batch 56 ends at 22:06:51.142333\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5743 - bce_dice_loss: 0.5743\n",
      "Training: batch 57 begins at 22:06:51.146336\n",
      "\n",
      "Training: batch 57 ends at 22:06:51.939861\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5724 - bce_dice_loss: 0.5724\n",
      "Training: batch 58 begins at 22:06:51.943140\n",
      "\n",
      "Training: batch 58 ends at 22:06:52.733425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.5723 - bce_dice_loss: 0.5723\n",
      "Training: batch 59 begins at 22:06:52.738623\n",
      "\n",
      "Training: batch 59 ends at 22:06:53.528637\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5710 - bce_dice_loss: 0.5710\n",
      "Evaluating: batch 0 begins at 22:06:53.563275\n",
      "\n",
      "Evaluating: batch 0 ends at 22:06:53.833424\n",
      "\n",
      "Evaluating: batch 1 begins at 22:06:53.834843\n",
      "\n",
      "Evaluating: batch 1 ends at 22:06:54.048026\n",
      "\n",
      "Evaluating: batch 2 begins at 22:06:54.049571\n",
      "\n",
      "Evaluating: batch 2 ends at 22:06:54.266365\n",
      "\n",
      "Evaluating: batch 3 begins at 22:06:54.267613\n",
      "\n",
      "Evaluating: batch 3 ends at 22:06:54.484039\n",
      "\n",
      "Evaluating: batch 4 begins at 22:06:54.486050\n",
      "\n",
      "Evaluating: batch 4 ends at 22:06:54.706221\n",
      "\n",
      "Evaluating: batch 5 begins at 22:06:54.707463\n",
      "\n",
      "Evaluating: batch 5 ends at 22:06:54.933445\n",
      "\n",
      "Evaluating: batch 6 begins at 22:06:54.934883\n",
      "\n",
      "Evaluating: batch 6 ends at 22:06:55.157968\n",
      "\n",
      "Evaluating: batch 7 begins at 22:06:55.159550\n",
      "\n",
      "Evaluating: batch 7 ends at 22:06:55.383937\n",
      "\n",
      "Evaluating: batch 8 begins at 22:06:55.385275\n",
      "\n",
      "Evaluating: batch 8 ends at 22:06:55.607427\n",
      "\n",
      "Evaluating: batch 9 begins at 22:06:55.608806\n",
      "\n",
      "Evaluating: batch 9 ends at 22:06:55.831374\n",
      "\n",
      "Evaluating: batch 10 begins at 22:06:55.833054\n",
      "\n",
      "Evaluating: batch 10 ends at 22:06:56.050805\n",
      "\n",
      "Evaluating: batch 11 begins at 22:06:56.052209\n",
      "\n",
      "Evaluating: batch 11 ends at 22:06:56.271375\n",
      "\n",
      "Evaluating: batch 12 begins at 22:06:56.272682\n",
      "\n",
      "Evaluating: batch 12 ends at 22:06:56.492001\n",
      "\n",
      "Evaluating: batch 13 begins at 22:06:56.493482\n",
      "\n",
      "Evaluating: batch 13 ends at 22:06:56.715135\n",
      "\n",
      "Evaluating: batch 14 begins at 22:06:56.716784\n",
      "\n",
      "Evaluating: batch 14 ends at 22:06:56.937301\n",
      "\n",
      "Evaluating: batch 15 begins at 22:06:56.938881\n",
      "\n",
      "Evaluating: batch 15 ends at 22:06:57.160415\n",
      "\n",
      "Evaluating: batch 16 begins at 22:06:57.161999\n",
      "\n",
      "Evaluating: batch 16 ends at 22:06:57.384314\n",
      "\n",
      "Evaluating: batch 17 begins at 22:06:57.385885\n",
      "\n",
      "Evaluating: batch 17 ends at 22:06:57.606736\n",
      "\n",
      "Evaluating: batch 18 begins at 22:06:57.607967\n",
      "\n",
      "Evaluating: batch 18 ends at 22:06:57.825468\n",
      "\n",
      "Evaluating: batch 19 begins at 22:06:57.826822\n",
      "\n",
      "Evaluating: batch 19 ends at 22:06:58.038777\n",
      "\n",
      "Evaluating: batch 20 begins at 22:06:58.040058\n",
      "\n",
      "Evaluating: batch 20 ends at 22:06:58.255349\n",
      "\n",
      "Evaluating: batch 21 begins at 22:06:58.256616\n",
      "\n",
      "Evaluating: batch 21 ends at 22:06:58.474705\n",
      "\n",
      "Evaluating: batch 22 begins at 22:06:58.476165\n",
      "\n",
      "Evaluating: batch 22 ends at 22:06:58.696680\n",
      "\n",
      "Evaluating: batch 23 begins at 22:06:58.698366\n",
      "\n",
      "Evaluating: batch 23 ends at 22:06:58.920019\n",
      "\n",
      "Evaluating: batch 24 begins at 22:06:58.921626\n",
      "\n",
      "Evaluating: batch 24 ends at 22:06:59.139867\n",
      "\n",
      "Evaluating: batch 25 begins at 22:06:59.142142\n",
      "\n",
      "Evaluating: batch 25 ends at 22:06:59.358207\n",
      "\n",
      "Evaluating: batch 26 begins at 22:06:59.359437\n",
      "\n",
      "Evaluating: batch 26 ends at 22:06:59.574720\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.55298\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.5710 - bce_dice_loss: 0.5710 - val_loss: 0.5681 - val_bce_dice_loss: 0.5681\n",
      "Epoch 15/25\n",
      "\n",
      "Training: batch 0 begins at 22:06:59.604481\n",
      "\n",
      "Training: batch 0 ends at 22:07:00.403514\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.7593 - bce_dice_loss: 0.7593\n",
      "Training: batch 1 begins at 22:07:00.407873\n",
      "\n",
      "Training: batch 1 ends at 22:07:01.203749\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.6345 - bce_dice_loss: 0.6345\n",
      "Training: batch 2 begins at 22:07:01.207979\n",
      "\n",
      "Training: batch 2 ends at 22:07:02.004010\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.6182 - bce_dice_loss: 0.6182\n",
      "Training: batch 3 begins at 22:07:02.007440\n",
      "\n",
      "Training: batch 3 ends at 22:07:02.799164\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.5907 - bce_dice_loss: 0.5907\n",
      "Training: batch 4 begins at 22:07:02.804013\n",
      "\n",
      "Training: batch 4 ends at 22:07:03.597584\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.5918 - bce_dice_loss: 0.5918\n",
      "Training: batch 5 begins at 22:07:03.601989\n",
      "\n",
      "Training: batch 5 ends at 22:07:04.403198\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6118 - bce_dice_loss: 0.6118\n",
      "Training: batch 6 begins at 22:07:04.406600\n",
      "\n",
      "Training: batch 6 ends at 22:07:05.199489\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5574 - bce_dice_loss: 0.5574\n",
      "Training: batch 7 begins at 22:07:05.204006\n",
      "\n",
      "Training: batch 7 ends at 22:07:05.989913\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5781 - bce_dice_loss: 0.5781\n",
      "Training: batch 8 begins at 22:07:05.993262\n",
      "\n",
      "Training: batch 8 ends at 22:07:06.789438\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5831 - bce_dice_loss: 0.5831\n",
      "Training: batch 9 begins at 22:07:06.793407\n",
      "\n",
      "Training: batch 9 ends at 22:07:07.611226\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5792 - bce_dice_loss: 0.5792\n",
      "Training: batch 10 begins at 22:07:07.615230\n",
      "\n",
      "Training: batch 10 ends at 22:07:08.416240\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5798 - bce_dice_loss: 0.5798\n",
      "Training: batch 11 begins at 22:07:08.419925\n",
      "\n",
      "Training: batch 11 ends at 22:07:09.234713\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5602 - bce_dice_loss: 0.5602\n",
      "Training: batch 12 begins at 22:07:09.238206\n",
      "\n",
      "Training: batch 12 ends at 22:07:10.033384\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5765 - bce_dice_loss: 0.5765\n",
      "Training: batch 13 begins at 22:07:10.037896\n",
      "\n",
      "Training: batch 13 ends at 22:07:10.828840\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5778 - bce_dice_loss: 0.5778\n",
      "Training: batch 14 begins at 22:07:10.833236\n",
      "\n",
      "Training: batch 14 ends at 22:07:11.636439\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5730 - bce_dice_loss: 0.5730\n",
      "Training: batch 15 begins at 22:07:11.640163\n",
      "\n",
      "Training: batch 15 ends at 22:07:12.438684\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5780 - bce_dice_loss: 0.5780\n",
      "Training: batch 16 begins at 22:07:12.441626\n",
      "\n",
      "Training: batch 16 ends at 22:07:13.237437\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5757 - bce_dice_loss: 0.5757\n",
      "Training: batch 17 begins at 22:07:13.241150\n",
      "\n",
      "Training: batch 17 ends at 22:07:14.030900\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5769 - bce_dice_loss: 0.5769\n",
      "Training: batch 18 begins at 22:07:14.035324\n",
      "\n",
      "Training: batch 18 ends at 22:07:14.831547\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5941 - bce_dice_loss: 0.5941\n",
      "Training: batch 19 begins at 22:07:14.836039\n",
      "\n",
      "Training: batch 19 ends at 22:07:15.624809\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5915 - bce_dice_loss: 0.5915\n",
      "Training: batch 20 begins at 22:07:15.629096\n",
      "\n",
      "Training: batch 20 ends at 22:07:16.435655\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5942 - bce_dice_loss: 0.5942\n",
      "Training: batch 21 begins at 22:07:16.438888\n",
      "\n",
      "Training: batch 21 ends at 22:07:17.231169\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5945 - bce_dice_loss: 0.5945\n",
      "Training: batch 22 begins at 22:07:17.235638\n",
      "\n",
      "Training: batch 22 ends at 22:07:18.035748\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5914 - bce_dice_loss: 0.5914\n",
      "Training: batch 23 begins at 22:07:18.039397\n",
      "\n",
      "Training: batch 23 ends at 22:07:18.838573\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.6025 - bce_dice_loss: 0.6025\n",
      "Training: batch 24 begins at 22:07:18.842244\n",
      "\n",
      "Training: batch 24 ends at 22:07:19.635738\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5951 - bce_dice_loss: 0.5951\n",
      "Training: batch 25 begins at 22:07:19.639057\n",
      "\n",
      "Training: batch 25 ends at 22:07:20.433291\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5865 - bce_dice_loss: 0.5865\n",
      "Training: batch 26 begins at 22:07:20.441164\n",
      "\n",
      "Training: batch 26 ends at 22:07:21.227909\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5858 - bce_dice_loss: 0.5858\n",
      "Training: batch 27 begins at 22:07:21.232433\n",
      "\n",
      "Training: batch 27 ends at 22:07:22.033799\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5783 - bce_dice_loss: 0.5783\n",
      "Training: batch 28 begins at 22:07:22.039516\n",
      "\n",
      "Training: batch 28 ends at 22:07:22.833907\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5773 - bce_dice_loss: 0.5773\n",
      "Training: batch 29 begins at 22:07:22.837754\n",
      "\n",
      "Training: batch 29 ends at 22:07:23.627276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5750 - bce_dice_loss: 0.5750\n",
      "Training: batch 30 begins at 22:07:23.631647\n",
      "\n",
      "Training: batch 30 ends at 22:07:24.448349\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5771 - bce_dice_loss: 0.5771\n",
      "Training: batch 31 begins at 22:07:24.453556\n",
      "\n",
      "Training: batch 31 ends at 22:07:25.249160\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5782 - bce_dice_loss: 0.5782\n",
      "Training: batch 32 begins at 22:07:25.254129\n",
      "\n",
      "Training: batch 32 ends at 22:07:26.054978\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5758 - bce_dice_loss: 0.5758\n",
      "Training: batch 33 begins at 22:07:26.058328\n",
      "\n",
      "Training: batch 33 ends at 22:07:26.849107\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5739 - bce_dice_loss: 0.5739\n",
      "Training: batch 34 begins at 22:07:26.853885\n",
      "\n",
      "Training: batch 34 ends at 22:07:27.647803\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5715 - bce_dice_loss: 0.5715\n",
      "Training: batch 35 begins at 22:07:27.653226\n",
      "\n",
      "Training: batch 35 ends at 22:07:28.451829\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5775 - bce_dice_loss: 0.5775\n",
      "Training: batch 36 begins at 22:07:28.455827\n",
      "\n",
      "Training: batch 36 ends at 22:07:29.264143\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5763 - bce_dice_loss: 0.5763\n",
      "Training: batch 37 begins at 22:07:29.268739\n",
      "\n",
      "Training: batch 37 ends at 22:07:30.062084\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5760 - bce_dice_loss: 0.5760\n",
      "Training: batch 38 begins at 22:07:30.066230\n",
      "\n",
      "Training: batch 38 ends at 22:07:30.886724\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5731 - bce_dice_loss: 0.5731\n",
      "Training: batch 39 begins at 22:07:30.890266\n",
      "\n",
      "Training: batch 39 ends at 22:07:31.683065\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5734 - bce_dice_loss: 0.5734\n",
      "Training: batch 40 begins at 22:07:31.687606\n",
      "\n",
      "Training: batch 40 ends at 22:07:32.479782\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5699 - bce_dice_loss: 0.5699\n",
      "Training: batch 41 begins at 22:07:32.484185\n",
      "\n",
      "Training: batch 41 ends at 22:07:33.278183\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5677 - bce_dice_loss: 0.5677\n",
      "Training: batch 42 begins at 22:07:33.282284\n",
      "\n",
      "Training: batch 42 ends at 22:07:34.089131\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5694 - bce_dice_loss: 0.5694\n",
      "Training: batch 43 begins at 22:07:34.092125\n",
      "\n",
      "Training: batch 43 ends at 22:07:34.884152\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5723 - bce_dice_loss: 0.5723\n",
      "Training: batch 44 begins at 22:07:34.888531\n",
      "\n",
      "Training: batch 44 ends at 22:07:35.679068\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5767 - bce_dice_loss: 0.5767\n",
      "Training: batch 45 begins at 22:07:35.683540\n",
      "\n",
      "Training: batch 45 ends at 22:07:36.485406\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5796 - bce_dice_loss: 0.5796\n",
      "Training: batch 46 begins at 22:07:36.489911\n",
      "\n",
      "Training: batch 46 ends at 22:07:37.299296\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5765 - bce_dice_loss: 0.5765\n",
      "Training: batch 47 begins at 22:07:37.303917\n",
      "\n",
      "Training: batch 47 ends at 22:07:38.098115\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5766 - bce_dice_loss: 0.5766 \n",
      "Training: batch 48 begins at 22:07:38.102783\n",
      "\n",
      "Training: batch 48 ends at 22:07:38.892619\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5775 - bce_dice_loss: 0.5775\n",
      "Training: batch 49 begins at 22:07:38.896037\n",
      "\n",
      "Training: batch 49 ends at 22:07:39.694016\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5755 - bce_dice_loss: 0.5755\n",
      "Training: batch 50 begins at 22:07:39.697424\n",
      "\n",
      "Training: batch 50 ends at 22:07:40.493857\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5718 - bce_dice_loss: 0.5718\n",
      "Training: batch 51 begins at 22:07:40.497179\n",
      "\n",
      "Training: batch 51 ends at 22:07:41.297684\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5724 - bce_dice_loss: 0.5724\n",
      "Training: batch 52 begins at 22:07:41.302337\n",
      "\n",
      "Training: batch 52 ends at 22:07:42.105195\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5723 - bce_dice_loss: 0.5723\n",
      "Training: batch 53 begins at 22:07:42.109467\n",
      "\n",
      "Training: batch 53 ends at 22:07:42.899815\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5708 - bce_dice_loss: 0.5708\n",
      "Training: batch 54 begins at 22:07:42.903852\n",
      "\n",
      "Training: batch 54 ends at 22:07:43.693029\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5720 - bce_dice_loss: 0.5720\n",
      "Training: batch 55 begins at 22:07:43.697369\n",
      "\n",
      "Training: batch 55 ends at 22:07:44.515250\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5702 - bce_dice_loss: 0.5702\n",
      "Training: batch 56 begins at 22:07:44.520135\n",
      "\n",
      "Training: batch 56 ends at 22:07:45.312056\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5705 - bce_dice_loss: 0.5705\n",
      "Training: batch 57 begins at 22:07:45.317222\n",
      "\n",
      "Training: batch 57 ends at 22:07:46.107928\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5703 - bce_dice_loss: 0.5703\n",
      "Training: batch 58 begins at 22:07:46.112340\n",
      "\n",
      "Training: batch 58 ends at 22:07:46.925831\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5671 - bce_dice_loss: 0.5671\n",
      "Training: batch 59 begins at 22:07:46.930053\n",
      "\n",
      "Training: batch 59 ends at 22:07:47.720652\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5673 - bce_dice_loss: 0.5673\n",
      "Evaluating: batch 0 begins at 22:07:47.753036\n",
      "\n",
      "Evaluating: batch 0 ends at 22:07:48.040493\n",
      "\n",
      "Evaluating: batch 1 begins at 22:07:48.042187\n",
      "\n",
      "Evaluating: batch 1 ends at 22:07:48.254475\n",
      "\n",
      "Evaluating: batch 2 begins at 22:07:48.255953\n",
      "\n",
      "Evaluating: batch 2 ends at 22:07:48.471824\n",
      "\n",
      "Evaluating: batch 3 begins at 22:07:48.473080\n",
      "\n",
      "Evaluating: batch 3 ends at 22:07:48.697690\n",
      "\n",
      "Evaluating: batch 4 begins at 22:07:48.699026\n",
      "\n",
      "Evaluating: batch 4 ends at 22:07:48.920665\n",
      "\n",
      "Evaluating: batch 5 begins at 22:07:48.922485\n",
      "\n",
      "Evaluating: batch 5 ends at 22:07:49.141732\n",
      "\n",
      "Evaluating: batch 6 begins at 22:07:49.143288\n",
      "\n",
      "Evaluating: batch 6 ends at 22:07:49.363192\n",
      "\n",
      "Evaluating: batch 7 begins at 22:07:49.364676\n",
      "\n",
      "Evaluating: batch 7 ends at 22:07:49.583407\n",
      "\n",
      "Evaluating: batch 8 begins at 22:07:49.584815\n",
      "\n",
      "Evaluating: batch 8 ends at 22:07:49.807436\n",
      "\n",
      "Evaluating: batch 9 begins at 22:07:49.808849\n",
      "\n",
      "Evaluating: batch 9 ends at 22:07:50.029700\n",
      "\n",
      "Evaluating: batch 10 begins at 22:07:50.031464\n",
      "\n",
      "Evaluating: batch 10 ends at 22:07:50.250887\n",
      "\n",
      "Evaluating: batch 11 begins at 22:07:50.252165\n",
      "\n",
      "Evaluating: batch 11 ends at 22:07:50.468713\n",
      "\n",
      "Evaluating: batch 12 begins at 22:07:50.469966\n",
      "\n",
      "Evaluating: batch 12 ends at 22:07:50.687729\n",
      "\n",
      "Evaluating: batch 13 begins at 22:07:50.689925\n",
      "\n",
      "Evaluating: batch 13 ends at 22:07:50.915761\n",
      "\n",
      "Evaluating: batch 14 begins at 22:07:50.917158\n",
      "\n",
      "Evaluating: batch 14 ends at 22:07:51.139436\n",
      "\n",
      "Evaluating: batch 15 begins at 22:07:51.140720\n",
      "\n",
      "Evaluating: batch 15 ends at 22:07:51.361554\n",
      "\n",
      "Evaluating: batch 16 begins at 22:07:51.362880\n",
      "\n",
      "Evaluating: batch 16 ends at 22:07:51.584935\n",
      "\n",
      "Evaluating: batch 17 begins at 22:07:51.586376\n",
      "\n",
      "Evaluating: batch 17 ends at 22:07:51.806969\n",
      "\n",
      "Evaluating: batch 18 begins at 22:07:51.808339\n",
      "\n",
      "Evaluating: batch 18 ends at 22:07:52.030756\n",
      "\n",
      "Evaluating: batch 19 begins at 22:07:52.032652\n",
      "\n",
      "Evaluating: batch 19 ends at 22:07:52.250533\n",
      "\n",
      "Evaluating: batch 20 begins at 22:07:52.251776\n",
      "\n",
      "Evaluating: batch 20 ends at 22:07:52.469464\n",
      "\n",
      "Evaluating: batch 21 begins at 22:07:52.470733\n",
      "\n",
      "Evaluating: batch 21 ends at 22:07:52.686622\n",
      "\n",
      "Evaluating: batch 22 begins at 22:07:52.688029\n",
      "\n",
      "Evaluating: batch 22 ends at 22:07:52.907495\n",
      "\n",
      "Evaluating: batch 23 begins at 22:07:52.909524\n",
      "\n",
      "Evaluating: batch 23 ends at 22:07:53.129810\n",
      "\n",
      "Evaluating: batch 24 begins at 22:07:53.132011\n",
      "\n",
      "Evaluating: batch 24 ends at 22:07:53.351270\n",
      "\n",
      "Evaluating: batch 25 begins at 22:07:53.352573\n",
      "\n",
      "Evaluating: batch 25 ends at 22:07:53.569594\n",
      "\n",
      "Evaluating: batch 26 begins at 22:07:53.570979\n",
      "\n",
      "Evaluating: batch 26 ends at 22:07:53.790999\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.55298 to 0.54892, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 933ms/step - loss: 0.5673 - bce_dice_loss: 0.5673 - val_loss: 0.5489 - val_bce_dice_loss: 0.5489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "\n",
      "Training: batch 0 begins at 22:07:55.450713\n",
      "\n",
      "Training: batch 0 ends at 22:07:56.266229\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.5127 - bce_dice_loss: 0.5127\n",
      "Training: batch 1 begins at 22:07:56.271386\n",
      "\n",
      "Training: batch 1 ends at 22:07:57.080322\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.5135 - bce_dice_loss: 0.5135\n",
      "Training: batch 2 begins at 22:07:57.085270\n",
      "\n",
      "Training: batch 2 ends at 22:07:57.887095\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4377 - bce_dice_loss: 0.4377\n",
      "Training: batch 3 begins at 22:07:57.890402\n",
      "\n",
      "Training: batch 3 ends at 22:07:58.680781\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5220 - bce_dice_loss: 0.5220\n",
      "Training: batch 4 begins at 22:07:58.683439\n",
      "\n",
      "Training: batch 4 ends at 22:07:59.491372\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4873 - bce_dice_loss: 0.4873\n",
      "Training: batch 5 begins at 22:07:59.494865\n",
      "\n",
      "Training: batch 5 ends at 22:08:00.294443\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4732 - bce_dice_loss: 0.4732\n",
      "Training: batch 6 begins at 22:08:00.298606\n",
      "\n",
      "Training: batch 6 ends at 22:08:01.088022\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5443 - bce_dice_loss: 0.5443\n",
      "Training: batch 7 begins at 22:08:01.091437\n",
      "\n",
      "Training: batch 7 ends at 22:08:01.888522\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5373 - bce_dice_loss: 0.5373\n",
      "Training: batch 8 begins at 22:08:01.891889\n",
      "\n",
      "Training: batch 8 ends at 22:08:02.687279\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5385 - bce_dice_loss: 0.5385\n",
      "Training: batch 9 begins at 22:08:02.691508\n",
      "\n",
      "Training: batch 9 ends at 22:08:03.492282\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5291 - bce_dice_loss: 0.5291\n",
      "Training: batch 10 begins at 22:08:03.496479\n",
      "\n",
      "Training: batch 10 ends at 22:08:04.291777\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5568 - bce_dice_loss: 0.5568\n",
      "Training: batch 11 begins at 22:08:04.295040\n",
      "\n",
      "Training: batch 11 ends at 22:08:05.079269\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5573 - bce_dice_loss: 0.5573\n",
      "Training: batch 12 begins at 22:08:05.084232\n",
      "\n",
      "Training: batch 12 ends at 22:08:05.878471\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5476 - bce_dice_loss: 0.5476\n",
      "Training: batch 13 begins at 22:08:05.883887\n",
      "\n",
      "Training: batch 13 ends at 22:08:06.680025\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5458 - bce_dice_loss: 0.5458\n",
      "Training: batch 14 begins at 22:08:06.685147\n",
      "\n",
      "Training: batch 14 ends at 22:08:07.471573\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5528 - bce_dice_loss: 0.5528\n",
      "Training: batch 15 begins at 22:08:07.474728\n",
      "\n",
      "Training: batch 15 ends at 22:08:08.299394\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5553 - bce_dice_loss: 0.5553\n",
      "Training: batch 16 begins at 22:08:08.303867\n",
      "\n",
      "Training: batch 16 ends at 22:08:09.103368\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5510 - bce_dice_loss: 0.5510\n",
      "Training: batch 17 begins at 22:08:09.106682\n",
      "\n",
      "Training: batch 17 ends at 22:08:09.937286\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5497 - bce_dice_loss: 0.5497\n",
      "Training: batch 18 begins at 22:08:09.940573\n",
      "\n",
      "Training: batch 18 ends at 22:08:10.730826\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5491 - bce_dice_loss: 0.5491\n",
      "Training: batch 19 begins at 22:08:10.734147\n",
      "\n",
      "Training: batch 19 ends at 22:08:11.530960\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5503 - bce_dice_loss: 0.5503\n",
      "Training: batch 20 begins at 22:08:11.535683\n",
      "\n",
      "Training: batch 20 ends at 22:08:12.325060\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5534 - bce_dice_loss: 0.5534\n",
      "Training: batch 21 begins at 22:08:12.329103\n",
      "\n",
      "Training: batch 21 ends at 22:08:13.125860\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5519 - bce_dice_loss: 0.5519\n",
      "Training: batch 22 begins at 22:08:13.130102\n",
      "\n",
      "Training: batch 22 ends at 22:08:13.950441\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5436 - bce_dice_loss: 0.5436\n",
      "Training: batch 23 begins at 22:08:13.954183\n",
      "\n",
      "Training: batch 23 ends at 22:08:14.746328\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5456 - bce_dice_loss: 0.5456\n",
      "Training: batch 24 begins at 22:08:14.750175\n",
      "\n",
      "Training: batch 24 ends at 22:08:15.549816\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5429 - bce_dice_loss: 0.5429\n",
      "Training: batch 25 begins at 22:08:15.554311\n",
      "\n",
      "Training: batch 25 ends at 22:08:16.350613\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5369 - bce_dice_loss: 0.5369\n",
      "Training: batch 26 begins at 22:08:16.355077\n",
      "\n",
      "Training: batch 26 ends at 22:08:17.153451\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5471 - bce_dice_loss: 0.5471\n",
      "Training: batch 27 begins at 22:08:17.157089\n",
      "\n",
      "Training: batch 27 ends at 22:08:17.948527\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5398 - bce_dice_loss: 0.5398\n",
      "Training: batch 28 begins at 22:08:17.952053\n",
      "\n",
      "Training: batch 28 ends at 22:08:18.743738\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5420 - bce_dice_loss: 0.5420\n",
      "Training: batch 29 begins at 22:08:18.747451\n",
      "\n",
      "Training: batch 29 ends at 22:08:19.558618\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5447 - bce_dice_loss: 0.5447\n",
      "Training: batch 30 begins at 22:08:19.562548\n",
      "\n",
      "Training: batch 30 ends at 22:08:20.353989\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5406 - bce_dice_loss: 0.5406\n",
      "Training: batch 31 begins at 22:08:20.357422\n",
      "\n",
      "Training: batch 31 ends at 22:08:21.152269\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5350 - bce_dice_loss: 0.5350\n",
      "Training: batch 32 begins at 22:08:21.155831\n",
      "\n",
      "Training: batch 32 ends at 22:08:21.960488\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5407 - bce_dice_loss: 0.5407\n",
      "Training: batch 33 begins at 22:08:21.963874\n",
      "\n",
      "Training: batch 33 ends at 22:08:22.754976\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5450 - bce_dice_loss: 0.5450\n",
      "Training: batch 34 begins at 22:08:22.759233\n",
      "\n",
      "Training: batch 34 ends at 22:08:23.552177\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5430 - bce_dice_loss: 0.5430\n",
      "Training: batch 35 begins at 22:08:23.556302\n",
      "\n",
      "Training: batch 35 ends at 22:08:24.356977\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5424 - bce_dice_loss: 0.5424\n",
      "Training: batch 36 begins at 22:08:24.360528\n",
      "\n",
      "Training: batch 36 ends at 22:08:25.153220\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5431 - bce_dice_loss: 0.5431\n",
      "Training: batch 37 begins at 22:08:25.157470\n",
      "\n",
      "Training: batch 37 ends at 22:08:25.956555\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5444 - bce_dice_loss: 0.5444\n",
      "Training: batch 38 begins at 22:08:25.962342\n",
      "\n",
      "Training: batch 38 ends at 22:08:26.753568\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5456 - bce_dice_loss: 0.5456\n",
      "Training: batch 39 begins at 22:08:26.756123\n",
      "\n",
      "Training: batch 39 ends at 22:08:27.565334\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5431 - bce_dice_loss: 0.5431\n",
      "Training: batch 40 begins at 22:08:27.569903\n",
      "\n",
      "Training: batch 40 ends at 22:08:28.365856\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5458 - bce_dice_loss: 0.5458\n",
      "Training: batch 41 begins at 22:08:28.369376\n",
      "\n",
      "Training: batch 41 ends at 22:08:29.169933\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5458 - bce_dice_loss: 0.5458\n",
      "Training: batch 42 begins at 22:08:29.174236\n",
      "\n",
      "Training: batch 42 ends at 22:08:29.970914\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5472 - bce_dice_loss: 0.5472\n",
      "Training: batch 43 begins at 22:08:29.976368\n",
      "\n",
      "Training: batch 43 ends at 22:08:30.772645\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5436 - bce_dice_loss: 0.5436\n",
      "Training: batch 44 begins at 22:08:30.777004\n",
      "\n",
      "Training: batch 44 ends at 22:08:31.566703\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5432 - bce_dice_loss: 0.5432\n",
      "Training: batch 45 begins at 22:08:31.569966\n",
      "\n",
      "Training: batch 45 ends at 22:08:32.362666\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5465 - bce_dice_loss: 0.5465\n",
      "Training: batch 46 begins at 22:08:32.367585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 46 ends at 22:08:33.160139\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5503 - bce_dice_loss: 0.5503\n",
      "Training: batch 47 begins at 22:08:33.164534\n",
      "\n",
      "Training: batch 47 ends at 22:08:33.955186\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5505 - bce_dice_loss: 0.5505 \n",
      "Training: batch 48 begins at 22:08:33.959665\n",
      "\n",
      "Training: batch 48 ends at 22:08:34.750547\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5506 - bce_dice_loss: 0.5506\n",
      "Training: batch 49 begins at 22:08:34.753484\n",
      "\n",
      "Training: batch 49 ends at 22:08:35.548340\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5530 - bce_dice_loss: 0.5530\n",
      "Training: batch 50 begins at 22:08:35.552976\n",
      "\n",
      "Training: batch 50 ends at 22:08:36.344364\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5516 - bce_dice_loss: 0.5516\n",
      "Training: batch 51 begins at 22:08:36.348124\n",
      "\n",
      "Training: batch 51 ends at 22:08:37.146967\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5531 - bce_dice_loss: 0.5531\n",
      "Training: batch 52 begins at 22:08:37.151614\n",
      "\n",
      "Training: batch 52 ends at 22:08:37.960166\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5508 - bce_dice_loss: 0.5508\n",
      "Training: batch 53 begins at 22:08:37.964111\n",
      "\n",
      "Training: batch 53 ends at 22:08:38.753658\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5480 - bce_dice_loss: 0.5480\n",
      "Training: batch 54 begins at 22:08:38.758246\n",
      "\n",
      "Training: batch 54 ends at 22:08:39.572138\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5500 - bce_dice_loss: 0.5500\n",
      "Training: batch 55 begins at 22:08:39.575887\n",
      "\n",
      "Training: batch 55 ends at 22:08:40.374101\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5515 - bce_dice_loss: 0.5515\n",
      "Training: batch 56 begins at 22:08:40.378701\n",
      "\n",
      "Training: batch 56 ends at 22:08:41.182244\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5518 - bce_dice_loss: 0.5518\n",
      "Training: batch 57 begins at 22:08:41.187028\n",
      "\n",
      "Training: batch 57 ends at 22:08:41.980865\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5531 - bce_dice_loss: 0.5531\n",
      "Training: batch 58 begins at 22:08:41.985563\n",
      "\n",
      "Training: batch 58 ends at 22:08:42.783940\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5517 - bce_dice_loss: 0.5517\n",
      "Training: batch 59 begins at 22:08:42.787824\n",
      "\n",
      "Training: batch 59 ends at 22:08:43.581727\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5528 - bce_dice_loss: 0.5528\n",
      "Evaluating: batch 0 begins at 22:08:43.613614\n",
      "\n",
      "Evaluating: batch 0 ends at 22:08:43.892685\n",
      "\n",
      "Evaluating: batch 1 begins at 22:08:43.894480\n",
      "\n",
      "Evaluating: batch 1 ends at 22:08:44.114529\n",
      "\n",
      "Evaluating: batch 2 begins at 22:08:44.116111\n",
      "\n",
      "Evaluating: batch 2 ends at 22:08:44.334015\n",
      "\n",
      "Evaluating: batch 3 begins at 22:08:44.335444\n",
      "\n",
      "Evaluating: batch 3 ends at 22:08:44.553677\n",
      "\n",
      "Evaluating: batch 4 begins at 22:08:44.555060\n",
      "\n",
      "Evaluating: batch 4 ends at 22:08:44.775942\n",
      "\n",
      "Evaluating: batch 5 begins at 22:08:44.777250\n",
      "\n",
      "Evaluating: batch 5 ends at 22:08:44.998431\n",
      "\n",
      "Evaluating: batch 6 begins at 22:08:45.000085\n",
      "\n",
      "Evaluating: batch 6 ends at 22:08:45.220355\n",
      "\n",
      "Evaluating: batch 7 begins at 22:08:45.221691\n",
      "\n",
      "Evaluating: batch 7 ends at 22:08:45.441195\n",
      "\n",
      "Evaluating: batch 8 begins at 22:08:45.442951\n",
      "\n",
      "Evaluating: batch 8 ends at 22:08:45.665205\n",
      "\n",
      "Evaluating: batch 9 begins at 22:08:45.667220\n",
      "\n",
      "Evaluating: batch 9 ends at 22:08:45.888308\n",
      "\n",
      "Evaluating: batch 10 begins at 22:08:45.889684\n",
      "\n",
      "Evaluating: batch 10 ends at 22:08:46.111902\n",
      "\n",
      "Evaluating: batch 11 begins at 22:08:46.113242\n",
      "\n",
      "Evaluating: batch 11 ends at 22:08:46.332031\n",
      "\n",
      "Evaluating: batch 12 begins at 22:08:46.333437\n",
      "\n",
      "Evaluating: batch 12 ends at 22:08:46.553046\n",
      "\n",
      "Evaluating: batch 13 begins at 22:08:46.554436\n",
      "\n",
      "Evaluating: batch 13 ends at 22:08:46.775880\n",
      "\n",
      "Evaluating: batch 14 begins at 22:08:46.777274\n",
      "\n",
      "Evaluating: batch 14 ends at 22:08:46.999434\n",
      "\n",
      "Evaluating: batch 15 begins at 22:08:47.000856\n",
      "\n",
      "Evaluating: batch 15 ends at 22:08:47.221728\n",
      "\n",
      "Evaluating: batch 16 begins at 22:08:47.223134\n",
      "\n",
      "Evaluating: batch 16 ends at 22:08:47.446282\n",
      "\n",
      "Evaluating: batch 17 begins at 22:08:47.447598\n",
      "\n",
      "Evaluating: batch 17 ends at 22:08:47.666308\n",
      "\n",
      "Evaluating: batch 18 begins at 22:08:47.667765\n",
      "\n",
      "Evaluating: batch 18 ends at 22:08:47.887143\n",
      "\n",
      "Evaluating: batch 19 begins at 22:08:47.888836\n",
      "\n",
      "Evaluating: batch 19 ends at 22:08:48.106023\n",
      "\n",
      "Evaluating: batch 20 begins at 22:08:48.107303\n",
      "\n",
      "Evaluating: batch 20 ends at 22:08:48.323000\n",
      "\n",
      "Evaluating: batch 21 begins at 22:08:48.324579\n",
      "\n",
      "Evaluating: batch 21 ends at 22:08:48.541416\n",
      "\n",
      "Evaluating: batch 22 begins at 22:08:48.542903\n",
      "\n",
      "Evaluating: batch 22 ends at 22:08:48.761302\n",
      "\n",
      "Evaluating: batch 23 begins at 22:08:48.763003\n",
      "\n",
      "Evaluating: batch 23 ends at 22:08:48.989213\n",
      "\n",
      "Evaluating: batch 24 begins at 22:08:48.990567\n",
      "\n",
      "Evaluating: batch 24 ends at 22:08:49.210282\n",
      "\n",
      "Evaluating: batch 25 begins at 22:08:49.211764\n",
      "\n",
      "Evaluating: batch 25 ends at 22:08:49.429988\n",
      "\n",
      "Evaluating: batch 26 begins at 22:08:49.431269\n",
      "\n",
      "Evaluating: batch 26 ends at 22:08:49.648544\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.54892\n",
      "60/60 [==============================] - 54s 905ms/step - loss: 0.5528 - bce_dice_loss: 0.5528 - val_loss: 0.5508 - val_bce_dice_loss: 0.5508\n",
      "Epoch 17/25\n",
      "\n",
      "Training: batch 0 begins at 22:08:49.676654\n",
      "\n",
      "Training: batch 0 ends at 22:08:50.474631\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.5867 - bce_dice_loss: 0.5867\n",
      "Training: batch 1 begins at 22:08:50.480502\n",
      "\n",
      "Training: batch 1 ends at 22:08:51.277402\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5530 - bce_dice_loss: 0.5530\n",
      "Training: batch 2 begins at 22:08:51.282086\n",
      "\n",
      "Training: batch 2 ends at 22:08:52.096708\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.5536 - bce_dice_loss: 0.5536\n",
      "Training: batch 3 begins at 22:08:52.099807\n",
      "\n",
      "Training: batch 3 ends at 22:08:52.893489\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5865 - bce_dice_loss: 0.5865\n",
      "Training: batch 4 begins at 22:08:52.896665\n",
      "\n",
      "Training: batch 4 ends at 22:08:53.700408\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.6020 - bce_dice_loss: 0.6020\n",
      "Training: batch 5 begins at 22:08:53.704711\n",
      "\n",
      "Training: batch 5 ends at 22:08:54.498067\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5873 - bce_dice_loss: 0.5873\n",
      "Training: batch 6 begins at 22:08:54.502133\n",
      "\n",
      "Training: batch 6 ends at 22:08:55.292140\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5928 - bce_dice_loss: 0.5928\n",
      "Training: batch 7 begins at 22:08:55.296314\n",
      "\n",
      "Training: batch 7 ends at 22:08:56.103138\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5996 - bce_dice_loss: 0.5996\n",
      "Training: batch 8 begins at 22:08:56.106029\n",
      "\n",
      "Training: batch 8 ends at 22:08:56.900235\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5862 - bce_dice_loss: 0.5862\n",
      "Training: batch 9 begins at 22:08:56.904312\n",
      "\n",
      "Training: batch 9 ends at 22:08:57.692931\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.6043 - bce_dice_loss: 0.6043\n",
      "Training: batch 10 begins at 22:08:57.697494\n",
      "\n",
      "Training: batch 10 ends at 22:08:58.505981\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5886 - bce_dice_loss: 0.5886\n",
      "Training: batch 11 begins at 22:08:58.511020\n",
      "\n",
      "Training: batch 11 ends at 22:08:59.312358\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5956 - bce_dice_loss: 0.5956\n",
      "Training: batch 12 begins at 22:08:59.317469\n",
      "\n",
      "Training: batch 12 ends at 22:09:00.109371\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5901 - bce_dice_loss: 0.5901\n",
      "Training: batch 13 begins at 22:09:00.112978\n",
      "\n",
      "Training: batch 13 ends at 22:09:00.905529\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5840 - bce_dice_loss: 0.5840\n",
      "Training: batch 14 begins at 22:09:00.909066\n",
      "\n",
      "Training: batch 14 ends at 22:09:01.722190\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5791 - bce_dice_loss: 0.5791\n",
      "Training: batch 15 begins at 22:09:01.725143\n",
      "\n",
      "Training: batch 15 ends at 22:09:02.521247\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5753 - bce_dice_loss: 0.5753\n",
      "Training: batch 16 begins at 22:09:02.524707\n",
      "\n",
      "Training: batch 16 ends at 22:09:03.322008\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5677 - bce_dice_loss: 0.5677\n",
      "Training: batch 17 begins at 22:09:03.325469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 17 ends at 22:09:04.141155\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5805 - bce_dice_loss: 0.5805\n",
      "Training: batch 18 begins at 22:09:04.144371\n",
      "\n",
      "Training: batch 18 ends at 22:09:04.936184\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5717 - bce_dice_loss: 0.5717\n",
      "Training: batch 19 begins at 22:09:04.942982\n",
      "\n",
      "Training: batch 19 ends at 22:09:05.729606\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5635 - bce_dice_loss: 0.5635\n",
      "Training: batch 20 begins at 22:09:05.732365\n",
      "\n",
      "Training: batch 20 ends at 22:09:06.519811\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5530 - bce_dice_loss: 0.5530\n",
      "Training: batch 21 begins at 22:09:06.523163\n",
      "\n",
      "Training: batch 21 ends at 22:09:07.332887\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5504 - bce_dice_loss: 0.5504\n",
      "Training: batch 22 begins at 22:09:07.336632\n",
      "\n",
      "Training: batch 22 ends at 22:09:08.129040\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5525 - bce_dice_loss: 0.5525\n",
      "Training: batch 23 begins at 22:09:08.133037\n",
      "\n",
      "Training: batch 23 ends at 22:09:08.932427\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5512 - bce_dice_loss: 0.5512\n",
      "Training: batch 24 begins at 22:09:08.935497\n",
      "\n",
      "Training: batch 24 ends at 22:09:09.762913\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5539 - bce_dice_loss: 0.5539\n",
      "Training: batch 25 begins at 22:09:09.767516\n",
      "\n",
      "Training: batch 25 ends at 22:09:10.557952\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5492 - bce_dice_loss: 0.5492\n",
      "Training: batch 26 begins at 22:09:10.561249\n",
      "\n",
      "Training: batch 26 ends at 22:09:11.356426\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5527 - bce_dice_loss: 0.5527\n",
      "Training: batch 27 begins at 22:09:11.359815\n",
      "\n",
      "Training: batch 27 ends at 22:09:12.176620\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5444 - bce_dice_loss: 0.5444\n",
      "Training: batch 28 begins at 22:09:12.180916\n",
      "\n",
      "Training: batch 28 ends at 22:09:12.971033\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5601 - bce_dice_loss: 0.5601\n",
      "Training: batch 29 begins at 22:09:12.975616\n",
      "\n",
      "Training: batch 29 ends at 22:09:13.769297\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5577 - bce_dice_loss: 0.5577\n",
      "Training: batch 30 begins at 22:09:13.773062\n",
      "\n",
      "Training: batch 30 ends at 22:09:14.561976\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5568 - bce_dice_loss: 0.5568\n",
      "Training: batch 31 begins at 22:09:14.567192\n",
      "\n",
      "Training: batch 31 ends at 22:09:15.358645\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5484 - bce_dice_loss: 0.5484\n",
      "Training: batch 32 begins at 22:09:15.363424\n",
      "\n",
      "Training: batch 32 ends at 22:09:16.149148\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5488 - bce_dice_loss: 0.5488\n",
      "Training: batch 33 begins at 22:09:16.153996\n",
      "\n",
      "Training: batch 33 ends at 22:09:16.950313\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5528 - bce_dice_loss: 0.5528\n",
      "Training: batch 34 begins at 22:09:16.954660\n",
      "\n",
      "Training: batch 34 ends at 22:09:17.759701\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5569 - bce_dice_loss: 0.5569\n",
      "Training: batch 35 begins at 22:09:17.763461\n",
      "\n",
      "Training: batch 35 ends at 22:09:18.553371\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5584 - bce_dice_loss: 0.5584\n",
      "Training: batch 36 begins at 22:09:18.556847\n",
      "\n",
      "Training: batch 36 ends at 22:09:19.355519\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5564 - bce_dice_loss: 0.5564\n",
      "Training: batch 37 begins at 22:09:19.361194\n",
      "\n",
      "Training: batch 37 ends at 22:09:20.159746\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5672 - bce_dice_loss: 0.5672\n",
      "Training: batch 38 begins at 22:09:20.165271\n",
      "\n",
      "Training: batch 38 ends at 22:09:20.958849\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5698 - bce_dice_loss: 0.5698\n",
      "Training: batch 39 begins at 22:09:20.962662\n",
      "\n",
      "Training: batch 39 ends at 22:09:21.758141\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5678 - bce_dice_loss: 0.5678\n",
      "Training: batch 40 begins at 22:09:21.761424\n",
      "\n",
      "Training: batch 40 ends at 22:09:22.574535\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5681 - bce_dice_loss: 0.5681\n",
      "Training: batch 41 begins at 22:09:22.579277\n",
      "\n",
      "Training: batch 41 ends at 22:09:23.371505\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5612 - bce_dice_loss: 0.5612\n",
      "Training: batch 42 begins at 22:09:23.375752\n",
      "\n",
      "Training: batch 42 ends at 22:09:24.167822\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5612 - bce_dice_loss: 0.5612\n",
      "Training: batch 43 begins at 22:09:24.171198\n",
      "\n",
      "Training: batch 43 ends at 22:09:24.973765\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5626 - bce_dice_loss: 0.5626\n",
      "Training: batch 44 begins at 22:09:24.978043\n",
      "\n",
      "Training: batch 44 ends at 22:09:25.775532\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5657 - bce_dice_loss: 0.5657\n",
      "Training: batch 45 begins at 22:09:25.778814\n",
      "\n",
      "Training: batch 45 ends at 22:09:26.568956\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5707 - bce_dice_loss: 0.5707\n",
      "Training: batch 46 begins at 22:09:26.572656\n",
      "\n",
      "Training: batch 46 ends at 22:09:27.368236\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5724 - bce_dice_loss: 0.5724\n",
      "Training: batch 47 begins at 22:09:27.371547\n",
      "\n",
      "Training: batch 47 ends at 22:09:28.164347\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5739 - bce_dice_loss: 0.5739 \n",
      "Training: batch 48 begins at 22:09:28.168628\n",
      "\n",
      "Training: batch 48 ends at 22:09:28.968260\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5720 - bce_dice_loss: 0.5720\n",
      "Training: batch 49 begins at 22:09:28.970894\n",
      "\n",
      "Training: batch 49 ends at 22:09:29.783578\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5646 - bce_dice_loss: 0.5646\n",
      "Training: batch 50 begins at 22:09:29.787248\n",
      "\n",
      "Training: batch 50 ends at 22:09:30.579653\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5644 - bce_dice_loss: 0.5644\n",
      "Training: batch 51 begins at 22:09:30.584111\n",
      "\n",
      "Training: batch 51 ends at 22:09:31.383931\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5640 - bce_dice_loss: 0.5640\n",
      "Training: batch 52 begins at 22:09:31.388095\n",
      "\n",
      "Training: batch 52 ends at 22:09:32.180679\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5645 - bce_dice_loss: 0.5645\n",
      "Training: batch 53 begins at 22:09:32.184978\n",
      "\n",
      "Training: batch 53 ends at 22:09:32.983669\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5626 - bce_dice_loss: 0.5626\n",
      "Training: batch 54 begins at 22:09:32.986209\n",
      "\n",
      "Training: batch 54 ends at 22:09:33.775595\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5626 - bce_dice_loss: 0.5626\n",
      "Training: batch 55 begins at 22:09:33.779160\n",
      "\n",
      "Training: batch 55 ends at 22:09:34.585359\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5608 - bce_dice_loss: 0.5608\n",
      "Training: batch 56 begins at 22:09:34.588947\n",
      "\n",
      "Training: batch 56 ends at 22:09:35.382531\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5569 - bce_dice_loss: 0.5569\n",
      "Training: batch 57 begins at 22:09:35.386190\n",
      "\n",
      "Training: batch 57 ends at 22:09:36.176491\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5567 - bce_dice_loss: 0.5567\n",
      "Training: batch 58 begins at 22:09:36.181339\n",
      "\n",
      "Training: batch 58 ends at 22:09:36.968857\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5562 - bce_dice_loss: 0.5562\n",
      "Training: batch 59 begins at 22:09:36.972314\n",
      "\n",
      "Training: batch 59 ends at 22:09:37.757834\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5585 - bce_dice_loss: 0.5585\n",
      "Evaluating: batch 0 begins at 22:09:37.787272\n",
      "\n",
      "Evaluating: batch 0 ends at 22:09:38.056488\n",
      "\n",
      "Evaluating: batch 1 begins at 22:09:38.058062\n",
      "\n",
      "Evaluating: batch 1 ends at 22:09:38.271390\n",
      "\n",
      "Evaluating: batch 2 begins at 22:09:38.272626\n",
      "\n",
      "Evaluating: batch 2 ends at 22:09:38.493003\n",
      "\n",
      "Evaluating: batch 3 begins at 22:09:38.495487\n",
      "\n",
      "Evaluating: batch 3 ends at 22:09:38.716763\n",
      "\n",
      "Evaluating: batch 4 begins at 22:09:38.718574\n",
      "\n",
      "Evaluating: batch 4 ends at 22:09:38.935382\n",
      "\n",
      "Evaluating: batch 5 begins at 22:09:38.936971\n",
      "\n",
      "Evaluating: batch 5 ends at 22:09:39.166271\n",
      "\n",
      "Evaluating: batch 6 begins at 22:09:39.167625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 6 ends at 22:09:39.386980\n",
      "\n",
      "Evaluating: batch 7 begins at 22:09:39.388325\n",
      "\n",
      "Evaluating: batch 7 ends at 22:09:39.610481\n",
      "\n",
      "Evaluating: batch 8 begins at 22:09:39.611824\n",
      "\n",
      "Evaluating: batch 8 ends at 22:09:39.835809\n",
      "\n",
      "Evaluating: batch 9 begins at 22:09:39.837063\n",
      "\n",
      "Evaluating: batch 9 ends at 22:09:40.057645\n",
      "\n",
      "Evaluating: batch 10 begins at 22:09:40.059059\n",
      "\n",
      "Evaluating: batch 10 ends at 22:09:40.283305\n",
      "\n",
      "Evaluating: batch 11 begins at 22:09:40.284726\n",
      "\n",
      "Evaluating: batch 11 ends at 22:09:40.506583\n",
      "\n",
      "Evaluating: batch 12 begins at 22:09:40.507958\n",
      "\n",
      "Evaluating: batch 12 ends at 22:09:40.729233\n",
      "\n",
      "Evaluating: batch 13 begins at 22:09:40.730609\n",
      "\n",
      "Evaluating: batch 13 ends at 22:09:40.951043\n",
      "\n",
      "Evaluating: batch 14 begins at 22:09:40.952631\n",
      "\n",
      "Evaluating: batch 14 ends at 22:09:41.173766\n",
      "\n",
      "Evaluating: batch 15 begins at 22:09:41.175073\n",
      "\n",
      "Evaluating: batch 15 ends at 22:09:41.393113\n",
      "\n",
      "Evaluating: batch 16 begins at 22:09:41.395051\n",
      "\n",
      "Evaluating: batch 16 ends at 22:09:41.615417\n",
      "\n",
      "Evaluating: batch 17 begins at 22:09:41.616850\n",
      "\n",
      "Evaluating: batch 17 ends at 22:09:41.837858\n",
      "\n",
      "Evaluating: batch 18 begins at 22:09:41.839260\n",
      "\n",
      "Evaluating: batch 18 ends at 22:09:42.058451\n",
      "\n",
      "Evaluating: batch 19 begins at 22:09:42.060389\n",
      "\n",
      "Evaluating: batch 19 ends at 22:09:42.279286\n",
      "\n",
      "Evaluating: batch 20 begins at 22:09:42.281343\n",
      "\n",
      "Evaluating: batch 20 ends at 22:09:42.500042\n",
      "\n",
      "Evaluating: batch 21 begins at 22:09:42.501966\n",
      "\n",
      "Evaluating: batch 21 ends at 22:09:42.718877\n",
      "\n",
      "Evaluating: batch 22 begins at 22:09:42.720424\n",
      "\n",
      "Evaluating: batch 22 ends at 22:09:42.935859\n",
      "\n",
      "Evaluating: batch 23 begins at 22:09:42.937372\n",
      "\n",
      "Evaluating: batch 23 ends at 22:09:43.152459\n",
      "\n",
      "Evaluating: batch 24 begins at 22:09:43.153699\n",
      "\n",
      "Evaluating: batch 24 ends at 22:09:43.373209\n",
      "\n",
      "Evaluating: batch 25 begins at 22:09:43.374525\n",
      "\n",
      "Evaluating: batch 25 ends at 22:09:43.591556\n",
      "\n",
      "Evaluating: batch 26 begins at 22:09:43.592794\n",
      "\n",
      "Evaluating: batch 26 ends at 22:09:43.809961\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.54892 to 0.53613, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 930ms/step - loss: 0.5585 - bce_dice_loss: 0.5585 - val_loss: 0.5361 - val_bce_dice_loss: 0.5361\n",
      "Epoch 18/25\n",
      "\n",
      "Training: batch 0 begins at 22:09:45.333198\n",
      "\n",
      "Training: batch 0 ends at 22:09:46.138393\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3385 - bce_dice_loss: 0.3385\n",
      "Training: batch 1 begins at 22:09:46.142919\n",
      "\n",
      "Training: batch 1 ends at 22:09:46.952221\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4001 - bce_dice_loss: 0.4001\n",
      "Training: batch 2 begins at 22:09:46.957110\n",
      "\n",
      "Training: batch 2 ends at 22:09:47.749502\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.4696 - bce_dice_loss: 0.4696\n",
      "Training: batch 3 begins at 22:09:47.753145\n",
      "\n",
      "Training: batch 3 ends at 22:09:48.539737\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.5839 - bce_dice_loss: 0.5839\n",
      "Training: batch 4 begins at 22:09:48.544791\n",
      "\n",
      "Training: batch 4 ends at 22:09:49.348377\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.6346 - bce_dice_loss: 0.6346\n",
      "Training: batch 5 begins at 22:09:49.352405\n",
      "\n",
      "Training: batch 5 ends at 22:09:50.154574\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5920 - bce_dice_loss: 0.5920\n",
      "Training: batch 6 begins at 22:09:50.159117\n",
      "\n",
      "Training: batch 6 ends at 22:09:50.953269\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5856 - bce_dice_loss: 0.5856\n",
      "Training: batch 7 begins at 22:09:50.956562\n",
      "\n",
      "Training: batch 7 ends at 22:09:51.750457\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5873 - bce_dice_loss: 0.5873\n",
      "Training: batch 8 begins at 22:09:51.754264\n",
      "\n",
      "Training: batch 8 ends at 22:09:52.546540\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5855 - bce_dice_loss: 0.5855\n",
      "Training: batch 9 begins at 22:09:52.550899\n",
      "\n",
      "Training: batch 9 ends at 22:09:53.361886\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5820 - bce_dice_loss: 0.5820\n",
      "Training: batch 10 begins at 22:09:53.365788\n",
      "\n",
      "Training: batch 10 ends at 22:09:54.186261\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5842 - bce_dice_loss: 0.5842\n",
      "Training: batch 11 begins at 22:09:54.190113\n",
      "\n",
      "Training: batch 11 ends at 22:09:54.988186\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5697 - bce_dice_loss: 0.5697\n",
      "Training: batch 12 begins at 22:09:54.991421\n",
      "\n",
      "Training: batch 12 ends at 22:09:55.795991\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5615 - bce_dice_loss: 0.5615\n",
      "Training: batch 13 begins at 22:09:55.800233\n",
      "\n",
      "Training: batch 13 ends at 22:09:56.594819\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5590 - bce_dice_loss: 0.5590\n",
      "Training: batch 14 begins at 22:09:56.597437\n",
      "\n",
      "Training: batch 14 ends at 22:09:57.395490\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5657 - bce_dice_loss: 0.5657\n",
      "Training: batch 15 begins at 22:09:57.399181\n",
      "\n",
      "Training: batch 15 ends at 22:09:58.194297\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5622 - bce_dice_loss: 0.5622\n",
      "Training: batch 16 begins at 22:09:58.198380\n",
      "\n",
      "Training: batch 16 ends at 22:09:59.004662\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5628 - bce_dice_loss: 0.5628\n",
      "Training: batch 17 begins at 22:09:59.007461\n",
      "\n",
      "Training: batch 17 ends at 22:09:59.808495\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5693 - bce_dice_loss: 0.5693\n",
      "Training: batch 18 begins at 22:09:59.811572\n",
      "\n",
      "Training: batch 18 ends at 22:10:00.609197\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5727 - bce_dice_loss: 0.5727\n",
      "Training: batch 19 begins at 22:10:00.612545\n",
      "\n",
      "Training: batch 19 ends at 22:10:01.409510\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5757 - bce_dice_loss: 0.5757\n",
      "Training: batch 20 begins at 22:10:01.412917\n",
      "\n",
      "Training: batch 20 ends at 22:10:02.225192\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5744 - bce_dice_loss: 0.5744\n",
      "Training: batch 21 begins at 22:10:02.228444\n",
      "\n",
      "Training: batch 21 ends at 22:10:03.025584\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5702 - bce_dice_loss: 0.5702\n",
      "Training: batch 22 begins at 22:10:03.029708\n",
      "\n",
      "Training: batch 22 ends at 22:10:03.827029\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5718 - bce_dice_loss: 0.5718\n",
      "Training: batch 23 begins at 22:10:03.831020\n",
      "\n",
      "Training: batch 23 ends at 22:10:04.649085\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5595 - bce_dice_loss: 0.5595\n",
      "Training: batch 24 begins at 22:10:04.654247\n",
      "\n",
      "Training: batch 24 ends at 22:10:05.445229\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5578 - bce_dice_loss: 0.5578\n",
      "Training: batch 25 begins at 22:10:05.449736\n",
      "\n",
      "Training: batch 25 ends at 22:10:06.242932\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5550 - bce_dice_loss: 0.5550\n",
      "Training: batch 26 begins at 22:10:06.247290\n",
      "\n",
      "Training: batch 26 ends at 22:10:07.041083\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5535 - bce_dice_loss: 0.5535\n",
      "Training: batch 27 begins at 22:10:07.044724\n",
      "\n",
      "Training: batch 27 ends at 22:10:07.837712\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5510 - bce_dice_loss: 0.5510\n",
      "Training: batch 28 begins at 22:10:07.841970\n",
      "\n",
      "Training: batch 28 ends at 22:10:08.637936\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5471 - bce_dice_loss: 0.5471\n",
      "Training: batch 29 begins at 22:10:08.641706\n",
      "\n",
      "Training: batch 29 ends at 22:10:09.465093\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5487 - bce_dice_loss: 0.5487\n",
      "Training: batch 30 begins at 22:10:09.470030\n",
      "\n",
      "Training: batch 30 ends at 22:10:10.272072\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5424 - bce_dice_loss: 0.5424\n",
      "Training: batch 31 begins at 22:10:10.279751\n",
      "\n",
      "Training: batch 31 ends at 22:10:11.074569\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5482 - bce_dice_loss: 0.5482\n",
      "Training: batch 32 begins at 22:10:11.077996\n",
      "\n",
      "Training: batch 32 ends at 22:10:11.870586\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5427 - bce_dice_loss: 0.5427\n",
      "Training: batch 33 begins at 22:10:11.874955\n",
      "\n",
      "Training: batch 33 ends at 22:10:12.668633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/60 [================>.............] - ETA: 20s - loss: 0.5412 - bce_dice_loss: 0.5412\n",
      "Training: batch 34 begins at 22:10:12.672558\n",
      "\n",
      "Training: batch 34 ends at 22:10:13.461060\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5402 - bce_dice_loss: 0.5402\n",
      "Training: batch 35 begins at 22:10:13.465346\n",
      "\n",
      "Training: batch 35 ends at 22:10:14.261539\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5417 - bce_dice_loss: 0.5417\n",
      "Training: batch 36 begins at 22:10:14.264061\n",
      "\n",
      "Training: batch 36 ends at 22:10:15.053788\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5396 - bce_dice_loss: 0.5396\n",
      "Training: batch 37 begins at 22:10:15.057565\n",
      "\n",
      "Training: batch 37 ends at 22:10:15.852252\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5381 - bce_dice_loss: 0.5381\n",
      "Training: batch 38 begins at 22:10:15.856378\n",
      "\n",
      "Training: batch 38 ends at 22:10:16.646709\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5386 - bce_dice_loss: 0.5386\n",
      "Training: batch 39 begins at 22:10:16.650054\n",
      "\n",
      "Training: batch 39 ends at 22:10:17.455465\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5412 - bce_dice_loss: 0.5412\n",
      "Training: batch 40 begins at 22:10:17.459617\n",
      "\n",
      "Training: batch 40 ends at 22:10:18.255743\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5413 - bce_dice_loss: 0.5413\n",
      "Training: batch 41 begins at 22:10:18.259005\n",
      "\n",
      "Training: batch 41 ends at 22:10:19.077303\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5423 - bce_dice_loss: 0.5423\n",
      "Training: batch 42 begins at 22:10:19.080627\n",
      "\n",
      "Training: batch 42 ends at 22:10:19.898229\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5471 - bce_dice_loss: 0.5471\n",
      "Training: batch 43 begins at 22:10:19.903247\n",
      "\n",
      "Training: batch 43 ends at 22:10:20.698829\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5430 - bce_dice_loss: 0.5430\n",
      "Training: batch 44 begins at 22:10:20.702675\n",
      "\n",
      "Training: batch 44 ends at 22:10:21.499011\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5416 - bce_dice_loss: 0.5416\n",
      "Training: batch 45 begins at 22:10:21.503024\n",
      "\n",
      "Training: batch 45 ends at 22:10:22.293811\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5434 - bce_dice_loss: 0.5434\n",
      "Training: batch 46 begins at 22:10:22.298490\n",
      "\n",
      "Training: batch 46 ends at 22:10:23.088457\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5461 - bce_dice_loss: 0.5461\n",
      "Training: batch 47 begins at 22:10:23.092520\n",
      "\n",
      "Training: batch 47 ends at 22:10:23.882487\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5441 - bce_dice_loss: 0.5441 \n",
      "Training: batch 48 begins at 22:10:23.887224\n",
      "\n",
      "Training: batch 48 ends at 22:10:24.665360\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5520 - bce_dice_loss: 0.5520\n",
      "Training: batch 49 begins at 22:10:24.670614\n",
      "\n",
      "Training: batch 49 ends at 22:10:25.487404\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5510 - bce_dice_loss: 0.5510\n",
      "Training: batch 50 begins at 22:10:25.490777\n",
      "\n",
      "Training: batch 50 ends at 22:10:26.290115\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5504 - bce_dice_loss: 0.5504\n",
      "Training: batch 51 begins at 22:10:26.293631\n",
      "\n",
      "Training: batch 51 ends at 22:10:27.076957\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5544 - bce_dice_loss: 0.5544\n",
      "Training: batch 52 begins at 22:10:27.080770\n",
      "\n",
      "Training: batch 52 ends at 22:10:27.909909\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5530 - bce_dice_loss: 0.5530\n",
      "Training: batch 53 begins at 22:10:27.913912\n",
      "\n",
      "Training: batch 53 ends at 22:10:28.699695\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5519 - bce_dice_loss: 0.5519\n",
      "Training: batch 54 begins at 22:10:28.704712\n",
      "\n",
      "Training: batch 54 ends at 22:10:29.504848\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5513 - bce_dice_loss: 0.5513\n",
      "Training: batch 55 begins at 22:10:29.508501\n",
      "\n",
      "Training: batch 55 ends at 22:10:30.321171\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5510 - bce_dice_loss: 0.5510\n",
      "Training: batch 56 begins at 22:10:30.324679\n",
      "\n",
      "Training: batch 56 ends at 22:10:31.119437\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5480 - bce_dice_loss: 0.5480\n",
      "Training: batch 57 begins at 22:10:31.123903\n",
      "\n",
      "Training: batch 57 ends at 22:10:31.908684\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5507 - bce_dice_loss: 0.5507\n",
      "Training: batch 58 begins at 22:10:31.912123\n",
      "\n",
      "Training: batch 58 ends at 22:10:32.699883\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5496 - bce_dice_loss: 0.5496\n",
      "Training: batch 59 begins at 22:10:32.704479\n",
      "\n",
      "Training: batch 59 ends at 22:10:33.496195\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5459 - bce_dice_loss: 0.5459\n",
      "Evaluating: batch 0 begins at 22:10:33.528391\n",
      "\n",
      "Evaluating: batch 0 ends at 22:10:33.797198\n",
      "\n",
      "Evaluating: batch 1 begins at 22:10:33.798506\n",
      "\n",
      "Evaluating: batch 1 ends at 22:10:34.015642\n",
      "\n",
      "Evaluating: batch 2 begins at 22:10:34.016899\n",
      "\n",
      "Evaluating: batch 2 ends at 22:10:34.231271\n",
      "\n",
      "Evaluating: batch 3 begins at 22:10:34.232514\n",
      "\n",
      "Evaluating: batch 3 ends at 22:10:34.453120\n",
      "\n",
      "Evaluating: batch 4 begins at 22:10:34.455516\n",
      "\n",
      "Evaluating: batch 4 ends at 22:10:34.673999\n",
      "\n",
      "Evaluating: batch 5 begins at 22:10:34.675411\n",
      "\n",
      "Evaluating: batch 5 ends at 22:10:34.900203\n",
      "\n",
      "Evaluating: batch 6 begins at 22:10:34.901808\n",
      "\n",
      "Evaluating: batch 6 ends at 22:10:35.119425\n",
      "\n",
      "Evaluating: batch 7 begins at 22:10:35.120693\n",
      "\n",
      "Evaluating: batch 7 ends at 22:10:35.343801\n",
      "\n",
      "Evaluating: batch 8 begins at 22:10:35.348925\n",
      "\n",
      "Evaluating: batch 8 ends at 22:10:35.571769\n",
      "\n",
      "Evaluating: batch 9 begins at 22:10:35.573123\n",
      "\n",
      "Evaluating: batch 9 ends at 22:10:35.793542\n",
      "\n",
      "Evaluating: batch 10 begins at 22:10:35.795059\n",
      "\n",
      "Evaluating: batch 10 ends at 22:10:36.017026\n",
      "\n",
      "Evaluating: batch 11 begins at 22:10:36.019352\n",
      "\n",
      "Evaluating: batch 11 ends at 22:10:36.238123\n",
      "\n",
      "Evaluating: batch 12 begins at 22:10:36.239565\n",
      "\n",
      "Evaluating: batch 12 ends at 22:10:36.459725\n",
      "\n",
      "Evaluating: batch 13 begins at 22:10:36.461238\n",
      "\n",
      "Evaluating: batch 13 ends at 22:10:36.681462\n",
      "\n",
      "Evaluating: batch 14 begins at 22:10:36.682905\n",
      "\n",
      "Evaluating: batch 14 ends at 22:10:36.905047\n",
      "\n",
      "Evaluating: batch 15 begins at 22:10:36.906601\n",
      "\n",
      "Evaluating: batch 15 ends at 22:10:37.125782\n",
      "\n",
      "Evaluating: batch 16 begins at 22:10:37.128396\n",
      "\n",
      "Evaluating: batch 16 ends at 22:10:37.350471\n",
      "\n",
      "Evaluating: batch 17 begins at 22:10:37.352462\n",
      "\n",
      "Evaluating: batch 17 ends at 22:10:37.572963\n",
      "\n",
      "Evaluating: batch 18 begins at 22:10:37.575347\n",
      "\n",
      "Evaluating: batch 18 ends at 22:10:37.800052\n",
      "\n",
      "Evaluating: batch 19 begins at 22:10:37.801421\n",
      "\n",
      "Evaluating: batch 19 ends at 22:10:38.017426\n",
      "\n",
      "Evaluating: batch 20 begins at 22:10:38.018995\n",
      "\n",
      "Evaluating: batch 20 ends at 22:10:38.235707\n",
      "\n",
      "Evaluating: batch 21 begins at 22:10:38.237178\n",
      "\n",
      "Evaluating: batch 21 ends at 22:10:38.455147\n",
      "\n",
      "Evaluating: batch 22 begins at 22:10:38.456955\n",
      "\n",
      "Evaluating: batch 22 ends at 22:10:38.676512\n",
      "\n",
      "Evaluating: batch 23 begins at 22:10:38.678253\n",
      "\n",
      "Evaluating: batch 23 ends at 22:10:38.897237\n",
      "\n",
      "Evaluating: batch 24 begins at 22:10:38.899743\n",
      "\n",
      "Evaluating: batch 24 ends at 22:10:39.122572\n",
      "\n",
      "Evaluating: batch 25 begins at 22:10:39.123849\n",
      "\n",
      "Evaluating: batch 25 ends at 22:10:39.344401\n",
      "\n",
      "Evaluating: batch 26 begins at 22:10:39.346866\n",
      "\n",
      "Evaluating: batch 26 ends at 22:10:39.567206\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.53613 to 0.53588, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 934ms/step - loss: 0.5459 - bce_dice_loss: 0.5459 - val_loss: 0.5359 - val_bce_dice_loss: 0.5359\n",
      "Epoch 19/25\n",
      "\n",
      "Training: batch 0 begins at 22:10:41.277057\n",
      "\n",
      "Training: batch 0 ends at 22:10:42.087394\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3625 - bce_dice_loss: 0.3625\n",
      "Training: batch 1 begins at 22:10:42.091476\n",
      "\n",
      "Training: batch 1 ends at 22:10:42.908800\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4220 - bce_dice_loss: 0.4220\n",
      "Training: batch 2 begins at 22:10:42.913330\n",
      "\n",
      "Training: batch 2 ends at 22:10:43.731342\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4589 - bce_dice_loss: 0.4589\n",
      "Training: batch 3 begins at 22:10:43.738211\n",
      "\n",
      "Training: batch 3 ends at 22:10:44.527078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5033 - bce_dice_loss: 0.5033\n",
      "Training: batch 4 begins at 22:10:44.530636\n",
      "\n",
      "Training: batch 4 ends at 22:10:45.355397\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4925 - bce_dice_loss: 0.4925\n",
      "Training: batch 5 begins at 22:10:45.358999\n",
      "\n",
      "Training: batch 5 ends at 22:10:46.151939\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5117 - bce_dice_loss: 0.5117\n",
      "Training: batch 6 begins at 22:10:46.155612\n",
      "\n",
      "Training: batch 6 ends at 22:10:46.950530\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4997 - bce_dice_loss: 0.4997\n",
      "Training: batch 7 begins at 22:10:46.954417\n",
      "\n",
      "Training: batch 7 ends at 22:10:47.773358\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.5057 - bce_dice_loss: 0.5057\n",
      "Training: batch 8 begins at 22:10:47.777409\n",
      "\n",
      "Training: batch 8 ends at 22:10:48.574163\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.4988 - bce_dice_loss: 0.4988\n",
      "Training: batch 9 begins at 22:10:48.577698\n",
      "\n",
      "Training: batch 9 ends at 22:10:49.390144\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4979 - bce_dice_loss: 0.4979\n",
      "Training: batch 10 begins at 22:10:49.393517\n",
      "\n",
      "Training: batch 10 ends at 22:10:50.189334\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5081 - bce_dice_loss: 0.5081\n",
      "Training: batch 11 begins at 22:10:50.193274\n",
      "\n",
      "Training: batch 11 ends at 22:10:50.999474\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5068 - bce_dice_loss: 0.5068\n",
      "Training: batch 12 begins at 22:10:51.003913\n",
      "\n",
      "Training: batch 12 ends at 22:10:51.795985\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.5089 - bce_dice_loss: 0.5089\n",
      "Training: batch 13 begins at 22:10:51.798619\n",
      "\n",
      "Training: batch 13 ends at 22:10:52.594260\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.4974 - bce_dice_loss: 0.4974\n",
      "Training: batch 14 begins at 22:10:52.598628\n",
      "\n",
      "Training: batch 14 ends at 22:10:53.421834\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4959 - bce_dice_loss: 0.4959\n",
      "Training: batch 15 begins at 22:10:53.425364\n",
      "\n",
      "Training: batch 15 ends at 22:10:54.222532\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5093 - bce_dice_loss: 0.5093\n",
      "Training: batch 16 begins at 22:10:54.225863\n",
      "\n",
      "Training: batch 16 ends at 22:10:55.023100\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5066 - bce_dice_loss: 0.5066\n",
      "Training: batch 17 begins at 22:10:55.026484\n",
      "\n",
      "Training: batch 17 ends at 22:10:55.851100\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.5062 - bce_dice_loss: 0.5062\n",
      "Training: batch 18 begins at 22:10:55.855087\n",
      "\n",
      "Training: batch 18 ends at 22:10:56.658093\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5054 - bce_dice_loss: 0.5054\n",
      "Training: batch 19 begins at 22:10:56.661478\n",
      "\n",
      "Training: batch 19 ends at 22:10:57.466824\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5038 - bce_dice_loss: 0.5038\n",
      "Training: batch 20 begins at 22:10:57.470536\n",
      "\n",
      "Training: batch 20 ends at 22:10:58.266591\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5171 - bce_dice_loss: 0.5171\n",
      "Training: batch 21 begins at 22:10:58.271529\n",
      "\n",
      "Training: batch 21 ends at 22:10:59.060400\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5155 - bce_dice_loss: 0.5155\n",
      "Training: batch 22 begins at 22:10:59.063712\n",
      "\n",
      "Training: batch 22 ends at 22:10:59.880726\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5253 - bce_dice_loss: 0.5253\n",
      "Training: batch 23 begins at 22:10:59.886423\n",
      "\n",
      "Training: batch 23 ends at 22:11:00.665913\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.5201 - bce_dice_loss: 0.5201\n",
      "Training: batch 24 begins at 22:11:00.670109\n",
      "\n",
      "Training: batch 24 ends at 22:11:01.469892\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5090 - bce_dice_loss: 0.5090\n",
      "Training: batch 25 begins at 22:11:01.472910\n",
      "\n",
      "Training: batch 25 ends at 22:11:02.299694\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5106 - bce_dice_loss: 0.5106\n",
      "Training: batch 26 begins at 22:11:02.304448\n",
      "\n",
      "Training: batch 26 ends at 22:11:03.096440\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5055 - bce_dice_loss: 0.5055\n",
      "Training: batch 27 begins at 22:11:03.100760\n",
      "\n",
      "Training: batch 27 ends at 22:11:03.888831\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5098 - bce_dice_loss: 0.5098\n",
      "Training: batch 28 begins at 22:11:03.893129\n",
      "\n",
      "Training: batch 28 ends at 22:11:04.685514\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.5196 - bce_dice_loss: 0.5196\n",
      "Training: batch 29 begins at 22:11:04.689022\n",
      "\n",
      "Training: batch 29 ends at 22:11:05.476928\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5257 - bce_dice_loss: 0.5257\n",
      "Training: batch 30 begins at 22:11:05.481440\n",
      "\n",
      "Training: batch 30 ends at 22:11:06.275596\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5299 - bce_dice_loss: 0.5299\n",
      "Training: batch 31 begins at 22:11:06.280465\n",
      "\n",
      "Training: batch 31 ends at 22:11:07.075730\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5278 - bce_dice_loss: 0.5278\n",
      "Training: batch 32 begins at 22:11:07.080137\n",
      "\n",
      "Training: batch 32 ends at 22:11:07.872567\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5302 - bce_dice_loss: 0.5302\n",
      "Training: batch 33 begins at 22:11:07.876111\n",
      "\n",
      "Training: batch 33 ends at 22:11:08.662239\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5312 - bce_dice_loss: 0.5312\n",
      "Training: batch 34 begins at 22:11:08.665789\n",
      "\n",
      "Training: batch 34 ends at 22:11:09.470588\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5284 - bce_dice_loss: 0.5284\n",
      "Training: batch 35 begins at 22:11:09.474638\n",
      "\n",
      "Training: batch 35 ends at 22:11:10.297921\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5256 - bce_dice_loss: 0.5256\n",
      "Training: batch 36 begins at 22:11:10.302707\n",
      "\n",
      "Training: batch 36 ends at 22:11:11.099751\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5279 - bce_dice_loss: 0.5279\n",
      "Training: batch 37 begins at 22:11:11.104609\n",
      "\n",
      "Training: batch 37 ends at 22:11:11.897049\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5300 - bce_dice_loss: 0.5300\n",
      "Training: batch 38 begins at 22:11:11.901468\n",
      "\n",
      "Training: batch 38 ends at 22:11:12.691514\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5300 - bce_dice_loss: 0.5300\n",
      "Training: batch 39 begins at 22:11:12.695734\n",
      "\n",
      "Training: batch 39 ends at 22:11:13.486803\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5292 - bce_dice_loss: 0.5292\n",
      "Training: batch 40 begins at 22:11:13.490385\n",
      "\n",
      "Training: batch 40 ends at 22:11:14.282239\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5358 - bce_dice_loss: 0.5358\n",
      "Training: batch 41 begins at 22:11:14.287527\n",
      "\n",
      "Training: batch 41 ends at 22:11:15.086495\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5372 - bce_dice_loss: 0.5372\n",
      "Training: batch 42 begins at 22:11:15.089916\n",
      "\n",
      "Training: batch 42 ends at 22:11:15.903108\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5391 - bce_dice_loss: 0.5391\n",
      "Training: batch 43 begins at 22:11:15.907164\n",
      "\n",
      "Training: batch 43 ends at 22:11:16.696290\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5387 - bce_dice_loss: 0.5387\n",
      "Training: batch 44 begins at 22:11:16.700330\n",
      "\n",
      "Training: batch 44 ends at 22:11:17.485426\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5452 - bce_dice_loss: 0.5452\n",
      "Training: batch 45 begins at 22:11:17.488945\n",
      "\n",
      "Training: batch 45 ends at 22:11:18.275313\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5422 - bce_dice_loss: 0.5422\n",
      "Training: batch 46 begins at 22:11:18.278805\n",
      "\n",
      "Training: batch 46 ends at 22:11:19.092599\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5414 - bce_dice_loss: 0.5414\n",
      "Training: batch 47 begins at 22:11:19.095590\n",
      "\n",
      "Training: batch 47 ends at 22:11:19.907384\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5361 - bce_dice_loss: 0.5361 \n",
      "Training: batch 48 begins at 22:11:19.911940\n",
      "\n",
      "Training: batch 48 ends at 22:11:20.704742\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5324 - bce_dice_loss: 0.5324\n",
      "Training: batch 49 begins at 22:11:20.709431\n",
      "\n",
      "Training: batch 49 ends at 22:11:21.503378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5340 - bce_dice_loss: 0.5340\n",
      "Training: batch 50 begins at 22:11:21.507501\n",
      "\n",
      "Training: batch 50 ends at 22:11:22.299956\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5343 - bce_dice_loss: 0.5343\n",
      "Training: batch 51 begins at 22:11:22.304956\n",
      "\n",
      "Training: batch 51 ends at 22:11:23.105164\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5348 - bce_dice_loss: 0.5348\n",
      "Training: batch 52 begins at 22:11:23.108534\n",
      "\n",
      "Training: batch 52 ends at 22:11:23.899648\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5371 - bce_dice_loss: 0.5371\n",
      "Training: batch 53 begins at 22:11:23.903355\n",
      "\n",
      "Training: batch 53 ends at 22:11:24.694641\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5338 - bce_dice_loss: 0.5338\n",
      "Training: batch 54 begins at 22:11:24.698269\n",
      "\n",
      "Training: batch 54 ends at 22:11:25.493329\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5332 - bce_dice_loss: 0.5332\n",
      "Training: batch 55 begins at 22:11:25.496219\n",
      "\n",
      "Training: batch 55 ends at 22:11:26.291643\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5317 - bce_dice_loss: 0.5317\n",
      "Training: batch 56 begins at 22:11:26.295845\n",
      "\n",
      "Training: batch 56 ends at 22:11:27.085774\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5310 - bce_dice_loss: 0.5310\n",
      "Training: batch 57 begins at 22:11:27.089503\n",
      "\n",
      "Training: batch 57 ends at 22:11:27.880880\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5339 - bce_dice_loss: 0.5339\n",
      "Training: batch 58 begins at 22:11:27.885559\n",
      "\n",
      "Training: batch 58 ends at 22:11:28.691003\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5355 - bce_dice_loss: 0.5355\n",
      "Training: batch 59 begins at 22:11:28.695267\n",
      "\n",
      "Training: batch 59 ends at 22:11:29.499141\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5349 - bce_dice_loss: 0.5349\n",
      "Evaluating: batch 0 begins at 22:11:29.531437\n",
      "\n",
      "Evaluating: batch 0 ends at 22:11:29.800546\n",
      "\n",
      "Evaluating: batch 1 begins at 22:11:29.802085\n",
      "\n",
      "Evaluating: batch 1 ends at 22:11:30.019396\n",
      "\n",
      "Evaluating: batch 2 begins at 22:11:30.020860\n",
      "\n",
      "Evaluating: batch 2 ends at 22:11:30.240654\n",
      "\n",
      "Evaluating: batch 3 begins at 22:11:30.243112\n",
      "\n",
      "Evaluating: batch 3 ends at 22:11:30.464146\n",
      "\n",
      "Evaluating: batch 4 begins at 22:11:30.466058\n",
      "\n",
      "Evaluating: batch 4 ends at 22:11:30.686396\n",
      "\n",
      "Evaluating: batch 5 begins at 22:11:30.688612\n",
      "\n",
      "Evaluating: batch 5 ends at 22:11:30.911989\n",
      "\n",
      "Evaluating: batch 6 begins at 22:11:30.913322\n",
      "\n",
      "Evaluating: batch 6 ends at 22:11:31.134379\n",
      "\n",
      "Evaluating: batch 7 begins at 22:11:31.136043\n",
      "\n",
      "Evaluating: batch 7 ends at 22:11:31.356633\n",
      "\n",
      "Evaluating: batch 8 begins at 22:11:31.358647\n",
      "\n",
      "Evaluating: batch 8 ends at 22:11:31.577369\n",
      "\n",
      "Evaluating: batch 9 begins at 22:11:31.579815\n",
      "\n",
      "Evaluating: batch 9 ends at 22:11:31.800591\n",
      "\n",
      "Evaluating: batch 10 begins at 22:11:31.802298\n",
      "\n",
      "Evaluating: batch 10 ends at 22:11:32.021786\n",
      "\n",
      "Evaluating: batch 11 begins at 22:11:32.024009\n",
      "\n",
      "Evaluating: batch 11 ends at 22:11:32.247710\n",
      "\n",
      "Evaluating: batch 12 begins at 22:11:32.249048\n",
      "\n",
      "Evaluating: batch 12 ends at 22:11:32.469486\n",
      "\n",
      "Evaluating: batch 13 begins at 22:11:32.471254\n",
      "\n",
      "Evaluating: batch 13 ends at 22:11:32.690095\n",
      "\n",
      "Evaluating: batch 14 begins at 22:11:32.691646\n",
      "\n",
      "Evaluating: batch 14 ends at 22:11:32.912718\n",
      "\n",
      "Evaluating: batch 15 begins at 22:11:32.914037\n",
      "\n",
      "Evaluating: batch 15 ends at 22:11:33.134671\n",
      "\n",
      "Evaluating: batch 16 begins at 22:11:33.136378\n",
      "\n",
      "Evaluating: batch 16 ends at 22:11:33.358596\n",
      "\n",
      "Evaluating: batch 17 begins at 22:11:33.360457\n",
      "\n",
      "Evaluating: batch 17 ends at 22:11:33.578919\n",
      "\n",
      "Evaluating: batch 18 begins at 22:11:33.580125\n",
      "\n",
      "Evaluating: batch 18 ends at 22:11:33.796395\n",
      "\n",
      "Evaluating: batch 19 begins at 22:11:33.797644\n",
      "\n",
      "Evaluating: batch 19 ends at 22:11:34.013829\n",
      "\n",
      "Evaluating: batch 20 begins at 22:11:34.015098\n",
      "\n",
      "Evaluating: batch 20 ends at 22:11:34.231167\n",
      "\n",
      "Evaluating: batch 21 begins at 22:11:34.232549\n",
      "\n",
      "Evaluating: batch 21 ends at 22:11:34.446885\n",
      "\n",
      "Evaluating: batch 22 begins at 22:11:34.448150\n",
      "\n",
      "Evaluating: batch 22 ends at 22:11:34.664354\n",
      "\n",
      "Evaluating: batch 23 begins at 22:11:34.665816\n",
      "\n",
      "Evaluating: batch 23 ends at 22:11:34.886191\n",
      "\n",
      "Evaluating: batch 24 begins at 22:11:34.887914\n",
      "\n",
      "Evaluating: batch 24 ends at 22:11:35.099701\n",
      "\n",
      "Evaluating: batch 25 begins at 22:11:35.101192\n",
      "\n",
      "Evaluating: batch 25 ends at 22:11:35.319272\n",
      "\n",
      "Evaluating: batch 26 begins at 22:11:35.322053\n",
      "\n",
      "Evaluating: batch 26 ends at 22:11:35.538591\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.53588 to 0.53453, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 932ms/step - loss: 0.5349 - bce_dice_loss: 0.5349 - val_loss: 0.5345 - val_bce_dice_loss: 0.5345\n",
      "Epoch 20/25\n",
      "\n",
      "Training: batch 0 begins at 22:11:37.055508\n",
      "\n",
      "Training: batch 0 ends at 22:11:37.858950\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.7176 - bce_dice_loss: 0.7176\n",
      "Training: batch 1 begins at 22:11:37.864472\n",
      "\n",
      "Training: batch 1 ends at 22:11:38.687052\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.7749 - bce_dice_loss: 0.7749\n",
      "Training: batch 2 begins at 22:11:38.690812\n",
      "\n",
      "Training: batch 2 ends at 22:11:39.504060\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.7640 - bce_dice_loss: 0.7640\n",
      "Training: batch 3 begins at 22:11:39.508618\n",
      "\n",
      "Training: batch 3 ends at 22:11:40.307553\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.6940 - bce_dice_loss: 0.6940\n",
      "Training: batch 4 begins at 22:11:40.311076\n",
      "\n",
      "Training: batch 4 ends at 22:11:41.108813\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.6160 - bce_dice_loss: 0.6160\n",
      "Training: batch 5 begins at 22:11:41.112934\n",
      "\n",
      "Training: batch 5 ends at 22:11:41.900715\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.6381 - bce_dice_loss: 0.6381\n",
      "Training: batch 6 begins at 22:11:41.904606\n",
      "\n",
      "Training: batch 6 ends at 22:11:42.695672\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.6249 - bce_dice_loss: 0.6249\n",
      "Training: batch 7 begins at 22:11:42.699748\n",
      "\n",
      "Training: batch 7 ends at 22:11:43.507535\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.6041 - bce_dice_loss: 0.6041\n",
      "Training: batch 8 begins at 22:11:43.511081\n",
      "\n",
      "Training: batch 8 ends at 22:11:44.306764\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5959 - bce_dice_loss: 0.5959\n",
      "Training: batch 9 begins at 22:11:44.309793\n",
      "\n",
      "Training: batch 9 ends at 22:11:45.118541\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5778 - bce_dice_loss: 0.5778\n",
      "Training: batch 10 begins at 22:11:45.122285\n",
      "\n",
      "Training: batch 10 ends at 22:11:45.938463\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5847 - bce_dice_loss: 0.5847\n",
      "Training: batch 11 begins at 22:11:45.941783\n",
      "\n",
      "Training: batch 11 ends at 22:11:46.734466\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5959 - bce_dice_loss: 0.5959\n",
      "Training: batch 12 begins at 22:11:46.738551\n",
      "\n",
      "Training: batch 12 ends at 22:11:47.553998\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5827 - bce_dice_loss: 0.5827\n",
      "Training: batch 13 begins at 22:11:47.557401\n",
      "\n",
      "Training: batch 13 ends at 22:11:48.344878\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.5724 - bce_dice_loss: 0.5724\n",
      "Training: batch 14 begins at 22:11:48.350094\n",
      "\n",
      "Training: batch 14 ends at 22:11:49.155956\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5749 - bce_dice_loss: 0.5749\n",
      "Training: batch 15 begins at 22:11:49.159356\n",
      "\n",
      "Training: batch 15 ends at 22:11:49.973027\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5729 - bce_dice_loss: 0.5729\n",
      "Training: batch 16 begins at 22:11:49.976118\n",
      "\n",
      "Training: batch 16 ends at 22:11:50.770493\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5705 - bce_dice_loss: 0.5705\n",
      "Training: batch 17 begins at 22:11:50.773745\n",
      "\n",
      "Training: batch 17 ends at 22:11:51.563024\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5672 - bce_dice_loss: 0.5672\n",
      "Training: batch 18 begins at 22:11:51.567608\n",
      "\n",
      "Training: batch 18 ends at 22:11:52.355589\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5727 - bce_dice_loss: 0.5727\n",
      "Training: batch 19 begins at 22:11:52.359298\n",
      "\n",
      "Training: batch 19 ends at 22:11:53.153375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5714 - bce_dice_loss: 0.5714\n",
      "Training: batch 20 begins at 22:11:53.157775\n",
      "\n",
      "Training: batch 20 ends at 22:11:53.949912\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5726 - bce_dice_loss: 0.5726\n",
      "Training: batch 21 begins at 22:11:53.954460\n",
      "\n",
      "Training: batch 21 ends at 22:11:54.751315\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5735 - bce_dice_loss: 0.5735\n",
      "Training: batch 22 begins at 22:11:54.753743\n",
      "\n",
      "Training: batch 22 ends at 22:11:55.531294\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5803 - bce_dice_loss: 0.5803\n",
      "Training: batch 23 begins at 22:11:55.535955\n",
      "\n",
      "Training: batch 23 ends at 22:11:56.355493\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5737 - bce_dice_loss: 0.5737\n",
      "Training: batch 24 begins at 22:11:56.359653\n",
      "\n",
      "Training: batch 24 ends at 22:11:57.155757\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5743 - bce_dice_loss: 0.5743\n",
      "Training: batch 25 begins at 22:11:57.159245\n",
      "\n",
      "Training: batch 25 ends at 22:11:57.954747\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5759 - bce_dice_loss: 0.5759\n",
      "Training: batch 26 begins at 22:11:57.958076\n",
      "\n",
      "Training: batch 26 ends at 22:11:58.757688\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5733 - bce_dice_loss: 0.5733\n",
      "Training: batch 27 begins at 22:11:58.762150\n",
      "\n",
      "Training: batch 27 ends at 22:11:59.571703\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5736 - bce_dice_loss: 0.5736\n",
      "Training: batch 28 begins at 22:11:59.576040\n",
      "\n",
      "Training: batch 28 ends at 22:12:00.391920\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5712 - bce_dice_loss: 0.5712\n",
      "Training: batch 29 begins at 22:12:00.399115\n",
      "\n",
      "Training: batch 29 ends at 22:12:01.194705\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5657 - bce_dice_loss: 0.5657\n",
      "Training: batch 30 begins at 22:12:01.199024\n",
      "\n",
      "Training: batch 30 ends at 22:12:01.997957\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5653 - bce_dice_loss: 0.5653\n",
      "Training: batch 31 begins at 22:12:02.002473\n",
      "\n",
      "Training: batch 31 ends at 22:12:02.798431\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5635 - bce_dice_loss: 0.5635\n",
      "Training: batch 32 begins at 22:12:02.803773\n",
      "\n",
      "Training: batch 32 ends at 22:12:03.603351\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5624 - bce_dice_loss: 0.5624\n",
      "Training: batch 33 begins at 22:12:03.607861\n",
      "\n",
      "Training: batch 33 ends at 22:12:04.401912\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5579 - bce_dice_loss: 0.5579\n",
      "Training: batch 34 begins at 22:12:04.405481\n",
      "\n",
      "Training: batch 34 ends at 22:12:05.217194\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5525 - bce_dice_loss: 0.5525\n",
      "Training: batch 35 begins at 22:12:05.220519\n",
      "\n",
      "Training: batch 35 ends at 22:12:06.014006\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5490 - bce_dice_loss: 0.5490\n",
      "Training: batch 36 begins at 22:12:06.019430\n",
      "\n",
      "Training: batch 36 ends at 22:12:06.813428\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5443 - bce_dice_loss: 0.5443\n",
      "Training: batch 37 begins at 22:12:06.817882\n",
      "\n",
      "Training: batch 37 ends at 22:12:07.613705\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5543 - bce_dice_loss: 0.5543\n",
      "Training: batch 38 begins at 22:12:07.619157\n",
      "\n",
      "Training: batch 38 ends at 22:12:08.410408\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5543 - bce_dice_loss: 0.5543\n",
      "Training: batch 39 begins at 22:12:08.414808\n",
      "\n",
      "Training: batch 39 ends at 22:12:09.225338\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5498 - bce_dice_loss: 0.5498\n",
      "Training: batch 40 begins at 22:12:09.229199\n",
      "\n",
      "Training: batch 40 ends at 22:12:10.037893\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5471 - bce_dice_loss: 0.5471\n",
      "Training: batch 41 begins at 22:12:10.041137\n",
      "\n",
      "Training: batch 41 ends at 22:12:10.838891\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5457 - bce_dice_loss: 0.5457\n",
      "Training: batch 42 begins at 22:12:10.843209\n",
      "\n",
      "Training: batch 42 ends at 22:12:11.629313\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5501 - bce_dice_loss: 0.5501\n",
      "Training: batch 43 begins at 22:12:11.634072\n",
      "\n",
      "Training: batch 43 ends at 22:12:12.424727\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5493 - bce_dice_loss: 0.5493\n",
      "Training: batch 44 begins at 22:12:12.428961\n",
      "\n",
      "Training: batch 44 ends at 22:12:13.245103\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5480 - bce_dice_loss: 0.5480\n",
      "Training: batch 45 begins at 22:12:13.249462\n",
      "\n",
      "Training: batch 45 ends at 22:12:14.042211\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5464 - bce_dice_loss: 0.5464\n",
      "Training: batch 46 begins at 22:12:14.046310\n",
      "\n",
      "Training: batch 46 ends at 22:12:14.837079\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5398 - bce_dice_loss: 0.5398\n",
      "Training: batch 47 begins at 22:12:14.839908\n",
      "\n",
      "Training: batch 47 ends at 22:12:15.636750\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5458 - bce_dice_loss: 0.5458 \n",
      "Training: batch 48 begins at 22:12:15.640027\n",
      "\n",
      "Training: batch 48 ends at 22:12:16.459056\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5458 - bce_dice_loss: 0.5458\n",
      "Training: batch 49 begins at 22:12:16.463285\n",
      "\n",
      "Training: batch 49 ends at 22:12:17.257995\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5419 - bce_dice_loss: 0.5419\n",
      "Training: batch 50 begins at 22:12:17.262331\n",
      "\n",
      "Training: batch 50 ends at 22:12:18.054540\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5429 - bce_dice_loss: 0.5429\n",
      "Training: batch 51 begins at 22:12:18.059349\n",
      "\n",
      "Training: batch 51 ends at 22:12:18.855759\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5416 - bce_dice_loss: 0.5416\n",
      "Training: batch 52 begins at 22:12:18.859595\n",
      "\n",
      "Training: batch 52 ends at 22:12:19.652160\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5424 - bce_dice_loss: 0.5424\n",
      "Training: batch 53 begins at 22:12:19.655647\n",
      "\n",
      "Training: batch 53 ends at 22:12:20.453410\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5393 - bce_dice_loss: 0.5393\n",
      "Training: batch 54 begins at 22:12:20.457044\n",
      "\n",
      "Training: batch 54 ends at 22:12:21.248267\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5373 - bce_dice_loss: 0.5373\n",
      "Training: batch 55 begins at 22:12:21.252342\n",
      "\n",
      "Training: batch 55 ends at 22:12:22.073192\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5417 - bce_dice_loss: 0.5417\n",
      "Training: batch 56 begins at 22:12:22.077545\n",
      "\n",
      "Training: batch 56 ends at 22:12:22.868614\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5412 - bce_dice_loss: 0.5412\n",
      "Training: batch 57 begins at 22:12:22.872373\n",
      "\n",
      "Training: batch 57 ends at 22:12:23.664086\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5356 - bce_dice_loss: 0.5356\n",
      "Training: batch 58 begins at 22:12:23.668835\n",
      "\n",
      "Training: batch 58 ends at 22:12:24.462548\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5340 - bce_dice_loss: 0.5340\n",
      "Training: batch 59 begins at 22:12:24.467171\n",
      "\n",
      "Training: batch 59 ends at 22:12:25.281260\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5322 - bce_dice_loss: 0.5322\n",
      "Evaluating: batch 0 begins at 22:12:25.312058\n",
      "\n",
      "Evaluating: batch 0 ends at 22:12:25.583819\n",
      "\n",
      "Evaluating: batch 1 begins at 22:12:25.585163\n",
      "\n",
      "Evaluating: batch 1 ends at 22:12:25.801417\n",
      "\n",
      "Evaluating: batch 2 begins at 22:12:25.802747\n",
      "\n",
      "Evaluating: batch 2 ends at 22:12:26.018808\n",
      "\n",
      "Evaluating: batch 3 begins at 22:12:26.020344\n",
      "\n",
      "Evaluating: batch 3 ends at 22:12:26.237255\n",
      "\n",
      "Evaluating: batch 4 begins at 22:12:26.239641\n",
      "\n",
      "Evaluating: batch 4 ends at 22:12:26.458682\n",
      "\n",
      "Evaluating: batch 5 begins at 22:12:26.461128\n",
      "\n",
      "Evaluating: batch 5 ends at 22:12:26.680214\n",
      "\n",
      "Evaluating: batch 6 begins at 22:12:26.682609\n",
      "\n",
      "Evaluating: batch 6 ends at 22:12:26.905607\n",
      "\n",
      "Evaluating: batch 7 begins at 22:12:26.906906\n",
      "\n",
      "Evaluating: batch 7 ends at 22:12:27.126891\n",
      "\n",
      "Evaluating: batch 8 begins at 22:12:27.128423\n",
      "\n",
      "Evaluating: batch 8 ends at 22:12:27.349470\n",
      "\n",
      "Evaluating: batch 9 begins at 22:12:27.351693\n",
      "\n",
      "Evaluating: batch 9 ends at 22:12:27.569643\n",
      "\n",
      "Evaluating: batch 10 begins at 22:12:27.572217\n",
      "\n",
      "Evaluating: batch 10 ends at 22:12:27.799852\n",
      "\n",
      "Evaluating: batch 11 begins at 22:12:27.801533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 11 ends at 22:12:28.021976\n",
      "\n",
      "Evaluating: batch 12 begins at 22:12:28.024615\n",
      "\n",
      "Evaluating: batch 12 ends at 22:12:28.243114\n",
      "\n",
      "Evaluating: batch 13 begins at 22:12:28.244585\n",
      "\n",
      "Evaluating: batch 13 ends at 22:12:28.465182\n",
      "\n",
      "Evaluating: batch 14 begins at 22:12:28.467519\n",
      "\n",
      "Evaluating: batch 14 ends at 22:12:28.686930\n",
      "\n",
      "Evaluating: batch 15 begins at 22:12:28.688769\n",
      "\n",
      "Evaluating: batch 15 ends at 22:12:28.911357\n",
      "\n",
      "Evaluating: batch 16 begins at 22:12:28.912642\n",
      "\n",
      "Evaluating: batch 16 ends at 22:12:29.132912\n",
      "\n",
      "Evaluating: batch 17 begins at 22:12:29.134863\n",
      "\n",
      "Evaluating: batch 17 ends at 22:12:29.354487\n",
      "\n",
      "Evaluating: batch 18 begins at 22:12:29.356057\n",
      "\n",
      "Evaluating: batch 18 ends at 22:12:29.587300\n",
      "\n",
      "Evaluating: batch 19 begins at 22:12:29.589636\n",
      "\n",
      "Evaluating: batch 19 ends at 22:12:29.810009\n",
      "\n",
      "Evaluating: batch 20 begins at 22:12:29.811482\n",
      "\n",
      "Evaluating: batch 20 ends at 22:12:30.027686\n",
      "\n",
      "Evaluating: batch 21 begins at 22:12:30.028924\n",
      "\n",
      "Evaluating: batch 21 ends at 22:12:30.244417\n",
      "\n",
      "Evaluating: batch 22 begins at 22:12:30.245691\n",
      "\n",
      "Evaluating: batch 22 ends at 22:12:30.462155\n",
      "\n",
      "Evaluating: batch 23 begins at 22:12:30.463421\n",
      "\n",
      "Evaluating: batch 23 ends at 22:12:30.681884\n",
      "\n",
      "Evaluating: batch 24 begins at 22:12:30.683239\n",
      "\n",
      "Evaluating: batch 24 ends at 22:12:30.906846\n",
      "\n",
      "Evaluating: batch 25 begins at 22:12:30.908444\n",
      "\n",
      "Evaluating: batch 25 ends at 22:12:31.128244\n",
      "\n",
      "Evaluating: batch 26 begins at 22:12:31.130436\n",
      "\n",
      "Evaluating: batch 26 ends at 22:12:31.346083\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.53453 to 0.52206, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 0.5322 - bce_dice_loss: 0.5322 - val_loss: 0.5221 - val_bce_dice_loss: 0.5221\n",
      "Epoch 21/25\n",
      "\n",
      "Training: batch 0 begins at 22:12:33.006184\n",
      "\n",
      "Training: batch 0 ends at 22:12:33.829690\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3922 - bce_dice_loss: 0.3922\n",
      "Training: batch 1 begins at 22:12:33.835117\n",
      "\n",
      "Training: batch 1 ends at 22:12:34.639387\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5711 - bce_dice_loss: 0.5711\n",
      "Training: batch 2 begins at 22:12:34.642811\n",
      "\n",
      "Training: batch 2 ends at 22:12:35.437220\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.5016 - bce_dice_loss: 0.5016\n",
      "Training: batch 3 begins at 22:12:35.441656\n",
      "\n",
      "Training: batch 3 ends at 22:12:36.235794\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.4788 - bce_dice_loss: 0.4788\n",
      "Training: batch 4 begins at 22:12:36.239487\n",
      "\n",
      "Training: batch 4 ends at 22:12:37.031098\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4827 - bce_dice_loss: 0.4827\n",
      "Training: batch 5 begins at 22:12:37.035736\n",
      "\n",
      "Training: batch 5 ends at 22:12:37.840038\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5123 - bce_dice_loss: 0.5123\n",
      "Training: batch 6 begins at 22:12:37.844398\n",
      "\n",
      "Training: batch 6 ends at 22:12:38.635724\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5209 - bce_dice_loss: 0.5209\n",
      "Training: batch 7 begins at 22:12:38.639267\n",
      "\n",
      "Training: batch 7 ends at 22:12:39.431275\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5033 - bce_dice_loss: 0.5033\n",
      "Training: batch 8 begins at 22:12:39.435500\n",
      "\n",
      "Training: batch 8 ends at 22:12:40.240806\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5170 - bce_dice_loss: 0.5170\n",
      "Training: batch 9 begins at 22:12:40.245038\n",
      "\n",
      "Training: batch 9 ends at 22:12:41.061926\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5179 - bce_dice_loss: 0.5179\n",
      "Training: batch 10 begins at 22:12:41.066911\n",
      "\n",
      "Training: batch 10 ends at 22:12:41.860110\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5155 - bce_dice_loss: 0.5155\n",
      "Training: batch 11 begins at 22:12:41.864248\n",
      "\n",
      "Training: batch 11 ends at 22:12:42.659760\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5047 - bce_dice_loss: 0.5047\n",
      "Training: batch 12 begins at 22:12:42.664168\n",
      "\n",
      "Training: batch 12 ends at 22:12:43.468437\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5045 - bce_dice_loss: 0.5045\n",
      "Training: batch 13 begins at 22:12:43.471860\n",
      "\n",
      "Training: batch 13 ends at 22:12:44.265435\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.4880 - bce_dice_loss: 0.4880\n",
      "Training: batch 14 begins at 22:12:44.269699\n",
      "\n",
      "Training: batch 14 ends at 22:12:45.062850\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4916 - bce_dice_loss: 0.4916\n",
      "Training: batch 15 begins at 22:12:45.067649\n",
      "\n",
      "Training: batch 15 ends at 22:12:45.860815\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4862 - bce_dice_loss: 0.4862\n",
      "Training: batch 16 begins at 22:12:45.865458\n",
      "\n",
      "Training: batch 16 ends at 22:12:46.685116\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5048 - bce_dice_loss: 0.5048\n",
      "Training: batch 17 begins at 22:12:46.689434\n",
      "\n",
      "Training: batch 17 ends at 22:12:47.488287\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5077 - bce_dice_loss: 0.5077\n",
      "Training: batch 18 begins at 22:12:47.492956\n",
      "\n",
      "Training: batch 18 ends at 22:12:48.314064\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5057 - bce_dice_loss: 0.5057\n",
      "Training: batch 19 begins at 22:12:48.318150\n",
      "\n",
      "Training: batch 19 ends at 22:12:49.112175\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5206 - bce_dice_loss: 0.5206\n",
      "Training: batch 20 begins at 22:12:49.116602\n",
      "\n",
      "Training: batch 20 ends at 22:12:49.917124\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5192 - bce_dice_loss: 0.5192\n",
      "Training: batch 21 begins at 22:12:49.920503\n",
      "\n",
      "Training: batch 21 ends at 22:12:50.719571\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5211 - bce_dice_loss: 0.5211\n",
      "Training: batch 22 begins at 22:12:50.723178\n",
      "\n",
      "Training: batch 22 ends at 22:12:51.536961\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5188 - bce_dice_loss: 0.5188\n",
      "Training: batch 23 begins at 22:12:51.540454\n",
      "\n",
      "Training: batch 23 ends at 22:12:52.334266\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5115 - bce_dice_loss: 0.5115\n",
      "Training: batch 24 begins at 22:12:52.338373\n",
      "\n",
      "Training: batch 24 ends at 22:12:53.129794\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5130 - bce_dice_loss: 0.5130\n",
      "Training: batch 25 begins at 22:12:53.134439\n",
      "\n",
      "Training: batch 25 ends at 22:12:53.953343\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5106 - bce_dice_loss: 0.5106\n",
      "Training: batch 26 begins at 22:12:53.956843\n",
      "\n",
      "Training: batch 26 ends at 22:12:54.747053\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5145 - bce_dice_loss: 0.5145\n",
      "Training: batch 27 begins at 22:12:54.751394\n",
      "\n",
      "Training: batch 27 ends at 22:12:55.546852\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5236 - bce_dice_loss: 0.5236\n",
      "Training: batch 28 begins at 22:12:55.551400\n",
      "\n",
      "Training: batch 28 ends at 22:12:56.368248\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5235 - bce_dice_loss: 0.5235\n",
      "Training: batch 29 begins at 22:12:56.372064\n",
      "\n",
      "Training: batch 29 ends at 22:12:57.165621\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5241 - bce_dice_loss: 0.5241\n",
      "Training: batch 30 begins at 22:12:57.170760\n",
      "\n",
      "Training: batch 30 ends at 22:12:57.952074\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5213 - bce_dice_loss: 0.5213\n",
      "Training: batch 31 begins at 22:12:57.957511\n",
      "\n",
      "Training: batch 31 ends at 22:12:58.737284\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5261 - bce_dice_loss: 0.5261\n",
      "Training: batch 32 begins at 22:12:58.740964\n",
      "\n",
      "Training: batch 32 ends at 22:12:59.548255\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5255 - bce_dice_loss: 0.5255\n",
      "Training: batch 33 begins at 22:12:59.552424\n",
      "\n",
      "Training: batch 33 ends at 22:13:00.349931\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5235 - bce_dice_loss: 0.5235\n",
      "Training: batch 34 begins at 22:13:00.354145\n",
      "\n",
      "Training: batch 34 ends at 22:13:01.151475\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5223 - bce_dice_loss: 0.5223\n",
      "Training: batch 35 begins at 22:13:01.155007\n",
      "\n",
      "Training: batch 35 ends at 22:13:01.953487\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5186 - bce_dice_loss: 0.5186\n",
      "Training: batch 36 begins at 22:13:01.957198\n",
      "\n",
      "Training: batch 36 ends at 22:13:02.751695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/60 [=================>............] - ETA: 18s - loss: 0.5185 - bce_dice_loss: 0.5185\n",
      "Training: batch 37 begins at 22:13:02.755390\n",
      "\n",
      "Training: batch 37 ends at 22:13:03.547668\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5197 - bce_dice_loss: 0.5197\n",
      "Training: batch 38 begins at 22:13:03.551965\n",
      "\n",
      "Training: batch 38 ends at 22:13:04.345400\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5183 - bce_dice_loss: 0.5183\n",
      "Training: batch 39 begins at 22:13:04.350115\n",
      "\n",
      "Training: batch 39 ends at 22:13:05.136973\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5156 - bce_dice_loss: 0.5156\n",
      "Training: batch 40 begins at 22:13:05.140448\n",
      "\n",
      "Training: batch 40 ends at 22:13:05.932641\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5123 - bce_dice_loss: 0.5123\n",
      "Training: batch 41 begins at 22:13:05.937153\n",
      "\n",
      "Training: batch 41 ends at 22:13:06.729261\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5103 - bce_dice_loss: 0.5103\n",
      "Training: batch 42 begins at 22:13:06.733871\n",
      "\n",
      "Training: batch 42 ends at 22:13:07.524271\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5085 - bce_dice_loss: 0.5085\n",
      "Training: batch 43 begins at 22:13:07.528875\n",
      "\n",
      "Training: batch 43 ends at 22:13:08.320868\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5044 - bce_dice_loss: 0.5044\n",
      "Training: batch 44 begins at 22:13:08.324344\n",
      "\n",
      "Training: batch 44 ends at 22:13:09.115012\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5138 - bce_dice_loss: 0.5138\n",
      "Training: batch 45 begins at 22:13:09.119495\n",
      "\n",
      "Training: batch 45 ends at 22:13:09.945896\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5144 - bce_dice_loss: 0.5144\n",
      "Training: batch 46 begins at 22:13:09.950628\n",
      "\n",
      "Training: batch 46 ends at 22:13:10.750475\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5192 - bce_dice_loss: 0.5192\n",
      "Training: batch 47 begins at 22:13:10.753632\n",
      "\n",
      "Training: batch 47 ends at 22:13:11.545082\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5159 - bce_dice_loss: 0.5159 \n",
      "Training: batch 48 begins at 22:13:11.549822\n",
      "\n",
      "Training: batch 48 ends at 22:13:12.338588\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5143 - bce_dice_loss: 0.5143\n",
      "Training: batch 49 begins at 22:13:12.341968\n",
      "\n",
      "Training: batch 49 ends at 22:13:13.142595\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5094 - bce_dice_loss: 0.5094\n",
      "Training: batch 50 begins at 22:13:13.146745\n",
      "\n",
      "Training: batch 50 ends at 22:13:13.945692\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5086 - bce_dice_loss: 0.5086\n",
      "Training: batch 51 begins at 22:13:13.951161\n",
      "\n",
      "Training: batch 51 ends at 22:13:14.745753\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5076 - bce_dice_loss: 0.5076\n",
      "Training: batch 52 begins at 22:13:14.749695\n",
      "\n",
      "Training: batch 52 ends at 22:13:15.545375\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5098 - bce_dice_loss: 0.5098\n",
      "Training: batch 53 begins at 22:13:15.549865\n",
      "\n",
      "Training: batch 53 ends at 22:13:16.342758\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5086 - bce_dice_loss: 0.5086\n",
      "Training: batch 54 begins at 22:13:16.346324\n",
      "\n",
      "Training: batch 54 ends at 22:13:17.135573\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5114 - bce_dice_loss: 0.5114\n",
      "Training: batch 55 begins at 22:13:17.138975\n",
      "\n",
      "Training: batch 55 ends at 22:13:17.945871\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5133 - bce_dice_loss: 0.5133\n",
      "Training: batch 56 begins at 22:13:17.950679\n",
      "\n",
      "Training: batch 56 ends at 22:13:18.738890\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5157 - bce_dice_loss: 0.5157\n",
      "Training: batch 57 begins at 22:13:18.742252\n",
      "\n",
      "Training: batch 57 ends at 22:13:19.535922\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5169 - bce_dice_loss: 0.5169\n",
      "Training: batch 58 begins at 22:13:19.538771\n",
      "\n",
      "Training: batch 58 ends at 22:13:20.358848\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5186 - bce_dice_loss: 0.5186\n",
      "Training: batch 59 begins at 22:13:20.363375\n",
      "\n",
      "Training: batch 59 ends at 22:13:21.154367\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5203 - bce_dice_loss: 0.5203\n",
      "Evaluating: batch 0 begins at 22:13:21.185783\n",
      "\n",
      "Evaluating: batch 0 ends at 22:13:21.456335\n",
      "\n",
      "Evaluating: batch 1 begins at 22:13:21.457737\n",
      "\n",
      "Evaluating: batch 1 ends at 22:13:21.673651\n",
      "\n",
      "Evaluating: batch 2 begins at 22:13:21.675485\n",
      "\n",
      "Evaluating: batch 2 ends at 22:13:21.892967\n",
      "\n",
      "Evaluating: batch 3 begins at 22:13:21.895673\n",
      "\n",
      "Evaluating: batch 3 ends at 22:13:22.116814\n",
      "\n",
      "Evaluating: batch 4 begins at 22:13:22.118441\n",
      "\n",
      "Evaluating: batch 4 ends at 22:13:22.338747\n",
      "\n",
      "Evaluating: batch 5 begins at 22:13:22.340659\n",
      "\n",
      "Evaluating: batch 5 ends at 22:13:22.561033\n",
      "\n",
      "Evaluating: batch 6 begins at 22:13:22.562827\n",
      "\n",
      "Evaluating: batch 6 ends at 22:13:22.785065\n",
      "\n",
      "Evaluating: batch 7 begins at 22:13:22.786707\n",
      "\n",
      "Evaluating: batch 7 ends at 22:13:23.006893\n",
      "\n",
      "Evaluating: batch 8 begins at 22:13:23.008259\n",
      "\n",
      "Evaluating: batch 8 ends at 22:13:23.227804\n",
      "\n",
      "Evaluating: batch 9 begins at 22:13:23.230158\n",
      "\n",
      "Evaluating: batch 9 ends at 22:13:23.449236\n",
      "\n",
      "Evaluating: batch 10 begins at 22:13:23.451797\n",
      "\n",
      "Evaluating: batch 10 ends at 22:13:23.671663\n",
      "\n",
      "Evaluating: batch 11 begins at 22:13:23.673929\n",
      "\n",
      "Evaluating: batch 11 ends at 22:13:23.894434\n",
      "\n",
      "Evaluating: batch 12 begins at 22:13:23.896213\n",
      "\n",
      "Evaluating: batch 12 ends at 22:13:24.112471\n",
      "\n",
      "Evaluating: batch 13 begins at 22:13:24.113818\n",
      "\n",
      "Evaluating: batch 13 ends at 22:13:24.334952\n",
      "\n",
      "Evaluating: batch 14 begins at 22:13:24.336138\n",
      "\n",
      "Evaluating: batch 14 ends at 22:13:24.552180\n",
      "\n",
      "Evaluating: batch 15 begins at 22:13:24.553438\n",
      "\n",
      "Evaluating: batch 15 ends at 22:13:24.775051\n",
      "\n",
      "Evaluating: batch 16 begins at 22:13:24.777480\n",
      "\n",
      "Evaluating: batch 16 ends at 22:13:24.997123\n",
      "\n",
      "Evaluating: batch 17 begins at 22:13:24.999415\n",
      "\n",
      "Evaluating: batch 17 ends at 22:13:25.216888\n",
      "\n",
      "Evaluating: batch 18 begins at 22:13:25.218972\n",
      "\n",
      "Evaluating: batch 18 ends at 22:13:25.436146\n",
      "\n",
      "Evaluating: batch 19 begins at 22:13:25.438102\n",
      "\n",
      "Evaluating: batch 19 ends at 22:13:25.653961\n",
      "\n",
      "Evaluating: batch 20 begins at 22:13:25.655982\n",
      "\n",
      "Evaluating: batch 20 ends at 22:13:25.875079\n",
      "\n",
      "Evaluating: batch 21 begins at 22:13:25.877523\n",
      "\n",
      "Evaluating: batch 21 ends at 22:13:26.094473\n",
      "\n",
      "Evaluating: batch 22 begins at 22:13:26.095948\n",
      "\n",
      "Evaluating: batch 22 ends at 22:13:26.311227\n",
      "\n",
      "Evaluating: batch 23 begins at 22:13:26.312499\n",
      "\n",
      "Evaluating: batch 23 ends at 22:13:26.529355\n",
      "\n",
      "Evaluating: batch 24 begins at 22:13:26.530776\n",
      "\n",
      "Evaluating: batch 24 ends at 22:13:26.750439\n",
      "\n",
      "Evaluating: batch 25 begins at 22:13:26.752584\n",
      "\n",
      "Evaluating: batch 25 ends at 22:13:26.971278\n",
      "\n",
      "Evaluating: batch 26 begins at 22:13:26.973107\n",
      "\n",
      "Evaluating: batch 26 ends at 22:13:27.192603\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.52206 to 0.51643, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 932ms/step - loss: 0.5203 - bce_dice_loss: 0.5203 - val_loss: 0.5164 - val_bce_dice_loss: 0.5164\n",
      "Epoch 22/25\n",
      "\n",
      "Training: batch 0 begins at 22:13:28.826259\n",
      "\n",
      "Training: batch 0 ends at 22:13:29.652337\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3479 - bce_dice_loss: 0.3479\n",
      "Training: batch 1 begins at 22:13:29.657225\n",
      "\n",
      "Training: batch 1 ends at 22:13:30.469307\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3677 - bce_dice_loss: 0.3677\n",
      "Training: batch 2 begins at 22:13:30.472936\n",
      "\n",
      "Training: batch 2 ends at 22:13:31.274881\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3984 - bce_dice_loss: 0.3984\n",
      "Training: batch 3 begins at 22:13:31.278806\n",
      "\n",
      "Training: batch 3 ends at 22:13:32.073434\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3865 - bce_dice_loss: 0.3865\n",
      "Training: batch 4 begins at 22:13:32.078142\n",
      "\n",
      "Training: batch 4 ends at 22:13:32.896899\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3885 - bce_dice_loss: 0.3885\n",
      "Training: batch 5 begins at 22:13:32.901029\n",
      "\n",
      "Training: batch 5 ends at 22:13:33.692254\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4547 - bce_dice_loss: 0.4547\n",
      "Training: batch 6 begins at 22:13:33.696319\n",
      "\n",
      "Training: batch 6 ends at 22:13:34.488731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4644 - bce_dice_loss: 0.4644\n",
      "Training: batch 7 begins at 22:13:34.492496\n",
      "\n",
      "Training: batch 7 ends at 22:13:35.284869\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4408 - bce_dice_loss: 0.4408\n",
      "Training: batch 8 begins at 22:13:35.288015\n",
      "\n",
      "Training: batch 8 ends at 22:13:36.074665\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.4569 - bce_dice_loss: 0.4569\n",
      "Training: batch 9 begins at 22:13:36.078751\n",
      "\n",
      "Training: batch 9 ends at 22:13:36.863092\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4575 - bce_dice_loss: 0.4575\n",
      "Training: batch 10 begins at 22:13:36.867736\n",
      "\n",
      "Training: batch 10 ends at 22:13:37.658562\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4590 - bce_dice_loss: 0.4590\n",
      "Training: batch 11 begins at 22:13:37.662814\n",
      "\n",
      "Training: batch 11 ends at 22:13:38.461993\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4856 - bce_dice_loss: 0.4856\n",
      "Training: batch 12 begins at 22:13:38.466999\n",
      "\n",
      "Training: batch 12 ends at 22:13:39.258673\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.4932 - bce_dice_loss: 0.4932\n",
      "Training: batch 13 begins at 22:13:39.262887\n",
      "\n",
      "Training: batch 13 ends at 22:13:40.115835\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.5037 - bce_dice_loss: 0.5037\n",
      "Training: batch 14 begins at 22:13:40.118294\n",
      "\n",
      "Training: batch 14 ends at 22:13:40.935670\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5265 - bce_dice_loss: 0.5265\n",
      "Training: batch 15 begins at 22:13:40.939354\n",
      "\n",
      "Training: batch 15 ends at 22:13:41.729133\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5236 - bce_dice_loss: 0.5236\n",
      "Training: batch 16 begins at 22:13:41.732670\n",
      "\n",
      "Training: batch 16 ends at 22:13:42.546884\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5234 - bce_dice_loss: 0.5234\n",
      "Training: batch 17 begins at 22:13:42.551101\n",
      "\n",
      "Training: batch 17 ends at 22:13:43.348380\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5280 - bce_dice_loss: 0.5280\n",
      "Training: batch 18 begins at 22:13:43.352707\n",
      "\n",
      "Training: batch 18 ends at 22:13:44.145293\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5206 - bce_dice_loss: 0.5206\n",
      "Training: batch 19 begins at 22:13:44.149802\n",
      "\n",
      "Training: batch 19 ends at 22:13:44.947288\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5110 - bce_dice_loss: 0.5110\n",
      "Training: batch 20 begins at 22:13:44.951583\n",
      "\n",
      "Training: batch 20 ends at 22:13:45.744346\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5114 - bce_dice_loss: 0.5114\n",
      "Training: batch 21 begins at 22:13:45.747059\n",
      "\n",
      "Training: batch 21 ends at 22:13:46.542677\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5091 - bce_dice_loss: 0.5091\n",
      "Training: batch 22 begins at 22:13:46.548017\n",
      "\n",
      "Training: batch 22 ends at 22:13:47.347275\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5063 - bce_dice_loss: 0.5063\n",
      "Training: batch 23 begins at 22:13:47.351942\n",
      "\n",
      "Training: batch 23 ends at 22:13:48.143070\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.5021 - bce_dice_loss: 0.5021\n",
      "Training: batch 24 begins at 22:13:48.147281\n",
      "\n",
      "Training: batch 24 ends at 22:13:48.932575\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4962 - bce_dice_loss: 0.4962\n",
      "Training: batch 25 begins at 22:13:48.936390\n",
      "\n",
      "Training: batch 25 ends at 22:13:49.750348\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5026 - bce_dice_loss: 0.5026\n",
      "Training: batch 26 begins at 22:13:49.753969\n",
      "\n",
      "Training: batch 26 ends at 22:13:50.545855\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5094 - bce_dice_loss: 0.5094\n",
      "Training: batch 27 begins at 22:13:50.551129\n",
      "\n",
      "Training: batch 27 ends at 22:13:51.342566\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.5070 - bce_dice_loss: 0.5070\n",
      "Training: batch 28 begins at 22:13:51.347437\n",
      "\n",
      "Training: batch 28 ends at 22:13:52.140768\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5127 - bce_dice_loss: 0.5127\n",
      "Training: batch 29 begins at 22:13:52.145153\n",
      "\n",
      "Training: batch 29 ends at 22:13:52.959923\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5072 - bce_dice_loss: 0.5072\n",
      "Training: batch 30 begins at 22:13:52.964789\n",
      "\n",
      "Training: batch 30 ends at 22:13:53.744000\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5087 - bce_dice_loss: 0.5087\n",
      "Training: batch 31 begins at 22:13:53.748012\n",
      "\n",
      "Training: batch 31 ends at 22:13:54.542417\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5046 - bce_dice_loss: 0.5046\n",
      "Training: batch 32 begins at 22:13:54.546031\n",
      "\n",
      "Training: batch 32 ends at 22:13:55.340426\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5050 - bce_dice_loss: 0.5050\n",
      "Training: batch 33 begins at 22:13:55.345358\n",
      "\n",
      "Training: batch 33 ends at 22:13:56.167847\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5038 - bce_dice_loss: 0.5038\n",
      "Training: batch 34 begins at 22:13:56.171489\n",
      "\n",
      "Training: batch 34 ends at 22:13:56.969903\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5044 - bce_dice_loss: 0.5044\n",
      "Training: batch 35 begins at 22:13:56.973991\n",
      "\n",
      "Training: batch 35 ends at 22:13:57.776766\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5046 - bce_dice_loss: 0.5046\n",
      "Training: batch 36 begins at 22:13:57.781447\n",
      "\n",
      "Training: batch 36 ends at 22:13:58.570303\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4997 - bce_dice_loss: 0.4997\n",
      "Training: batch 37 begins at 22:13:58.574306\n",
      "\n",
      "Training: batch 37 ends at 22:13:59.366040\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4978 - bce_dice_loss: 0.4978\n",
      "Training: batch 38 begins at 22:13:59.370440\n",
      "\n",
      "Training: batch 38 ends at 22:14:00.178061\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4949 - bce_dice_loss: 0.4949\n",
      "Training: batch 39 begins at 22:14:00.182905\n",
      "\n",
      "Training: batch 39 ends at 22:14:00.976083\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4969 - bce_dice_loss: 0.4969\n",
      "Training: batch 40 begins at 22:14:00.980513\n",
      "\n",
      "Training: batch 40 ends at 22:14:01.801586\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4973 - bce_dice_loss: 0.4973\n",
      "Training: batch 41 begins at 22:14:01.804102\n",
      "\n",
      "Training: batch 41 ends at 22:14:02.615195\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4984 - bce_dice_loss: 0.4984\n",
      "Training: batch 42 begins at 22:14:02.618335\n",
      "\n",
      "Training: batch 42 ends at 22:14:03.441155\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4980 - bce_dice_loss: 0.4980\n",
      "Training: batch 43 begins at 22:14:03.445143\n",
      "\n",
      "Training: batch 43 ends at 22:14:04.246045\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4971 - bce_dice_loss: 0.4971\n",
      "Training: batch 44 begins at 22:14:04.250350\n",
      "\n",
      "Training: batch 44 ends at 22:14:05.051066\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4957 - bce_dice_loss: 0.4957\n",
      "Training: batch 45 begins at 22:14:05.053661\n",
      "\n",
      "Training: batch 45 ends at 22:14:05.845292\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4931 - bce_dice_loss: 0.4931\n",
      "Training: batch 46 begins at 22:14:05.850052\n",
      "\n",
      "Training: batch 46 ends at 22:14:06.646671\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4908 - bce_dice_loss: 0.4908\n",
      "Training: batch 47 begins at 22:14:06.651454\n",
      "\n",
      "Training: batch 47 ends at 22:14:07.444941\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4905 - bce_dice_loss: 0.4905 \n",
      "Training: batch 48 begins at 22:14:07.449191\n",
      "\n",
      "Training: batch 48 ends at 22:14:08.246447\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4906 - bce_dice_loss: 0.4906\n",
      "Training: batch 49 begins at 22:14:08.250685\n",
      "\n",
      "Training: batch 49 ends at 22:14:09.064336\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4909 - bce_dice_loss: 0.4909\n",
      "Training: batch 50 begins at 22:14:09.069169\n",
      "\n",
      "Training: batch 50 ends at 22:14:09.874399\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4960 - bce_dice_loss: 0.4960\n",
      "Training: batch 51 begins at 22:14:09.878611\n",
      "\n",
      "Training: batch 51 ends at 22:14:10.702383\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4923 - bce_dice_loss: 0.4923\n",
      "Training: batch 52 begins at 22:14:10.709656\n",
      "\n",
      "Training: batch 52 ends at 22:14:11.505561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4986 - bce_dice_loss: 0.4986\n",
      "Training: batch 53 begins at 22:14:11.509729\n",
      "\n",
      "Training: batch 53 ends at 22:14:12.302352\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5051 - bce_dice_loss: 0.5051\n",
      "Training: batch 54 begins at 22:14:12.306327\n",
      "\n",
      "Training: batch 54 ends at 22:14:13.099469\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5068 - bce_dice_loss: 0.5068\n",
      "Training: batch 55 begins at 22:14:13.103011\n",
      "\n",
      "Training: batch 55 ends at 22:14:13.916776\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5141 - bce_dice_loss: 0.5141\n",
      "Training: batch 56 begins at 22:14:13.920235\n",
      "\n",
      "Training: batch 56 ends at 22:14:14.704645\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5138 - bce_dice_loss: 0.5138\n",
      "Training: batch 57 begins at 22:14:14.708431\n",
      "\n",
      "Training: batch 57 ends at 22:14:15.502190\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5126 - bce_dice_loss: 0.5126\n",
      "Training: batch 58 begins at 22:14:15.504935\n",
      "\n",
      "Training: batch 58 ends at 22:14:16.299362\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5114 - bce_dice_loss: 0.5114\n",
      "Training: batch 59 begins at 22:14:16.303046\n",
      "\n",
      "Training: batch 59 ends at 22:14:17.091393\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5119 - bce_dice_loss: 0.5119\n",
      "Evaluating: batch 0 begins at 22:14:17.124631\n",
      "\n",
      "Evaluating: batch 0 ends at 22:14:17.396309\n",
      "\n",
      "Evaluating: batch 1 begins at 22:14:17.397463\n",
      "\n",
      "Evaluating: batch 1 ends at 22:14:17.615376\n",
      "\n",
      "Evaluating: batch 2 begins at 22:14:17.616774\n",
      "\n",
      "Evaluating: batch 2 ends at 22:14:17.837032\n",
      "\n",
      "Evaluating: batch 3 begins at 22:14:17.838402\n",
      "\n",
      "Evaluating: batch 3 ends at 22:14:18.058489\n",
      "\n",
      "Evaluating: batch 4 begins at 22:14:18.061214\n",
      "\n",
      "Evaluating: batch 4 ends at 22:14:18.281090\n",
      "\n",
      "Evaluating: batch 5 begins at 22:14:18.282604\n",
      "\n",
      "Evaluating: batch 5 ends at 22:14:18.502667\n",
      "\n",
      "Evaluating: batch 6 begins at 22:14:18.504403\n",
      "\n",
      "Evaluating: batch 6 ends at 22:14:18.723501\n",
      "\n",
      "Evaluating: batch 7 begins at 22:14:18.724760\n",
      "\n",
      "Evaluating: batch 7 ends at 22:14:18.944885\n",
      "\n",
      "Evaluating: batch 8 begins at 22:14:18.947207\n",
      "\n",
      "Evaluating: batch 8 ends at 22:14:19.167279\n",
      "\n",
      "Evaluating: batch 9 begins at 22:14:19.168715\n",
      "\n",
      "Evaluating: batch 9 ends at 22:14:19.389866\n",
      "\n",
      "Evaluating: batch 10 begins at 22:14:19.392006\n",
      "\n",
      "Evaluating: batch 10 ends at 22:14:19.611295\n",
      "\n",
      "Evaluating: batch 11 begins at 22:14:19.614896\n",
      "\n",
      "Evaluating: batch 11 ends at 22:14:19.843296\n",
      "\n",
      "Evaluating: batch 12 begins at 22:14:19.844710\n",
      "\n",
      "Evaluating: batch 12 ends at 22:14:20.065259\n",
      "\n",
      "Evaluating: batch 13 begins at 22:14:20.066589\n",
      "\n",
      "Evaluating: batch 13 ends at 22:14:20.287680\n",
      "\n",
      "Evaluating: batch 14 begins at 22:14:20.289059\n",
      "\n",
      "Evaluating: batch 14 ends at 22:14:20.508819\n",
      "\n",
      "Evaluating: batch 15 begins at 22:14:20.510233\n",
      "\n",
      "Evaluating: batch 15 ends at 22:14:20.730945\n",
      "\n",
      "Evaluating: batch 16 begins at 22:14:20.732540\n",
      "\n",
      "Evaluating: batch 16 ends at 22:14:20.953660\n",
      "\n",
      "Evaluating: batch 17 begins at 22:14:20.955009\n",
      "\n",
      "Evaluating: batch 17 ends at 22:14:21.175988\n",
      "\n",
      "Evaluating: batch 18 begins at 22:14:21.177550\n",
      "\n",
      "Evaluating: batch 18 ends at 22:14:21.398376\n",
      "\n",
      "Evaluating: batch 19 begins at 22:14:21.400253\n",
      "\n",
      "Evaluating: batch 19 ends at 22:14:21.617754\n",
      "\n",
      "Evaluating: batch 20 begins at 22:14:21.620163\n",
      "\n",
      "Evaluating: batch 20 ends at 22:14:21.833538\n",
      "\n",
      "Evaluating: batch 21 begins at 22:14:21.834845\n",
      "\n",
      "Evaluating: batch 21 ends at 22:14:22.053304\n",
      "\n",
      "Evaluating: batch 22 begins at 22:14:22.055888\n",
      "\n",
      "Evaluating: batch 22 ends at 22:14:22.271984\n",
      "\n",
      "Evaluating: batch 23 begins at 22:14:22.274891\n",
      "\n",
      "Evaluating: batch 23 ends at 22:14:22.493971\n",
      "\n",
      "Evaluating: batch 24 begins at 22:14:22.496162\n",
      "\n",
      "Evaluating: batch 24 ends at 22:14:22.713312\n",
      "\n",
      "Evaluating: batch 25 begins at 22:14:22.716001\n",
      "\n",
      "Evaluating: batch 25 ends at 22:14:22.933923\n",
      "\n",
      "Evaluating: batch 26 begins at 22:14:22.936035\n",
      "\n",
      "Evaluating: batch 26 ends at 22:14:23.152054\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.51643 to 0.51326, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 934ms/step - loss: 0.5119 - bce_dice_loss: 0.5119 - val_loss: 0.5133 - val_bce_dice_loss: 0.5133\n",
      "Epoch 23/25\n",
      "\n",
      "Training: batch 0 begins at 22:14:24.791529\n",
      "\n",
      "Training: batch 0 ends at 22:14:25.603386\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.5960 - bce_dice_loss: 0.5960\n",
      "Training: batch 1 begins at 22:14:25.607931\n",
      "\n",
      "Training: batch 1 ends at 22:14:26.434790\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.6354 - bce_dice_loss: 0.6354\n",
      "Training: batch 2 begins at 22:14:26.438174\n",
      "\n",
      "Training: batch 2 ends at 22:14:27.237871\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.5845 - bce_dice_loss: 0.5845\n",
      "Training: batch 3 begins at 22:14:27.241410\n",
      "\n",
      "Training: batch 3 ends at 22:14:28.040194\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5618 - bce_dice_loss: 0.5618\n",
      "Training: batch 4 begins at 22:14:28.044645\n",
      "\n",
      "Training: batch 4 ends at 22:14:28.845365\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.5285 - bce_dice_loss: 0.5285\n",
      "Training: batch 5 begins at 22:14:28.850375\n",
      "\n",
      "Training: batch 5 ends at 22:14:29.640814\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5197 - bce_dice_loss: 0.5197\n",
      "Training: batch 6 begins at 22:14:29.643435\n",
      "\n",
      "Training: batch 6 ends at 22:14:30.455295\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5158 - bce_dice_loss: 0.5158\n",
      "Training: batch 7 begins at 22:14:30.461184\n",
      "\n",
      "Training: batch 7 ends at 22:14:31.252194\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5377 - bce_dice_loss: 0.5377\n",
      "Training: batch 8 begins at 22:14:31.257191\n",
      "\n",
      "Training: batch 8 ends at 22:14:32.044174\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5305 - bce_dice_loss: 0.5305\n",
      "Training: batch 9 begins at 22:14:32.048976\n",
      "\n",
      "Training: batch 9 ends at 22:14:32.842306\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5275 - bce_dice_loss: 0.5275\n",
      "Training: batch 10 begins at 22:14:32.846671\n",
      "\n",
      "Training: batch 10 ends at 22:14:33.634234\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5206 - bce_dice_loss: 0.5206\n",
      "Training: batch 11 begins at 22:14:33.638362\n",
      "\n",
      "Training: batch 11 ends at 22:14:34.435421\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5137 - bce_dice_loss: 0.5137\n",
      "Training: batch 12 begins at 22:14:34.439068\n",
      "\n",
      "Training: batch 12 ends at 22:14:35.234246\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5136 - bce_dice_loss: 0.5136\n",
      "Training: batch 13 begins at 22:14:35.238276\n",
      "\n",
      "Training: batch 13 ends at 22:14:36.032714\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5091 - bce_dice_loss: 0.5091\n",
      "Training: batch 14 begins at 22:14:36.037034\n",
      "\n",
      "Training: batch 14 ends at 22:14:36.831853\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5027 - bce_dice_loss: 0.5027\n",
      "Training: batch 15 begins at 22:14:36.835940\n",
      "\n",
      "Training: batch 15 ends at 22:14:37.624615\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5070 - bce_dice_loss: 0.5070\n",
      "Training: batch 16 begins at 22:14:37.628789\n",
      "\n",
      "Training: batch 16 ends at 22:14:38.416907\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5075 - bce_dice_loss: 0.5075\n",
      "Training: batch 17 begins at 22:14:38.420309\n",
      "\n",
      "Training: batch 17 ends at 22:14:39.239335\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5051 - bce_dice_loss: 0.5051\n",
      "Training: batch 18 begins at 22:14:39.242211\n",
      "\n",
      "Training: batch 18 ends at 22:14:40.042196\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.5080 - bce_dice_loss: 0.5080\n",
      "Training: batch 19 begins at 22:14:40.046826\n",
      "\n",
      "Training: batch 19 ends at 22:14:40.865476\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5092 - bce_dice_loss: 0.5092\n",
      "Training: batch 20 begins at 22:14:40.868930\n",
      "\n",
      "Training: batch 20 ends at 22:14:41.659692\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5042 - bce_dice_loss: 0.5042\n",
      "Training: batch 21 begins at 22:14:41.664285\n",
      "\n",
      "Training: batch 21 ends at 22:14:42.484984\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5037 - bce_dice_loss: 0.5037\n",
      "Training: batch 22 begins at 22:14:42.489232\n",
      "\n",
      "Training: batch 22 ends at 22:14:43.287666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/60 [==========>...................] - ETA: 29s - loss: 0.5018 - bce_dice_loss: 0.5018\n",
      "Training: batch 23 begins at 22:14:43.290187\n",
      "\n",
      "Training: batch 23 ends at 22:14:44.082631\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4989 - bce_dice_loss: 0.4989\n",
      "Training: batch 24 begins at 22:14:44.086115\n",
      "\n",
      "Training: batch 24 ends at 22:14:44.908812\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5047 - bce_dice_loss: 0.5047\n",
      "Training: batch 25 begins at 22:14:44.913694\n",
      "\n",
      "Training: batch 25 ends at 22:14:45.694367\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5075 - bce_dice_loss: 0.5075\n",
      "Training: batch 26 begins at 22:14:45.699625\n",
      "\n",
      "Training: batch 26 ends at 22:14:46.518857\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4995 - bce_dice_loss: 0.4995\n",
      "Training: batch 27 begins at 22:14:46.522593\n",
      "\n",
      "Training: batch 27 ends at 22:14:47.309132\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4951 - bce_dice_loss: 0.4951\n",
      "Training: batch 28 begins at 22:14:47.313582\n",
      "\n",
      "Training: batch 28 ends at 22:14:48.097916\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4982 - bce_dice_loss: 0.4982\n",
      "Training: batch 29 begins at 22:14:48.104182\n",
      "\n",
      "Training: batch 29 ends at 22:14:48.904349\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4937 - bce_dice_loss: 0.4937\n",
      "Training: batch 30 begins at 22:14:48.908280\n",
      "\n",
      "Training: batch 30 ends at 22:14:49.697482\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4865 - bce_dice_loss: 0.4865\n",
      "Training: batch 31 begins at 22:14:49.701704\n",
      "\n",
      "Training: batch 31 ends at 22:14:50.528047\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4833 - bce_dice_loss: 0.4833\n",
      "Training: batch 32 begins at 22:14:50.532633\n",
      "\n",
      "Training: batch 32 ends at 22:14:51.324807\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4814 - bce_dice_loss: 0.4814\n",
      "Training: batch 33 begins at 22:14:51.329901\n",
      "\n",
      "Training: batch 33 ends at 22:14:52.121963\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4809 - bce_dice_loss: 0.4809\n",
      "Training: batch 34 begins at 22:14:52.125639\n",
      "\n",
      "Training: batch 34 ends at 22:14:52.940959\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4902 - bce_dice_loss: 0.4902\n",
      "Training: batch 35 begins at 22:14:52.945487\n",
      "\n",
      "Training: batch 35 ends at 22:14:53.736383\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4882 - bce_dice_loss: 0.4882\n",
      "Training: batch 36 begins at 22:14:53.739727\n",
      "\n",
      "Training: batch 36 ends at 22:14:54.532493\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4902 - bce_dice_loss: 0.4902\n",
      "Training: batch 37 begins at 22:14:54.536715\n",
      "\n",
      "Training: batch 37 ends at 22:14:55.331793\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4893 - bce_dice_loss: 0.4893\n",
      "Training: batch 38 begins at 22:14:55.335293\n",
      "\n",
      "Training: batch 38 ends at 22:14:56.125225\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4880 - bce_dice_loss: 0.4880\n",
      "Training: batch 39 begins at 22:14:56.131065\n",
      "\n",
      "Training: batch 39 ends at 22:14:56.917258\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4848 - bce_dice_loss: 0.4848\n",
      "Training: batch 40 begins at 22:14:56.920765\n",
      "\n",
      "Training: batch 40 ends at 22:14:57.716592\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4926 - bce_dice_loss: 0.4926\n",
      "Training: batch 41 begins at 22:14:57.720463\n",
      "\n",
      "Training: batch 41 ends at 22:14:58.533568\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5008 - bce_dice_loss: 0.5008\n",
      "Training: batch 42 begins at 22:14:58.537986\n",
      "\n",
      "Training: batch 42 ends at 22:14:59.334879\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4996 - bce_dice_loss: 0.4996\n",
      "Training: batch 43 begins at 22:14:59.339176\n",
      "\n",
      "Training: batch 43 ends at 22:15:00.137585\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4989 - bce_dice_loss: 0.4989\n",
      "Training: batch 44 begins at 22:15:00.141382\n",
      "\n",
      "Training: batch 44 ends at 22:15:00.965776\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4967 - bce_dice_loss: 0.4967\n",
      "Training: batch 45 begins at 22:15:00.969312\n",
      "\n",
      "Training: batch 45 ends at 22:15:01.763437\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4946 - bce_dice_loss: 0.4946\n",
      "Training: batch 46 begins at 22:15:01.766818\n",
      "\n",
      "Training: batch 46 ends at 22:15:02.580962\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4923 - bce_dice_loss: 0.4923\n",
      "Training: batch 47 begins at 22:15:02.585151\n",
      "\n",
      "Training: batch 47 ends at 22:15:03.417488\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4929 - bce_dice_loss: 0.4929 \n",
      "Training: batch 48 begins at 22:15:03.420315\n",
      "\n",
      "Training: batch 48 ends at 22:15:04.250783\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4985 - bce_dice_loss: 0.4985\n",
      "Training: batch 49 begins at 22:15:04.253703\n",
      "\n",
      "Training: batch 49 ends at 22:15:05.053300\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4971 - bce_dice_loss: 0.4971\n",
      "Training: batch 50 begins at 22:15:05.057092\n",
      "\n",
      "Training: batch 50 ends at 22:15:05.850070\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4976 - bce_dice_loss: 0.4976\n",
      "Training: batch 51 begins at 22:15:05.854689\n",
      "\n",
      "Training: batch 51 ends at 22:15:06.659438\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4960 - bce_dice_loss: 0.4960\n",
      "Training: batch 52 begins at 22:15:06.663044\n",
      "\n",
      "Training: batch 52 ends at 22:15:07.458394\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4915 - bce_dice_loss: 0.4915\n",
      "Training: batch 53 begins at 22:15:07.461854\n",
      "\n",
      "Training: batch 53 ends at 22:15:08.256487\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4931 - bce_dice_loss: 0.4931\n",
      "Training: batch 54 begins at 22:15:08.259374\n",
      "\n",
      "Training: batch 54 ends at 22:15:09.054748\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4909 - bce_dice_loss: 0.4909\n",
      "Training: batch 55 begins at 22:15:09.057759\n",
      "\n",
      "Training: batch 55 ends at 22:15:09.855121\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4928 - bce_dice_loss: 0.4928\n",
      "Training: batch 56 begins at 22:15:09.858667\n",
      "\n",
      "Training: batch 56 ends at 22:15:10.673399\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4991 - bce_dice_loss: 0.4991\n",
      "Training: batch 57 begins at 22:15:10.677917\n",
      "\n",
      "Training: batch 57 ends at 22:15:11.474967\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4991 - bce_dice_loss: 0.4991\n",
      "Training: batch 58 begins at 22:15:11.478448\n",
      "\n",
      "Training: batch 58 ends at 22:15:12.275776\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5059 - bce_dice_loss: 0.5059\n",
      "Training: batch 59 begins at 22:15:12.278089\n",
      "\n",
      "Training: batch 59 ends at 22:15:13.073262\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5060 - bce_dice_loss: 0.5060\n",
      "Evaluating: batch 0 begins at 22:15:13.104729\n",
      "\n",
      "Evaluating: batch 0 ends at 22:15:13.375258\n",
      "\n",
      "Evaluating: batch 1 begins at 22:15:13.376706\n",
      "\n",
      "Evaluating: batch 1 ends at 22:15:13.588802\n",
      "\n",
      "Evaluating: batch 2 begins at 22:15:13.589922\n",
      "\n",
      "Evaluating: batch 2 ends at 22:15:13.809245\n",
      "\n",
      "Evaluating: batch 3 begins at 22:15:13.811596\n",
      "\n",
      "Evaluating: batch 3 ends at 22:15:14.032361\n",
      "\n",
      "Evaluating: batch 4 begins at 22:15:14.034322\n",
      "\n",
      "Evaluating: batch 4 ends at 22:15:14.252874\n",
      "\n",
      "Evaluating: batch 5 begins at 22:15:14.254235\n",
      "\n",
      "Evaluating: batch 5 ends at 22:15:14.472388\n",
      "\n",
      "Evaluating: batch 6 begins at 22:15:14.473542\n",
      "\n",
      "Evaluating: batch 6 ends at 22:15:14.694267\n",
      "\n",
      "Evaluating: batch 7 begins at 22:15:14.695726\n",
      "\n",
      "Evaluating: batch 7 ends at 22:15:14.920963\n",
      "\n",
      "Evaluating: batch 8 begins at 22:15:14.922272\n",
      "\n",
      "Evaluating: batch 8 ends at 22:15:15.139588\n",
      "\n",
      "Evaluating: batch 9 begins at 22:15:15.141905\n",
      "\n",
      "Evaluating: batch 9 ends at 22:15:15.362535\n",
      "\n",
      "Evaluating: batch 10 begins at 22:15:15.364947\n",
      "\n",
      "Evaluating: batch 10 ends at 22:15:15.582671\n",
      "\n",
      "Evaluating: batch 11 begins at 22:15:15.584369\n",
      "\n",
      "Evaluating: batch 11 ends at 22:15:15.806444\n",
      "\n",
      "Evaluating: batch 12 begins at 22:15:15.808232\n",
      "\n",
      "Evaluating: batch 12 ends at 22:15:16.030401\n",
      "\n",
      "Evaluating: batch 13 begins at 22:15:16.031810\n",
      "\n",
      "Evaluating: batch 13 ends at 22:15:16.252973\n",
      "\n",
      "Evaluating: batch 14 begins at 22:15:16.257021\n",
      "\n",
      "Evaluating: batch 14 ends at 22:15:16.475819\n",
      "\n",
      "Evaluating: batch 15 begins at 22:15:16.477302\n",
      "\n",
      "Evaluating: batch 15 ends at 22:15:16.696260\n",
      "\n",
      "Evaluating: batch 16 begins at 22:15:16.697929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 16 ends at 22:15:16.919910\n",
      "\n",
      "Evaluating: batch 17 begins at 22:15:16.922772\n",
      "\n",
      "Evaluating: batch 17 ends at 22:15:17.142979\n",
      "\n",
      "Evaluating: batch 18 begins at 22:15:17.144969\n",
      "\n",
      "Evaluating: batch 18 ends at 22:15:17.366589\n",
      "\n",
      "Evaluating: batch 19 begins at 22:15:17.368270\n",
      "\n",
      "Evaluating: batch 19 ends at 22:15:17.585027\n",
      "\n",
      "Evaluating: batch 20 begins at 22:15:17.586340\n",
      "\n",
      "Evaluating: batch 20 ends at 22:15:17.804066\n",
      "\n",
      "Evaluating: batch 21 begins at 22:15:17.806181\n",
      "\n",
      "Evaluating: batch 21 ends at 22:15:18.020317\n",
      "\n",
      "Evaluating: batch 22 begins at 22:15:18.021678\n",
      "\n",
      "Evaluating: batch 22 ends at 22:15:18.238060\n",
      "\n",
      "Evaluating: batch 23 begins at 22:15:18.239333\n",
      "\n",
      "Evaluating: batch 23 ends at 22:15:18.456559\n",
      "\n",
      "Evaluating: batch 24 begins at 22:15:18.457765\n",
      "\n",
      "Evaluating: batch 24 ends at 22:15:18.673732\n",
      "\n",
      "Evaluating: batch 25 begins at 22:15:18.675018\n",
      "\n",
      "Evaluating: batch 25 ends at 22:15:18.891036\n",
      "\n",
      "Evaluating: batch 26 begins at 22:15:18.892386\n",
      "\n",
      "Evaluating: batch 26 ends at 22:15:19.108380\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51326\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.5060 - bce_dice_loss: 0.5060 - val_loss: 0.5195 - val_bce_dice_loss: 0.5195\n",
      "Epoch 24/25\n",
      "\n",
      "Training: batch 0 begins at 22:15:19.138922\n",
      "\n",
      "Training: batch 0 ends at 22:15:19.941429\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3219 - bce_dice_loss: 0.3219\n",
      "Training: batch 1 begins at 22:15:19.944837\n",
      "\n",
      "Training: batch 1 ends at 22:15:20.746779\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3581 - bce_dice_loss: 0.3581\n",
      "Training: batch 2 begins at 22:15:20.751620\n",
      "\n",
      "Training: batch 2 ends at 22:15:21.547224\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.4208 - bce_dice_loss: 0.4208\n",
      "Training: batch 3 begins at 22:15:21.551669\n",
      "\n",
      "Training: batch 3 ends at 22:15:22.344775\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.4274 - bce_dice_loss: 0.4274\n",
      "Training: batch 4 begins at 22:15:22.349042\n",
      "\n",
      "Training: batch 4 ends at 22:15:23.164614\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4437 - bce_dice_loss: 0.4437\n",
      "Training: batch 5 begins at 22:15:23.169066\n",
      "\n",
      "Training: batch 5 ends at 22:15:23.967328\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4553 - bce_dice_loss: 0.4553\n",
      "Training: batch 6 begins at 22:15:23.973483\n",
      "\n",
      "Training: batch 6 ends at 22:15:24.762458\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5099 - bce_dice_loss: 0.5099\n",
      "Training: batch 7 begins at 22:15:24.766522\n",
      "\n",
      "Training: batch 7 ends at 22:15:25.556356\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5286 - bce_dice_loss: 0.5286\n",
      "Training: batch 8 begins at 22:15:25.558961\n",
      "\n",
      "Training: batch 8 ends at 22:15:26.381973\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5340 - bce_dice_loss: 0.5340\n",
      "Training: batch 9 begins at 22:15:26.386870\n",
      "\n",
      "Training: batch 9 ends at 22:15:27.179567\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5167 - bce_dice_loss: 0.5167\n",
      "Training: batch 10 begins at 22:15:27.182730\n",
      "\n",
      "Training: batch 10 ends at 22:15:28.009752\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5096 - bce_dice_loss: 0.5096\n",
      "Training: batch 11 begins at 22:15:28.014386\n",
      "\n",
      "Training: batch 11 ends at 22:15:28.809610\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4987 - bce_dice_loss: 0.4987\n",
      "Training: batch 12 begins at 22:15:28.813329\n",
      "\n",
      "Training: batch 12 ends at 22:15:29.608818\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5098 - bce_dice_loss: 0.5098\n",
      "Training: batch 13 begins at 22:15:29.612612\n",
      "\n",
      "Training: batch 13 ends at 22:15:30.422640\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.5316 - bce_dice_loss: 0.5316\n",
      "Training: batch 14 begins at 22:15:30.426123\n",
      "\n",
      "Training: batch 14 ends at 22:15:31.227732\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5261 - bce_dice_loss: 0.5261\n",
      "Training: batch 15 begins at 22:15:31.231954\n",
      "\n",
      "Training: batch 15 ends at 22:15:32.064076\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5260 - bce_dice_loss: 0.5260\n",
      "Training: batch 16 begins at 22:15:32.068860\n",
      "\n",
      "Training: batch 16 ends at 22:15:32.857470\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.5148 - bce_dice_loss: 0.5148\n",
      "Training: batch 17 begins at 22:15:32.860203\n",
      "\n",
      "Training: batch 17 ends at 22:15:33.654795\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.5030 - bce_dice_loss: 0.5030\n",
      "Training: batch 18 begins at 22:15:33.659159\n",
      "\n",
      "Training: batch 18 ends at 22:15:34.453740\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5065 - bce_dice_loss: 0.5065\n",
      "Training: batch 19 begins at 22:15:34.457672\n",
      "\n",
      "Training: batch 19 ends at 22:15:35.257783\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5021 - bce_dice_loss: 0.5021\n",
      "Training: batch 20 begins at 22:15:35.262131\n",
      "\n",
      "Training: batch 20 ends at 22:15:36.059884\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5001 - bce_dice_loss: 0.5001\n",
      "Training: batch 21 begins at 22:15:36.063575\n",
      "\n",
      "Training: batch 21 ends at 22:15:36.868412\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4993 - bce_dice_loss: 0.4993\n",
      "Training: batch 22 begins at 22:15:36.872569\n",
      "\n",
      "Training: batch 22 ends at 22:15:37.662328\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4996 - bce_dice_loss: 0.4996\n",
      "Training: batch 23 begins at 22:15:37.666464\n",
      "\n",
      "Training: batch 23 ends at 22:15:38.459904\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4990 - bce_dice_loss: 0.4990\n",
      "Training: batch 24 begins at 22:15:38.463595\n",
      "\n",
      "Training: batch 24 ends at 22:15:39.284462\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4903 - bce_dice_loss: 0.4903\n",
      "Training: batch 25 begins at 22:15:39.289607\n",
      "\n",
      "Training: batch 25 ends at 22:15:40.089268\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4981 - bce_dice_loss: 0.4981\n",
      "Training: batch 26 begins at 22:15:40.092219\n",
      "\n",
      "Training: batch 26 ends at 22:15:40.889806\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4956 - bce_dice_loss: 0.4956\n",
      "Training: batch 27 begins at 22:15:40.893774\n",
      "\n",
      "Training: batch 27 ends at 22:15:41.686382\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4952 - bce_dice_loss: 0.4952\n",
      "Training: batch 28 begins at 22:15:41.690441\n",
      "\n",
      "Training: batch 28 ends at 22:15:42.507668\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.5000 - bce_dice_loss: 0.5000\n",
      "Training: batch 29 begins at 22:15:42.511709\n",
      "\n",
      "Training: batch 29 ends at 22:15:43.315391\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5078 - bce_dice_loss: 0.5078\n",
      "Training: batch 30 begins at 22:15:43.319495\n",
      "\n",
      "Training: batch 30 ends at 22:15:44.111343\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5073 - bce_dice_loss: 0.5073\n",
      "Training: batch 31 begins at 22:15:44.115806\n",
      "\n",
      "Training: batch 31 ends at 22:15:44.904771\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5121 - bce_dice_loss: 0.5121\n",
      "Training: batch 32 begins at 22:15:44.908413\n",
      "\n",
      "Training: batch 32 ends at 22:15:45.703157\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5061 - bce_dice_loss: 0.5061\n",
      "Training: batch 33 begins at 22:15:45.705688\n",
      "\n",
      "Training: batch 33 ends at 22:15:46.507685\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.5082 - bce_dice_loss: 0.5082\n",
      "Training: batch 34 begins at 22:15:46.511926\n",
      "\n",
      "Training: batch 34 ends at 22:15:47.304562\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5009 - bce_dice_loss: 0.5009\n",
      "Training: batch 35 begins at 22:15:47.308846\n",
      "\n",
      "Training: batch 35 ends at 22:15:48.101481\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4996 - bce_dice_loss: 0.4996\n",
      "Training: batch 36 begins at 22:15:48.105996\n",
      "\n",
      "Training: batch 36 ends at 22:15:48.899470\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5012 - bce_dice_loss: 0.5012\n",
      "Training: batch 37 begins at 22:15:48.904347\n",
      "\n",
      "Training: batch 37 ends at 22:15:49.732549\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5062 - bce_dice_loss: 0.5062\n",
      "Training: batch 38 begins at 22:15:49.733534\n",
      "\n",
      "Training: batch 38 ends at 22:15:50.527989\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.5085 - bce_dice_loss: 0.5085\n",
      "Training: batch 39 begins at 22:15:50.532097\n",
      "\n",
      "Training: batch 39 ends at 22:15:51.328617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5092 - bce_dice_loss: 0.5092\n",
      "Training: batch 40 begins at 22:15:51.332885\n",
      "\n",
      "Training: batch 40 ends at 22:15:52.123275\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5033 - bce_dice_loss: 0.5033\n",
      "Training: batch 41 begins at 22:15:52.127709\n",
      "\n",
      "Training: batch 41 ends at 22:15:52.923174\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5048 - bce_dice_loss: 0.5048\n",
      "Training: batch 42 begins at 22:15:52.927607\n",
      "\n",
      "Training: batch 42 ends at 22:15:53.719429\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5026 - bce_dice_loss: 0.5026\n",
      "Training: batch 43 begins at 22:15:53.723484\n",
      "\n",
      "Training: batch 43 ends at 22:15:54.515487\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5014 - bce_dice_loss: 0.5014\n",
      "Training: batch 44 begins at 22:15:54.518020\n",
      "\n",
      "Training: batch 44 ends at 22:15:55.338251\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5020 - bce_dice_loss: 0.5020\n",
      "Training: batch 45 begins at 22:15:55.341630\n",
      "\n",
      "Training: batch 45 ends at 22:15:56.147220\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5015 - bce_dice_loss: 0.5015\n",
      "Training: batch 46 begins at 22:15:56.150751\n",
      "\n",
      "Training: batch 46 ends at 22:15:56.953708\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5049 - bce_dice_loss: 0.5049\n",
      "Training: batch 47 begins at 22:15:56.957792\n",
      "\n",
      "Training: batch 47 ends at 22:15:57.755870\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5018 - bce_dice_loss: 0.5018 \n",
      "Training: batch 48 begins at 22:15:57.758990\n",
      "\n",
      "Training: batch 48 ends at 22:15:58.553912\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5025 - bce_dice_loss: 0.5025\n",
      "Training: batch 49 begins at 22:15:58.558050\n",
      "\n",
      "Training: batch 49 ends at 22:15:59.352833\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5031 - bce_dice_loss: 0.5031\n",
      "Training: batch 50 begins at 22:15:59.356360\n",
      "\n",
      "Training: batch 50 ends at 22:16:00.154257\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5047 - bce_dice_loss: 0.5047\n",
      "Training: batch 51 begins at 22:16:00.157872\n",
      "\n",
      "Training: batch 51 ends at 22:16:00.949526\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5053 - bce_dice_loss: 0.5053\n",
      "Training: batch 52 begins at 22:16:00.954156\n",
      "\n",
      "Training: batch 52 ends at 22:16:01.747290\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5021 - bce_dice_loss: 0.5021\n",
      "Training: batch 53 begins at 22:16:01.752915\n",
      "\n",
      "Training: batch 53 ends at 22:16:02.546046\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5017 - bce_dice_loss: 0.5017\n",
      "Training: batch 54 begins at 22:16:02.549745\n",
      "\n",
      "Training: batch 54 ends at 22:16:03.343518\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5020 - bce_dice_loss: 0.5020\n",
      "Training: batch 55 begins at 22:16:03.347007\n",
      "\n",
      "Training: batch 55 ends at 22:16:04.154417\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5035 - bce_dice_loss: 0.5035\n",
      "Training: batch 56 begins at 22:16:04.157967\n",
      "\n",
      "Training: batch 56 ends at 22:16:04.947501\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4982 - bce_dice_loss: 0.4982\n",
      "Training: batch 57 begins at 22:16:04.953269\n",
      "\n",
      "Training: batch 57 ends at 22:16:05.760222\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4964 - bce_dice_loss: 0.4964\n",
      "Training: batch 58 begins at 22:16:05.763330\n",
      "\n",
      "Training: batch 58 ends at 22:16:06.566870\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5023 - bce_dice_loss: 0.5023\n",
      "Training: batch 59 begins at 22:16:06.570273\n",
      "\n",
      "Training: batch 59 ends at 22:16:07.362999\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5074 - bce_dice_loss: 0.5074\n",
      "Evaluating: batch 0 begins at 22:16:07.394053\n",
      "\n",
      "Evaluating: batch 0 ends at 22:16:07.660158\n",
      "\n",
      "Evaluating: batch 1 begins at 22:16:07.661355\n",
      "\n",
      "Evaluating: batch 1 ends at 22:16:07.877378\n",
      "\n",
      "Evaluating: batch 2 begins at 22:16:07.878644\n",
      "\n",
      "Evaluating: batch 2 ends at 22:16:08.099464\n",
      "\n",
      "Evaluating: batch 3 begins at 22:16:08.101873\n",
      "\n",
      "Evaluating: batch 3 ends at 22:16:08.323663\n",
      "\n",
      "Evaluating: batch 4 begins at 22:16:08.325135\n",
      "\n",
      "Evaluating: batch 4 ends at 22:16:08.547752\n",
      "\n",
      "Evaluating: batch 5 begins at 22:16:08.549370\n",
      "\n",
      "Evaluating: batch 5 ends at 22:16:08.770831\n",
      "\n",
      "Evaluating: batch 6 begins at 22:16:08.773238\n",
      "\n",
      "Evaluating: batch 6 ends at 22:16:08.988686\n",
      "\n",
      "Evaluating: batch 7 begins at 22:16:08.991141\n",
      "\n",
      "Evaluating: batch 7 ends at 22:16:09.214427\n",
      "\n",
      "Evaluating: batch 8 begins at 22:16:09.215782\n",
      "\n",
      "Evaluating: batch 8 ends at 22:16:09.435470\n",
      "\n",
      "Evaluating: batch 9 begins at 22:16:09.437195\n",
      "\n",
      "Evaluating: batch 9 ends at 22:16:09.657031\n",
      "\n",
      "Evaluating: batch 10 begins at 22:16:09.659584\n",
      "\n",
      "Evaluating: batch 10 ends at 22:16:09.885285\n",
      "\n",
      "Evaluating: batch 11 begins at 22:16:09.886869\n",
      "\n",
      "Evaluating: batch 11 ends at 22:16:10.111987\n",
      "\n",
      "Evaluating: batch 12 begins at 22:16:10.113706\n",
      "\n",
      "Evaluating: batch 12 ends at 22:16:10.333248\n",
      "\n",
      "Evaluating: batch 13 begins at 22:16:10.335339\n",
      "\n",
      "Evaluating: batch 13 ends at 22:16:10.554287\n",
      "\n",
      "Evaluating: batch 14 begins at 22:16:10.556053\n",
      "\n",
      "Evaluating: batch 14 ends at 22:16:10.781689\n",
      "\n",
      "Evaluating: batch 15 begins at 22:16:10.783536\n",
      "\n",
      "Evaluating: batch 15 ends at 22:16:11.005729\n",
      "\n",
      "Evaluating: batch 16 begins at 22:16:11.007629\n",
      "\n",
      "Evaluating: batch 16 ends at 22:16:11.231595\n",
      "\n",
      "Evaluating: batch 17 begins at 22:16:11.233044\n",
      "\n",
      "Evaluating: batch 17 ends at 22:16:11.453792\n",
      "\n",
      "Evaluating: batch 18 begins at 22:16:11.456070\n",
      "\n",
      "Evaluating: batch 18 ends at 22:16:11.673725\n",
      "\n",
      "Evaluating: batch 19 begins at 22:16:11.675407\n",
      "\n",
      "Evaluating: batch 19 ends at 22:16:11.893176\n",
      "\n",
      "Evaluating: batch 20 begins at 22:16:11.895089\n",
      "\n",
      "Evaluating: batch 20 ends at 22:16:12.113807\n",
      "\n",
      "Evaluating: batch 21 begins at 22:16:12.116306\n",
      "\n",
      "Evaluating: batch 21 ends at 22:16:12.332345\n",
      "\n",
      "Evaluating: batch 22 begins at 22:16:12.333974\n",
      "\n",
      "Evaluating: batch 22 ends at 22:16:12.550420\n",
      "\n",
      "Evaluating: batch 23 begins at 22:16:12.552074\n",
      "\n",
      "Evaluating: batch 23 ends at 22:16:12.773199\n",
      "\n",
      "Evaluating: batch 24 begins at 22:16:12.775456\n",
      "\n",
      "Evaluating: batch 24 ends at 22:16:12.995201\n",
      "\n",
      "Evaluating: batch 25 begins at 22:16:12.997552\n",
      "\n",
      "Evaluating: batch 25 ends at 22:16:13.215736\n",
      "\n",
      "Evaluating: batch 26 begins at 22:16:13.218200\n",
      "\n",
      "Evaluating: batch 26 ends at 22:16:13.434091\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51326\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.5074 - bce_dice_loss: 0.5074 - val_loss: 0.5135 - val_bce_dice_loss: 0.5135\n",
      "Epoch 25/25\n",
      "\n",
      "Training: batch 0 begins at 22:16:13.465886\n",
      "\n",
      "Training: batch 0 ends at 22:16:14.286275\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.6177 - bce_dice_loss: 0.6177\n",
      "Training: batch 1 begins at 22:16:14.290629\n",
      "\n",
      "Training: batch 1 ends at 22:16:15.092668\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.6401 - bce_dice_loss: 0.6401\n",
      "Training: batch 2 begins at 22:16:15.097020\n",
      "\n",
      "Training: batch 2 ends at 22:16:15.888284\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.5849 - bce_dice_loss: 0.5849\n",
      "Training: batch 3 begins at 22:16:15.892014\n",
      "\n",
      "Training: batch 3 ends at 22:16:16.681735\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.5536 - bce_dice_loss: 0.5536\n",
      "Training: batch 4 begins at 22:16:16.687307\n",
      "\n",
      "Training: batch 4 ends at 22:16:17.469849\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.5092 - bce_dice_loss: 0.5092\n",
      "Training: batch 5 begins at 22:16:17.473598\n",
      "\n",
      "Training: batch 5 ends at 22:16:18.258813\n",
      " 6/60 [==>...........................] - ETA: 42s - loss: 0.4943 - bce_dice_loss: 0.4943\n",
      "Training: batch 6 begins at 22:16:18.262465\n",
      "\n",
      "Training: batch 6 ends at 22:16:19.057121\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4833 - bce_dice_loss: 0.4833\n",
      "Training: batch 7 begins at 22:16:19.061239\n",
      "\n",
      "Training: batch 7 ends at 22:16:19.870602\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4823 - bce_dice_loss: 0.4823\n",
      "Training: batch 8 begins at 22:16:19.871659\n",
      "\n",
      "Training: batch 8 ends at 22:16:20.669820\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.5078 - bce_dice_loss: 0.5078\n",
      "Training: batch 9 begins at 22:16:20.673090\n",
      "\n",
      "Training: batch 9 ends at 22:16:21.493548\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5254 - bce_dice_loss: 0.5254\n",
      "Training: batch 10 begins at 22:16:21.497643\n",
      "\n",
      "Training: batch 10 ends at 22:16:22.313201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5361 - bce_dice_loss: 0.5361\n",
      "Training: batch 11 begins at 22:16:22.318246\n",
      "\n",
      "Training: batch 11 ends at 22:16:23.111341\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5381 - bce_dice_loss: 0.5381\n",
      "Training: batch 12 begins at 22:16:23.115576\n",
      "\n",
      "Training: batch 12 ends at 22:16:23.940071\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5228 - bce_dice_loss: 0.5228\n",
      "Training: batch 13 begins at 22:16:23.943705\n",
      "\n",
      "Training: batch 13 ends at 22:16:24.734253\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5215 - bce_dice_loss: 0.5215\n",
      "Training: batch 14 begins at 22:16:24.737755\n",
      "\n",
      "Training: batch 14 ends at 22:16:25.543626\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5044 - bce_dice_loss: 0.5044\n",
      "Training: batch 15 begins at 22:16:25.547097\n",
      "\n",
      "Training: batch 15 ends at 22:16:26.344003\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4950 - bce_dice_loss: 0.4950\n",
      "Training: batch 16 begins at 22:16:26.347751\n",
      "\n",
      "Training: batch 16 ends at 22:16:27.160743\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4836 - bce_dice_loss: 0.4836\n",
      "Training: batch 17 begins at 22:16:27.164918\n",
      "\n",
      "Training: batch 17 ends at 22:16:27.961368\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.4779 - bce_dice_loss: 0.4779\n",
      "Training: batch 18 begins at 22:16:27.965590\n",
      "\n",
      "Training: batch 18 ends at 22:16:28.771741\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.4749 - bce_dice_loss: 0.4749\n",
      "Training: batch 19 begins at 22:16:28.776065\n",
      "\n",
      "Training: batch 19 ends at 22:16:29.570766\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4667 - bce_dice_loss: 0.4667\n",
      "Training: batch 20 begins at 22:16:29.575204\n",
      "\n",
      "Training: batch 20 ends at 22:16:30.378222\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4619 - bce_dice_loss: 0.4619\n",
      "Training: batch 21 begins at 22:16:30.382705\n",
      "\n",
      "Training: batch 21 ends at 22:16:31.189596\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4675 - bce_dice_loss: 0.4675\n",
      "Training: batch 22 begins at 22:16:31.194109\n",
      "\n",
      "Training: batch 22 ends at 22:16:31.986705\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4720 - bce_dice_loss: 0.4720\n",
      "Training: batch 23 begins at 22:16:31.990829\n",
      "\n",
      "Training: batch 23 ends at 22:16:32.793958\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4734 - bce_dice_loss: 0.4734\n",
      "Training: batch 24 begins at 22:16:32.798501\n",
      "\n",
      "Training: batch 24 ends at 22:16:33.600568\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4810 - bce_dice_loss: 0.4810\n",
      "Training: batch 25 begins at 22:16:33.604932\n",
      "\n",
      "Training: batch 25 ends at 22:16:34.396036\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4749 - bce_dice_loss: 0.4749\n",
      "Training: batch 26 begins at 22:16:34.400806\n",
      "\n",
      "Training: batch 26 ends at 22:16:35.186462\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4736 - bce_dice_loss: 0.4736\n",
      "Training: batch 27 begins at 22:16:35.190365\n",
      "\n",
      "Training: batch 27 ends at 22:16:35.983555\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4791 - bce_dice_loss: 0.4791\n",
      "Training: batch 28 begins at 22:16:35.987426\n",
      "\n",
      "Training: batch 28 ends at 22:16:36.786143\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4773 - bce_dice_loss: 0.4773\n",
      "Training: batch 29 begins at 22:16:36.788965\n",
      "\n",
      "Training: batch 29 ends at 22:16:37.589011\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4747 - bce_dice_loss: 0.4747\n",
      "Training: batch 30 begins at 22:16:37.595122\n",
      "\n",
      "Training: batch 30 ends at 22:16:38.407797\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4764 - bce_dice_loss: 0.4764\n",
      "Training: batch 31 begins at 22:16:38.411411\n",
      "\n",
      "Training: batch 31 ends at 22:16:39.201816\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4765 - bce_dice_loss: 0.4765\n",
      "Training: batch 32 begins at 22:16:39.205444\n",
      "\n",
      "Training: batch 32 ends at 22:16:40.014145\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4727 - bce_dice_loss: 0.4727\n",
      "Training: batch 33 begins at 22:16:40.017513\n",
      "\n",
      "Training: batch 33 ends at 22:16:40.814044\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4702 - bce_dice_loss: 0.4702\n",
      "Training: batch 34 begins at 22:16:40.818724\n",
      "\n",
      "Training: batch 34 ends at 22:16:41.609781\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4619 - bce_dice_loss: 0.4619\n",
      "Training: batch 35 begins at 22:16:41.613174\n",
      "\n",
      "Training: batch 35 ends at 22:16:42.411691\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4615 - bce_dice_loss: 0.4615\n",
      "Training: batch 36 begins at 22:16:42.416433\n",
      "\n",
      "Training: batch 36 ends at 22:16:43.237503\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4619 - bce_dice_loss: 0.4619\n",
      "Training: batch 37 begins at 22:16:43.240956\n",
      "\n",
      "Training: batch 37 ends at 22:16:44.029904\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4627 - bce_dice_loss: 0.4627\n",
      "Training: batch 38 begins at 22:16:44.036724\n",
      "\n",
      "Training: batch 38 ends at 22:16:44.851686\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4596 - bce_dice_loss: 0.4596\n",
      "Training: batch 39 begins at 22:16:44.855260\n",
      "\n",
      "Training: batch 39 ends at 22:16:45.647166\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4578 - bce_dice_loss: 0.4578\n",
      "Training: batch 40 begins at 22:16:45.652076\n",
      "\n",
      "Training: batch 40 ends at 22:16:46.463053\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4602 - bce_dice_loss: 0.4602\n",
      "Training: batch 41 begins at 22:16:46.468742\n",
      "\n",
      "Training: batch 41 ends at 22:16:47.260173\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4604 - bce_dice_loss: 0.4604\n",
      "Training: batch 42 begins at 22:16:47.264247\n",
      "\n",
      "Training: batch 42 ends at 22:16:48.058799\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4595 - bce_dice_loss: 0.4595\n",
      "Training: batch 43 begins at 22:16:48.062359\n",
      "\n",
      "Training: batch 43 ends at 22:16:48.881888\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4593 - bce_dice_loss: 0.4593\n",
      "Training: batch 44 begins at 22:16:48.886699\n",
      "\n",
      "Training: batch 44 ends at 22:16:49.676188\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4636 - bce_dice_loss: 0.4636\n",
      "Training: batch 45 begins at 22:16:49.680557\n",
      "\n",
      "Training: batch 45 ends at 22:16:50.474072\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4647 - bce_dice_loss: 0.4647\n",
      "Training: batch 46 begins at 22:16:50.478167\n",
      "\n",
      "Training: batch 46 ends at 22:16:51.304780\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4675 - bce_dice_loss: 0.4675\n",
      "Training: batch 47 begins at 22:16:51.309421\n",
      "\n",
      "Training: batch 47 ends at 22:16:52.102811\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4682 - bce_dice_loss: 0.4682 \n",
      "Training: batch 48 begins at 22:16:52.106410\n",
      "\n",
      "Training: batch 48 ends at 22:16:52.901227\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4685 - bce_dice_loss: 0.4685\n",
      "Training: batch 49 begins at 22:16:52.904985\n",
      "\n",
      "Training: batch 49 ends at 22:16:53.694057\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4688 - bce_dice_loss: 0.4688\n",
      "Training: batch 50 begins at 22:16:53.698125\n",
      "\n",
      "Training: batch 50 ends at 22:16:54.488053\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4705 - bce_dice_loss: 0.4705\n",
      "Training: batch 51 begins at 22:16:54.491625\n",
      "\n",
      "Training: batch 51 ends at 22:16:55.284171\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4699 - bce_dice_loss: 0.4699\n",
      "Training: batch 52 begins at 22:16:55.288082\n",
      "\n",
      "Training: batch 52 ends at 22:16:56.100780\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4768 - bce_dice_loss: 0.4768\n",
      "Training: batch 53 begins at 22:16:56.104598\n",
      "\n",
      "Training: batch 53 ends at 22:16:56.890954\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4759 - bce_dice_loss: 0.4759\n",
      "Training: batch 54 begins at 22:16:56.894224\n",
      "\n",
      "Training: batch 54 ends at 22:16:57.682917\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4776 - bce_dice_loss: 0.4776\n",
      "Training: batch 55 begins at 22:16:57.686120\n",
      "\n",
      "Training: batch 55 ends at 22:16:58.475061\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4769 - bce_dice_loss: 0.4769\n",
      "Training: batch 56 begins at 22:16:58.479638\n",
      "\n",
      "Training: batch 56 ends at 22:16:59.270689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4781 - bce_dice_loss: 0.4781\n",
      "Training: batch 57 begins at 22:16:59.274926\n",
      "\n",
      "Training: batch 57 ends at 22:17:00.061087\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4786 - bce_dice_loss: 0.4786\n",
      "Training: batch 58 begins at 22:17:00.064419\n",
      "\n",
      "Training: batch 58 ends at 22:17:00.858553\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4790 - bce_dice_loss: 0.4790\n",
      "Training: batch 59 begins at 22:17:00.862440\n",
      "\n",
      "Training: batch 59 ends at 22:17:01.679112\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4864 - bce_dice_loss: 0.4864\n",
      "Evaluating: batch 0 begins at 22:17:01.711486\n",
      "\n",
      "Evaluating: batch 0 ends at 22:17:01.978539\n",
      "\n",
      "Evaluating: batch 1 begins at 22:17:01.979515\n",
      "\n",
      "Evaluating: batch 1 ends at 22:17:02.196098\n",
      "\n",
      "Evaluating: batch 2 begins at 22:17:02.197375\n",
      "\n",
      "Evaluating: batch 2 ends at 22:17:02.418113\n",
      "\n",
      "Evaluating: batch 3 begins at 22:17:02.419553\n",
      "\n",
      "Evaluating: batch 3 ends at 22:17:02.639123\n",
      "\n",
      "Evaluating: batch 4 begins at 22:17:02.641409\n",
      "\n",
      "Evaluating: batch 4 ends at 22:17:02.861919\n",
      "\n",
      "Evaluating: batch 5 begins at 22:17:02.863548\n",
      "\n",
      "Evaluating: batch 5 ends at 22:17:03.083840\n",
      "\n",
      "Evaluating: batch 6 begins at 22:17:03.085277\n",
      "\n",
      "Evaluating: batch 6 ends at 22:17:03.299934\n",
      "\n",
      "Evaluating: batch 7 begins at 22:17:03.301327\n",
      "\n",
      "Evaluating: batch 7 ends at 22:17:03.522435\n",
      "\n",
      "Evaluating: batch 8 begins at 22:17:03.524183\n",
      "\n",
      "Evaluating: batch 8 ends at 22:17:03.748281\n",
      "\n",
      "Evaluating: batch 9 begins at 22:17:03.751185\n",
      "\n",
      "Evaluating: batch 9 ends at 22:17:03.971778\n",
      "\n",
      "Evaluating: batch 10 begins at 22:17:03.973060\n",
      "\n",
      "Evaluating: batch 10 ends at 22:17:04.195824\n",
      "\n",
      "Evaluating: batch 11 begins at 22:17:04.197399\n",
      "\n",
      "Evaluating: batch 11 ends at 22:17:04.416489\n",
      "\n",
      "Evaluating: batch 12 begins at 22:17:04.418769\n",
      "\n",
      "Evaluating: batch 12 ends at 22:17:04.639608\n",
      "\n",
      "Evaluating: batch 13 begins at 22:17:04.640848\n",
      "\n",
      "Evaluating: batch 13 ends at 22:17:04.865086\n",
      "\n",
      "Evaluating: batch 14 begins at 22:17:04.866493\n",
      "\n",
      "Evaluating: batch 14 ends at 22:17:05.084460\n",
      "\n",
      "Evaluating: batch 15 begins at 22:17:05.086199\n",
      "\n",
      "Evaluating: batch 15 ends at 22:17:05.305549\n",
      "\n",
      "Evaluating: batch 16 begins at 22:17:05.306977\n",
      "\n",
      "Evaluating: batch 16 ends at 22:17:05.527186\n",
      "\n",
      "Evaluating: batch 17 begins at 22:17:05.529347\n",
      "\n",
      "Evaluating: batch 17 ends at 22:17:05.753011\n",
      "\n",
      "Evaluating: batch 18 begins at 22:17:05.754348\n",
      "\n",
      "Evaluating: batch 18 ends at 22:17:05.972620\n",
      "\n",
      "Evaluating: batch 19 begins at 22:17:05.973930\n",
      "\n",
      "Evaluating: batch 19 ends at 22:17:06.191571\n",
      "\n",
      "Evaluating: batch 20 begins at 22:17:06.193745\n",
      "\n",
      "Evaluating: batch 20 ends at 22:17:06.412628\n",
      "\n",
      "Evaluating: batch 21 begins at 22:17:06.414980\n",
      "\n",
      "Evaluating: batch 21 ends at 22:17:06.630646\n",
      "\n",
      "Evaluating: batch 22 begins at 22:17:06.632020\n",
      "\n",
      "Evaluating: batch 22 ends at 22:17:06.849467\n",
      "\n",
      "Evaluating: batch 23 begins at 22:17:06.850821\n",
      "\n",
      "Evaluating: batch 23 ends at 22:17:07.068925\n",
      "\n",
      "Evaluating: batch 24 begins at 22:17:07.070739\n",
      "\n",
      "Evaluating: batch 24 ends at 22:17:07.289149\n",
      "\n",
      "Evaluating: batch 25 begins at 22:17:07.291618\n",
      "\n",
      "Evaluating: batch 25 ends at 22:17:07.507342\n",
      "\n",
      "Evaluating: batch 26 begins at 22:17:07.508579\n",
      "\n",
      "Evaluating: batch 26 ends at 22:17:07.726919\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.51326 to 0.48615, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 933ms/step - loss: 0.4864 - bce_dice_loss: 0.4864 - val_loss: 0.4862 - val_bce_dice_loss: 0.4862\n",
      "Train hist recorded for batch -  0\n",
      "Saving the model\n",
      "Saved the model at saved_models/training_25/batch_0/\n",
      "Reading train data\n",
      "x train ----  120\n",
      "x val ----  54\n",
      "Train and Validation data created\n",
      "Epoch 1/25\n",
      "\n",
      "Training: batch 0 begins at 22:17:29.950505\n",
      "\n",
      "Training: batch 0 ends at 22:17:31.018202\n",
      " 1/60 [..............................] - ETA: 1:06 - loss: 0.5627 - bce_dice_loss: 0.5627\n",
      "Training: batch 1 begins at 22:17:31.079861\n",
      "\n",
      "Training: batch 1 ends at 22:17:31.877096\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5307 - bce_dice_loss: 0.5307 \n",
      "Training: batch 2 begins at 22:17:31.880670\n",
      "\n",
      "Training: batch 2 ends at 22:17:32.685882\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.4939 - bce_dice_loss: 0.4939\n",
      "Training: batch 3 begins at 22:17:32.689762\n",
      "\n",
      "Training: batch 3 ends at 22:17:33.503325\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5172 - bce_dice_loss: 0.5172\n",
      "Training: batch 4 begins at 22:17:33.508289\n",
      "\n",
      "Training: batch 4 ends at 22:17:34.311566\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.5212 - bce_dice_loss: 0.5212\n",
      "Training: batch 5 begins at 22:17:34.316288\n",
      "\n",
      "Training: batch 5 ends at 22:17:35.136576\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4983 - bce_dice_loss: 0.4983WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1673s vs `on_train_batch_end` time: 0.6590s). Check your callbacks.\n",
      "\n",
      "Training: batch 6 begins at 22:17:35.141411\n",
      "\n",
      "Training: batch 6 ends at 22:17:35.945479\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.5126 - bce_dice_loss: 0.5126\n",
      "Training: batch 7 begins at 22:17:35.950576\n",
      "\n",
      "Training: batch 7 ends at 22:17:36.754143\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.4960 - bce_dice_loss: 0.4960\n",
      "Training: batch 8 begins at 22:17:36.758382\n",
      "\n",
      "Training: batch 8 ends at 22:17:37.557798\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.4908 - bce_dice_loss: 0.4908\n",
      "Training: batch 9 begins at 22:17:37.561387\n",
      "\n",
      "Training: batch 9 ends at 22:17:38.388754\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5061 - bce_dice_loss: 0.5061\n",
      "Training: batch 10 begins at 22:17:38.392064\n",
      "\n",
      "Training: batch 10 ends at 22:17:39.186734\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4922 - bce_dice_loss: 0.4922\n",
      "Training: batch 11 begins at 22:17:39.190136\n",
      "\n",
      "Training: batch 11 ends at 22:17:39.989349\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4893 - bce_dice_loss: 0.4893\n",
      "Training: batch 12 begins at 22:17:39.993379\n",
      "\n",
      "Training: batch 12 ends at 22:17:40.812898\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.4841 - bce_dice_loss: 0.4841\n",
      "Training: batch 13 begins at 22:17:40.817532\n",
      "\n",
      "Training: batch 13 ends at 22:17:41.613020\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.4766 - bce_dice_loss: 0.4766\n",
      "Training: batch 14 begins at 22:17:41.617800\n",
      "\n",
      "Training: batch 14 ends at 22:17:42.438659\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4830 - bce_dice_loss: 0.4830\n",
      "Training: batch 15 begins at 22:17:42.442416\n",
      "\n",
      "Training: batch 15 ends at 22:17:43.244010\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4831 - bce_dice_loss: 0.4831\n",
      "Training: batch 16 begins at 22:17:43.248231\n",
      "\n",
      "Training: batch 16 ends at 22:17:44.047417\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4738 - bce_dice_loss: 0.4738\n",
      "Training: batch 17 begins at 22:17:44.052656\n",
      "\n",
      "Training: batch 17 ends at 22:17:44.844187\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.4814 - bce_dice_loss: 0.4814\n",
      "Training: batch 18 begins at 22:17:44.848738\n",
      "\n",
      "Training: batch 18 ends at 22:17:45.647728\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.4829 - bce_dice_loss: 0.4829\n",
      "Training: batch 19 begins at 22:17:45.652524\n",
      "\n",
      "Training: batch 19 ends at 22:17:46.451858\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4795 - bce_dice_loss: 0.4795\n",
      "Training: batch 20 begins at 22:17:46.456577\n",
      "\n",
      "Training: batch 20 ends at 22:17:47.275009\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4812 - bce_dice_loss: 0.4812\n",
      "Training: batch 21 begins at 22:17:47.278472\n",
      "\n",
      "Training: batch 21 ends at 22:17:48.079915\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4852 - bce_dice_loss: 0.4852\n",
      "Training: batch 22 begins at 22:17:48.084520\n",
      "\n",
      "Training: batch 22 ends at 22:17:48.887089\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4878 - bce_dice_loss: 0.4878\n",
      "Training: batch 23 begins at 22:17:48.891493\n",
      "\n",
      "Training: batch 23 ends at 22:17:49.693105\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4957 - bce_dice_loss: 0.4957\n",
      "Training: batch 24 begins at 22:17:49.698100\n",
      "\n",
      "Training: batch 24 ends at 22:17:50.507054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4933 - bce_dice_loss: 0.4933\n",
      "Training: batch 25 begins at 22:17:50.511955\n",
      "\n",
      "Training: batch 25 ends at 22:17:51.338382\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4939 - bce_dice_loss: 0.4939\n",
      "Training: batch 26 begins at 22:17:51.342505\n",
      "\n",
      "Training: batch 26 ends at 22:17:52.143922\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4952 - bce_dice_loss: 0.4952\n",
      "Training: batch 27 begins at 22:17:52.148715\n",
      "\n",
      "Training: batch 27 ends at 22:17:52.964757\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4900 - bce_dice_loss: 0.4900\n",
      "Training: batch 28 begins at 22:17:52.969185\n",
      "\n",
      "Training: batch 28 ends at 22:17:53.769388\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.4873 - bce_dice_loss: 0.4873\n",
      "Training: batch 29 begins at 22:17:53.775017\n",
      "\n",
      "Training: batch 29 ends at 22:17:54.579721\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4958 - bce_dice_loss: 0.4958\n",
      "Training: batch 30 begins at 22:17:54.582594\n",
      "\n",
      "Training: batch 30 ends at 22:17:55.383673\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4940 - bce_dice_loss: 0.4940\n",
      "Training: batch 31 begins at 22:17:55.387357\n",
      "\n",
      "Training: batch 31 ends at 22:17:56.194065\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5018 - bce_dice_loss: 0.5018\n",
      "Training: batch 32 begins at 22:17:56.198631\n",
      "\n",
      "Training: batch 32 ends at 22:17:57.008199\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4980 - bce_dice_loss: 0.4980\n",
      "Training: batch 33 begins at 22:17:57.012475\n",
      "\n",
      "Training: batch 33 ends at 22:17:57.813453\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.4986 - bce_dice_loss: 0.4986\n",
      "Training: batch 34 begins at 22:17:57.818281\n",
      "\n",
      "Training: batch 34 ends at 22:17:58.624870\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4965 - bce_dice_loss: 0.4965\n",
      "Training: batch 35 begins at 22:17:58.629099\n",
      "\n",
      "Training: batch 35 ends at 22:17:59.432162\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4945 - bce_dice_loss: 0.4945\n",
      "Training: batch 36 begins at 22:17:59.435901\n",
      "\n",
      "Training: batch 36 ends at 22:18:00.242806\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4981 - bce_dice_loss: 0.4981\n",
      "Training: batch 37 begins at 22:18:00.247848\n",
      "\n",
      "Training: batch 37 ends at 22:18:01.063252\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5072 - bce_dice_loss: 0.5072\n",
      "Training: batch 38 begins at 22:18:01.069701\n",
      "\n",
      "Training: batch 38 ends at 22:18:01.884722\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.5071 - bce_dice_loss: 0.5071\n",
      "Training: batch 39 begins at 22:18:01.888060\n",
      "\n",
      "Training: batch 39 ends at 22:18:02.694900\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5111 - bce_dice_loss: 0.5111\n",
      "Training: batch 40 begins at 22:18:02.698476\n",
      "\n",
      "Training: batch 40 ends at 22:18:03.502168\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.5142 - bce_dice_loss: 0.5142\n",
      "Training: batch 41 begins at 22:18:03.504767\n",
      "\n",
      "Training: batch 41 ends at 22:18:04.346400\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5111 - bce_dice_loss: 0.5111\n",
      "Training: batch 42 begins at 22:18:04.350757\n",
      "\n",
      "Training: batch 42 ends at 22:18:05.148337\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5140 - bce_dice_loss: 0.5140\n",
      "Training: batch 43 begins at 22:18:05.152067\n",
      "\n",
      "Training: batch 43 ends at 22:18:05.952767\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5201 - bce_dice_loss: 0.5201\n",
      "Training: batch 44 begins at 22:18:05.956444\n",
      "\n",
      "Training: batch 44 ends at 22:18:06.753427\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.5175 - bce_dice_loss: 0.5175\n",
      "Training: batch 45 begins at 22:18:06.756240\n",
      "\n",
      "Training: batch 45 ends at 22:18:07.574184\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5185 - bce_dice_loss: 0.5185\n",
      "Training: batch 46 begins at 22:18:07.578224\n",
      "\n",
      "Training: batch 46 ends at 22:18:08.378851\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5187 - bce_dice_loss: 0.5187\n",
      "Training: batch 47 begins at 22:18:08.383253\n",
      "\n",
      "Training: batch 47 ends at 22:18:09.183406\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5183 - bce_dice_loss: 0.5183 \n",
      "Training: batch 48 begins at 22:18:09.186750\n",
      "\n",
      "Training: batch 48 ends at 22:18:09.987101\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5187 - bce_dice_loss: 0.5187\n",
      "Training: batch 49 begins at 22:18:09.991425\n",
      "\n",
      "Training: batch 49 ends at 22:18:10.833120\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5216 - bce_dice_loss: 0.5216\n",
      "Training: batch 50 begins at 22:18:10.836799\n",
      "\n",
      "Training: batch 50 ends at 22:18:11.641668\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5207 - bce_dice_loss: 0.5207\n",
      "Training: batch 51 begins at 22:18:11.646350\n",
      "\n",
      "Training: batch 51 ends at 22:18:12.450646\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5209 - bce_dice_loss: 0.5209\n",
      "Training: batch 52 begins at 22:18:12.455219\n",
      "\n",
      "Training: batch 52 ends at 22:18:13.268961\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5220 - bce_dice_loss: 0.5220\n",
      "Training: batch 53 begins at 22:18:13.271850\n",
      "\n",
      "Training: batch 53 ends at 22:18:14.087424\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5218 - bce_dice_loss: 0.5218\n",
      "Training: batch 54 begins at 22:18:14.091698\n",
      "\n",
      "Training: batch 54 ends at 22:18:14.894841\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5218 - bce_dice_loss: 0.5218\n",
      "Training: batch 55 begins at 22:18:14.899620\n",
      "\n",
      "Training: batch 55 ends at 22:18:15.700859\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5234 - bce_dice_loss: 0.5234\n",
      "Training: batch 56 begins at 22:18:15.704737\n",
      "\n",
      "Training: batch 56 ends at 22:18:16.500309\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5229 - bce_dice_loss: 0.5229\n",
      "Training: batch 57 begins at 22:18:16.504058\n",
      "\n",
      "Training: batch 57 ends at 22:18:17.322147\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5254 - bce_dice_loss: 0.5254\n",
      "Training: batch 58 begins at 22:18:17.326441\n",
      "\n",
      "Training: batch 58 ends at 22:18:18.126269\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5276 - bce_dice_loss: 0.5276\n",
      "Training: batch 59 begins at 22:18:18.130577\n",
      "\n",
      "Training: batch 59 ends at 22:18:18.920285\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5280 - bce_dice_loss: 0.5280\n",
      "Evaluating: batch 0 begins at 22:18:19.654734\n",
      "\n",
      "Evaluating: batch 0 ends at 22:18:19.932288\n",
      "\n",
      "Evaluating: batch 1 begins at 22:18:19.933410\n",
      "\n",
      "Evaluating: batch 1 ends at 22:18:20.156461\n",
      "\n",
      "Evaluating: batch 2 begins at 22:18:20.157439\n",
      "\n",
      "Evaluating: batch 2 ends at 22:18:20.391479\n",
      "\n",
      "Evaluating: batch 3 begins at 22:18:20.393023\n",
      "\n",
      "Evaluating: batch 3 ends at 22:18:20.622350\n",
      "\n",
      "Evaluating: batch 4 begins at 22:18:20.623454\n",
      "\n",
      "Evaluating: batch 4 ends at 22:18:20.846011\n",
      "\n",
      "Evaluating: batch 5 begins at 22:18:20.847346\n",
      "\n",
      "Evaluating: batch 5 ends at 22:18:21.075238\n",
      "\n",
      "Evaluating: batch 6 begins at 22:18:21.076401\n",
      "\n",
      "Evaluating: batch 6 ends at 22:18:21.309277\n",
      "\n",
      "Evaluating: batch 7 begins at 22:18:21.310419\n",
      "\n",
      "Evaluating: batch 7 ends at 22:18:21.541543\n",
      "\n",
      "Evaluating: batch 8 begins at 22:18:21.542681\n",
      "\n",
      "Evaluating: batch 8 ends at 22:18:21.772555\n",
      "\n",
      "Evaluating: batch 9 begins at 22:18:21.773565\n",
      "\n",
      "Evaluating: batch 9 ends at 22:18:22.000879\n",
      "\n",
      "Evaluating: batch 10 begins at 22:18:22.002110\n",
      "\n",
      "Evaluating: batch 10 ends at 22:18:22.229019\n",
      "\n",
      "Evaluating: batch 11 begins at 22:18:22.230234\n",
      "\n",
      "Evaluating: batch 11 ends at 22:18:22.456954\n",
      "\n",
      "Evaluating: batch 12 begins at 22:18:22.458111\n",
      "\n",
      "Evaluating: batch 12 ends at 22:18:22.684280\n",
      "\n",
      "Evaluating: batch 13 begins at 22:18:22.685457\n",
      "\n",
      "Evaluating: batch 13 ends at 22:18:22.914252\n",
      "\n",
      "Evaluating: batch 14 begins at 22:18:22.915386\n",
      "\n",
      "Evaluating: batch 14 ends at 22:18:23.141719\n",
      "\n",
      "Evaluating: batch 15 begins at 22:18:23.142876\n",
      "\n",
      "Evaluating: batch 15 ends at 22:18:23.371671\n",
      "\n",
      "Evaluating: batch 16 begins at 22:18:23.372900\n",
      "\n",
      "Evaluating: batch 16 ends at 22:18:23.600980\n",
      "\n",
      "Evaluating: batch 17 begins at 22:18:23.602117\n",
      "\n",
      "Evaluating: batch 17 ends at 22:18:23.831641\n",
      "\n",
      "Evaluating: batch 18 begins at 22:18:23.832810\n",
      "\n",
      "Evaluating: batch 18 ends at 22:18:24.061253\n",
      "\n",
      "Evaluating: batch 19 begins at 22:18:24.062459\n",
      "\n",
      "Evaluating: batch 19 ends at 22:18:24.288806\n",
      "\n",
      "Evaluating: batch 20 begins at 22:18:24.290025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 20 ends at 22:18:24.514988\n",
      "\n",
      "Evaluating: batch 21 begins at 22:18:24.516058\n",
      "\n",
      "Evaluating: batch 21 ends at 22:18:24.743351\n",
      "\n",
      "Evaluating: batch 22 begins at 22:18:24.744587\n",
      "\n",
      "Evaluating: batch 22 ends at 22:18:24.970215\n",
      "\n",
      "Evaluating: batch 23 begins at 22:18:24.971764\n",
      "\n",
      "Evaluating: batch 23 ends at 22:18:25.197179\n",
      "\n",
      "Evaluating: batch 24 begins at 22:18:25.198302\n",
      "\n",
      "Evaluating: batch 24 ends at 22:18:25.424526\n",
      "\n",
      "Evaluating: batch 25 begins at 22:18:25.425673\n",
      "\n",
      "Evaluating: batch 25 ends at 22:18:25.652340\n",
      "\n",
      "Evaluating: batch 26 begins at 22:18:25.653613\n",
      "\n",
      "Evaluating: batch 26 ends at 22:18:25.879775\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.48615\n",
      "60/60 [==============================] - 56s 929ms/step - loss: 0.5280 - bce_dice_loss: 0.5280 - val_loss: 0.5014 - val_bce_dice_loss: 0.5014\n",
      "Epoch 2/25\n",
      "\n",
      "Training: batch 0 begins at 22:18:25.898504\n",
      "\n",
      "Training: batch 0 ends at 22:18:26.706843\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.5030 - bce_dice_loss: 0.5030\n",
      "Training: batch 1 begins at 22:18:26.711242\n",
      "\n",
      "Training: batch 1 ends at 22:18:27.527803\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.5106 - bce_dice_loss: 0.5106\n",
      "Training: batch 2 begins at 22:18:27.532901\n",
      "\n",
      "Training: batch 2 ends at 22:18:28.335693\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4920 - bce_dice_loss: 0.4920\n",
      "Training: batch 3 begins at 22:18:28.339609\n",
      "\n",
      "Training: batch 3 ends at 22:18:29.151263\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5010 - bce_dice_loss: 0.5010\n",
      "Training: batch 4 begins at 22:18:29.155590\n",
      "\n",
      "Training: batch 4 ends at 22:18:29.962665\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.5198 - bce_dice_loss: 0.5198\n",
      "Training: batch 5 begins at 22:18:29.965031\n",
      "\n",
      "Training: batch 5 ends at 22:18:30.782464\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.4951 - bce_dice_loss: 0.4951\n",
      "Training: batch 6 begins at 22:18:30.785813\n",
      "\n",
      "Training: batch 6 ends at 22:18:31.585538\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.5003 - bce_dice_loss: 0.5003\n",
      "Training: batch 7 begins at 22:18:31.589910\n",
      "\n",
      "Training: batch 7 ends at 22:18:32.396986\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.5127 - bce_dice_loss: 0.5127\n",
      "Training: batch 8 begins at 22:18:32.401084\n",
      "\n",
      "Training: batch 8 ends at 22:18:33.201467\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5142 - bce_dice_loss: 0.5142\n",
      "Training: batch 9 begins at 22:18:33.204781\n",
      "\n",
      "Training: batch 9 ends at 22:18:34.002500\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4985 - bce_dice_loss: 0.4985\n",
      "Training: batch 10 begins at 22:18:34.005229\n",
      "\n",
      "Training: batch 10 ends at 22:18:34.806061\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4967 - bce_dice_loss: 0.4967\n",
      "Training: batch 11 begins at 22:18:34.809494\n",
      "\n",
      "Training: batch 11 ends at 22:18:35.607249\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4917 - bce_dice_loss: 0.4917\n",
      "Training: batch 12 begins at 22:18:35.611804\n",
      "\n",
      "Training: batch 12 ends at 22:18:36.418245\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.4779 - bce_dice_loss: 0.4779\n",
      "Training: batch 13 begins at 22:18:36.421286\n",
      "\n",
      "Training: batch 13 ends at 22:18:37.237102\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.4734 - bce_dice_loss: 0.4734\n",
      "Training: batch 14 begins at 22:18:37.240630\n",
      "\n",
      "Training: batch 14 ends at 22:18:38.041388\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4715 - bce_dice_loss: 0.4715\n",
      "Training: batch 15 begins at 22:18:38.045209\n",
      "\n",
      "Training: batch 15 ends at 22:18:38.841820\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4838 - bce_dice_loss: 0.4838\n",
      "Training: batch 16 begins at 22:18:38.846258\n",
      "\n",
      "Training: batch 16 ends at 22:18:39.667295\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4838 - bce_dice_loss: 0.4838\n",
      "Training: batch 17 begins at 22:18:39.670459\n",
      "\n",
      "Training: batch 17 ends at 22:18:40.477497\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.4788 - bce_dice_loss: 0.4788\n",
      "Training: batch 18 begins at 22:18:40.481621\n",
      "\n",
      "Training: batch 18 ends at 22:18:41.280430\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.4694 - bce_dice_loss: 0.4694\n",
      "Training: batch 19 begins at 22:18:41.285375\n",
      "\n",
      "Training: batch 19 ends at 22:18:42.088735\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4762 - bce_dice_loss: 0.4762\n",
      "Training: batch 20 begins at 22:18:42.093453\n",
      "\n",
      "Training: batch 20 ends at 22:18:42.895288\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4734 - bce_dice_loss: 0.4734\n",
      "Training: batch 21 begins at 22:18:42.899046\n",
      "\n",
      "Training: batch 21 ends at 22:18:43.724122\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4768 - bce_dice_loss: 0.4768\n",
      "Training: batch 22 begins at 22:18:43.728343\n",
      "\n",
      "Training: batch 22 ends at 22:18:44.535447\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4722 - bce_dice_loss: 0.4722\n",
      "Training: batch 23 begins at 22:18:44.538847\n",
      "\n",
      "Training: batch 23 ends at 22:18:45.339029\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4711 - bce_dice_loss: 0.4711\n",
      "Training: batch 24 begins at 22:18:45.342871\n",
      "\n",
      "Training: batch 24 ends at 22:18:46.171499\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4717 - bce_dice_loss: 0.4717\n",
      "Training: batch 25 begins at 22:18:46.175040\n",
      "\n",
      "Training: batch 25 ends at 22:18:46.970282\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4753 - bce_dice_loss: 0.4753\n",
      "Training: batch 26 begins at 22:18:46.973807\n",
      "\n",
      "Training: batch 26 ends at 22:18:47.774515\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4821 - bce_dice_loss: 0.4821\n",
      "Training: batch 27 begins at 22:18:47.778744\n",
      "\n",
      "Training: batch 27 ends at 22:18:48.582868\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4827 - bce_dice_loss: 0.4827\n",
      "Training: batch 28 begins at 22:18:48.586178\n",
      "\n",
      "Training: batch 28 ends at 22:18:49.384386\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.4825 - bce_dice_loss: 0.4825\n",
      "Training: batch 29 begins at 22:18:49.387212\n",
      "\n",
      "Training: batch 29 ends at 22:18:50.203692\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4922 - bce_dice_loss: 0.4922\n",
      "Training: batch 30 begins at 22:18:50.207005\n",
      "\n",
      "Training: batch 30 ends at 22:18:51.021506\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4905 - bce_dice_loss: 0.4905\n",
      "Training: batch 31 begins at 22:18:51.025054\n",
      "\n",
      "Training: batch 31 ends at 22:18:51.835029\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4997 - bce_dice_loss: 0.4997\n",
      "Training: batch 32 begins at 22:18:51.838007\n",
      "\n",
      "Training: batch 32 ends at 22:18:52.645119\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5051 - bce_dice_loss: 0.5051\n",
      "Training: batch 33 begins at 22:18:52.650159\n",
      "\n",
      "Training: batch 33 ends at 22:18:53.453295\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.5029 - bce_dice_loss: 0.5029\n",
      "Training: batch 34 begins at 22:18:53.456846\n",
      "\n",
      "Training: batch 34 ends at 22:18:54.257715\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5032 - bce_dice_loss: 0.5032\n",
      "Training: batch 35 begins at 22:18:54.262219\n",
      "\n",
      "Training: batch 35 ends at 22:18:55.083118\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5019 - bce_dice_loss: 0.5019\n",
      "Training: batch 36 begins at 22:18:55.087118\n",
      "\n",
      "Training: batch 36 ends at 22:18:55.905596\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5006 - bce_dice_loss: 0.5006\n",
      "Training: batch 37 begins at 22:18:55.909825\n",
      "\n",
      "Training: batch 37 ends at 22:18:56.707736\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5008 - bce_dice_loss: 0.5008\n",
      "Training: batch 38 begins at 22:18:56.712158\n",
      "\n",
      "Training: batch 38 ends at 22:18:57.511830\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.4995 - bce_dice_loss: 0.4995\n",
      "Training: batch 39 begins at 22:18:57.516819\n",
      "\n",
      "Training: batch 39 ends at 22:18:58.316243\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4983 - bce_dice_loss: 0.4983\n",
      "Training: batch 40 begins at 22:18:58.319905\n",
      "\n",
      "Training: batch 40 ends at 22:18:59.121692\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4978 - bce_dice_loss: 0.4978\n",
      "Training: batch 41 begins at 22:18:59.125113\n",
      "\n",
      "Training: batch 41 ends at 22:18:59.933933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/60 [====================>.........] - ETA: 14s - loss: 0.5017 - bce_dice_loss: 0.5017\n",
      "Training: batch 42 begins at 22:18:59.937600\n",
      "\n",
      "Training: batch 42 ends at 22:19:00.754607\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.5045 - bce_dice_loss: 0.5045\n",
      "Training: batch 43 begins at 22:19:00.758350\n",
      "\n",
      "Training: batch 43 ends at 22:19:01.575583\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.5023 - bce_dice_loss: 0.5023\n",
      "Training: batch 44 begins at 22:19:01.579990\n",
      "\n",
      "Training: batch 44 ends at 22:19:02.387637\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4992 - bce_dice_loss: 0.4992\n",
      "Training: batch 45 begins at 22:19:02.390849\n",
      "\n",
      "Training: batch 45 ends at 22:19:03.185314\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5006 - bce_dice_loss: 0.5006\n",
      "Training: batch 46 begins at 22:19:03.188771\n",
      "\n",
      "Training: batch 46 ends at 22:19:03.985403\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5038 - bce_dice_loss: 0.5038\n",
      "Training: batch 47 begins at 22:19:03.988388\n",
      "\n",
      "Training: batch 47 ends at 22:19:04.789360\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5092 - bce_dice_loss: 0.5092 \n",
      "Training: batch 48 begins at 22:19:04.793580\n",
      "\n",
      "Training: batch 48 ends at 22:19:05.601291\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.5100 - bce_dice_loss: 0.5100\n",
      "Training: batch 49 begins at 22:19:05.605207\n",
      "\n",
      "Training: batch 49 ends at 22:19:06.402941\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5129 - bce_dice_loss: 0.5129\n",
      "Training: batch 50 begins at 22:19:06.407048\n",
      "\n",
      "Training: batch 50 ends at 22:19:07.224094\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5093 - bce_dice_loss: 0.5093\n",
      "Training: batch 51 begins at 22:19:07.228297\n",
      "\n",
      "Training: batch 51 ends at 22:19:08.031714\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.5101 - bce_dice_loss: 0.5101\n",
      "Training: batch 52 begins at 22:19:08.035648\n",
      "\n",
      "Training: batch 52 ends at 22:19:08.837936\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.5121 - bce_dice_loss: 0.5121\n",
      "Training: batch 53 begins at 22:19:08.842408\n",
      "\n",
      "Training: batch 53 ends at 22:19:09.643467\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.5105 - bce_dice_loss: 0.5105\n",
      "Training: batch 54 begins at 22:19:09.648105\n",
      "\n",
      "Training: batch 54 ends at 22:19:10.455633\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.5115 - bce_dice_loss: 0.5115\n",
      "Training: batch 55 begins at 22:19:10.460208\n",
      "\n",
      "Training: batch 55 ends at 22:19:11.275196\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.5112 - bce_dice_loss: 0.5112\n",
      "Training: batch 56 begins at 22:19:11.278880\n",
      "\n",
      "Training: batch 56 ends at 22:19:12.097664\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.5153 - bce_dice_loss: 0.5153\n",
      "Training: batch 57 begins at 22:19:12.101516\n",
      "\n",
      "Training: batch 57 ends at 22:19:12.903721\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.5166 - bce_dice_loss: 0.5166\n",
      "Training: batch 58 begins at 22:19:12.907288\n",
      "\n",
      "Training: batch 58 ends at 22:19:13.708237\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5155 - bce_dice_loss: 0.5155\n",
      "Training: batch 59 begins at 22:19:13.713202\n",
      "\n",
      "Training: batch 59 ends at 22:19:14.512314\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5150 - bce_dice_loss: 0.5150\n",
      "Evaluating: batch 0 begins at 22:19:14.546396\n",
      "\n",
      "Evaluating: batch 0 ends at 22:19:14.834441\n",
      "\n",
      "Evaluating: batch 1 begins at 22:19:14.835584\n",
      "\n",
      "Evaluating: batch 1 ends at 22:19:15.062510\n",
      "\n",
      "Evaluating: batch 2 begins at 22:19:15.063720\n",
      "\n",
      "Evaluating: batch 2 ends at 22:19:15.293237\n",
      "\n",
      "Evaluating: batch 3 begins at 22:19:15.294374\n",
      "\n",
      "Evaluating: batch 3 ends at 22:19:15.519820\n",
      "\n",
      "Evaluating: batch 4 begins at 22:19:15.520805\n",
      "\n",
      "Evaluating: batch 4 ends at 22:19:15.747149\n",
      "\n",
      "Evaluating: batch 5 begins at 22:19:15.748316\n",
      "\n",
      "Evaluating: batch 5 ends at 22:19:15.975450\n",
      "\n",
      "Evaluating: batch 6 begins at 22:19:15.976570\n",
      "\n",
      "Evaluating: batch 6 ends at 22:19:16.203418\n",
      "\n",
      "Evaluating: batch 7 begins at 22:19:16.205077\n",
      "\n",
      "Evaluating: batch 7 ends at 22:19:16.431930\n",
      "\n",
      "Evaluating: batch 8 begins at 22:19:16.433051\n",
      "\n",
      "Evaluating: batch 8 ends at 22:19:16.660402\n",
      "\n",
      "Evaluating: batch 9 begins at 22:19:16.661618\n",
      "\n",
      "Evaluating: batch 9 ends at 22:19:16.893325\n",
      "\n",
      "Evaluating: batch 10 begins at 22:19:16.894479\n",
      "\n",
      "Evaluating: batch 10 ends at 22:19:17.123065\n",
      "\n",
      "Evaluating: batch 11 begins at 22:19:17.124203\n",
      "\n",
      "Evaluating: batch 11 ends at 22:19:17.352137\n",
      "\n",
      "Evaluating: batch 12 begins at 22:19:17.353243\n",
      "\n",
      "Evaluating: batch 12 ends at 22:19:17.580629\n",
      "\n",
      "Evaluating: batch 13 begins at 22:19:17.581855\n",
      "\n",
      "Evaluating: batch 13 ends at 22:19:17.805945\n",
      "\n",
      "Evaluating: batch 14 begins at 22:19:17.807226\n",
      "\n",
      "Evaluating: batch 14 ends at 22:19:18.034909\n",
      "\n",
      "Evaluating: batch 15 begins at 22:19:18.036111\n",
      "\n",
      "Evaluating: batch 15 ends at 22:19:18.261801\n",
      "\n",
      "Evaluating: batch 16 begins at 22:19:18.262926\n",
      "\n",
      "Evaluating: batch 16 ends at 22:19:18.491975\n",
      "\n",
      "Evaluating: batch 17 begins at 22:19:18.493087\n",
      "\n",
      "Evaluating: batch 17 ends at 22:19:18.722768\n",
      "\n",
      "Evaluating: batch 18 begins at 22:19:18.723959\n",
      "\n",
      "Evaluating: batch 18 ends at 22:19:18.952664\n",
      "\n",
      "Evaluating: batch 19 begins at 22:19:18.953796\n",
      "\n",
      "Evaluating: batch 19 ends at 22:19:19.179153\n",
      "\n",
      "Evaluating: batch 20 begins at 22:19:19.180261\n",
      "\n",
      "Evaluating: batch 20 ends at 22:19:19.405622\n",
      "\n",
      "Evaluating: batch 21 begins at 22:19:19.406774\n",
      "\n",
      "Evaluating: batch 21 ends at 22:19:19.632223\n",
      "\n",
      "Evaluating: batch 22 begins at 22:19:19.633333\n",
      "\n",
      "Evaluating: batch 22 ends at 22:19:19.858491\n",
      "\n",
      "Evaluating: batch 23 begins at 22:19:19.859627\n",
      "\n",
      "Evaluating: batch 23 ends at 22:19:20.085192\n",
      "\n",
      "Evaluating: batch 24 begins at 22:19:20.086331\n",
      "\n",
      "Evaluating: batch 24 ends at 22:19:20.311835\n",
      "\n",
      "Evaluating: batch 25 begins at 22:19:20.312983\n",
      "\n",
      "Evaluating: batch 25 ends at 22:19:20.545101\n",
      "\n",
      "Evaluating: batch 26 begins at 22:19:20.546416\n",
      "\n",
      "Evaluating: batch 26 ends at 22:19:20.771759\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48615\n",
      "60/60 [==============================] - 55s 917ms/step - loss: 0.5150 - bce_dice_loss: 0.5150 - val_loss: 0.5234 - val_bce_dice_loss: 0.5234\n",
      "Epoch 3/25\n",
      "\n",
      "Training: batch 0 begins at 22:19:20.790987\n",
      "\n",
      "Training: batch 0 ends at 22:19:21.593192\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.5012 - bce_dice_loss: 0.5012\n",
      "Training: batch 1 begins at 22:19:21.599723\n",
      "\n",
      "Training: batch 1 ends at 22:19:22.413652\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.5720 - bce_dice_loss: 0.5720\n",
      "Training: batch 2 begins at 22:19:22.416485\n",
      "\n",
      "Training: batch 2 ends at 22:19:23.258076\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.5940 - bce_dice_loss: 0.5940\n",
      "Training: batch 3 begins at 22:19:23.261530\n",
      "\n",
      "Training: batch 3 ends at 22:19:24.081842\n",
      " 4/60 [=>............................] - ETA: 46s - loss: 0.5691 - bce_dice_loss: 0.5691\n",
      "Training: batch 4 begins at 22:19:24.086050\n",
      "\n",
      "Training: batch 4 ends at 22:19:24.888517\n",
      " 5/60 [=>............................] - ETA: 45s - loss: 0.5997 - bce_dice_loss: 0.5997\n",
      "Training: batch 5 begins at 22:19:24.892872\n",
      "\n",
      "Training: batch 5 ends at 22:19:25.724158\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.5807 - bce_dice_loss: 0.5807\n",
      "Training: batch 6 begins at 22:19:25.728634\n",
      "\n",
      "Training: batch 6 ends at 22:19:26.532965\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.5541 - bce_dice_loss: 0.5541\n",
      "Training: batch 7 begins at 22:19:26.536393\n",
      "\n",
      "Training: batch 7 ends at 22:19:27.336496\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.5513 - bce_dice_loss: 0.5513\n",
      "Training: batch 8 begins at 22:19:27.340192\n",
      "\n",
      "Training: batch 8 ends at 22:19:28.141703\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5665 - bce_dice_loss: 0.5665\n",
      "Training: batch 9 begins at 22:19:28.146686\n",
      "\n",
      "Training: batch 9 ends at 22:19:28.957596\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5495 - bce_dice_loss: 0.5495\n",
      "Training: batch 10 begins at 22:19:28.962013\n",
      "\n",
      "Training: batch 10 ends at 22:19:29.763737\n",
      "11/60 [====>.........................] - ETA: 40s - loss: 0.5824 - bce_dice_loss: 0.5824\n",
      "Training: batch 11 begins at 22:19:29.767432\n",
      "\n",
      "Training: batch 11 ends at 22:19:30.568489\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.5731 - bce_dice_loss: 0.5731\n",
      "Training: batch 12 begins at 22:19:30.572191\n",
      "\n",
      "Training: batch 12 ends at 22:19:31.382235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/60 [=====>........................] - ETA: 38s - loss: 0.5794 - bce_dice_loss: 0.5794\n",
      "Training: batch 13 begins at 22:19:31.385405\n",
      "\n",
      "Training: batch 13 ends at 22:19:32.203196\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.5824 - bce_dice_loss: 0.5824\n",
      "Training: batch 14 begins at 22:19:32.206659\n",
      "\n",
      "Training: batch 14 ends at 22:19:33.011219\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.5803 - bce_dice_loss: 0.5803\n",
      "Training: batch 15 begins at 22:19:33.013890\n",
      "\n",
      "Training: batch 15 ends at 22:19:33.812534\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.5637 - bce_dice_loss: 0.5637\n",
      "Training: batch 16 begins at 22:19:33.816192\n",
      "\n",
      "Training: batch 16 ends at 22:19:34.619862\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.5686 - bce_dice_loss: 0.5686\n",
      "Training: batch 17 begins at 22:19:34.623907\n",
      "\n",
      "Training: batch 17 ends at 22:19:35.426023\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.5723 - bce_dice_loss: 0.5723\n",
      "Training: batch 18 begins at 22:19:35.430874\n",
      "\n",
      "Training: batch 18 ends at 22:19:36.249813\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.5618 - bce_dice_loss: 0.5618\n",
      "Training: batch 19 begins at 22:19:36.253251\n",
      "\n",
      "Training: batch 19 ends at 22:19:37.054041\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.5530 - bce_dice_loss: 0.5530\n",
      "Training: batch 20 begins at 22:19:37.058047\n",
      "\n",
      "Training: batch 20 ends at 22:19:37.866307\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.5441 - bce_dice_loss: 0.5441\n",
      "Training: batch 21 begins at 22:19:37.870002\n",
      "\n",
      "Training: batch 21 ends at 22:19:38.664601\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.5366 - bce_dice_loss: 0.5366\n",
      "Training: batch 22 begins at 22:19:38.669000\n",
      "\n",
      "Training: batch 22 ends at 22:19:39.477727\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.5332 - bce_dice_loss: 0.5332\n",
      "Training: batch 23 begins at 22:19:39.482566\n",
      "\n",
      "Training: batch 23 ends at 22:19:40.309957\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.5265 - bce_dice_loss: 0.5265\n",
      "Training: batch 24 begins at 22:19:40.315384\n",
      "\n",
      "Training: batch 24 ends at 22:19:41.128471\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.5188 - bce_dice_loss: 0.5188\n",
      "Training: batch 25 begins at 22:19:41.134154\n",
      "\n",
      "Training: batch 25 ends at 22:19:41.931853\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.5136 - bce_dice_loss: 0.5136\n",
      "Training: batch 26 begins at 22:19:41.935605\n",
      "\n",
      "Training: batch 26 ends at 22:19:42.735802\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.5103 - bce_dice_loss: 0.5103\n",
      "Training: batch 27 begins at 22:19:42.738439\n",
      "\n",
      "Training: batch 27 ends at 22:19:43.594745\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.5118 - bce_dice_loss: 0.5118\n",
      "Training: batch 28 begins at 22:19:43.598160\n",
      "\n",
      "Training: batch 28 ends at 22:19:44.407042\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.5110 - bce_dice_loss: 0.5110\n",
      "Training: batch 29 begins at 22:19:44.411845\n",
      "\n",
      "Training: batch 29 ends at 22:19:45.208052\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.5133 - bce_dice_loss: 0.5133\n",
      "Training: batch 30 begins at 22:19:45.213820\n",
      "\n",
      "Training: batch 30 ends at 22:19:46.011546\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.5190 - bce_dice_loss: 0.5190\n",
      "Training: batch 31 begins at 22:19:46.016357\n",
      "\n",
      "Training: batch 31 ends at 22:19:46.820451\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.5131 - bce_dice_loss: 0.5131\n",
      "Training: batch 32 begins at 22:19:46.824751\n",
      "\n",
      "Training: batch 32 ends at 22:19:47.638695\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.5082 - bce_dice_loss: 0.5082\n",
      "Training: batch 33 begins at 22:19:47.643155\n",
      "\n",
      "Training: batch 33 ends at 22:19:48.440251\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.5082 - bce_dice_loss: 0.5082\n",
      "Training: batch 34 begins at 22:19:48.444553\n",
      "\n",
      "Training: batch 34 ends at 22:19:49.279030\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.5071 - bce_dice_loss: 0.5071\n",
      "Training: batch 35 begins at 22:19:49.283276\n",
      "\n",
      "Training: batch 35 ends at 22:19:50.077309\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.5065 - bce_dice_loss: 0.5065\n",
      "Training: batch 36 begins at 22:19:50.082176\n",
      "\n",
      "Training: batch 36 ends at 22:19:50.898725\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.5067 - bce_dice_loss: 0.5067\n",
      "Training: batch 37 begins at 22:19:50.901957\n",
      "\n",
      "Training: batch 37 ends at 22:19:51.703545\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.5023 - bce_dice_loss: 0.5023\n",
      "Training: batch 38 begins at 22:19:51.706971\n",
      "\n",
      "Training: batch 38 ends at 22:19:52.506415\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.4993 - bce_dice_loss: 0.4993\n",
      "Training: batch 39 begins at 22:19:52.510777\n",
      "\n",
      "Training: batch 39 ends at 22:19:53.346954\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.5008 - bce_dice_loss: 0.5008\n",
      "Training: batch 40 begins at 22:19:53.350549\n",
      "\n",
      "Training: batch 40 ends at 22:19:54.146494\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4990 - bce_dice_loss: 0.4990\n",
      "Training: batch 41 begins at 22:19:54.150810\n",
      "\n",
      "Training: batch 41 ends at 22:19:54.958321\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4968 - bce_dice_loss: 0.4968\n",
      "Training: batch 42 begins at 22:19:54.962761\n",
      "\n",
      "Training: batch 42 ends at 22:19:55.767997\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4967 - bce_dice_loss: 0.4967\n",
      "Training: batch 43 begins at 22:19:55.771198\n",
      "\n",
      "Training: batch 43 ends at 22:19:56.575613\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.4982 - bce_dice_loss: 0.4982\n",
      "Training: batch 44 begins at 22:19:56.580426\n",
      "\n",
      "Training: batch 44 ends at 22:19:57.406314\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4980 - bce_dice_loss: 0.4980\n",
      "Training: batch 45 begins at 22:19:57.410461\n",
      "\n",
      "Training: batch 45 ends at 22:19:58.235688\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.5039 - bce_dice_loss: 0.5039\n",
      "Training: batch 46 begins at 22:19:58.238824\n",
      "\n",
      "Training: batch 46 ends at 22:19:59.037256\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.5044 - bce_dice_loss: 0.5044\n",
      "Training: batch 47 begins at 22:19:59.040838\n",
      "\n",
      "Training: batch 47 ends at 22:19:59.839235\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.5014 - bce_dice_loss: 0.5014 \n",
      "Training: batch 48 begins at 22:19:59.841691\n",
      "\n",
      "Training: batch 48 ends at 22:20:00.649270\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4993 - bce_dice_loss: 0.4993\n",
      "Training: batch 49 begins at 22:20:00.652815\n",
      "\n",
      "Training: batch 49 ends at 22:20:01.470226\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.5023 - bce_dice_loss: 0.5023\n",
      "Training: batch 50 begins at 22:20:01.473977\n",
      "\n",
      "Training: batch 50 ends at 22:20:02.281461\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.5004 - bce_dice_loss: 0.5004\n",
      "Training: batch 51 begins at 22:20:02.285925\n",
      "\n",
      "Training: batch 51 ends at 22:20:03.088940\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4979 - bce_dice_loss: 0.4979\n",
      "Training: batch 52 begins at 22:20:03.093145\n",
      "\n",
      "Training: batch 52 ends at 22:20:03.914661\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4956 - bce_dice_loss: 0.4956\n",
      "Training: batch 53 begins at 22:20:03.918098\n",
      "\n",
      "Training: batch 53 ends at 22:20:04.714196\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4972 - bce_dice_loss: 0.4972\n",
      "Training: batch 54 begins at 22:20:04.719014\n",
      "\n",
      "Training: batch 54 ends at 22:20:05.533730\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4935 - bce_dice_loss: 0.4935\n",
      "Training: batch 55 begins at 22:20:05.538416\n",
      "\n",
      "Training: batch 55 ends at 22:20:06.338905\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4926 - bce_dice_loss: 0.4926\n",
      "Training: batch 56 begins at 22:20:06.343047\n",
      "\n",
      "Training: batch 56 ends at 22:20:07.151578\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4924 - bce_dice_loss: 0.4924\n",
      "Training: batch 57 begins at 22:20:07.155113\n",
      "\n",
      "Training: batch 57 ends at 22:20:07.965423\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4873 - bce_dice_loss: 0.4873\n",
      "Training: batch 58 begins at 22:20:07.970201\n",
      "\n",
      "Training: batch 58 ends at 22:20:08.766799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.4885 - bce_dice_loss: 0.4885\n",
      "Training: batch 59 begins at 22:20:08.769657\n",
      "\n",
      "Training: batch 59 ends at 22:20:09.585073\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4869 - bce_dice_loss: 0.4869\n",
      "Evaluating: batch 0 begins at 22:20:09.616824\n",
      "\n",
      "Evaluating: batch 0 ends at 22:20:09.897230\n",
      "\n",
      "Evaluating: batch 1 begins at 22:20:09.898369\n",
      "\n",
      "Evaluating: batch 1 ends at 22:20:10.121778\n",
      "\n",
      "Evaluating: batch 2 begins at 22:20:10.122874\n",
      "\n",
      "Evaluating: batch 2 ends at 22:20:10.348998\n",
      "\n",
      "Evaluating: batch 3 begins at 22:20:10.350228\n",
      "\n",
      "Evaluating: batch 3 ends at 22:20:10.579694\n",
      "\n",
      "Evaluating: batch 4 begins at 22:20:10.580875\n",
      "\n",
      "Evaluating: batch 4 ends at 22:20:10.818278\n",
      "\n",
      "Evaluating: batch 5 begins at 22:20:10.819502\n",
      "\n",
      "Evaluating: batch 5 ends at 22:20:11.046944\n",
      "\n",
      "Evaluating: batch 6 begins at 22:20:11.048110\n",
      "\n",
      "Evaluating: batch 6 ends at 22:20:11.274913\n",
      "\n",
      "Evaluating: batch 7 begins at 22:20:11.276078\n",
      "\n",
      "Evaluating: batch 7 ends at 22:20:11.504921\n",
      "\n",
      "Evaluating: batch 8 begins at 22:20:11.506366\n",
      "\n",
      "Evaluating: batch 8 ends at 22:20:11.731042\n",
      "\n",
      "Evaluating: batch 9 begins at 22:20:11.732224\n",
      "\n",
      "Evaluating: batch 9 ends at 22:20:11.956564\n",
      "\n",
      "Evaluating: batch 10 begins at 22:20:11.957678\n",
      "\n",
      "Evaluating: batch 10 ends at 22:20:12.186000\n",
      "\n",
      "Evaluating: batch 11 begins at 22:20:12.187114\n",
      "\n",
      "Evaluating: batch 11 ends at 22:20:12.414282\n",
      "\n",
      "Evaluating: batch 12 begins at 22:20:12.415439\n",
      "\n",
      "Evaluating: batch 12 ends at 22:20:12.643356\n",
      "\n",
      "Evaluating: batch 13 begins at 22:20:12.644508\n",
      "\n",
      "Evaluating: batch 13 ends at 22:20:12.873217\n",
      "\n",
      "Evaluating: batch 14 begins at 22:20:12.874336\n",
      "\n",
      "Evaluating: batch 14 ends at 22:20:13.098296\n",
      "\n",
      "Evaluating: batch 15 begins at 22:20:13.099408\n",
      "\n",
      "Evaluating: batch 15 ends at 22:20:13.325835\n",
      "\n",
      "Evaluating: batch 16 begins at 22:20:13.326967\n",
      "\n",
      "Evaluating: batch 16 ends at 22:20:13.552645\n",
      "\n",
      "Evaluating: batch 17 begins at 22:20:13.553827\n",
      "\n",
      "Evaluating: batch 17 ends at 22:20:13.780410\n",
      "\n",
      "Evaluating: batch 18 begins at 22:20:13.781589\n",
      "\n",
      "Evaluating: batch 18 ends at 22:20:14.009807\n",
      "\n",
      "Evaluating: batch 19 begins at 22:20:14.010968\n",
      "\n",
      "Evaluating: batch 19 ends at 22:20:14.238268\n",
      "\n",
      "Evaluating: batch 20 begins at 22:20:14.239409\n",
      "\n",
      "Evaluating: batch 20 ends at 22:20:14.465932\n",
      "\n",
      "Evaluating: batch 21 begins at 22:20:14.467050\n",
      "\n",
      "Evaluating: batch 21 ends at 22:20:14.692163\n",
      "\n",
      "Evaluating: batch 22 begins at 22:20:14.693323\n",
      "\n",
      "Evaluating: batch 22 ends at 22:20:14.917657\n",
      "\n",
      "Evaluating: batch 23 begins at 22:20:14.918934\n",
      "\n",
      "Evaluating: batch 23 ends at 22:20:15.143751\n",
      "\n",
      "Evaluating: batch 24 begins at 22:20:15.144990\n",
      "\n",
      "Evaluating: batch 24 ends at 22:20:15.370793\n",
      "\n",
      "Evaluating: batch 25 begins at 22:20:15.371932\n",
      "\n",
      "Evaluating: batch 25 ends at 22:20:15.593443\n",
      "\n",
      "Evaluating: batch 26 begins at 22:20:15.594562\n",
      "\n",
      "Evaluating: batch 26 ends at 22:20:15.821136\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48615 to 0.44481, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 57s 947ms/step - loss: 0.4869 - bce_dice_loss: 0.4869 - val_loss: 0.4448 - val_bce_dice_loss: 0.4448\n",
      "Epoch 4/25\n",
      "\n",
      "Training: batch 0 begins at 22:20:17.564359\n",
      "\n",
      "Training: batch 0 ends at 22:20:18.409018\n",
      " 1/60 [..............................] - ETA: 49s - loss: 0.5658 - bce_dice_loss: 0.5658\n",
      "Training: batch 1 begins at 22:20:18.414806\n",
      "\n",
      "Training: batch 1 ends at 22:20:19.214523\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5043 - bce_dice_loss: 0.5043\n",
      "Training: batch 2 begins at 22:20:19.217938\n",
      "\n",
      "Training: batch 2 ends at 22:20:20.012257\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.4508 - bce_dice_loss: 0.4508\n",
      "Training: batch 3 begins at 22:20:20.017033\n",
      "\n",
      "Training: batch 3 ends at 22:20:20.840829\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.4341 - bce_dice_loss: 0.4341\n",
      "Training: batch 4 begins at 22:20:20.847068\n",
      "\n",
      "Training: batch 4 ends at 22:20:21.641871\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4797 - bce_dice_loss: 0.4797\n",
      "Training: batch 5 begins at 22:20:21.646131\n",
      "\n",
      "Training: batch 5 ends at 22:20:22.452876\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.5146 - bce_dice_loss: 0.5146\n",
      "Training: batch 6 begins at 22:20:22.457117\n",
      "\n",
      "Training: batch 6 ends at 22:20:23.248495\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.5415 - bce_dice_loss: 0.5415\n",
      "Training: batch 7 begins at 22:20:23.251929\n",
      "\n",
      "Training: batch 7 ends at 22:20:24.047934\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.5306 - bce_dice_loss: 0.5306\n",
      "Training: batch 8 begins at 22:20:24.051334\n",
      "\n",
      "Training: batch 8 ends at 22:20:24.840909\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.5046 - bce_dice_loss: 0.5046\n",
      "Training: batch 9 begins at 22:20:24.845497\n",
      "\n",
      "Training: batch 9 ends at 22:20:25.634198\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.5163 - bce_dice_loss: 0.5163\n",
      "Training: batch 10 begins at 22:20:25.637667\n",
      "\n",
      "Training: batch 10 ends at 22:20:26.441801\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.5231 - bce_dice_loss: 0.5231\n",
      "Training: batch 11 begins at 22:20:26.446727\n",
      "\n",
      "Training: batch 11 ends at 22:20:27.239253\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.5244 - bce_dice_loss: 0.5244\n",
      "Training: batch 12 begins at 22:20:27.243820\n",
      "\n",
      "Training: batch 12 ends at 22:20:28.038357\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.5142 - bce_dice_loss: 0.5142\n",
      "Training: batch 13 begins at 22:20:28.042523\n",
      "\n",
      "Training: batch 13 ends at 22:20:28.853106\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.5007 - bce_dice_loss: 0.5007\n",
      "Training: batch 14 begins at 22:20:28.856490\n",
      "\n",
      "Training: batch 14 ends at 22:20:29.649127\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4896 - bce_dice_loss: 0.4896\n",
      "Training: batch 15 begins at 22:20:29.652541\n",
      "\n",
      "Training: batch 15 ends at 22:20:30.477445\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4910 - bce_dice_loss: 0.4910\n",
      "Training: batch 16 begins at 22:20:30.481873\n",
      "\n",
      "Training: batch 16 ends at 22:20:31.279896\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4848 - bce_dice_loss: 0.4848\n",
      "Training: batch 17 begins at 22:20:31.283476\n",
      "\n",
      "Training: batch 17 ends at 22:20:32.083026\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.4745 - bce_dice_loss: 0.4745\n",
      "Training: batch 18 begins at 22:20:32.086488\n",
      "\n",
      "Training: batch 18 ends at 22:20:32.908047\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.4741 - bce_dice_loss: 0.4741\n",
      "Training: batch 19 begins at 22:20:32.913024\n",
      "\n",
      "Training: batch 19 ends at 22:20:33.705280\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4718 - bce_dice_loss: 0.4718\n",
      "Training: batch 20 begins at 22:20:33.708994\n",
      "\n",
      "Training: batch 20 ends at 22:20:34.497254\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4644 - bce_dice_loss: 0.4644\n",
      "Training: batch 21 begins at 22:20:34.501688\n",
      "\n",
      "Training: batch 21 ends at 22:20:35.377376\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4632 - bce_dice_loss: 0.4632\n",
      "Training: batch 22 begins at 22:20:35.379542\n",
      "\n",
      "Training: batch 22 ends at 22:20:36.206418\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4627 - bce_dice_loss: 0.4627\n",
      "Training: batch 23 begins at 22:20:36.208719\n",
      "\n",
      "Training: batch 23 ends at 22:20:37.009150\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4638 - bce_dice_loss: 0.4638\n",
      "Training: batch 24 begins at 22:20:37.011949\n",
      "\n",
      "Training: batch 24 ends at 22:20:37.804495\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4664 - bce_dice_loss: 0.4664\n",
      "Training: batch 25 begins at 22:20:37.807831\n",
      "\n",
      "Training: batch 25 ends at 22:20:38.599783\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4630 - bce_dice_loss: 0.4630\n",
      "Training: batch 26 begins at 22:20:38.603497\n",
      "\n",
      "Training: batch 26 ends at 22:20:39.396226\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4610 - bce_dice_loss: 0.4610\n",
      "Training: batch 27 begins at 22:20:39.402442\n",
      "\n",
      "Training: batch 27 ends at 22:20:40.191842\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4612 - bce_dice_loss: 0.4612\n",
      "Training: batch 28 begins at 22:20:40.195818\n",
      "\n",
      "Training: batch 28 ends at 22:20:40.989773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/60 [=============>................] - ETA: 24s - loss: 0.4648 - bce_dice_loss: 0.4648\n",
      "Training: batch 29 begins at 22:20:40.993643\n",
      "\n",
      "Training: batch 29 ends at 22:20:41.791255\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4603 - bce_dice_loss: 0.4603\n",
      "Training: batch 30 begins at 22:20:41.796152\n",
      "\n",
      "Training: batch 30 ends at 22:20:42.584087\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4562 - bce_dice_loss: 0.4562\n",
      "Training: batch 31 begins at 22:20:42.587724\n",
      "\n",
      "Training: batch 31 ends at 22:20:43.383188\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4554 - bce_dice_loss: 0.4554\n",
      "Training: batch 32 begins at 22:20:43.386726\n",
      "\n",
      "Training: batch 32 ends at 22:20:44.183669\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4564 - bce_dice_loss: 0.4564\n",
      "Training: batch 33 begins at 22:20:44.187943\n",
      "\n",
      "Training: batch 33 ends at 22:20:44.981695\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4547 - bce_dice_loss: 0.4547\n",
      "Training: batch 34 begins at 22:20:44.985124\n",
      "\n",
      "Training: batch 34 ends at 22:20:45.771656\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4567 - bce_dice_loss: 0.4567\n",
      "Training: batch 35 begins at 22:20:45.774342\n",
      "\n",
      "Training: batch 35 ends at 22:20:46.565790\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4661 - bce_dice_loss: 0.4661\n",
      "Training: batch 36 begins at 22:20:46.570018\n",
      "\n",
      "Training: batch 36 ends at 22:20:47.363052\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4630 - bce_dice_loss: 0.4630\n",
      "Training: batch 37 begins at 22:20:47.366744\n",
      "\n",
      "Training: batch 37 ends at 22:20:48.159234\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4647 - bce_dice_loss: 0.4647\n",
      "Training: batch 38 begins at 22:20:48.163758\n",
      "\n",
      "Training: batch 38 ends at 22:20:48.951390\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4628 - bce_dice_loss: 0.4628\n",
      "Training: batch 39 begins at 22:20:48.954531\n",
      "\n",
      "Training: batch 39 ends at 22:20:49.744138\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4608 - bce_dice_loss: 0.4608\n",
      "Training: batch 40 begins at 22:20:49.747786\n",
      "\n",
      "Training: batch 40 ends at 22:20:50.536446\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4623 - bce_dice_loss: 0.4623\n",
      "Training: batch 41 begins at 22:20:50.540166\n",
      "\n",
      "Training: batch 41 ends at 22:20:51.361564\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4648 - bce_dice_loss: 0.4648\n",
      "Training: batch 42 begins at 22:20:51.365037\n",
      "\n",
      "Training: batch 42 ends at 22:20:52.158328\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4655 - bce_dice_loss: 0.4655\n",
      "Training: batch 43 begins at 22:20:52.161822\n",
      "\n",
      "Training: batch 43 ends at 22:20:52.954297\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4681 - bce_dice_loss: 0.4681\n",
      "Training: batch 44 begins at 22:20:52.959166\n",
      "\n",
      "Training: batch 44 ends at 22:20:53.734633\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4687 - bce_dice_loss: 0.4687\n",
      "Training: batch 45 begins at 22:20:53.738408\n",
      "\n",
      "Training: batch 45 ends at 22:20:54.552918\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4674 - bce_dice_loss: 0.4674\n",
      "Training: batch 46 begins at 22:20:54.558150\n",
      "\n",
      "Training: batch 46 ends at 22:20:55.336418\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4669 - bce_dice_loss: 0.4669\n",
      "Training: batch 47 begins at 22:20:55.343554\n",
      "\n",
      "Training: batch 47 ends at 22:20:56.142327\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4666 - bce_dice_loss: 0.4666 \n",
      "Training: batch 48 begins at 22:20:56.145896\n",
      "\n",
      "Training: batch 48 ends at 22:20:56.933321\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4660 - bce_dice_loss: 0.4660\n",
      "Training: batch 49 begins at 22:20:56.937797\n",
      "\n",
      "Training: batch 49 ends at 22:20:57.724873\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4640 - bce_dice_loss: 0.4640\n",
      "Training: batch 50 begins at 22:20:57.729228\n",
      "\n",
      "Training: batch 50 ends at 22:20:58.524239\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4650 - bce_dice_loss: 0.4650\n",
      "Training: batch 51 begins at 22:20:58.528017\n",
      "\n",
      "Training: batch 51 ends at 22:20:59.323458\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4629 - bce_dice_loss: 0.4629\n",
      "Training: batch 52 begins at 22:20:59.327487\n",
      "\n",
      "Training: batch 52 ends at 22:21:00.108671\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4603 - bce_dice_loss: 0.4603\n",
      "Training: batch 53 begins at 22:21:00.112208\n",
      "\n",
      "Training: batch 53 ends at 22:21:00.921629\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4634 - bce_dice_loss: 0.4634\n",
      "Training: batch 54 begins at 22:21:00.925819\n",
      "\n",
      "Training: batch 54 ends at 22:21:01.719569\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4693 - bce_dice_loss: 0.4693\n",
      "Training: batch 55 begins at 22:21:01.726954\n",
      "\n",
      "Training: batch 55 ends at 22:21:02.534841\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4693 - bce_dice_loss: 0.4693\n",
      "Training: batch 56 begins at 22:21:02.539596\n",
      "\n",
      "Training: batch 56 ends at 22:21:03.338840\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4679 - bce_dice_loss: 0.4679\n",
      "Training: batch 57 begins at 22:21:03.343558\n",
      "\n",
      "Training: batch 57 ends at 22:21:04.119767\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4677 - bce_dice_loss: 0.4677\n",
      "Training: batch 58 begins at 22:21:04.124357\n",
      "\n",
      "Training: batch 58 ends at 22:21:04.942548\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4680 - bce_dice_loss: 0.4680\n",
      "Training: batch 59 begins at 22:21:04.946016\n",
      "\n",
      "Training: batch 59 ends at 22:21:05.728771\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4686 - bce_dice_loss: 0.4686\n",
      "Evaluating: batch 0 begins at 22:21:05.759744\n",
      "\n",
      "Evaluating: batch 0 ends at 22:21:06.033421\n",
      "\n",
      "Evaluating: batch 1 begins at 22:21:06.034610\n",
      "\n",
      "Evaluating: batch 1 ends at 22:21:06.252181\n",
      "\n",
      "Evaluating: batch 2 begins at 22:21:06.253622\n",
      "\n",
      "Evaluating: batch 2 ends at 22:21:06.471676\n",
      "\n",
      "Evaluating: batch 3 begins at 22:21:06.473544\n",
      "\n",
      "Evaluating: batch 3 ends at 22:21:06.686183\n",
      "\n",
      "Evaluating: batch 4 begins at 22:21:06.687394\n",
      "\n",
      "Evaluating: batch 4 ends at 22:21:06.907590\n",
      "\n",
      "Evaluating: batch 5 begins at 22:21:06.908862\n",
      "\n",
      "Evaluating: batch 5 ends at 22:21:07.127551\n",
      "\n",
      "Evaluating: batch 6 begins at 22:21:07.129591\n",
      "\n",
      "Evaluating: batch 6 ends at 22:21:07.350411\n",
      "\n",
      "Evaluating: batch 7 begins at 22:21:07.352668\n",
      "\n",
      "Evaluating: batch 7 ends at 22:21:07.569138\n",
      "\n",
      "Evaluating: batch 8 begins at 22:21:07.570341\n",
      "\n",
      "Evaluating: batch 8 ends at 22:21:07.789365\n",
      "\n",
      "Evaluating: batch 9 begins at 22:21:07.790873\n",
      "\n",
      "Evaluating: batch 9 ends at 22:21:08.009824\n",
      "\n",
      "Evaluating: batch 10 begins at 22:21:08.012276\n",
      "\n",
      "Evaluating: batch 10 ends at 22:21:08.235602\n",
      "\n",
      "Evaluating: batch 11 begins at 22:21:08.237124\n",
      "\n",
      "Evaluating: batch 11 ends at 22:21:08.454482\n",
      "\n",
      "Evaluating: batch 12 begins at 22:21:08.456981\n",
      "\n",
      "Evaluating: batch 12 ends at 22:21:08.674715\n",
      "\n",
      "Evaluating: batch 13 begins at 22:21:08.676412\n",
      "\n",
      "Evaluating: batch 13 ends at 22:21:08.893600\n",
      "\n",
      "Evaluating: batch 14 begins at 22:21:08.894802\n",
      "\n",
      "Evaluating: batch 14 ends at 22:21:09.112274\n",
      "\n",
      "Evaluating: batch 15 begins at 22:21:09.114678\n",
      "\n",
      "Evaluating: batch 15 ends at 22:21:09.334160\n",
      "\n",
      "Evaluating: batch 16 begins at 22:21:09.336155\n",
      "\n",
      "Evaluating: batch 16 ends at 22:21:09.555725\n",
      "\n",
      "Evaluating: batch 17 begins at 22:21:09.558424\n",
      "\n",
      "Evaluating: batch 17 ends at 22:21:09.775400\n",
      "\n",
      "Evaluating: batch 18 begins at 22:21:09.777580\n",
      "\n",
      "Evaluating: batch 18 ends at 22:21:10.000422\n",
      "\n",
      "Evaluating: batch 19 begins at 22:21:10.002057\n",
      "\n",
      "Evaluating: batch 19 ends at 22:21:10.219992\n",
      "\n",
      "Evaluating: batch 20 begins at 22:21:10.221615\n",
      "\n",
      "Evaluating: batch 20 ends at 22:21:10.438018\n",
      "\n",
      "Evaluating: batch 21 begins at 22:21:10.439846\n",
      "\n",
      "Evaluating: batch 21 ends at 22:21:10.655715\n",
      "\n",
      "Evaluating: batch 22 begins at 22:21:10.657739\n",
      "\n",
      "Evaluating: batch 22 ends at 22:21:10.884274\n",
      "\n",
      "Evaluating: batch 23 begins at 22:21:10.885780\n",
      "\n",
      "Evaluating: batch 23 ends at 22:21:11.101486\n",
      "\n",
      "Evaluating: batch 24 begins at 22:21:11.102600\n",
      "\n",
      "Evaluating: batch 24 ends at 22:21:11.317903\n",
      "\n",
      "Evaluating: batch 25 begins at 22:21:11.319180\n",
      "\n",
      "Evaluating: batch 25 ends at 22:21:11.536605\n",
      "\n",
      "Evaluating: batch 26 begins at 22:21:11.537950\n",
      "\n",
      "Evaluating: batch 26 ends at 22:21:11.757905\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44481 to 0.44273, saving model to ./keras.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 932ms/step - loss: 0.4686 - bce_dice_loss: 0.4686 - val_loss: 0.4427 - val_bce_dice_loss: 0.4427\n",
      "Epoch 5/25\n",
      "\n",
      "Training: batch 0 begins at 22:21:13.416991\n",
      "\n",
      "Training: batch 0 ends at 22:21:14.253912\n",
      " 1/60 [..............................] - ETA: 49s - loss: 0.4099 - bce_dice_loss: 0.4099\n",
      "Training: batch 1 begins at 22:21:14.258714\n",
      "\n",
      "Training: batch 1 ends at 22:21:15.068052\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4245 - bce_dice_loss: 0.4245\n",
      "Training: batch 2 begins at 22:21:15.074393\n",
      "\n",
      "Training: batch 2 ends at 22:21:15.875334\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4488 - bce_dice_loss: 0.4488\n",
      "Training: batch 3 begins at 22:21:15.879211\n",
      "\n",
      "Training: batch 3 ends at 22:21:16.669095\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.4561 - bce_dice_loss: 0.4561\n",
      "Training: batch 4 begins at 22:21:16.673600\n",
      "\n",
      "Training: batch 4 ends at 22:21:17.461834\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4608 - bce_dice_loss: 0.4608\n",
      "Training: batch 5 begins at 22:21:17.465933\n",
      "\n",
      "Training: batch 5 ends at 22:21:18.260303\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4751 - bce_dice_loss: 0.4751\n",
      "Training: batch 6 begins at 22:21:18.264655\n",
      "\n",
      "Training: batch 6 ends at 22:21:19.059005\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4509 - bce_dice_loss: 0.4509\n",
      "Training: batch 7 begins at 22:21:19.062624\n",
      "\n",
      "Training: batch 7 ends at 22:21:19.850549\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4366 - bce_dice_loss: 0.4366\n",
      "Training: batch 8 begins at 22:21:19.854992\n",
      "\n",
      "Training: batch 8 ends at 22:21:20.644545\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.4551 - bce_dice_loss: 0.4551\n",
      "Training: batch 9 begins at 22:21:20.648942\n",
      "\n",
      "Training: batch 9 ends at 22:21:21.457305\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4589 - bce_dice_loss: 0.4589\n",
      "Training: batch 10 begins at 22:21:21.460692\n",
      "\n",
      "Training: batch 10 ends at 22:21:22.273815\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4611 - bce_dice_loss: 0.4611\n",
      "Training: batch 11 begins at 22:21:22.277071\n",
      "\n",
      "Training: batch 11 ends at 22:21:23.075387\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4643 - bce_dice_loss: 0.4643\n",
      "Training: batch 12 begins at 22:21:23.080482\n",
      "\n",
      "Training: batch 12 ends at 22:21:23.862391\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.4502 - bce_dice_loss: 0.4502\n",
      "Training: batch 13 begins at 22:21:23.867060\n",
      "\n",
      "Training: batch 13 ends at 22:21:24.655572\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.4508 - bce_dice_loss: 0.4508\n",
      "Training: batch 14 begins at 22:21:24.658865\n",
      "\n",
      "Training: batch 14 ends at 22:21:25.451202\n",
      "15/60 [======>.......................] - ETA: 35s - loss: 0.4479 - bce_dice_loss: 0.4479\n",
      "Training: batch 15 begins at 22:21:25.455976\n",
      "\n",
      "Training: batch 15 ends at 22:21:26.274528\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4475 - bce_dice_loss: 0.4475\n",
      "Training: batch 16 begins at 22:21:26.277785\n",
      "\n",
      "Training: batch 16 ends at 22:21:27.071223\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4738 - bce_dice_loss: 0.4738\n",
      "Training: batch 17 begins at 22:21:27.075504\n",
      "\n",
      "Training: batch 17 ends at 22:21:27.865007\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.4728 - bce_dice_loss: 0.4728\n",
      "Training: batch 18 begins at 22:21:27.869789\n",
      "\n",
      "Training: batch 18 ends at 22:21:28.656455\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.4678 - bce_dice_loss: 0.4678\n",
      "Training: batch 19 begins at 22:21:28.660698\n",
      "\n",
      "Training: batch 19 ends at 22:21:29.475014\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4687 - bce_dice_loss: 0.4687\n",
      "Training: batch 20 begins at 22:21:29.478795\n",
      "\n",
      "Training: batch 20 ends at 22:21:30.271184\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4760 - bce_dice_loss: 0.4760\n",
      "Training: batch 21 begins at 22:21:30.274844\n",
      "\n",
      "Training: batch 21 ends at 22:21:31.071943\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4753 - bce_dice_loss: 0.4753\n",
      "Training: batch 22 begins at 22:21:31.075545\n",
      "\n",
      "Training: batch 22 ends at 22:21:31.871710\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4821 - bce_dice_loss: 0.4821\n",
      "Training: batch 23 begins at 22:21:31.875372\n",
      "\n",
      "Training: batch 23 ends at 22:21:32.663797\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4832 - bce_dice_loss: 0.4832\n",
      "Training: batch 24 begins at 22:21:32.668002\n",
      "\n",
      "Training: batch 24 ends at 22:21:33.460872\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4821 - bce_dice_loss: 0.4821\n",
      "Training: batch 25 begins at 22:21:33.465221\n",
      "\n",
      "Training: batch 25 ends at 22:21:34.261209\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4931 - bce_dice_loss: 0.4931\n",
      "Training: batch 26 begins at 22:21:34.264858\n",
      "\n",
      "Training: batch 26 ends at 22:21:35.056840\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4804 - bce_dice_loss: 0.4804\n",
      "Training: batch 27 begins at 22:21:35.060329\n",
      "\n",
      "Training: batch 27 ends at 22:21:35.848721\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4749 - bce_dice_loss: 0.4749\n",
      "Training: batch 28 begins at 22:21:35.853502\n",
      "\n",
      "Training: batch 28 ends at 22:21:36.647630\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4716 - bce_dice_loss: 0.4716\n",
      "Training: batch 29 begins at 22:21:36.652141\n",
      "\n",
      "Training: batch 29 ends at 22:21:37.448273\n",
      "30/60 [==============>...............] - ETA: 23s - loss: 0.4689 - bce_dice_loss: 0.4689\n",
      "Training: batch 30 begins at 22:21:37.452927\n",
      "\n",
      "Training: batch 30 ends at 22:21:38.256914\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4743 - bce_dice_loss: 0.4743\n",
      "Training: batch 31 begins at 22:21:38.261380\n",
      "\n",
      "Training: batch 31 ends at 22:21:39.053816\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4722 - bce_dice_loss: 0.4722\n",
      "Training: batch 32 begins at 22:21:39.057677\n",
      "\n",
      "Training: batch 32 ends at 22:21:39.844814\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4683 - bce_dice_loss: 0.4683\n",
      "Training: batch 33 begins at 22:21:39.849251\n",
      "\n",
      "Training: batch 33 ends at 22:21:40.664320\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4749 - bce_dice_loss: 0.4749\n",
      "Training: batch 34 begins at 22:21:40.671862\n",
      "\n",
      "Training: batch 34 ends at 22:21:41.472789\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4722 - bce_dice_loss: 0.4722\n",
      "Training: batch 35 begins at 22:21:41.476135\n",
      "\n",
      "Training: batch 35 ends at 22:21:42.272970\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4680 - bce_dice_loss: 0.4680\n",
      "Training: batch 36 begins at 22:21:42.277483\n",
      "\n",
      "Training: batch 36 ends at 22:21:43.069653\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4691 - bce_dice_loss: 0.4691\n",
      "Training: batch 37 begins at 22:21:43.074016\n",
      "\n",
      "Training: batch 37 ends at 22:21:43.869708\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4727 - bce_dice_loss: 0.4727\n",
      "Training: batch 38 begins at 22:21:43.874125\n",
      "\n",
      "Training: batch 38 ends at 22:21:44.667389\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4697 - bce_dice_loss: 0.4697\n",
      "Training: batch 39 begins at 22:21:44.671883\n",
      "\n",
      "Training: batch 39 ends at 22:21:45.462022\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4667 - bce_dice_loss: 0.4667\n",
      "Training: batch 40 begins at 22:21:45.466540\n",
      "\n",
      "Training: batch 40 ends at 22:21:46.258049\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4648 - bce_dice_loss: 0.4648\n",
      "Training: batch 41 begins at 22:21:46.262349\n",
      "\n",
      "Training: batch 41 ends at 22:21:47.059754\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4632 - bce_dice_loss: 0.4632\n",
      "Training: batch 42 begins at 22:21:47.064057\n",
      "\n",
      "Training: batch 42 ends at 22:21:47.883368\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4644 - bce_dice_loss: 0.4644\n",
      "Training: batch 43 begins at 22:21:47.888457\n",
      "\n",
      "Training: batch 43 ends at 22:21:48.677895\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4639 - bce_dice_loss: 0.4639\n",
      "Training: batch 44 begins at 22:21:48.682221\n",
      "\n",
      "Training: batch 44 ends at 22:21:49.495539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4646 - bce_dice_loss: 0.4646\n",
      "Training: batch 45 begins at 22:21:49.499792\n",
      "\n",
      "Training: batch 45 ends at 22:21:50.290498\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4641 - bce_dice_loss: 0.4641\n",
      "Training: batch 46 begins at 22:21:50.294111\n",
      "\n",
      "Training: batch 46 ends at 22:21:51.088566\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4624 - bce_dice_loss: 0.4624\n",
      "Training: batch 47 begins at 22:21:51.092097\n",
      "\n",
      "Training: batch 47 ends at 22:21:51.886805\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4612 - bce_dice_loss: 0.4612 \n",
      "Training: batch 48 begins at 22:21:51.890967\n",
      "\n",
      "Training: batch 48 ends at 22:21:52.712838\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4583 - bce_dice_loss: 0.4583\n",
      "Training: batch 49 begins at 22:21:52.717585\n",
      "\n",
      "Training: batch 49 ends at 22:21:53.508276\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4566 - bce_dice_loss: 0.4566\n",
      "Training: batch 50 begins at 22:21:53.511662\n",
      "\n",
      "Training: batch 50 ends at 22:21:54.306409\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4546 - bce_dice_loss: 0.4546\n",
      "Training: batch 51 begins at 22:21:54.310738\n",
      "\n",
      "Training: batch 51 ends at 22:21:55.111293\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4591 - bce_dice_loss: 0.4591\n",
      "Training: batch 52 begins at 22:21:55.115384\n",
      "\n",
      "Training: batch 52 ends at 22:21:55.915673\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4601 - bce_dice_loss: 0.4601\n",
      "Training: batch 53 begins at 22:21:55.917952\n",
      "\n",
      "Training: batch 53 ends at 22:21:56.724144\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4594 - bce_dice_loss: 0.4594\n",
      "Training: batch 54 begins at 22:21:56.728348\n",
      "\n",
      "Training: batch 54 ends at 22:21:57.517027\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4596 - bce_dice_loss: 0.4596\n",
      "Training: batch 55 begins at 22:21:57.521714\n",
      "\n",
      "Training: batch 55 ends at 22:21:58.343415\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4616 - bce_dice_loss: 0.4616\n",
      "Training: batch 56 begins at 22:21:58.347677\n",
      "\n",
      "Training: batch 56 ends at 22:21:59.136410\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4587 - bce_dice_loss: 0.4587\n",
      "Training: batch 57 begins at 22:21:59.140188\n",
      "\n",
      "Training: batch 57 ends at 22:21:59.923709\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4570 - bce_dice_loss: 0.4570\n",
      "Training: batch 58 begins at 22:21:59.926696\n",
      "\n",
      "Training: batch 58 ends at 22:22:00.736587\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4570 - bce_dice_loss: 0.4570\n",
      "Training: batch 59 begins at 22:22:00.739738\n",
      "\n",
      "Training: batch 59 ends at 22:22:01.544226\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4564 - bce_dice_loss: 0.4564\n",
      "Evaluating: batch 0 begins at 22:22:01.576455\n",
      "\n",
      "Evaluating: batch 0 ends at 22:22:01.845558\n",
      "\n",
      "Evaluating: batch 1 begins at 22:22:01.847507\n",
      "\n",
      "Evaluating: batch 1 ends at 22:22:02.066336\n",
      "\n",
      "Evaluating: batch 2 begins at 22:22:02.068484\n",
      "\n",
      "Evaluating: batch 2 ends at 22:22:02.285912\n",
      "\n",
      "Evaluating: batch 3 begins at 22:22:02.287397\n",
      "\n",
      "Evaluating: batch 3 ends at 22:22:02.524635\n",
      "\n",
      "Evaluating: batch 4 begins at 22:22:02.525740\n",
      "\n",
      "Evaluating: batch 4 ends at 22:22:02.750581\n",
      "\n",
      "Evaluating: batch 5 begins at 22:22:02.752023\n",
      "\n",
      "Evaluating: batch 5 ends at 22:22:02.971754\n",
      "\n",
      "Evaluating: batch 6 begins at 22:22:02.973813\n",
      "\n",
      "Evaluating: batch 6 ends at 22:22:03.190650\n",
      "\n",
      "Evaluating: batch 7 begins at 22:22:03.192318\n",
      "\n",
      "Evaluating: batch 7 ends at 22:22:03.414643\n",
      "\n",
      "Evaluating: batch 8 begins at 22:22:03.416916\n",
      "\n",
      "Evaluating: batch 8 ends at 22:22:03.632688\n",
      "\n",
      "Evaluating: batch 9 begins at 22:22:03.633920\n",
      "\n",
      "Evaluating: batch 9 ends at 22:22:03.855634\n",
      "\n",
      "Evaluating: batch 10 begins at 22:22:03.857176\n",
      "\n",
      "Evaluating: batch 10 ends at 22:22:04.077504\n",
      "\n",
      "Evaluating: batch 11 begins at 22:22:04.079856\n",
      "\n",
      "Evaluating: batch 11 ends at 22:22:04.299682\n",
      "\n",
      "Evaluating: batch 12 begins at 22:22:04.301922\n",
      "\n",
      "Evaluating: batch 12 ends at 22:22:04.519596\n",
      "\n",
      "Evaluating: batch 13 begins at 22:22:04.521953\n",
      "\n",
      "Evaluating: batch 13 ends at 22:22:04.744507\n",
      "\n",
      "Evaluating: batch 14 begins at 22:22:04.745882\n",
      "\n",
      "Evaluating: batch 14 ends at 22:22:04.961085\n",
      "\n",
      "Evaluating: batch 15 begins at 22:22:04.962374\n",
      "\n",
      "Evaluating: batch 15 ends at 22:22:05.181757\n",
      "\n",
      "Evaluating: batch 16 begins at 22:22:05.183993\n",
      "\n",
      "Evaluating: batch 16 ends at 22:22:05.404758\n",
      "\n",
      "Evaluating: batch 17 begins at 22:22:05.406450\n",
      "\n",
      "Evaluating: batch 17 ends at 22:22:05.624106\n",
      "\n",
      "Evaluating: batch 18 begins at 22:22:05.625383\n",
      "\n",
      "Evaluating: batch 18 ends at 22:22:05.852025\n",
      "\n",
      "Evaluating: batch 19 begins at 22:22:05.854803\n",
      "\n",
      "Evaluating: batch 19 ends at 22:22:06.067670\n",
      "\n",
      "Evaluating: batch 20 begins at 22:22:06.068985\n",
      "\n",
      "Evaluating: batch 20 ends at 22:22:06.286089\n",
      "\n",
      "Evaluating: batch 21 begins at 22:22:06.287655\n",
      "\n",
      "Evaluating: batch 21 ends at 22:22:06.502704\n",
      "\n",
      "Evaluating: batch 22 begins at 22:22:06.504001\n",
      "\n",
      "Evaluating: batch 22 ends at 22:22:06.719487\n",
      "\n",
      "Evaluating: batch 23 begins at 22:22:06.721228\n",
      "\n",
      "Evaluating: batch 23 ends at 22:22:06.941639\n",
      "\n",
      "Evaluating: batch 24 begins at 22:22:06.943620\n",
      "\n",
      "Evaluating: batch 24 ends at 22:22:07.160002\n",
      "\n",
      "Evaluating: batch 25 begins at 22:22:07.161211\n",
      "\n",
      "Evaluating: batch 25 ends at 22:22:07.383027\n",
      "\n",
      "Evaluating: batch 26 begins at 22:22:07.385204\n",
      "\n",
      "Evaluating: batch 26 ends at 22:22:07.600898\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44273 to 0.42019, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 932ms/step - loss: 0.4564 - bce_dice_loss: 0.4564 - val_loss: 0.4202 - val_bce_dice_loss: 0.4202\n",
      "Epoch 6/25\n",
      "\n",
      "Training: batch 0 begins at 22:22:09.238074\n",
      "\n",
      "Training: batch 0 ends at 22:22:10.055695\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.5859 - bce_dice_loss: 0.5859\n",
      "Training: batch 1 begins at 22:22:10.061170\n",
      "\n",
      "Training: batch 1 ends at 22:22:10.875346\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4427 - bce_dice_loss: 0.4427\n",
      "Training: batch 2 begins at 22:22:10.879916\n",
      "\n",
      "Training: batch 2 ends at 22:22:11.696242\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4593 - bce_dice_loss: 0.4593\n",
      "Training: batch 3 begins at 22:22:11.701018\n",
      "\n",
      "Training: batch 3 ends at 22:22:12.503269\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.4182 - bce_dice_loss: 0.4182\n",
      "Training: batch 4 begins at 22:22:12.507672\n",
      "\n",
      "Training: batch 4 ends at 22:22:13.303460\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4226 - bce_dice_loss: 0.4226\n",
      "Training: batch 5 begins at 22:22:13.307180\n",
      "\n",
      "Training: batch 5 ends at 22:22:14.100195\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4468 - bce_dice_loss: 0.4468\n",
      "Training: batch 6 begins at 22:22:14.104011\n",
      "\n",
      "Training: batch 6 ends at 22:22:14.893072\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4580 - bce_dice_loss: 0.4580\n",
      "Training: batch 7 begins at 22:22:14.897333\n",
      "\n",
      "Training: batch 7 ends at 22:22:15.687746\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4479 - bce_dice_loss: 0.4479\n",
      "Training: batch 8 begins at 22:22:15.691356\n",
      "\n",
      "Training: batch 8 ends at 22:22:16.485101\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.4610 - bce_dice_loss: 0.4610\n",
      "Training: batch 9 begins at 22:22:16.489289\n",
      "\n",
      "Training: batch 9 ends at 22:22:17.281956\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4599 - bce_dice_loss: 0.4599\n",
      "Training: batch 10 begins at 22:22:17.286107\n",
      "\n",
      "Training: batch 10 ends at 22:22:18.097510\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4589 - bce_dice_loss: 0.4589\n",
      "Training: batch 11 begins at 22:22:18.102475\n",
      "\n",
      "Training: batch 11 ends at 22:22:18.880074\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4844 - bce_dice_loss: 0.4844\n",
      "Training: batch 12 begins at 22:22:18.884815\n",
      "\n",
      "Training: batch 12 ends at 22:22:19.699775\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.4803 - bce_dice_loss: 0.4803\n",
      "Training: batch 13 begins at 22:22:19.704694\n",
      "\n",
      "Training: batch 13 ends at 22:22:20.493918\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.4703 - bce_dice_loss: 0.4703\n",
      "Training: batch 14 begins at 22:22:20.498185\n",
      "\n",
      "Training: batch 14 ends at 22:22:21.282706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4591 - bce_dice_loss: 0.4591\n",
      "Training: batch 15 begins at 22:22:21.287439\n",
      "\n",
      "Training: batch 15 ends at 22:22:22.078416\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4522 - bce_dice_loss: 0.4522\n",
      "Training: batch 16 begins at 22:22:22.082780\n",
      "\n",
      "Training: batch 16 ends at 22:22:22.876599\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4432 - bce_dice_loss: 0.4432\n",
      "Training: batch 17 begins at 22:22:22.880943\n",
      "\n",
      "Training: batch 17 ends at 22:22:23.695799\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.4345 - bce_dice_loss: 0.4345\n",
      "Training: batch 18 begins at 22:22:23.700033\n",
      "\n",
      "Training: batch 18 ends at 22:22:24.496467\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.4398 - bce_dice_loss: 0.4398\n",
      "Training: batch 19 begins at 22:22:24.501283\n",
      "\n",
      "Training: batch 19 ends at 22:22:25.289419\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4436 - bce_dice_loss: 0.4436\n",
      "Training: batch 20 begins at 22:22:25.292879\n",
      "\n",
      "Training: batch 20 ends at 22:22:26.080576\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4385 - bce_dice_loss: 0.4385\n",
      "Training: batch 21 begins at 22:22:26.086366\n",
      "\n",
      "Training: batch 21 ends at 22:22:26.876317\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4381 - bce_dice_loss: 0.4381\n",
      "Training: batch 22 begins at 22:22:26.880735\n",
      "\n",
      "Training: batch 22 ends at 22:22:27.698142\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4497 - bce_dice_loss: 0.4497\n",
      "Training: batch 23 begins at 22:22:27.702614\n",
      "\n",
      "Training: batch 23 ends at 22:22:28.478538\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4520 - bce_dice_loss: 0.4520\n",
      "Training: batch 24 begins at 22:22:28.483004\n",
      "\n",
      "Training: batch 24 ends at 22:22:29.272284\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4458 - bce_dice_loss: 0.4458\n",
      "Training: batch 25 begins at 22:22:29.275597\n",
      "\n",
      "Training: batch 25 ends at 22:22:30.067084\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4437 - bce_dice_loss: 0.4437\n",
      "Training: batch 26 begins at 22:22:30.071673\n",
      "\n",
      "Training: batch 26 ends at 22:22:30.868164\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4434 - bce_dice_loss: 0.4434\n",
      "Training: batch 27 begins at 22:22:30.872393\n",
      "\n",
      "Training: batch 27 ends at 22:22:31.672274\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4374 - bce_dice_loss: 0.4374\n",
      "Training: batch 28 begins at 22:22:31.676662\n",
      "\n",
      "Training: batch 28 ends at 22:22:32.468140\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4330 - bce_dice_loss: 0.4330\n",
      "Training: batch 29 begins at 22:22:32.472363\n",
      "\n",
      "Training: batch 29 ends at 22:22:33.267211\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4306 - bce_dice_loss: 0.4306\n",
      "Training: batch 30 begins at 22:22:33.271388\n",
      "\n",
      "Training: batch 30 ends at 22:22:34.085700\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4256 - bce_dice_loss: 0.4256\n",
      "Training: batch 31 begins at 22:22:34.090931\n",
      "\n",
      "Training: batch 31 ends at 22:22:34.876384\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4215 - bce_dice_loss: 0.4215\n",
      "Training: batch 32 begins at 22:22:34.880258\n",
      "\n",
      "Training: batch 32 ends at 22:22:35.663990\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4216 - bce_dice_loss: 0.4216\n",
      "Training: batch 33 begins at 22:22:35.668672\n",
      "\n",
      "Training: batch 33 ends at 22:22:36.454450\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4213 - bce_dice_loss: 0.4213\n",
      "Training: batch 34 begins at 22:22:36.457966\n",
      "\n",
      "Training: batch 34 ends at 22:22:37.253691\n",
      "35/60 [================>.............] - ETA: 19s - loss: 0.4277 - bce_dice_loss: 0.4277\n",
      "Training: batch 35 begins at 22:22:37.257579\n",
      "\n",
      "Training: batch 35 ends at 22:22:38.051480\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4259 - bce_dice_loss: 0.4259\n",
      "Training: batch 36 begins at 22:22:38.055771\n",
      "\n",
      "Training: batch 36 ends at 22:22:38.860443\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4246 - bce_dice_loss: 0.4246\n",
      "Training: batch 37 begins at 22:22:38.864960\n",
      "\n",
      "Training: batch 37 ends at 22:22:39.650793\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4234 - bce_dice_loss: 0.4234\n",
      "Training: batch 38 begins at 22:22:39.654956\n",
      "\n",
      "Training: batch 38 ends at 22:22:40.446667\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4228 - bce_dice_loss: 0.4228\n",
      "Training: batch 39 begins at 22:22:40.451524\n",
      "\n",
      "Training: batch 39 ends at 22:22:41.249334\n",
      "40/60 [===================>..........] - ETA: 15s - loss: 0.4208 - bce_dice_loss: 0.4208\n",
      "Training: batch 40 begins at 22:22:41.253482\n",
      "\n",
      "Training: batch 40 ends at 22:22:42.054414\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4220 - bce_dice_loss: 0.4220\n",
      "Training: batch 41 begins at 22:22:42.058378\n",
      "\n",
      "Training: batch 41 ends at 22:22:42.850174\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4208 - bce_dice_loss: 0.4208\n",
      "Training: batch 42 begins at 22:22:42.854832\n",
      "\n",
      "Training: batch 42 ends at 22:22:43.645633\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4182 - bce_dice_loss: 0.4182\n",
      "Training: batch 43 begins at 22:22:43.650415\n",
      "\n",
      "Training: batch 43 ends at 22:22:44.459398\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4157 - bce_dice_loss: 0.4157\n",
      "Training: batch 44 begins at 22:22:44.463771\n",
      "\n",
      "Training: batch 44 ends at 22:22:45.253207\n",
      "45/60 [=====================>........] - ETA: 11s - loss: 0.4146 - bce_dice_loss: 0.4146\n",
      "Training: batch 45 begins at 22:22:45.257504\n",
      "\n",
      "Training: batch 45 ends at 22:22:46.049956\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4124 - bce_dice_loss: 0.4124\n",
      "Training: batch 46 begins at 22:22:46.054062\n",
      "\n",
      "Training: batch 46 ends at 22:22:46.853708\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4129 - bce_dice_loss: 0.4129\n",
      "Training: batch 47 begins at 22:22:46.857168\n",
      "\n",
      "Training: batch 47 ends at 22:22:47.648008\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4193 - bce_dice_loss: 0.4193 \n",
      "Training: batch 48 begins at 22:22:47.652756\n",
      "\n",
      "Training: batch 48 ends at 22:22:48.472965\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4170 - bce_dice_loss: 0.4170\n",
      "Training: batch 49 begins at 22:22:48.477430\n",
      "\n",
      "Training: batch 49 ends at 22:22:49.265692\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4166 - bce_dice_loss: 0.4166\n",
      "Training: batch 50 begins at 22:22:49.270763\n",
      "\n",
      "Training: batch 50 ends at 22:22:50.059987\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4214 - bce_dice_loss: 0.4214\n",
      "Training: batch 51 begins at 22:22:50.064249\n",
      "\n",
      "Training: batch 51 ends at 22:22:50.855200\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4236 - bce_dice_loss: 0.4236\n",
      "Training: batch 52 begins at 22:22:50.859630\n",
      "\n",
      "Training: batch 52 ends at 22:22:51.656677\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4235 - bce_dice_loss: 0.4235\n",
      "Training: batch 53 begins at 22:22:51.660095\n",
      "\n",
      "Training: batch 53 ends at 22:22:52.456893\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4213 - bce_dice_loss: 0.4213\n",
      "Training: batch 54 begins at 22:22:52.461528\n",
      "\n",
      "Training: batch 54 ends at 22:22:53.269913\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4200 - bce_dice_loss: 0.4200\n",
      "Training: batch 55 begins at 22:22:53.273583\n",
      "\n",
      "Training: batch 55 ends at 22:22:54.063718\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4167 - bce_dice_loss: 0.4167\n",
      "Training: batch 56 begins at 22:22:54.068570\n",
      "\n",
      "Training: batch 56 ends at 22:22:54.874288\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4138 - bce_dice_loss: 0.4138\n",
      "Training: batch 57 begins at 22:22:54.878585\n",
      "\n",
      "Training: batch 57 ends at 22:22:55.669223\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4113 - bce_dice_loss: 0.4113\n",
      "Training: batch 58 begins at 22:22:55.672530\n",
      "\n",
      "Training: batch 58 ends at 22:22:56.461501\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4103 - bce_dice_loss: 0.4103\n",
      "Training: batch 59 begins at 22:22:56.466265\n",
      "\n",
      "Training: batch 59 ends at 22:22:57.252748\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4135 - bce_dice_loss: 0.4135\n",
      "Evaluating: batch 0 begins at 22:22:57.284344\n",
      "\n",
      "Evaluating: batch 0 ends at 22:22:57.557967\n",
      "\n",
      "Evaluating: batch 1 begins at 22:22:57.559154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 1 ends at 22:22:57.779142\n",
      "\n",
      "Evaluating: batch 2 begins at 22:22:57.781214\n",
      "\n",
      "Evaluating: batch 2 ends at 22:22:57.999507\n",
      "\n",
      "Evaluating: batch 3 begins at 22:22:58.001679\n",
      "\n",
      "Evaluating: batch 3 ends at 22:22:58.220248\n",
      "\n",
      "Evaluating: batch 4 begins at 22:22:58.222278\n",
      "\n",
      "Evaluating: batch 4 ends at 22:22:58.439949\n",
      "\n",
      "Evaluating: batch 5 begins at 22:22:58.441562\n",
      "\n",
      "Evaluating: batch 5 ends at 22:22:58.660999\n",
      "\n",
      "Evaluating: batch 6 begins at 22:22:58.662593\n",
      "\n",
      "Evaluating: batch 6 ends at 22:22:58.883285\n",
      "\n",
      "Evaluating: batch 7 begins at 22:22:58.884753\n",
      "\n",
      "Evaluating: batch 7 ends at 22:22:59.103987\n",
      "\n",
      "Evaluating: batch 8 begins at 22:22:59.106094\n",
      "\n",
      "Evaluating: batch 8 ends at 22:22:59.324497\n",
      "\n",
      "Evaluating: batch 9 begins at 22:22:59.325844\n",
      "\n",
      "Evaluating: batch 9 ends at 22:22:59.545101\n",
      "\n",
      "Evaluating: batch 10 begins at 22:22:59.546776\n",
      "\n",
      "Evaluating: batch 10 ends at 22:22:59.768730\n",
      "\n",
      "Evaluating: batch 11 begins at 22:22:59.770419\n",
      "\n",
      "Evaluating: batch 11 ends at 22:22:59.989541\n",
      "\n",
      "Evaluating: batch 12 begins at 22:22:59.991360\n",
      "\n",
      "Evaluating: batch 12 ends at 22:23:00.217984\n",
      "\n",
      "Evaluating: batch 13 begins at 22:23:00.219940\n",
      "\n",
      "Evaluating: batch 13 ends at 22:23:00.438809\n",
      "\n",
      "Evaluating: batch 14 begins at 22:23:00.441047\n",
      "\n",
      "Evaluating: batch 14 ends at 22:23:00.662827\n",
      "\n",
      "Evaluating: batch 15 begins at 22:23:00.664048\n",
      "\n",
      "Evaluating: batch 15 ends at 22:23:00.885016\n",
      "\n",
      "Evaluating: batch 16 begins at 22:23:00.886368\n",
      "\n",
      "Evaluating: batch 16 ends at 22:23:01.107314\n",
      "\n",
      "Evaluating: batch 17 begins at 22:23:01.108741\n",
      "\n",
      "Evaluating: batch 17 ends at 22:23:01.339567\n",
      "\n",
      "Evaluating: batch 18 begins at 22:23:01.340942\n",
      "\n",
      "Evaluating: batch 18 ends at 22:23:01.563821\n",
      "\n",
      "Evaluating: batch 19 begins at 22:23:01.565165\n",
      "\n",
      "Evaluating: batch 19 ends at 22:23:01.781898\n",
      "\n",
      "Evaluating: batch 20 begins at 22:23:01.783520\n",
      "\n",
      "Evaluating: batch 20 ends at 22:23:01.999323\n",
      "\n",
      "Evaluating: batch 21 begins at 22:23:02.000648\n",
      "\n",
      "Evaluating: batch 21 ends at 22:23:02.216272\n",
      "\n",
      "Evaluating: batch 22 begins at 22:23:02.217379\n",
      "\n",
      "Evaluating: batch 22 ends at 22:23:02.435868\n",
      "\n",
      "Evaluating: batch 23 begins at 22:23:02.437427\n",
      "\n",
      "Evaluating: batch 23 ends at 22:23:02.656505\n",
      "\n",
      "Evaluating: batch 24 begins at 22:23:02.658000\n",
      "\n",
      "Evaluating: batch 24 ends at 22:23:02.874922\n",
      "\n",
      "Evaluating: batch 25 begins at 22:23:02.876235\n",
      "\n",
      "Evaluating: batch 25 ends at 22:23:03.093423\n",
      "\n",
      "Evaluating: batch 26 begins at 22:23:03.095831\n",
      "\n",
      "Evaluating: batch 26 ends at 22:23:03.311432\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42019 to 0.41529, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 928ms/step - loss: 0.4135 - bce_dice_loss: 0.4135 - val_loss: 0.4153 - val_bce_dice_loss: 0.4153\n",
      "Epoch 7/25\n",
      "\n",
      "Training: batch 0 begins at 22:23:04.836334\n",
      "\n",
      "Training: batch 0 ends at 22:23:05.656183\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3423 - bce_dice_loss: 0.3423\n",
      "Training: batch 1 begins at 22:23:05.664078\n",
      "\n",
      "Training: batch 1 ends at 22:23:06.466100\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3812 - bce_dice_loss: 0.3812\n",
      "Training: batch 2 begins at 22:23:06.470737\n",
      "\n",
      "Training: batch 2 ends at 22:23:07.258250\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 3 begins at 22:23:07.263501\n",
      "\n",
      "Training: batch 3 ends at 22:23:08.059863\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.3336 - bce_dice_loss: 0.3336\n",
      "Training: batch 4 begins at 22:23:08.064324\n",
      "\n",
      "Training: batch 4 ends at 22:23:08.854293\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.3306 - bce_dice_loss: 0.3306\n",
      "Training: batch 5 begins at 22:23:08.858729\n",
      "\n",
      "Training: batch 5 ends at 22:23:09.647921\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3265 - bce_dice_loss: 0.3265\n",
      "Training: batch 6 begins at 22:23:09.652485\n",
      "\n",
      "Training: batch 6 ends at 22:23:10.442432\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3429 - bce_dice_loss: 0.3429\n",
      "Training: batch 7 begins at 22:23:10.446842\n",
      "\n",
      "Training: batch 7 ends at 22:23:11.266513\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3247 - bce_dice_loss: 0.3247\n",
      "Training: batch 8 begins at 22:23:11.270141\n",
      "\n",
      "Training: batch 8 ends at 22:23:12.069647\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.3203 - bce_dice_loss: 0.3203\n",
      "Training: batch 9 begins at 22:23:12.073022\n",
      "\n",
      "Training: batch 9 ends at 22:23:12.890491\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3479 - bce_dice_loss: 0.3479\n",
      "Training: batch 10 begins at 22:23:12.894869\n",
      "\n",
      "Training: batch 10 ends at 22:23:13.687256\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3400 - bce_dice_loss: 0.3400\n",
      "Training: batch 11 begins at 22:23:13.691483\n",
      "\n",
      "Training: batch 11 ends at 22:23:14.476754\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3514 - bce_dice_loss: 0.3514\n",
      "Training: batch 12 begins at 22:23:14.481174\n",
      "\n",
      "Training: batch 12 ends at 22:23:15.280981\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3584 - bce_dice_loss: 0.3584\n",
      "Training: batch 13 begins at 22:23:15.285718\n",
      "\n",
      "Training: batch 13 ends at 22:23:16.071754\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.3723 - bce_dice_loss: 0.3723\n",
      "Training: batch 14 begins at 22:23:16.075301\n",
      "\n",
      "Training: batch 14 ends at 22:23:16.866186\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3688 - bce_dice_loss: 0.3688\n",
      "Training: batch 15 begins at 22:23:16.870586\n",
      "\n",
      "Training: batch 15 ends at 22:23:17.658666\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3592 - bce_dice_loss: 0.3592\n",
      "Training: batch 16 begins at 22:23:17.663127\n",
      "\n",
      "Training: batch 16 ends at 22:23:18.480090\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3634 - bce_dice_loss: 0.3634\n",
      "Training: batch 17 begins at 22:23:18.485206\n",
      "\n",
      "Training: batch 17 ends at 22:23:19.278093\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3580 - bce_dice_loss: 0.3580\n",
      "Training: batch 18 begins at 22:23:19.282593\n",
      "\n",
      "Training: batch 18 ends at 22:23:20.075762\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.3556 - bce_dice_loss: 0.3556\n",
      "Training: batch 19 begins at 22:23:20.079837\n",
      "\n",
      "Training: batch 19 ends at 22:23:20.856940\n",
      "20/60 [=========>....................] - ETA: 31s - loss: 0.3522 - bce_dice_loss: 0.3522\n",
      "Training: batch 20 begins at 22:23:20.861200\n",
      "\n",
      "Training: batch 20 ends at 22:23:21.678380\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3473 - bce_dice_loss: 0.3473\n",
      "Training: batch 21 begins at 22:23:21.683196\n",
      "\n",
      "Training: batch 21 ends at 22:23:22.476161\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3501 - bce_dice_loss: 0.3501\n",
      "Training: batch 22 begins at 22:23:22.480637\n",
      "\n",
      "Training: batch 22 ends at 22:23:23.272473\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3523 - bce_dice_loss: 0.3523\n",
      "Training: batch 23 begins at 22:23:23.275879\n",
      "\n",
      "Training: batch 23 ends at 22:23:24.065181\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3572 - bce_dice_loss: 0.3572\n",
      "Training: batch 24 begins at 22:23:24.069456\n",
      "\n",
      "Training: batch 24 ends at 22:23:24.858194\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3574 - bce_dice_loss: 0.3574\n",
      "Training: batch 25 begins at 22:23:24.862520\n",
      "\n",
      "Training: batch 25 ends at 22:23:25.656564\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3587 - bce_dice_loss: 0.3587\n",
      "Training: batch 26 begins at 22:23:25.660913\n",
      "\n",
      "Training: batch 26 ends at 22:23:26.470540\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3687 - bce_dice_loss: 0.3687\n",
      "Training: batch 27 begins at 22:23:26.475052\n",
      "\n",
      "Training: batch 27 ends at 22:23:27.268728\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3730 - bce_dice_loss: 0.3730\n",
      "Training: batch 28 begins at 22:23:27.272107\n",
      "\n",
      "Training: batch 28 ends at 22:23:28.066948\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3750 - bce_dice_loss: 0.3750\n",
      "Training: batch 29 begins at 22:23:28.071404\n",
      "\n",
      "Training: batch 29 ends at 22:23:28.858129\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3743 - bce_dice_loss: 0.3743\n",
      "Training: batch 30 begins at 22:23:28.862314\n",
      "\n",
      "Training: batch 30 ends at 22:23:29.649878\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3739 - bce_dice_loss: 0.3739\n",
      "Training: batch 31 begins at 22:23:29.653737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 31 ends at 22:23:30.446508\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3722 - bce_dice_loss: 0.3722\n",
      "Training: batch 32 begins at 22:23:30.451205\n",
      "\n",
      "Training: batch 32 ends at 22:23:31.248190\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3797 - bce_dice_loss: 0.3797\n",
      "Training: batch 33 begins at 22:23:31.252432\n",
      "\n",
      "Training: batch 33 ends at 22:23:32.048619\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3848 - bce_dice_loss: 0.3848\n",
      "Training: batch 34 begins at 22:23:32.052437\n",
      "\n",
      "Training: batch 34 ends at 22:23:32.843609\n",
      "35/60 [================>.............] - ETA: 19s - loss: 0.3884 - bce_dice_loss: 0.3884\n",
      "Training: batch 35 begins at 22:23:32.848076\n",
      "\n",
      "Training: batch 35 ends at 22:23:33.655977\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3899 - bce_dice_loss: 0.3899\n",
      "Training: batch 36 begins at 22:23:33.659454\n",
      "\n",
      "Training: batch 36 ends at 22:23:34.453394\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3924 - bce_dice_loss: 0.3924\n",
      "Training: batch 37 begins at 22:23:34.457675\n",
      "\n",
      "Training: batch 37 ends at 22:23:35.248559\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3964 - bce_dice_loss: 0.3964\n",
      "Training: batch 38 begins at 22:23:35.253003\n",
      "\n",
      "Training: batch 38 ends at 22:23:36.046329\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3986 - bce_dice_loss: 0.3986\n",
      "Training: batch 39 begins at 22:23:36.051083\n",
      "\n",
      "Training: batch 39 ends at 22:23:36.871205\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3994 - bce_dice_loss: 0.3994\n",
      "Training: batch 40 begins at 22:23:36.876355\n",
      "\n",
      "Training: batch 40 ends at 22:23:37.660750\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3972 - bce_dice_loss: 0.3972\n",
      "Training: batch 41 begins at 22:23:37.665840\n",
      "\n",
      "Training: batch 41 ends at 22:23:38.451537\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3999 - bce_dice_loss: 0.3999\n",
      "Training: batch 42 begins at 22:23:38.455993\n",
      "\n",
      "Training: batch 42 ends at 22:23:39.272059\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4018 - bce_dice_loss: 0.4018\n",
      "Training: batch 43 begins at 22:23:39.276628\n",
      "\n",
      "Training: batch 43 ends at 22:23:40.064890\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4030 - bce_dice_loss: 0.4030\n",
      "Training: batch 44 begins at 22:23:40.069399\n",
      "\n",
      "Training: batch 44 ends at 22:23:40.881245\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4033 - bce_dice_loss: 0.4033\n",
      "Training: batch 45 begins at 22:23:40.885903\n",
      "\n",
      "Training: batch 45 ends at 22:23:41.679287\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4077 - bce_dice_loss: 0.4077\n",
      "Training: batch 46 begins at 22:23:41.684206\n",
      "\n",
      "Training: batch 46 ends at 22:23:42.467746\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4053 - bce_dice_loss: 0.4053\n",
      "Training: batch 47 begins at 22:23:42.472335\n",
      "\n",
      "Training: batch 47 ends at 22:23:43.270622\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4058 - bce_dice_loss: 0.4058 \n",
      "Training: batch 48 begins at 22:23:43.274758\n",
      "\n",
      "Training: batch 48 ends at 22:23:44.091399\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4047 - bce_dice_loss: 0.4047\n",
      "Training: batch 49 begins at 22:23:44.095523\n",
      "\n",
      "Training: batch 49 ends at 22:23:44.881290\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4047 - bce_dice_loss: 0.4047\n",
      "Training: batch 50 begins at 22:23:44.885995\n",
      "\n",
      "Training: batch 50 ends at 22:23:45.705813\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4033 - bce_dice_loss: 0.4033\n",
      "Training: batch 51 begins at 22:23:45.710213\n",
      "\n",
      "Training: batch 51 ends at 22:23:46.501901\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4044 - bce_dice_loss: 0.4044\n",
      "Training: batch 52 begins at 22:23:46.506195\n",
      "\n",
      "Training: batch 52 ends at 22:23:47.286132\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4124 - bce_dice_loss: 0.4124\n",
      "Training: batch 53 begins at 22:23:47.290530\n",
      "\n",
      "Training: batch 53 ends at 22:23:48.081188\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4121 - bce_dice_loss: 0.4121\n",
      "Training: batch 54 begins at 22:23:48.085993\n",
      "\n",
      "Training: batch 54 ends at 22:23:48.872732\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4179 - bce_dice_loss: 0.4179\n",
      "Training: batch 55 begins at 22:23:48.877035\n",
      "\n",
      "Training: batch 55 ends at 22:23:49.665755\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4186 - bce_dice_loss: 0.4186\n",
      "Training: batch 56 begins at 22:23:49.670207\n",
      "\n",
      "Training: batch 56 ends at 22:23:50.461111\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4202 - bce_dice_loss: 0.4202\n",
      "Training: batch 57 begins at 22:23:50.466432\n",
      "\n",
      "Training: batch 57 ends at 22:23:51.289004\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4229 - bce_dice_loss: 0.4229\n",
      "Training: batch 58 begins at 22:23:51.292600\n",
      "\n",
      "Training: batch 58 ends at 22:23:52.082030\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4260 - bce_dice_loss: 0.4260\n",
      "Training: batch 59 begins at 22:23:52.086168\n",
      "\n",
      "Training: batch 59 ends at 22:23:52.878202\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4315 - bce_dice_loss: 0.4315\n",
      "Evaluating: batch 0 begins at 22:23:52.908241\n",
      "\n",
      "Evaluating: batch 0 ends at 22:23:53.172702\n",
      "\n",
      "Evaluating: batch 1 begins at 22:23:53.173902\n",
      "\n",
      "Evaluating: batch 1 ends at 22:23:53.386535\n",
      "\n",
      "Evaluating: batch 2 begins at 22:23:53.387742\n",
      "\n",
      "Evaluating: batch 2 ends at 22:23:53.602833\n",
      "\n",
      "Evaluating: batch 3 begins at 22:23:53.603900\n",
      "\n",
      "Evaluating: batch 3 ends at 22:23:53.822438\n",
      "\n",
      "Evaluating: batch 4 begins at 22:23:53.823749\n",
      "\n",
      "Evaluating: batch 4 ends at 22:23:54.041161\n",
      "\n",
      "Evaluating: batch 5 begins at 22:23:54.042441\n",
      "\n",
      "Evaluating: batch 5 ends at 22:23:54.267281\n",
      "\n",
      "Evaluating: batch 6 begins at 22:23:54.269082\n",
      "\n",
      "Evaluating: batch 6 ends at 22:23:54.484790\n",
      "\n",
      "Evaluating: batch 7 begins at 22:23:54.485972\n",
      "\n",
      "Evaluating: batch 7 ends at 22:23:54.714686\n",
      "\n",
      "Evaluating: batch 8 begins at 22:23:54.715636\n",
      "\n",
      "Evaluating: batch 8 ends at 22:23:54.929032\n",
      "\n",
      "Evaluating: batch 9 begins at 22:23:54.930173\n",
      "\n",
      "Evaluating: batch 9 ends at 22:23:55.145169\n",
      "\n",
      "Evaluating: batch 10 begins at 22:23:55.146411\n",
      "\n",
      "Evaluating: batch 10 ends at 22:23:55.363851\n",
      "\n",
      "Evaluating: batch 11 begins at 22:23:55.366478\n",
      "\n",
      "Evaluating: batch 11 ends at 22:23:55.584154\n",
      "\n",
      "Evaluating: batch 12 begins at 22:23:55.586597\n",
      "\n",
      "Evaluating: batch 12 ends at 22:23:55.808907\n",
      "\n",
      "Evaluating: batch 13 begins at 22:23:55.810205\n",
      "\n",
      "Evaluating: batch 13 ends at 22:23:56.029302\n",
      "\n",
      "Evaluating: batch 14 begins at 22:23:56.031746\n",
      "\n",
      "Evaluating: batch 14 ends at 22:23:56.251341\n",
      "\n",
      "Evaluating: batch 15 begins at 22:23:56.253464\n",
      "\n",
      "Evaluating: batch 15 ends at 22:23:56.472710\n",
      "\n",
      "Evaluating: batch 16 begins at 22:23:56.475093\n",
      "\n",
      "Evaluating: batch 16 ends at 22:23:56.701076\n",
      "\n",
      "Evaluating: batch 17 begins at 22:23:56.703071\n",
      "\n",
      "Evaluating: batch 17 ends at 22:23:56.925245\n",
      "\n",
      "Evaluating: batch 18 begins at 22:23:56.926923\n",
      "\n",
      "Evaluating: batch 18 ends at 22:23:57.150708\n",
      "\n",
      "Evaluating: batch 19 begins at 22:23:57.152121\n",
      "\n",
      "Evaluating: batch 19 ends at 22:23:57.368130\n",
      "\n",
      "Evaluating: batch 20 begins at 22:23:57.369339\n",
      "\n",
      "Evaluating: batch 20 ends at 22:23:57.586506\n",
      "\n",
      "Evaluating: batch 21 begins at 22:23:57.588608\n",
      "\n",
      "Evaluating: batch 21 ends at 22:23:57.805721\n",
      "\n",
      "Evaluating: batch 22 begins at 22:23:57.808207\n",
      "\n",
      "Evaluating: batch 22 ends at 22:23:58.024606\n",
      "\n",
      "Evaluating: batch 23 begins at 22:23:58.025852\n",
      "\n",
      "Evaluating: batch 23 ends at 22:23:58.245493\n",
      "\n",
      "Evaluating: batch 24 begins at 22:23:58.247916\n",
      "\n",
      "Evaluating: batch 24 ends at 22:23:58.464020\n",
      "\n",
      "Evaluating: batch 25 begins at 22:23:58.466699\n",
      "\n",
      "Evaluating: batch 25 ends at 22:23:58.683153\n",
      "\n",
      "Evaluating: batch 26 begins at 22:23:58.685645\n",
      "\n",
      "Evaluating: batch 26 ends at 22:23:58.904343\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41529\n",
      "60/60 [==============================] - 54s 903ms/step - loss: 0.4315 - bce_dice_loss: 0.4315 - val_loss: 0.4827 - val_bce_dice_loss: 0.4827\n",
      "Epoch 8/25\n",
      "\n",
      "Training: batch 0 begins at 22:23:58.933168\n",
      "\n",
      "Training: batch 0 ends at 22:23:59.724200\n",
      " 1/60 [..............................] - ETA: 46s - loss: 0.5898 - bce_dice_loss: 0.5898\n",
      "Training: batch 1 begins at 22:23:59.727667\n",
      "\n",
      "Training: batch 1 ends at 22:24:00.530305\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.5101 - bce_dice_loss: 0.5101\n",
      "Training: batch 2 begins at 22:24:00.536969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 2 ends at 22:24:01.339036\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.5414 - bce_dice_loss: 0.5414\n",
      "Training: batch 3 begins at 22:24:01.342930\n",
      "\n",
      "Training: batch 3 ends at 22:24:02.142472\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.5285 - bce_dice_loss: 0.5285\n",
      "Training: batch 4 begins at 22:24:02.146411\n",
      "\n",
      "Training: batch 4 ends at 22:24:02.940067\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4907 - bce_dice_loss: 0.4907\n",
      "Training: batch 5 begins at 22:24:02.944607\n",
      "\n",
      "Training: batch 5 ends at 22:24:03.731693\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4496 - bce_dice_loss: 0.4496\n",
      "Training: batch 6 begins at 22:24:03.736351\n",
      "\n",
      "Training: batch 6 ends at 22:24:04.545447\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4576 - bce_dice_loss: 0.4576\n",
      "Training: batch 7 begins at 22:24:04.550487\n",
      "\n",
      "Training: batch 7 ends at 22:24:05.343856\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.4557 - bce_dice_loss: 0.4557\n",
      "Training: batch 8 begins at 22:24:05.348723\n",
      "\n",
      "Training: batch 8 ends at 22:24:06.135755\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.4566 - bce_dice_loss: 0.4566\n",
      "Training: batch 9 begins at 22:24:06.140191\n",
      "\n",
      "Training: batch 9 ends at 22:24:06.936992\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4813 - bce_dice_loss: 0.4813\n",
      "Training: batch 10 begins at 22:24:06.940464\n",
      "\n",
      "Training: batch 10 ends at 22:24:07.739051\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4809 - bce_dice_loss: 0.4809\n",
      "Training: batch 11 begins at 22:24:07.741842\n",
      "\n",
      "Training: batch 11 ends at 22:24:08.564910\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.4585 - bce_dice_loss: 0.4585\n",
      "Training: batch 12 begins at 22:24:08.569459\n",
      "\n",
      "Training: batch 12 ends at 22:24:09.362936\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.4536 - bce_dice_loss: 0.4536\n",
      "Training: batch 13 begins at 22:24:09.367863\n",
      "\n",
      "Training: batch 13 ends at 22:24:10.162099\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.4532 - bce_dice_loss: 0.4532\n",
      "Training: batch 14 begins at 22:24:10.167054\n",
      "\n",
      "Training: batch 14 ends at 22:24:10.956302\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4493 - bce_dice_loss: 0.4493\n",
      "Training: batch 15 begins at 22:24:10.960683\n",
      "\n",
      "Training: batch 15 ends at 22:24:11.759045\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.4496 - bce_dice_loss: 0.4496\n",
      "Training: batch 16 begins at 22:24:11.762400\n",
      "\n",
      "Training: batch 16 ends at 22:24:12.548763\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.4548 - bce_dice_loss: 0.4548\n",
      "Training: batch 17 begins at 22:24:12.553109\n",
      "\n",
      "Training: batch 17 ends at 22:24:13.350136\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.4400 - bce_dice_loss: 0.4400\n",
      "Training: batch 18 begins at 22:24:13.353995\n",
      "\n",
      "Training: batch 18 ends at 22:24:14.172093\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.4375 - bce_dice_loss: 0.4375\n",
      "Training: batch 19 begins at 22:24:14.175957\n",
      "\n",
      "Training: batch 19 ends at 22:24:14.971078\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4471 - bce_dice_loss: 0.4471\n",
      "Training: batch 20 begins at 22:24:14.975157\n",
      "\n",
      "Training: batch 20 ends at 22:24:15.764130\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4531 - bce_dice_loss: 0.4531\n",
      "Training: batch 21 begins at 22:24:15.768425\n",
      "\n",
      "Training: batch 21 ends at 22:24:16.584108\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4621 - bce_dice_loss: 0.4621\n",
      "Training: batch 22 begins at 22:24:16.588429\n",
      "\n",
      "Training: batch 22 ends at 22:24:17.400828\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4544 - bce_dice_loss: 0.4544\n",
      "Training: batch 23 begins at 22:24:17.404934\n",
      "\n",
      "Training: batch 23 ends at 22:24:18.199789\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4547 - bce_dice_loss: 0.4547\n",
      "Training: batch 24 begins at 22:24:18.204079\n",
      "\n",
      "Training: batch 24 ends at 22:24:18.993224\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4507 - bce_dice_loss: 0.4507\n",
      "Training: batch 25 begins at 22:24:18.998104\n",
      "\n",
      "Training: batch 25 ends at 22:24:19.807613\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4559 - bce_dice_loss: 0.4559\n",
      "Training: batch 26 begins at 22:24:19.811292\n",
      "\n",
      "Training: batch 26 ends at 22:24:20.599224\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4626 - bce_dice_loss: 0.4626\n",
      "Training: batch 27 begins at 22:24:20.603578\n",
      "\n",
      "Training: batch 27 ends at 22:24:21.401032\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4609 - bce_dice_loss: 0.4609\n",
      "Training: batch 28 begins at 22:24:21.404438\n",
      "\n",
      "Training: batch 28 ends at 22:24:22.197240\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4590 - bce_dice_loss: 0.4590\n",
      "Training: batch 29 begins at 22:24:22.201966\n",
      "\n",
      "Training: batch 29 ends at 22:24:22.988973\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4549 - bce_dice_loss: 0.4549\n",
      "Training: batch 30 begins at 22:24:22.992847\n",
      "\n",
      "Training: batch 30 ends at 22:24:23.785033\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4539 - bce_dice_loss: 0.4539\n",
      "Training: batch 31 begins at 22:24:23.788518\n",
      "\n",
      "Training: batch 31 ends at 22:24:24.575614\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4546 - bce_dice_loss: 0.4546\n",
      "Training: batch 32 begins at 22:24:24.580210\n",
      "\n",
      "Training: batch 32 ends at 22:24:25.385504\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4511 - bce_dice_loss: 0.4511\n",
      "Training: batch 33 begins at 22:24:25.389112\n",
      "\n",
      "Training: batch 33 ends at 22:24:26.196589\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.4463 - bce_dice_loss: 0.4463\n",
      "Training: batch 34 begins at 22:24:26.199647\n",
      "\n",
      "Training: batch 34 ends at 22:24:27.019809\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4458 - bce_dice_loss: 0.4458\n",
      "Training: batch 35 begins at 22:24:27.022810\n",
      "\n",
      "Training: batch 35 ends at 22:24:27.824328\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4476 - bce_dice_loss: 0.4476\n",
      "Training: batch 36 begins at 22:24:27.830820\n",
      "\n",
      "Training: batch 36 ends at 22:24:28.625109\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4457 - bce_dice_loss: 0.4457\n",
      "Training: batch 37 begins at 22:24:28.629964\n",
      "\n",
      "Training: batch 37 ends at 22:24:29.433686\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4453 - bce_dice_loss: 0.4453\n",
      "Training: batch 38 begins at 22:24:29.438065\n",
      "\n",
      "Training: batch 38 ends at 22:24:30.223651\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.4446 - bce_dice_loss: 0.4446\n",
      "Training: batch 39 begins at 22:24:30.227809\n",
      "\n",
      "Training: batch 39 ends at 22:24:31.024960\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4438 - bce_dice_loss: 0.4438\n",
      "Training: batch 40 begins at 22:24:31.029642\n",
      "\n",
      "Training: batch 40 ends at 22:24:31.831309\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4432 - bce_dice_loss: 0.4432\n",
      "Training: batch 41 begins at 22:24:31.835704\n",
      "\n",
      "Training: batch 41 ends at 22:24:32.623343\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4388 - bce_dice_loss: 0.4388\n",
      "Training: batch 42 begins at 22:24:32.628117\n",
      "\n",
      "Training: batch 42 ends at 22:24:33.433515\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4400 - bce_dice_loss: 0.4400\n",
      "Training: batch 43 begins at 22:24:33.437919\n",
      "\n",
      "Training: batch 43 ends at 22:24:34.230712\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.4429 - bce_dice_loss: 0.4429\n",
      "Training: batch 44 begins at 22:24:34.235112\n",
      "\n",
      "Training: batch 44 ends at 22:24:35.022505\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4431 - bce_dice_loss: 0.4431\n",
      "Training: batch 45 begins at 22:24:35.026743\n",
      "\n",
      "Training: batch 45 ends at 22:24:35.820703\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4406 - bce_dice_loss: 0.4406\n",
      "Training: batch 46 begins at 22:24:35.825236\n",
      "\n",
      "Training: batch 46 ends at 22:24:36.616620\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4376 - bce_dice_loss: 0.4376\n",
      "Training: batch 47 begins at 22:24:36.621121\n",
      "\n",
      "Training: batch 47 ends at 22:24:37.415240\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4343 - bce_dice_loss: 0.4343 \n",
      "Training: batch 48 begins at 22:24:37.419455\n",
      "\n",
      "Training: batch 48 ends at 22:24:38.211938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4334 - bce_dice_loss: 0.4334\n",
      "Training: batch 49 begins at 22:24:38.216179\n",
      "\n",
      "Training: batch 49 ends at 22:24:39.002768\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4304 - bce_dice_loss: 0.4304\n",
      "Training: batch 50 begins at 22:24:39.006130\n",
      "\n",
      "Training: batch 50 ends at 22:24:39.796389\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4338 - bce_dice_loss: 0.4338\n",
      "Training: batch 51 begins at 22:24:39.800581\n",
      "\n",
      "Training: batch 51 ends at 22:24:40.602517\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4322 - bce_dice_loss: 0.4322\n",
      "Training: batch 52 begins at 22:24:40.606065\n",
      "\n",
      "Training: batch 52 ends at 22:24:41.395676\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4293 - bce_dice_loss: 0.4293\n",
      "Training: batch 53 begins at 22:24:41.400425\n",
      "\n",
      "Training: batch 53 ends at 22:24:42.206913\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4269 - bce_dice_loss: 0.4269\n",
      "Training: batch 54 begins at 22:24:42.211446\n",
      "\n",
      "Training: batch 54 ends at 22:24:42.997618\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4242 - bce_dice_loss: 0.4242\n",
      "Training: batch 55 begins at 22:24:43.001440\n",
      "\n",
      "Training: batch 55 ends at 22:24:43.790642\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4273 - bce_dice_loss: 0.4273\n",
      "Training: batch 56 begins at 22:24:43.794802\n",
      "\n",
      "Training: batch 56 ends at 22:24:44.587251\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4254 - bce_dice_loss: 0.4254\n",
      "Training: batch 57 begins at 22:24:44.590936\n",
      "\n",
      "Training: batch 57 ends at 22:24:45.384383\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4250 - bce_dice_loss: 0.4250\n",
      "Training: batch 58 begins at 22:24:45.388691\n",
      "\n",
      "Training: batch 58 ends at 22:24:46.181065\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4263 - bce_dice_loss: 0.4263\n",
      "Training: batch 59 begins at 22:24:46.185242\n",
      "\n",
      "Training: batch 59 ends at 22:24:46.975083\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4254 - bce_dice_loss: 0.4254\n",
      "Evaluating: batch 0 begins at 22:24:47.009482\n",
      "\n",
      "Evaluating: batch 0 ends at 22:24:47.275571\n",
      "\n",
      "Evaluating: batch 1 begins at 22:24:47.276740\n",
      "\n",
      "Evaluating: batch 1 ends at 22:24:47.490129\n",
      "\n",
      "Evaluating: batch 2 begins at 22:24:47.491451\n",
      "\n",
      "Evaluating: batch 2 ends at 22:24:47.708566\n",
      "\n",
      "Evaluating: batch 3 begins at 22:24:47.710011\n",
      "\n",
      "Evaluating: batch 3 ends at 22:24:47.927094\n",
      "\n",
      "Evaluating: batch 4 begins at 22:24:47.928325\n",
      "\n",
      "Evaluating: batch 4 ends at 22:24:48.148469\n",
      "\n",
      "Evaluating: batch 5 begins at 22:24:48.151013\n",
      "\n",
      "Evaluating: batch 5 ends at 22:24:48.369806\n",
      "\n",
      "Evaluating: batch 6 begins at 22:24:48.371162\n",
      "\n",
      "Evaluating: batch 6 ends at 22:24:48.589471\n",
      "\n",
      "Evaluating: batch 7 begins at 22:24:48.590753\n",
      "\n",
      "Evaluating: batch 7 ends at 22:24:48.812660\n",
      "\n",
      "Evaluating: batch 8 begins at 22:24:48.814305\n",
      "\n",
      "Evaluating: batch 8 ends at 22:24:49.031409\n",
      "\n",
      "Evaluating: batch 9 begins at 22:24:49.032635\n",
      "\n",
      "Evaluating: batch 9 ends at 22:24:49.254299\n",
      "\n",
      "Evaluating: batch 10 begins at 22:24:49.255605\n",
      "\n",
      "Evaluating: batch 10 ends at 22:24:49.475547\n",
      "\n",
      "Evaluating: batch 11 begins at 22:24:49.477808\n",
      "\n",
      "Evaluating: batch 11 ends at 22:24:49.697982\n",
      "\n",
      "Evaluating: batch 12 begins at 22:24:49.700006\n",
      "\n",
      "Evaluating: batch 12 ends at 22:24:49.919132\n",
      "\n",
      "Evaluating: batch 13 begins at 22:24:49.921132\n",
      "\n",
      "Evaluating: batch 13 ends at 22:24:50.140825\n",
      "\n",
      "Evaluating: batch 14 begins at 22:24:50.143296\n",
      "\n",
      "Evaluating: batch 14 ends at 22:24:50.360425\n",
      "\n",
      "Evaluating: batch 15 begins at 22:24:50.362777\n",
      "\n",
      "Evaluating: batch 15 ends at 22:24:50.581699\n",
      "\n",
      "Evaluating: batch 16 begins at 22:24:50.583780\n",
      "\n",
      "Evaluating: batch 16 ends at 22:24:50.801243\n",
      "\n",
      "Evaluating: batch 17 begins at 22:24:50.803830\n",
      "\n",
      "Evaluating: batch 17 ends at 22:24:51.021780\n",
      "\n",
      "Evaluating: batch 18 begins at 22:24:51.023735\n",
      "\n",
      "Evaluating: batch 18 ends at 22:24:51.243647\n",
      "\n",
      "Evaluating: batch 19 begins at 22:24:51.245974\n",
      "\n",
      "Evaluating: batch 19 ends at 22:24:51.463459\n",
      "\n",
      "Evaluating: batch 20 begins at 22:24:51.465220\n",
      "\n",
      "Evaluating: batch 20 ends at 22:24:51.686141\n",
      "\n",
      "Evaluating: batch 21 begins at 22:24:51.687760\n",
      "\n",
      "Evaluating: batch 21 ends at 22:24:51.909905\n",
      "\n",
      "Evaluating: batch 22 begins at 22:24:51.911927\n",
      "\n",
      "Evaluating: batch 22 ends at 22:24:52.127252\n",
      "\n",
      "Evaluating: batch 23 begins at 22:24:52.128498\n",
      "\n",
      "Evaluating: batch 23 ends at 22:24:52.345259\n",
      "\n",
      "Evaluating: batch 24 begins at 22:24:52.346609\n",
      "\n",
      "Evaluating: batch 24 ends at 22:24:52.562902\n",
      "\n",
      "Evaluating: batch 25 begins at 22:24:52.565445\n",
      "\n",
      "Evaluating: batch 25 ends at 22:24:52.785390\n",
      "\n",
      "Evaluating: batch 26 begins at 22:24:52.786898\n",
      "\n",
      "Evaluating: batch 26 ends at 22:24:53.005805\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.41529 to 0.39337, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 931ms/step - loss: 0.4254 - bce_dice_loss: 0.4254 - val_loss: 0.3934 - val_bce_dice_loss: 0.3934\n",
      "Epoch 9/25\n",
      "\n",
      "Training: batch 0 begins at 22:24:54.669745\n",
      "\n",
      "Training: batch 0 ends at 22:24:55.492916\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2400 - bce_dice_loss: 0.2400\n",
      "Training: batch 1 begins at 22:24:55.499981\n",
      "\n",
      "Training: batch 1 ends at 22:24:56.308595\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2865 - bce_dice_loss: 0.2865\n",
      "Training: batch 2 begins at 22:24:56.313138\n",
      "\n",
      "Training: batch 2 ends at 22:24:57.113677\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3153 - bce_dice_loss: 0.3153\n",
      "Training: batch 3 begins at 22:24:57.117583\n",
      "\n",
      "Training: batch 3 ends at 22:24:57.909981\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3454 - bce_dice_loss: 0.3454\n",
      "Training: batch 4 begins at 22:24:57.914582\n",
      "\n",
      "Training: batch 4 ends at 22:24:58.704790\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3955 - bce_dice_loss: 0.3955\n",
      "Training: batch 5 begins at 22:24:58.708874\n",
      "\n",
      "Training: batch 5 ends at 22:24:59.493850\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3911 - bce_dice_loss: 0.3911\n",
      "Training: batch 6 begins at 22:24:59.498512\n",
      "\n",
      "Training: batch 6 ends at 22:25:00.289869\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3795 - bce_dice_loss: 0.3795\n",
      "Training: batch 7 begins at 22:25:00.294383\n",
      "\n",
      "Training: batch 7 ends at 22:25:01.106146\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3730 - bce_dice_loss: 0.3730\n",
      "Training: batch 8 begins at 22:25:01.110351\n",
      "\n",
      "Training: batch 8 ends at 22:25:01.911362\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.3683 - bce_dice_loss: 0.3683\n",
      "Training: batch 9 begins at 22:25:01.915815\n",
      "\n",
      "Training: batch 9 ends at 22:25:02.707239\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3608 - bce_dice_loss: 0.3608\n",
      "Training: batch 10 begins at 22:25:02.711656\n",
      "\n",
      "Training: batch 10 ends at 22:25:03.515080\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3710 - bce_dice_loss: 0.3710\n",
      "Training: batch 11 begins at 22:25:03.519189\n",
      "\n",
      "Training: batch 11 ends at 22:25:04.310352\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3618 - bce_dice_loss: 0.3618\n",
      "Training: batch 12 begins at 22:25:04.314861\n",
      "\n",
      "Training: batch 12 ends at 22:25:05.105703\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3705 - bce_dice_loss: 0.3705\n",
      "Training: batch 13 begins at 22:25:05.110248\n",
      "\n",
      "Training: batch 13 ends at 22:25:05.897876\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.3633 - bce_dice_loss: 0.3633\n",
      "Training: batch 14 begins at 22:25:05.902081\n",
      "\n",
      "Training: batch 14 ends at 22:25:06.717289\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3758 - bce_dice_loss: 0.3758\n",
      "Training: batch 15 begins at 22:25:06.721218\n",
      "\n",
      "Training: batch 15 ends at 22:25:07.516357\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3820 - bce_dice_loss: 0.3820\n",
      "Training: batch 16 begins at 22:25:07.520547\n",
      "\n",
      "Training: batch 16 ends at 22:25:08.314691\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3941 - bce_dice_loss: 0.3941\n",
      "Training: batch 17 begins at 22:25:08.319112\n",
      "\n",
      "Training: batch 17 ends at 22:25:09.106526\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3869 - bce_dice_loss: 0.3869\n",
      "Training: batch 18 begins at 22:25:09.110550\n",
      "\n",
      "Training: batch 18 ends at 22:25:09.913145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/60 [========>.....................] - ETA: 32s - loss: 0.3853 - bce_dice_loss: 0.3853\n",
      "Training: batch 19 begins at 22:25:09.916760\n",
      "\n",
      "Training: batch 19 ends at 22:25:10.704852\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3802 - bce_dice_loss: 0.3802\n",
      "Training: batch 20 begins at 22:25:10.709595\n",
      "\n",
      "Training: batch 20 ends at 22:25:11.502509\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3727 - bce_dice_loss: 0.3727\n",
      "Training: batch 21 begins at 22:25:11.506598\n",
      "\n",
      "Training: batch 21 ends at 22:25:12.314793\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3777 - bce_dice_loss: 0.3777\n",
      "Training: batch 22 begins at 22:25:12.319057\n",
      "\n",
      "Training: batch 22 ends at 22:25:13.119203\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3822 - bce_dice_loss: 0.3822\n",
      "Training: batch 23 begins at 22:25:13.120149\n",
      "\n",
      "Training: batch 23 ends at 22:25:13.898804\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3798 - bce_dice_loss: 0.3798\n",
      "Training: batch 24 begins at 22:25:13.902544\n",
      "\n",
      "Training: batch 24 ends at 22:25:14.699285\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3840 - bce_dice_loss: 0.3840\n",
      "Training: batch 25 begins at 22:25:14.702645\n",
      "\n",
      "Training: batch 25 ends at 22:25:15.492708\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3871 - bce_dice_loss: 0.3871\n",
      "Training: batch 26 begins at 22:25:15.497448\n",
      "\n",
      "Training: batch 26 ends at 22:25:16.287534\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3862 - bce_dice_loss: 0.3862\n",
      "Training: batch 27 begins at 22:25:16.291817\n",
      "\n",
      "Training: batch 27 ends at 22:25:17.081506\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3839 - bce_dice_loss: 0.3839\n",
      "Training: batch 28 begins at 22:25:17.085628\n",
      "\n",
      "Training: batch 28 ends at 22:25:17.896477\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3803 - bce_dice_loss: 0.3803\n",
      "Training: batch 29 begins at 22:25:17.900237\n",
      "\n",
      "Training: batch 29 ends at 22:25:18.691763\n",
      "30/60 [==============>...............] - ETA: 23s - loss: 0.3749 - bce_dice_loss: 0.3749\n",
      "Training: batch 30 begins at 22:25:18.696419\n",
      "\n",
      "Training: batch 30 ends at 22:25:19.477870\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3707 - bce_dice_loss: 0.3707\n",
      "Training: batch 31 begins at 22:25:19.482512\n",
      "\n",
      "Training: batch 31 ends at 22:25:20.297406\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3738 - bce_dice_loss: 0.3738\n",
      "Training: batch 32 begins at 22:25:20.301412\n",
      "\n",
      "Training: batch 32 ends at 22:25:21.089794\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3688 - bce_dice_loss: 0.3688\n",
      "Training: batch 33 begins at 22:25:21.093940\n",
      "\n",
      "Training: batch 33 ends at 22:25:21.902408\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3718 - bce_dice_loss: 0.3718\n",
      "Training: batch 34 begins at 22:25:21.906536\n",
      "\n",
      "Training: batch 34 ends at 22:25:22.699443\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3731 - bce_dice_loss: 0.3731\n",
      "Training: batch 35 begins at 22:25:22.703491\n",
      "\n",
      "Training: batch 35 ends at 22:25:23.510529\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3740 - bce_dice_loss: 0.3740\n",
      "Training: batch 36 begins at 22:25:23.515923\n",
      "\n",
      "Training: batch 36 ends at 22:25:24.307021\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3723 - bce_dice_loss: 0.3723\n",
      "Training: batch 37 begins at 22:25:24.311221\n",
      "\n",
      "Training: batch 37 ends at 22:25:25.117203\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3762 - bce_dice_loss: 0.3762\n",
      "Training: batch 38 begins at 22:25:25.122073\n",
      "\n",
      "Training: batch 38 ends at 22:25:25.910887\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3860 - bce_dice_loss: 0.3860\n",
      "Training: batch 39 begins at 22:25:25.915622\n",
      "\n",
      "Training: batch 39 ends at 22:25:26.711331\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3888 - bce_dice_loss: 0.3888\n",
      "Training: batch 40 begins at 22:25:26.715495\n",
      "\n",
      "Training: batch 40 ends at 22:25:27.505231\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3930 - bce_dice_loss: 0.3930\n",
      "Training: batch 41 begins at 22:25:27.509532\n",
      "\n",
      "Training: batch 41 ends at 22:25:28.294749\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3928 - bce_dice_loss: 0.3928\n",
      "Training: batch 42 begins at 22:25:28.299280\n",
      "\n",
      "Training: batch 42 ends at 22:25:29.092553\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3910 - bce_dice_loss: 0.3910\n",
      "Training: batch 43 begins at 22:25:29.097408\n",
      "\n",
      "Training: batch 43 ends at 22:25:29.886975\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3892 - bce_dice_loss: 0.3892\n",
      "Training: batch 44 begins at 22:25:29.890506\n",
      "\n",
      "Training: batch 44 ends at 22:25:30.669526\n",
      "45/60 [=====================>........] - ETA: 11s - loss: 0.3888 - bce_dice_loss: 0.3888\n",
      "Training: batch 45 begins at 22:25:30.674190\n",
      "\n",
      "Training: batch 45 ends at 22:25:31.461437\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3864 - bce_dice_loss: 0.3864\n",
      "Training: batch 46 begins at 22:25:31.466763\n",
      "\n",
      "Training: batch 46 ends at 22:25:32.268201\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3885 - bce_dice_loss: 0.3885\n",
      "Training: batch 47 begins at 22:25:32.272494\n",
      "\n",
      "Training: batch 47 ends at 22:25:33.080716\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3863 - bce_dice_loss: 0.3863 \n",
      "Training: batch 48 begins at 22:25:33.084854\n",
      "\n",
      "Training: batch 48 ends at 22:25:33.875090\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3870 - bce_dice_loss: 0.3870\n",
      "Training: batch 49 begins at 22:25:33.879308\n",
      "\n",
      "Training: batch 49 ends at 22:25:34.670493\n",
      "50/60 [========================>.....] - ETA: 7s - loss: 0.3847 - bce_dice_loss: 0.3847\n",
      "Training: batch 50 begins at 22:25:34.674301\n",
      "\n",
      "Training: batch 50 ends at 22:25:35.495849\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3843 - bce_dice_loss: 0.3843\n",
      "Training: batch 51 begins at 22:25:35.500180\n",
      "\n",
      "Training: batch 51 ends at 22:25:36.281794\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3849 - bce_dice_loss: 0.3849\n",
      "Training: batch 52 begins at 22:25:36.286065\n",
      "\n",
      "Training: batch 52 ends at 22:25:37.081146\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3814 - bce_dice_loss: 0.3814\n",
      "Training: batch 53 begins at 22:25:37.085582\n",
      "\n",
      "Training: batch 53 ends at 22:25:37.897638\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3798 - bce_dice_loss: 0.3798\n",
      "Training: batch 54 begins at 22:25:37.901643\n",
      "\n",
      "Training: batch 54 ends at 22:25:38.685391\n",
      "55/60 [==========================>...] - ETA: 3s - loss: 0.3802 - bce_dice_loss: 0.3802\n",
      "Training: batch 55 begins at 22:25:38.689288\n",
      "\n",
      "Training: batch 55 ends at 22:25:39.484976\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3807 - bce_dice_loss: 0.3807\n",
      "Training: batch 56 begins at 22:25:39.488298\n",
      "\n",
      "Training: batch 56 ends at 22:25:40.281744\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3832 - bce_dice_loss: 0.3832\n",
      "Training: batch 57 begins at 22:25:40.285152\n",
      "\n",
      "Training: batch 57 ends at 22:25:41.079550\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3843 - bce_dice_loss: 0.3843\n",
      "Training: batch 58 begins at 22:25:41.083357\n",
      "\n",
      "Training: batch 58 ends at 22:25:41.900247\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3888 - bce_dice_loss: 0.3888\n",
      "Training: batch 59 begins at 22:25:41.904586\n",
      "\n",
      "Training: batch 59 ends at 22:25:42.692282\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3864 - bce_dice_loss: 0.3864\n",
      "Evaluating: batch 0 begins at 22:25:42.726512\n",
      "\n",
      "Evaluating: batch 0 ends at 22:25:42.995451\n",
      "\n",
      "Evaluating: batch 1 begins at 22:25:42.996736\n",
      "\n",
      "Evaluating: batch 1 ends at 22:25:43.209631\n",
      "\n",
      "Evaluating: batch 2 begins at 22:25:43.210830\n",
      "\n",
      "Evaluating: batch 2 ends at 22:25:43.422846\n",
      "\n",
      "Evaluating: batch 3 begins at 22:25:43.424054\n",
      "\n",
      "Evaluating: batch 3 ends at 22:25:43.639187\n",
      "\n",
      "Evaluating: batch 4 begins at 22:25:43.640399\n",
      "\n",
      "Evaluating: batch 4 ends at 22:25:43.862656\n",
      "\n",
      "Evaluating: batch 5 begins at 22:25:43.864813\n",
      "\n",
      "Evaluating: batch 5 ends at 22:25:44.087665\n",
      "\n",
      "Evaluating: batch 6 begins at 22:25:44.088685\n",
      "\n",
      "Evaluating: batch 6 ends at 22:25:44.311194\n",
      "\n",
      "Evaluating: batch 7 begins at 22:25:44.313622\n",
      "\n",
      "Evaluating: batch 7 ends at 22:25:44.531147\n",
      "\n",
      "Evaluating: batch 8 begins at 22:25:44.533242\n",
      "\n",
      "Evaluating: batch 8 ends at 22:25:44.752186\n",
      "\n",
      "Evaluating: batch 9 begins at 22:25:44.754194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 9 ends at 22:25:44.975400\n",
      "\n",
      "Evaluating: batch 10 begins at 22:25:44.976485\n",
      "\n",
      "Evaluating: batch 10 ends at 22:25:45.196158\n",
      "\n",
      "Evaluating: batch 11 begins at 22:25:45.198762\n",
      "\n",
      "Evaluating: batch 11 ends at 22:25:45.416160\n",
      "\n",
      "Evaluating: batch 12 begins at 22:25:45.418081\n",
      "\n",
      "Evaluating: batch 12 ends at 22:25:45.637339\n",
      "\n",
      "Evaluating: batch 13 begins at 22:25:45.638649\n",
      "\n",
      "Evaluating: batch 13 ends at 22:25:45.860630\n",
      "\n",
      "Evaluating: batch 14 begins at 22:25:45.862044\n",
      "\n",
      "Evaluating: batch 14 ends at 22:25:46.080742\n",
      "\n",
      "Evaluating: batch 15 begins at 22:25:46.082521\n",
      "\n",
      "Evaluating: batch 15 ends at 22:25:46.302528\n",
      "\n",
      "Evaluating: batch 16 begins at 22:25:46.303970\n",
      "\n",
      "Evaluating: batch 16 ends at 22:25:46.524508\n",
      "\n",
      "Evaluating: batch 17 begins at 22:25:46.525835\n",
      "\n",
      "Evaluating: batch 17 ends at 22:25:46.746267\n",
      "\n",
      "Evaluating: batch 18 begins at 22:25:46.747710\n",
      "\n",
      "Evaluating: batch 18 ends at 22:25:46.967654\n",
      "\n",
      "Evaluating: batch 19 begins at 22:25:46.968655\n",
      "\n",
      "Evaluating: batch 19 ends at 22:25:47.185383\n",
      "\n",
      "Evaluating: batch 20 begins at 22:25:47.186835\n",
      "\n",
      "Evaluating: batch 20 ends at 22:25:47.402032\n",
      "\n",
      "Evaluating: batch 21 begins at 22:25:47.403585\n",
      "\n",
      "Evaluating: batch 21 ends at 22:25:47.622350\n",
      "\n",
      "Evaluating: batch 22 begins at 22:25:47.624890\n",
      "\n",
      "Evaluating: batch 22 ends at 22:25:47.839822\n",
      "\n",
      "Evaluating: batch 23 begins at 22:25:47.841122\n",
      "\n",
      "Evaluating: batch 23 ends at 22:25:48.058685\n",
      "\n",
      "Evaluating: batch 24 begins at 22:25:48.060023\n",
      "\n",
      "Evaluating: batch 24 ends at 22:25:48.275950\n",
      "\n",
      "Evaluating: batch 25 begins at 22:25:48.277587\n",
      "\n",
      "Evaluating: batch 25 ends at 22:25:48.494910\n",
      "\n",
      "Evaluating: batch 26 begins at 22:25:48.497366\n",
      "\n",
      "Evaluating: batch 26 ends at 22:25:48.715811\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39337\n",
      "60/60 [==============================] - 54s 902ms/step - loss: 0.3864 - bce_dice_loss: 0.3864 - val_loss: 0.3960 - val_bce_dice_loss: 0.3960\n",
      "Epoch 10/25\n",
      "\n",
      "Training: batch 0 begins at 22:25:48.744348\n",
      "\n",
      "Training: batch 0 ends at 22:25:49.536565\n",
      " 1/60 [..............................] - ETA: 46s - loss: 0.3551 - bce_dice_loss: 0.3551\n",
      "Training: batch 1 begins at 22:25:49.540835\n",
      "\n",
      "Training: batch 1 ends at 22:25:50.367785\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.4332 - bce_dice_loss: 0.4332\n",
      "Training: batch 2 begins at 22:25:50.373543\n",
      "\n",
      "Training: batch 2 ends at 22:25:51.167766\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4068 - bce_dice_loss: 0.4068\n",
      "Training: batch 3 begins at 22:25:51.171997\n",
      "\n",
      "Training: batch 3 ends at 22:25:51.992717\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.4574 - bce_dice_loss: 0.4574\n",
      "Training: batch 4 begins at 22:25:51.997565\n",
      "\n",
      "Training: batch 4 ends at 22:25:52.781031\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4266 - bce_dice_loss: 0.4266\n",
      "Training: batch 5 begins at 22:25:52.785410\n",
      "\n",
      "Training: batch 5 ends at 22:25:53.577292\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4030 - bce_dice_loss: 0.4030\n",
      "Training: batch 6 begins at 22:25:53.581388\n",
      "\n",
      "Training: batch 6 ends at 22:25:54.374648\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.4063 - bce_dice_loss: 0.4063\n",
      "Training: batch 7 begins at 22:25:54.379060\n",
      "\n",
      "Training: batch 7 ends at 22:25:55.193194\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.4064 - bce_dice_loss: 0.4064\n",
      "Training: batch 8 begins at 22:25:55.197582\n",
      "\n",
      "Training: batch 8 ends at 22:25:55.997425\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3861 - bce_dice_loss: 0.3861\n",
      "Training: batch 9 begins at 22:25:56.001105\n",
      "\n",
      "Training: batch 9 ends at 22:25:56.795065\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3789 - bce_dice_loss: 0.3789\n",
      "Training: batch 10 begins at 22:25:56.798643\n",
      "\n",
      "Training: batch 10 ends at 22:25:57.589296\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3818 - bce_dice_loss: 0.3818\n",
      "Training: batch 11 begins at 22:25:57.593524\n",
      "\n",
      "Training: batch 11 ends at 22:25:58.402323\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3948 - bce_dice_loss: 0.3948\n",
      "Training: batch 12 begins at 22:25:58.406563\n",
      "\n",
      "Training: batch 12 ends at 22:25:59.197143\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3879 - bce_dice_loss: 0.3879\n",
      "Training: batch 13 begins at 22:25:59.201864\n",
      "\n",
      "Training: batch 13 ends at 22:25:59.996704\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3911 - bce_dice_loss: 0.3911\n",
      "Training: batch 14 begins at 22:26:00.000231\n",
      "\n",
      "Training: batch 14 ends at 22:26:00.798368\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3882 - bce_dice_loss: 0.3882\n",
      "Training: batch 15 begins at 22:26:00.801897\n",
      "\n",
      "Training: batch 15 ends at 22:26:01.592418\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3842 - bce_dice_loss: 0.3842\n",
      "Training: batch 16 begins at 22:26:01.596958\n",
      "\n",
      "Training: batch 16 ends at 22:26:02.430595\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3857 - bce_dice_loss: 0.3857\n",
      "Training: batch 17 begins at 22:26:02.435042\n",
      "\n",
      "Training: batch 17 ends at 22:26:03.230404\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3802 - bce_dice_loss: 0.3802\n",
      "Training: batch 18 begins at 22:26:03.234536\n",
      "\n",
      "Training: batch 18 ends at 22:26:04.055182\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3785 - bce_dice_loss: 0.3785\n",
      "Training: batch 19 begins at 22:26:04.059735\n",
      "\n",
      "Training: batch 19 ends at 22:26:04.850140\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3922 - bce_dice_loss: 0.3922\n",
      "Training: batch 20 begins at 22:26:04.854190\n",
      "\n",
      "Training: batch 20 ends at 22:26:05.635604\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4020 - bce_dice_loss: 0.4020\n",
      "Training: batch 21 begins at 22:26:05.639751\n",
      "\n",
      "Training: batch 21 ends at 22:26:06.431609\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4045 - bce_dice_loss: 0.4045\n",
      "Training: batch 22 begins at 22:26:06.435981\n",
      "\n",
      "Training: batch 22 ends at 22:26:07.228890\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4054 - bce_dice_loss: 0.4054\n",
      "Training: batch 23 begins at 22:26:07.233033\n",
      "\n",
      "Training: batch 23 ends at 22:26:08.026499\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.4064 - bce_dice_loss: 0.4064\n",
      "Training: batch 24 begins at 22:26:08.031140\n",
      "\n",
      "Training: batch 24 ends at 22:26:08.816826\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4039 - bce_dice_loss: 0.4039\n",
      "Training: batch 25 begins at 22:26:08.820354\n",
      "\n",
      "Training: batch 25 ends at 22:26:09.617383\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4015 - bce_dice_loss: 0.4015\n",
      "Training: batch 26 begins at 22:26:09.621527\n",
      "\n",
      "Training: batch 26 ends at 22:26:10.440500\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4012 - bce_dice_loss: 0.4012\n",
      "Training: batch 27 begins at 22:26:10.445356\n",
      "\n",
      "Training: batch 27 ends at 22:26:11.236136\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3943 - bce_dice_loss: 0.3943\n",
      "Training: batch 28 begins at 22:26:11.240449\n",
      "\n",
      "Training: batch 28 ends at 22:26:12.038931\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3858 - bce_dice_loss: 0.3858\n",
      "Training: batch 29 begins at 22:26:12.043247\n",
      "\n",
      "Training: batch 29 ends at 22:26:12.834383\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3853 - bce_dice_loss: 0.3853\n",
      "Training: batch 30 begins at 22:26:12.838681\n",
      "\n",
      "Training: batch 30 ends at 22:26:13.656479\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3850 - bce_dice_loss: 0.3850\n",
      "Training: batch 31 begins at 22:26:13.661647\n",
      "\n",
      "Training: batch 31 ends at 22:26:14.451037\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3874 - bce_dice_loss: 0.3874\n",
      "Training: batch 32 begins at 22:26:14.454705\n",
      "\n",
      "Training: batch 32 ends at 22:26:15.266851\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3866 - bce_dice_loss: 0.3866\n",
      "Training: batch 33 begins at 22:26:15.269886\n",
      "\n",
      "Training: batch 33 ends at 22:26:16.062863\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3907 - bce_dice_loss: 0.3907\n",
      "Training: batch 34 begins at 22:26:16.066744\n",
      "\n",
      "Training: batch 34 ends at 22:26:16.854481\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3870 - bce_dice_loss: 0.3870\n",
      "Training: batch 35 begins at 22:26:16.858848\n",
      "\n",
      "Training: batch 35 ends at 22:26:17.665450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/60 [=================>............] - ETA: 19s - loss: 0.3848 - bce_dice_loss: 0.3848\n",
      "Training: batch 36 begins at 22:26:17.669940\n",
      "\n",
      "Training: batch 36 ends at 22:26:18.461165\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3919 - bce_dice_loss: 0.3919\n",
      "Training: batch 37 begins at 22:26:18.465329\n",
      "\n",
      "Training: batch 37 ends at 22:26:19.246941\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3915 - bce_dice_loss: 0.3915\n",
      "Training: batch 38 begins at 22:26:19.251273\n",
      "\n",
      "Training: batch 38 ends at 22:26:20.067257\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3892 - bce_dice_loss: 0.3892\n",
      "Training: batch 39 begins at 22:26:20.072721\n",
      "\n",
      "Training: batch 39 ends at 22:26:20.864275\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3877 - bce_dice_loss: 0.3877\n",
      "Training: batch 40 begins at 22:26:20.868019\n",
      "\n",
      "Training: batch 40 ends at 22:26:21.657182\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3870 - bce_dice_loss: 0.3870\n",
      "Training: batch 41 begins at 22:26:21.662406\n",
      "\n",
      "Training: batch 41 ends at 22:26:22.464983\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3888 - bce_dice_loss: 0.3888\n",
      "Training: batch 42 begins at 22:26:22.468236\n",
      "\n",
      "Training: batch 42 ends at 22:26:23.260094\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3899 - bce_dice_loss: 0.3899\n",
      "Training: batch 43 begins at 22:26:23.264250\n",
      "\n",
      "Training: batch 43 ends at 22:26:24.053886\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3921 - bce_dice_loss: 0.3921\n",
      "Training: batch 44 begins at 22:26:24.058229\n",
      "\n",
      "Training: batch 44 ends at 22:26:24.849568\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3907 - bce_dice_loss: 0.3907\n",
      "Training: batch 45 begins at 22:26:24.853532\n",
      "\n",
      "Training: batch 45 ends at 22:26:25.656202\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3886 - bce_dice_loss: 0.3886\n",
      "Training: batch 46 begins at 22:26:25.661327\n",
      "\n",
      "Training: batch 46 ends at 22:26:26.452739\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3902 - bce_dice_loss: 0.3902\n",
      "Training: batch 47 begins at 22:26:26.457129\n",
      "\n",
      "Training: batch 47 ends at 22:26:27.257079\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3916 - bce_dice_loss: 0.3916 \n",
      "Training: batch 48 begins at 22:26:27.262573\n",
      "\n",
      "Training: batch 48 ends at 22:26:28.065613\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3927 - bce_dice_loss: 0.3927\n",
      "Training: batch 49 begins at 22:26:28.069276\n",
      "\n",
      "Training: batch 49 ends at 22:26:28.872741\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3945 - bce_dice_loss: 0.3945\n",
      "Training: batch 50 begins at 22:26:28.877078\n",
      "\n",
      "Training: batch 50 ends at 22:26:29.662248\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3927 - bce_dice_loss: 0.3927\n",
      "Training: batch 51 begins at 22:26:29.666142\n",
      "\n",
      "Training: batch 51 ends at 22:26:30.463074\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3879 - bce_dice_loss: 0.3879\n",
      "Training: batch 52 begins at 22:26:30.467442\n",
      "\n",
      "Training: batch 52 ends at 22:26:31.277055\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3878 - bce_dice_loss: 0.3878\n",
      "Training: batch 53 begins at 22:26:31.280933\n",
      "\n",
      "Training: batch 53 ends at 22:26:32.075280\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3926 - bce_dice_loss: 0.3926\n",
      "Training: batch 54 begins at 22:26:32.079769\n",
      "\n",
      "Training: batch 54 ends at 22:26:32.866595\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3893 - bce_dice_loss: 0.3893\n",
      "Training: batch 55 begins at 22:26:32.871105\n",
      "\n",
      "Training: batch 55 ends at 22:26:33.665935\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3888 - bce_dice_loss: 0.3888\n",
      "Training: batch 56 begins at 22:26:33.670292\n",
      "\n",
      "Training: batch 56 ends at 22:26:34.469216\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3876 - bce_dice_loss: 0.3876\n",
      "Training: batch 57 begins at 22:26:34.473696\n",
      "\n",
      "Training: batch 57 ends at 22:26:35.264099\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3871 - bce_dice_loss: 0.3871\n",
      "Training: batch 58 begins at 22:26:35.267868\n",
      "\n",
      "Training: batch 58 ends at 22:26:36.052054\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3864 - bce_dice_loss: 0.3864\n",
      "Training: batch 59 begins at 22:26:36.056409\n",
      "\n",
      "Training: batch 59 ends at 22:26:36.844852\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3874 - bce_dice_loss: 0.3874\n",
      "Evaluating: batch 0 begins at 22:26:36.876538\n",
      "\n",
      "Evaluating: batch 0 ends at 22:26:37.141898\n",
      "\n",
      "Evaluating: batch 1 begins at 22:26:37.143059\n",
      "\n",
      "Evaluating: batch 1 ends at 22:26:37.355616\n",
      "\n",
      "Evaluating: batch 2 begins at 22:26:37.356732\n",
      "\n",
      "Evaluating: batch 2 ends at 22:26:37.572526\n",
      "\n",
      "Evaluating: batch 3 begins at 22:26:37.573772\n",
      "\n",
      "Evaluating: batch 3 ends at 22:26:37.791954\n",
      "\n",
      "Evaluating: batch 4 begins at 22:26:37.794425\n",
      "\n",
      "Evaluating: batch 4 ends at 22:26:38.011462\n",
      "\n",
      "Evaluating: batch 5 begins at 22:26:38.013445\n",
      "\n",
      "Evaluating: batch 5 ends at 22:26:38.231066\n",
      "\n",
      "Evaluating: batch 6 begins at 22:26:38.233353\n",
      "\n",
      "Evaluating: batch 6 ends at 22:26:38.456510\n",
      "\n",
      "Evaluating: batch 7 begins at 22:26:38.457565\n",
      "\n",
      "Evaluating: batch 7 ends at 22:26:38.676984\n",
      "\n",
      "Evaluating: batch 8 begins at 22:26:38.679480\n",
      "\n",
      "Evaluating: batch 8 ends at 22:26:38.900745\n",
      "\n",
      "Evaluating: batch 9 begins at 22:26:38.902324\n",
      "\n",
      "Evaluating: batch 9 ends at 22:26:39.122835\n",
      "\n",
      "Evaluating: batch 10 begins at 22:26:39.124625\n",
      "\n",
      "Evaluating: batch 10 ends at 22:26:39.343121\n",
      "\n",
      "Evaluating: batch 11 begins at 22:26:39.345656\n",
      "\n",
      "Evaluating: batch 11 ends at 22:26:39.565625\n",
      "\n",
      "Evaluating: batch 12 begins at 22:26:39.567435\n",
      "\n",
      "Evaluating: batch 12 ends at 22:26:39.790880\n",
      "\n",
      "Evaluating: batch 13 begins at 22:26:39.792137\n",
      "\n",
      "Evaluating: batch 13 ends at 22:26:40.013612\n",
      "\n",
      "Evaluating: batch 14 begins at 22:26:40.015984\n",
      "\n",
      "Evaluating: batch 14 ends at 22:26:40.230283\n",
      "\n",
      "Evaluating: batch 15 begins at 22:26:40.232682\n",
      "\n",
      "Evaluating: batch 15 ends at 22:26:40.454187\n",
      "\n",
      "Evaluating: batch 16 begins at 22:26:40.456715\n",
      "\n",
      "Evaluating: batch 16 ends at 22:26:40.675655\n",
      "\n",
      "Evaluating: batch 17 begins at 22:26:40.677318\n",
      "\n",
      "Evaluating: batch 17 ends at 22:26:40.898894\n",
      "\n",
      "Evaluating: batch 18 begins at 22:26:40.900565\n",
      "\n",
      "Evaluating: batch 18 ends at 22:26:41.117848\n",
      "\n",
      "Evaluating: batch 19 begins at 22:26:41.120124\n",
      "\n",
      "Evaluating: batch 19 ends at 22:26:41.335259\n",
      "\n",
      "Evaluating: batch 20 begins at 22:26:41.336836\n",
      "\n",
      "Evaluating: batch 20 ends at 22:26:41.554647\n",
      "\n",
      "Evaluating: batch 21 begins at 22:26:41.556963\n",
      "\n",
      "Evaluating: batch 21 ends at 22:26:41.775366\n",
      "\n",
      "Evaluating: batch 22 begins at 22:26:41.777217\n",
      "\n",
      "Evaluating: batch 22 ends at 22:26:42.000729\n",
      "\n",
      "Evaluating: batch 23 begins at 22:26:42.002132\n",
      "\n",
      "Evaluating: batch 23 ends at 22:26:42.220584\n",
      "\n",
      "Evaluating: batch 24 begins at 22:26:42.222975\n",
      "\n",
      "Evaluating: batch 24 ends at 22:26:42.440128\n",
      "\n",
      "Evaluating: batch 25 begins at 22:26:42.442601\n",
      "\n",
      "Evaluating: batch 25 ends at 22:26:42.657571\n",
      "\n",
      "Evaluating: batch 26 begins at 22:26:42.659918\n",
      "\n",
      "Evaluating: batch 26 ends at 22:26:42.879873\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39337\n",
      "60/60 [==============================] - 54s 904ms/step - loss: 0.3874 - bce_dice_loss: 0.3874 - val_loss: 0.3971 - val_bce_dice_loss: 0.3971\n",
      "Epoch 11/25\n",
      "\n",
      "Training: batch 0 begins at 22:26:42.906331\n",
      "\n",
      "Training: batch 0 ends at 22:26:43.699185\n",
      " 1/60 [..............................] - ETA: 46s - loss: 0.5373 - bce_dice_loss: 0.5373\n",
      "Training: batch 1 begins at 22:26:43.705025\n",
      "\n",
      "Training: batch 1 ends at 22:26:44.503583\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3781 - bce_dice_loss: 0.3781\n",
      "Training: batch 2 begins at 22:26:44.508144\n",
      "\n",
      "Training: batch 2 ends at 22:26:45.322895\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3552 - bce_dice_loss: 0.3552\n",
      "Training: batch 3 begins at 22:26:45.327742\n",
      "\n",
      "Training: batch 3 ends at 22:26:46.120070\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3425 - bce_dice_loss: 0.3425\n",
      "Training: batch 4 begins at 22:26:46.124730\n",
      "\n",
      "Training: batch 4 ends at 22:26:46.914430\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3686 - bce_dice_loss: 0.3686\n",
      "Training: batch 5 begins at 22:26:46.918598\n",
      "\n",
      "Training: batch 5 ends at 22:26:47.704791\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3585 - bce_dice_loss: 0.3585\n",
      "Training: batch 6 begins at 22:26:47.710144\n",
      "\n",
      "Training: batch 6 ends at 22:26:48.505232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3653 - bce_dice_loss: 0.3653\n",
      "Training: batch 7 begins at 22:26:48.509582\n",
      "\n",
      "Training: batch 7 ends at 22:26:49.300771\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3467 - bce_dice_loss: 0.3467\n",
      "Training: batch 8 begins at 22:26:49.305059\n",
      "\n",
      "Training: batch 8 ends at 22:26:50.117582\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.3375 - bce_dice_loss: 0.3375\n",
      "Training: batch 9 begins at 22:26:50.122317\n",
      "\n",
      "Training: batch 9 ends at 22:26:50.914157\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3421 - bce_dice_loss: 0.3421\n",
      "Training: batch 10 begins at 22:26:50.917867\n",
      "\n",
      "Training: batch 10 ends at 22:26:51.710498\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3337 - bce_dice_loss: 0.3337\n",
      "Training: batch 11 begins at 22:26:51.713915\n",
      "\n",
      "Training: batch 11 ends at 22:26:52.514871\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3384 - bce_dice_loss: 0.3384\n",
      "Training: batch 12 begins at 22:26:52.518946\n",
      "\n",
      "Training: batch 12 ends at 22:26:53.310947\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3630 - bce_dice_loss: 0.3630\n",
      "Training: batch 13 begins at 22:26:53.314622\n",
      "\n",
      "Training: batch 13 ends at 22:26:54.136018\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.3648 - bce_dice_loss: 0.3648\n",
      "Training: batch 14 begins at 22:26:54.140061\n",
      "\n",
      "Training: batch 14 ends at 22:26:54.931210\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3679 - bce_dice_loss: 0.3679\n",
      "Training: batch 15 begins at 22:26:54.935293\n",
      "\n",
      "Training: batch 15 ends at 22:26:55.725289\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3571 - bce_dice_loss: 0.3571\n",
      "Training: batch 16 begins at 22:26:55.728066\n",
      "\n",
      "Training: batch 16 ends at 22:26:56.518794\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3454 - bce_dice_loss: 0.3454\n",
      "Training: batch 17 begins at 22:26:56.522907\n",
      "\n",
      "Training: batch 17 ends at 22:26:57.320776\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3415 - bce_dice_loss: 0.3415\n",
      "Training: batch 18 begins at 22:26:57.324856\n",
      "\n",
      "Training: batch 18 ends at 22:26:58.137976\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.3373 - bce_dice_loss: 0.3373\n",
      "Training: batch 19 begins at 22:26:58.142376\n",
      "\n",
      "Training: batch 19 ends at 22:26:58.930089\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3460 - bce_dice_loss: 0.3460\n",
      "Training: batch 20 begins at 22:26:58.934531\n",
      "\n",
      "Training: batch 20 ends at 22:26:59.722528\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 21 begins at 22:26:59.726708\n",
      "\n",
      "Training: batch 21 ends at 22:27:00.542127\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3481 - bce_dice_loss: 0.3481\n",
      "Training: batch 22 begins at 22:27:00.546530\n",
      "\n",
      "Training: batch 22 ends at 22:27:01.343703\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3572 - bce_dice_loss: 0.3572\n",
      "Training: batch 23 begins at 22:27:01.348217\n",
      "\n",
      "Training: batch 23 ends at 22:27:02.194258\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3539 - bce_dice_loss: 0.3539\n",
      "Training: batch 24 begins at 22:27:02.197383\n",
      "\n",
      "Training: batch 24 ends at 22:27:03.017882\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3566 - bce_dice_loss: 0.3566\n",
      "Training: batch 25 begins at 22:27:03.021325\n",
      "\n",
      "Training: batch 25 ends at 22:27:03.925705\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3553 - bce_dice_loss: 0.3553\n",
      "Training: batch 26 begins at 22:27:03.928234\n",
      "\n",
      "Training: batch 26 ends at 22:27:04.714196\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3572 - bce_dice_loss: 0.3572\n",
      "Training: batch 27 begins at 22:27:04.717667\n",
      "\n",
      "Training: batch 27 ends at 22:27:05.565040\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3594 - bce_dice_loss: 0.3594\n",
      "Training: batch 28 begins at 22:27:05.569469\n",
      "\n",
      "Training: batch 28 ends at 22:27:06.364834\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3613 - bce_dice_loss: 0.3613\n",
      "Training: batch 29 begins at 22:27:06.368304\n",
      "\n",
      "Training: batch 29 ends at 22:27:07.151250\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3579 - bce_dice_loss: 0.3579\n",
      "Training: batch 30 begins at 22:27:07.154628\n",
      "\n",
      "Training: batch 30 ends at 22:27:07.945260\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3588 - bce_dice_loss: 0.3588\n",
      "Training: batch 31 begins at 22:27:07.947756\n",
      "\n",
      "Training: batch 31 ends at 22:27:08.854349\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3571 - bce_dice_loss: 0.3571\n",
      "Training: batch 32 begins at 22:27:08.856875\n",
      "\n",
      "Training: batch 32 ends at 22:27:09.731774\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3522 - bce_dice_loss: 0.3522\n",
      "Training: batch 33 begins at 22:27:09.735749\n",
      "\n",
      "Training: batch 33 ends at 22:27:10.638337\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3485 - bce_dice_loss: 0.3485\n",
      "Training: batch 34 begins at 22:27:10.643525\n",
      "\n",
      "Training: batch 34 ends at 22:27:11.530375\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3498 - bce_dice_loss: 0.3498\n",
      "Training: batch 35 begins at 22:27:11.533452\n",
      "\n",
      "Training: batch 35 ends at 22:27:12.401454\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3496 - bce_dice_loss: 0.3496\n",
      "Training: batch 36 begins at 22:27:12.404301\n",
      "\n",
      "Training: batch 36 ends at 22:27:13.235829\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3503 - bce_dice_loss: 0.3503\n",
      "Training: batch 37 begins at 22:27:13.242855\n",
      "\n",
      "Training: batch 37 ends at 22:27:14.073056\n",
      "38/60 [==================>...........] - ETA: 18s - loss: 0.3535 - bce_dice_loss: 0.3535\n",
      "Training: batch 38 begins at 22:27:14.076878\n",
      "\n",
      "Training: batch 38 ends at 22:27:14.889721\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3560 - bce_dice_loss: 0.3560\n",
      "Training: batch 39 begins at 22:27:14.892905\n",
      "\n",
      "Training: batch 39 ends at 22:27:15.708745\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3566 - bce_dice_loss: 0.3566\n",
      "Training: batch 40 begins at 22:27:15.713073\n",
      "\n",
      "Training: batch 40 ends at 22:27:16.509401\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3536 - bce_dice_loss: 0.3536\n",
      "Training: batch 41 begins at 22:27:16.511977\n",
      "\n",
      "Training: batch 41 ends at 22:27:17.320118\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3593 - bce_dice_loss: 0.3593\n",
      "Training: batch 42 begins at 22:27:17.323089\n",
      "\n",
      "Training: batch 42 ends at 22:27:18.117476\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3605 - bce_dice_loss: 0.3605\n",
      "Training: batch 43 begins at 22:27:18.120710\n",
      "\n",
      "Training: batch 43 ends at 22:27:18.935698\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3604 - bce_dice_loss: 0.3604\n",
      "Training: batch 44 begins at 22:27:18.938927\n",
      "\n",
      "Training: batch 44 ends at 22:27:19.733921\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3637 - bce_dice_loss: 0.3637\n",
      "Training: batch 45 begins at 22:27:19.738546\n",
      "\n",
      "Training: batch 45 ends at 22:27:20.532726\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3684 - bce_dice_loss: 0.3684\n",
      "Training: batch 46 begins at 22:27:20.537481\n",
      "\n",
      "Training: batch 46 ends at 22:27:21.339745\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3673 - bce_dice_loss: 0.3673\n",
      "Training: batch 47 begins at 22:27:21.342450\n",
      "\n",
      "Training: batch 47 ends at 22:27:22.162890\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3689 - bce_dice_loss: 0.3689 \n",
      "Training: batch 48 begins at 22:27:22.167670\n",
      "\n",
      "Training: batch 48 ends at 22:27:22.987813\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.3686 - bce_dice_loss: 0.3686\n",
      "Training: batch 49 begins at 22:27:22.992083\n",
      "\n",
      "Training: batch 49 ends at 22:27:23.797502\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3679 - bce_dice_loss: 0.3679\n",
      "Training: batch 50 begins at 22:27:23.800962\n",
      "\n",
      "Training: batch 50 ends at 22:27:24.630145\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3699 - bce_dice_loss: 0.3699\n",
      "Training: batch 51 begins at 22:27:24.634314\n",
      "\n",
      "Training: batch 51 ends at 22:27:25.436383\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3736 - bce_dice_loss: 0.3736\n",
      "Training: batch 52 begins at 22:27:25.439633\n",
      "\n",
      "Training: batch 52 ends at 22:27:26.290003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3722 - bce_dice_loss: 0.3722\n",
      "Training: batch 53 begins at 22:27:26.293201\n",
      "\n",
      "Training: batch 53 ends at 22:27:27.126457\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3697 - bce_dice_loss: 0.3697\n",
      "Training: batch 54 begins at 22:27:27.131127\n",
      "\n",
      "Training: batch 54 ends at 22:27:27.972329\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3695 - bce_dice_loss: 0.3695\n",
      "Training: batch 55 begins at 22:27:27.975379\n",
      "\n",
      "Training: batch 55 ends at 22:27:28.845235\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3727 - bce_dice_loss: 0.3727\n",
      "Training: batch 56 begins at 22:27:28.849161\n",
      "\n",
      "Training: batch 56 ends at 22:27:29.751180\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3717 - bce_dice_loss: 0.3717\n",
      "Training: batch 57 begins at 22:27:29.754034\n",
      "\n",
      "Training: batch 57 ends at 22:27:30.631855\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3710 - bce_dice_loss: 0.3710\n",
      "Training: batch 58 begins at 22:27:30.634986\n",
      "\n",
      "Training: batch 58 ends at 22:27:31.502180\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3726 - bce_dice_loss: 0.3726\n",
      "Training: batch 59 begins at 22:27:31.505130\n",
      "\n",
      "Training: batch 59 ends at 22:27:32.392391\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3721 - bce_dice_loss: 0.3721\n",
      "Evaluating: batch 0 begins at 22:27:32.432795\n",
      "\n",
      "Evaluating: batch 0 ends at 22:27:32.724777\n",
      "\n",
      "Evaluating: batch 1 begins at 22:27:32.725975\n",
      "\n",
      "Evaluating: batch 1 ends at 22:27:32.943407\n",
      "\n",
      "Evaluating: batch 2 begins at 22:27:32.944959\n",
      "\n",
      "Evaluating: batch 2 ends at 22:27:33.171942\n",
      "\n",
      "Evaluating: batch 3 begins at 22:27:33.173213\n",
      "\n",
      "Evaluating: batch 3 ends at 22:27:33.393577\n",
      "\n",
      "Evaluating: batch 4 begins at 22:27:33.395271\n",
      "\n",
      "Evaluating: batch 4 ends at 22:27:33.618347\n",
      "\n",
      "Evaluating: batch 5 begins at 22:27:33.620941\n",
      "\n",
      "Evaluating: batch 5 ends at 22:27:33.843035\n",
      "\n",
      "Evaluating: batch 6 begins at 22:27:33.844326\n",
      "\n",
      "Evaluating: batch 6 ends at 22:27:34.067232\n",
      "\n",
      "Evaluating: batch 7 begins at 22:27:34.068520\n",
      "\n",
      "Evaluating: batch 7 ends at 22:27:34.289274\n",
      "\n",
      "Evaluating: batch 8 begins at 22:27:34.292467\n",
      "\n",
      "Evaluating: batch 8 ends at 22:27:34.511483\n",
      "\n",
      "Evaluating: batch 9 begins at 22:27:34.512757\n",
      "\n",
      "Evaluating: batch 9 ends at 22:27:34.735031\n",
      "\n",
      "Evaluating: batch 10 begins at 22:27:34.736767\n",
      "\n",
      "Evaluating: batch 10 ends at 22:27:34.957024\n",
      "\n",
      "Evaluating: batch 11 begins at 22:27:34.958427\n",
      "\n",
      "Evaluating: batch 11 ends at 22:27:35.180330\n",
      "\n",
      "Evaluating: batch 12 begins at 22:27:35.181690\n",
      "\n",
      "Evaluating: batch 12 ends at 22:27:35.403807\n",
      "\n",
      "Evaluating: batch 13 begins at 22:27:35.405196\n",
      "\n",
      "Evaluating: batch 13 ends at 22:27:35.626696\n",
      "\n",
      "Evaluating: batch 14 begins at 22:27:35.627996\n",
      "\n",
      "Evaluating: batch 14 ends at 22:27:35.854218\n",
      "\n",
      "Evaluating: batch 15 begins at 22:27:35.855760\n",
      "\n",
      "Evaluating: batch 15 ends at 22:27:36.076356\n",
      "\n",
      "Evaluating: batch 16 begins at 22:27:36.077559\n",
      "\n",
      "Evaluating: batch 16 ends at 22:27:36.297555\n",
      "\n",
      "Evaluating: batch 17 begins at 22:27:36.300178\n",
      "\n",
      "Evaluating: batch 17 ends at 22:27:36.520854\n",
      "\n",
      "Evaluating: batch 18 begins at 22:27:36.522087\n",
      "\n",
      "Evaluating: batch 18 ends at 22:27:36.743399\n",
      "\n",
      "Evaluating: batch 19 begins at 22:27:36.745533\n",
      "\n",
      "Evaluating: batch 19 ends at 22:27:36.967884\n",
      "\n",
      "Evaluating: batch 20 begins at 22:27:36.969296\n",
      "\n",
      "Evaluating: batch 20 ends at 22:27:37.187326\n",
      "\n",
      "Evaluating: batch 21 begins at 22:27:37.188570\n",
      "\n",
      "Evaluating: batch 21 ends at 22:27:37.406331\n",
      "\n",
      "Evaluating: batch 22 begins at 22:27:37.407586\n",
      "\n",
      "Evaluating: batch 22 ends at 22:27:37.634211\n",
      "\n",
      "Evaluating: batch 23 begins at 22:27:37.635650\n",
      "\n",
      "Evaluating: batch 23 ends at 22:27:37.856643\n",
      "\n",
      "Evaluating: batch 24 begins at 22:27:37.857974\n",
      "\n",
      "Evaluating: batch 24 ends at 22:27:38.081849\n",
      "\n",
      "Evaluating: batch 25 begins at 22:27:38.083217\n",
      "\n",
      "Evaluating: batch 25 ends at 22:27:38.304849\n",
      "\n",
      "Evaluating: batch 26 begins at 22:27:38.306132\n",
      "\n",
      "Evaluating: batch 26 ends at 22:27:38.523841\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.39337 to 0.38795, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 57s 957ms/step - loss: 0.3721 - bce_dice_loss: 0.3721 - val_loss: 0.3879 - val_bce_dice_loss: 0.3879\n",
      "Epoch 12/25\n",
      "\n",
      "Training: batch 0 begins at 22:27:40.168328\n",
      "\n",
      "Training: batch 0 ends at 22:27:40.973106\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.4101 - bce_dice_loss: 0.4101\n",
      "Training: batch 1 begins at 22:27:40.977127\n",
      "\n",
      "Training: batch 1 ends at 22:27:41.779515\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3090 - bce_dice_loss: 0.3090\n",
      "Training: batch 2 begins at 22:27:41.784745\n",
      "\n",
      "Training: batch 2 ends at 22:27:42.608367\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3289 - bce_dice_loss: 0.3289\n",
      "Training: batch 3 begins at 22:27:42.611806\n",
      "\n",
      "Training: batch 3 ends at 22:27:43.430241\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3152 - bce_dice_loss: 0.3152\n",
      "Training: batch 4 begins at 22:27:43.435858\n",
      "\n",
      "Training: batch 4 ends at 22:27:44.239706\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3776 - bce_dice_loss: 0.3776\n",
      "Training: batch 5 begins at 22:27:44.242458\n",
      "\n",
      "Training: batch 5 ends at 22:27:45.047776\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.3708 - bce_dice_loss: 0.3708\n",
      "Training: batch 6 begins at 22:27:45.053011\n",
      "\n",
      "Training: batch 6 ends at 22:27:45.854674\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.3705 - bce_dice_loss: 0.3705\n",
      "Training: batch 7 begins at 22:27:45.859341\n",
      "\n",
      "Training: batch 7 ends at 22:27:46.687189\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3909 - bce_dice_loss: 0.3909\n",
      "Training: batch 8 begins at 22:27:46.689858\n",
      "\n",
      "Training: batch 8 ends at 22:27:47.488748\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3777 - bce_dice_loss: 0.3777\n",
      "Training: batch 9 begins at 22:27:47.492001\n",
      "\n",
      "Training: batch 9 ends at 22:27:48.299831\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3568 - bce_dice_loss: 0.3568\n",
      "Training: batch 10 begins at 22:27:48.303868\n",
      "\n",
      "Training: batch 10 ends at 22:27:49.099338\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3569 - bce_dice_loss: 0.3569\n",
      "Training: batch 11 begins at 22:27:49.102441\n",
      "\n",
      "Training: batch 11 ends at 22:27:49.896699\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3494 - bce_dice_loss: 0.3494\n",
      "Training: batch 12 begins at 22:27:49.900216\n",
      "\n",
      "Training: batch 12 ends at 22:27:50.686204\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3486 - bce_dice_loss: 0.3486\n",
      "Training: batch 13 begins at 22:27:50.690323\n",
      "\n",
      "Training: batch 13 ends at 22:27:51.500255\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3534 - bce_dice_loss: 0.3534\n",
      "Training: batch 14 begins at 22:27:51.504421\n",
      "\n",
      "Training: batch 14 ends at 22:27:52.318092\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3610 - bce_dice_loss: 0.3610\n",
      "Training: batch 15 begins at 22:27:52.321524\n",
      "\n",
      "Training: batch 15 ends at 22:27:53.118870\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3737 - bce_dice_loss: 0.3737\n",
      "Training: batch 16 begins at 22:27:53.121837\n",
      "\n",
      "Training: batch 16 ends at 22:27:53.922705\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3725 - bce_dice_loss: 0.3725\n",
      "Training: batch 17 begins at 22:27:53.926838\n",
      "\n",
      "Training: batch 17 ends at 22:27:54.729502\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3721 - bce_dice_loss: 0.3721\n",
      "Training: batch 18 begins at 22:27:54.731208\n",
      "\n",
      "Training: batch 18 ends at 22:27:55.526196\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3680 - bce_dice_loss: 0.3680\n",
      "Training: batch 19 begins at 22:27:55.528790\n",
      "\n",
      "Training: batch 19 ends at 22:27:56.319936\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3632 - bce_dice_loss: 0.3632\n",
      "Training: batch 20 begins at 22:27:56.322774\n",
      "\n",
      "Training: batch 20 ends at 22:27:57.119039\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3649 - bce_dice_loss: 0.3649\n",
      "Training: batch 21 begins at 22:27:57.122950\n",
      "\n",
      "Training: batch 21 ends at 22:27:57.921637\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3690 - bce_dice_loss: 0.3690\n",
      "Training: batch 22 begins at 22:27:57.925980\n",
      "\n",
      "Training: batch 22 ends at 22:27:58.747706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3679 - bce_dice_loss: 0.3679\n",
      "Training: batch 23 begins at 22:27:58.751412\n",
      "\n",
      "Training: batch 23 ends at 22:27:59.545948\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3655 - bce_dice_loss: 0.3655\n",
      "Training: batch 24 begins at 22:27:59.552944\n",
      "\n",
      "Training: batch 24 ends at 22:28:00.374984\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3679 - bce_dice_loss: 0.3679\n",
      "Training: batch 25 begins at 22:28:00.378517\n",
      "\n",
      "Training: batch 25 ends at 22:28:01.178266\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3657 - bce_dice_loss: 0.3657\n",
      "Training: batch 26 begins at 22:28:01.182637\n",
      "\n",
      "Training: batch 26 ends at 22:28:01.992017\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3623 - bce_dice_loss: 0.3623\n",
      "Training: batch 27 begins at 22:28:01.994809\n",
      "\n",
      "Training: batch 27 ends at 22:28:02.802311\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3658 - bce_dice_loss: 0.3658\n",
      "Training: batch 28 begins at 22:28:02.807253\n",
      "\n",
      "Training: batch 28 ends at 22:28:03.606573\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3653 - bce_dice_loss: 0.3653\n",
      "Training: batch 29 begins at 22:28:03.609282\n",
      "\n",
      "Training: batch 29 ends at 22:28:04.406275\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3620 - bce_dice_loss: 0.3620\n",
      "Training: batch 30 begins at 22:28:04.409850\n",
      "\n",
      "Training: batch 30 ends at 22:28:05.205732\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3609 - bce_dice_loss: 0.3609\n",
      "Training: batch 31 begins at 22:28:05.210104\n",
      "\n",
      "Training: batch 31 ends at 22:28:06.004943\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3657 - bce_dice_loss: 0.3657\n",
      "Training: batch 32 begins at 22:28:06.008886\n",
      "\n",
      "Training: batch 32 ends at 22:28:06.810394\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3649 - bce_dice_loss: 0.3649\n",
      "Training: batch 33 begins at 22:28:06.814960\n",
      "\n",
      "Training: batch 33 ends at 22:28:07.611152\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3626 - bce_dice_loss: 0.3626\n",
      "Training: batch 34 begins at 22:28:07.614164\n",
      "\n",
      "Training: batch 34 ends at 22:28:08.409873\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3648 - bce_dice_loss: 0.3648\n",
      "Training: batch 35 begins at 22:28:08.412379\n",
      "\n",
      "Training: batch 35 ends at 22:28:09.219893\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3616 - bce_dice_loss: 0.3616\n",
      "Training: batch 36 begins at 22:28:09.223208\n",
      "\n",
      "Training: batch 36 ends at 22:28:10.028310\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3564 - bce_dice_loss: 0.3564\n",
      "Training: batch 37 begins at 22:28:10.032703\n",
      "\n",
      "Training: batch 37 ends at 22:28:10.834295\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3537 - bce_dice_loss: 0.3537\n",
      "Training: batch 38 begins at 22:28:10.837849\n",
      "\n",
      "Training: batch 38 ends at 22:28:11.637628\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3518 - bce_dice_loss: 0.3518\n",
      "Training: batch 39 begins at 22:28:11.640209\n",
      "\n",
      "Training: batch 39 ends at 22:28:12.445774\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3501 - bce_dice_loss: 0.3501\n",
      "Training: batch 40 begins at 22:28:12.449981\n",
      "\n",
      "Training: batch 40 ends at 22:28:13.249766\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 41 begins at 22:28:13.253542\n",
      "\n",
      "Training: batch 41 ends at 22:28:14.045014\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3595 - bce_dice_loss: 0.3595\n",
      "Training: batch 42 begins at 22:28:14.047882\n",
      "\n",
      "Training: batch 42 ends at 22:28:14.846444\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3650 - bce_dice_loss: 0.3650\n",
      "Training: batch 43 begins at 22:28:14.850834\n",
      "\n",
      "Training: batch 43 ends at 22:28:15.646742\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3622 - bce_dice_loss: 0.3622\n",
      "Training: batch 44 begins at 22:28:15.651147\n",
      "\n",
      "Training: batch 44 ends at 22:28:16.449112\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3628 - bce_dice_loss: 0.3628\n",
      "Training: batch 45 begins at 22:28:16.453664\n",
      "\n",
      "Training: batch 45 ends at 22:28:17.259847\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3640 - bce_dice_loss: 0.3640\n",
      "Training: batch 46 begins at 22:28:17.264231\n",
      "\n",
      "Training: batch 46 ends at 22:28:18.076644\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3634 - bce_dice_loss: 0.3634\n",
      "Training: batch 47 begins at 22:28:18.079758\n",
      "\n",
      "Training: batch 47 ends at 22:28:18.875068\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3667 - bce_dice_loss: 0.3667 \n",
      "Training: batch 48 begins at 22:28:18.877837\n",
      "\n",
      "Training: batch 48 ends at 22:28:19.668761\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3699 - bce_dice_loss: 0.3699\n",
      "Training: batch 49 begins at 22:28:19.674317\n",
      "\n",
      "Training: batch 49 ends at 22:28:20.470218\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3688 - bce_dice_loss: 0.3688\n",
      "Training: batch 50 begins at 22:28:20.473155\n",
      "\n",
      "Training: batch 50 ends at 22:28:21.268618\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3666 - bce_dice_loss: 0.3666\n",
      "Training: batch 51 begins at 22:28:21.272614\n",
      "\n",
      "Training: batch 51 ends at 22:28:22.078374\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3678 - bce_dice_loss: 0.3678\n",
      "Training: batch 52 begins at 22:28:22.080752\n",
      "\n",
      "Training: batch 52 ends at 22:28:22.888610\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3683 - bce_dice_loss: 0.3683\n",
      "Training: batch 53 begins at 22:28:22.893650\n",
      "\n",
      "Training: batch 53 ends at 22:28:23.715325\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3668 - bce_dice_loss: 0.3668\n",
      "Training: batch 54 begins at 22:28:23.721821\n",
      "\n",
      "Training: batch 54 ends at 22:28:24.510473\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3663 - bce_dice_loss: 0.3663\n",
      "Training: batch 55 begins at 22:28:24.513108\n",
      "\n",
      "Training: batch 55 ends at 22:28:25.305021\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3660 - bce_dice_loss: 0.3660\n",
      "Training: batch 56 begins at 22:28:25.307487\n",
      "\n",
      "Training: batch 56 ends at 22:28:26.131648\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3654 - bce_dice_loss: 0.3654\n",
      "Training: batch 57 begins at 22:28:26.136445\n",
      "\n",
      "Training: batch 57 ends at 22:28:26.931855\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3655 - bce_dice_loss: 0.3655\n",
      "Training: batch 58 begins at 22:28:26.936877\n",
      "\n",
      "Training: batch 58 ends at 22:28:27.737188\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3645 - bce_dice_loss: 0.3645\n",
      "Training: batch 59 begins at 22:28:27.740879\n",
      "\n",
      "Training: batch 59 ends at 22:28:28.537358\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3682 - bce_dice_loss: 0.3682\n",
      "Evaluating: batch 0 begins at 22:28:28.568620\n",
      "\n",
      "Evaluating: batch 0 ends at 22:28:28.835152\n",
      "\n",
      "Evaluating: batch 1 begins at 22:28:28.836656\n",
      "\n",
      "Evaluating: batch 1 ends at 22:28:29.052209\n",
      "\n",
      "Evaluating: batch 2 begins at 22:28:29.053606\n",
      "\n",
      "Evaluating: batch 2 ends at 22:28:29.273793\n",
      "\n",
      "Evaluating: batch 3 begins at 22:28:29.275261\n",
      "\n",
      "Evaluating: batch 3 ends at 22:28:29.497959\n",
      "\n",
      "Evaluating: batch 4 begins at 22:28:29.503585\n",
      "\n",
      "Evaluating: batch 4 ends at 22:28:29.724885\n",
      "\n",
      "Evaluating: batch 5 begins at 22:28:29.726210\n",
      "\n",
      "Evaluating: batch 5 ends at 22:28:29.950566\n",
      "\n",
      "Evaluating: batch 6 begins at 22:28:29.952999\n",
      "\n",
      "Evaluating: batch 6 ends at 22:28:30.172442\n",
      "\n",
      "Evaluating: batch 7 begins at 22:28:30.173721\n",
      "\n",
      "Evaluating: batch 7 ends at 22:28:30.394447\n",
      "\n",
      "Evaluating: batch 8 begins at 22:28:30.396863\n",
      "\n",
      "Evaluating: batch 8 ends at 22:28:30.619887\n",
      "\n",
      "Evaluating: batch 9 begins at 22:28:30.621565\n",
      "\n",
      "Evaluating: batch 9 ends at 22:28:30.844556\n",
      "\n",
      "Evaluating: batch 10 begins at 22:28:30.845928\n",
      "\n",
      "Evaluating: batch 10 ends at 22:28:31.067132\n",
      "\n",
      "Evaluating: batch 11 begins at 22:28:31.069368\n",
      "\n",
      "Evaluating: batch 11 ends at 22:28:31.293032\n",
      "\n",
      "Evaluating: batch 12 begins at 22:28:31.294434\n",
      "\n",
      "Evaluating: batch 12 ends at 22:28:31.518346\n",
      "\n",
      "Evaluating: batch 13 begins at 22:28:31.519774\n",
      "\n",
      "Evaluating: batch 13 ends at 22:28:31.745223\n",
      "\n",
      "Evaluating: batch 14 begins at 22:28:31.746838\n",
      "\n",
      "Evaluating: batch 14 ends at 22:28:31.971143\n",
      "\n",
      "Evaluating: batch 15 begins at 22:28:31.972431\n",
      "\n",
      "Evaluating: batch 15 ends at 22:28:32.201379\n",
      "\n",
      "Evaluating: batch 16 begins at 22:28:32.203336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 16 ends at 22:28:32.427567\n",
      "\n",
      "Evaluating: batch 17 begins at 22:28:32.429037\n",
      "\n",
      "Evaluating: batch 17 ends at 22:28:32.650977\n",
      "\n",
      "Evaluating: batch 18 begins at 22:28:32.653737\n",
      "\n",
      "Evaluating: batch 18 ends at 22:28:32.874692\n",
      "\n",
      "Evaluating: batch 19 begins at 22:28:32.875917\n",
      "\n",
      "Evaluating: batch 19 ends at 22:28:33.094671\n",
      "\n",
      "Evaluating: batch 20 begins at 22:28:33.095884\n",
      "\n",
      "Evaluating: batch 20 ends at 22:28:33.314932\n",
      "\n",
      "Evaluating: batch 21 begins at 22:28:33.317029\n",
      "\n",
      "Evaluating: batch 21 ends at 22:28:33.531182\n",
      "\n",
      "Evaluating: batch 22 begins at 22:28:33.532486\n",
      "\n",
      "Evaluating: batch 22 ends at 22:28:33.749241\n",
      "\n",
      "Evaluating: batch 23 begins at 22:28:33.750466\n",
      "\n",
      "Evaluating: batch 23 ends at 22:28:33.970486\n",
      "\n",
      "Evaluating: batch 24 begins at 22:28:33.971788\n",
      "\n",
      "Evaluating: batch 24 ends at 22:28:34.189343\n",
      "\n",
      "Evaluating: batch 25 begins at 22:28:34.190901\n",
      "\n",
      "Evaluating: batch 25 ends at 22:28:34.407143\n",
      "\n",
      "Evaluating: batch 26 begins at 22:28:34.408505\n",
      "\n",
      "Evaluating: batch 26 ends at 22:28:34.625957\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38795\n",
      "60/60 [==============================] - 54s 910ms/step - loss: 0.3682 - bce_dice_loss: 0.3682 - val_loss: 0.4141 - val_bce_dice_loss: 0.4141\n",
      "Epoch 13/25\n",
      "\n",
      "Training: batch 0 begins at 22:28:34.650616\n",
      "\n",
      "Training: batch 0 ends at 22:28:35.454383\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3807 - bce_dice_loss: 0.3807\n",
      "Training: batch 1 begins at 22:28:35.458245\n",
      "\n",
      "Training: batch 1 ends at 22:28:36.289461\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.3599 - bce_dice_loss: 0.3599\n",
      "Training: batch 2 begins at 22:28:36.292811\n",
      "\n",
      "Training: batch 2 ends at 22:28:37.103320\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.3771 - bce_dice_loss: 0.3771\n",
      "Training: batch 3 begins at 22:28:37.106940\n",
      "\n",
      "Training: batch 3 ends at 22:28:37.908992\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3723 - bce_dice_loss: 0.3723\n",
      "Training: batch 4 begins at 22:28:37.912432\n",
      "\n",
      "Training: batch 4 ends at 22:28:38.738119\n",
      " 5/60 [=>............................] - ETA: 45s - loss: 0.3666 - bce_dice_loss: 0.3666\n",
      "Training: batch 5 begins at 22:28:38.740987\n",
      "\n",
      "Training: batch 5 ends at 22:28:39.558112\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.3850 - bce_dice_loss: 0.3850\n",
      "Training: batch 6 begins at 22:28:39.560952\n",
      "\n",
      "Training: batch 6 ends at 22:28:40.354524\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.3800 - bce_dice_loss: 0.3800\n",
      "Training: batch 7 begins at 22:28:40.357057\n",
      "\n",
      "Training: batch 7 ends at 22:28:41.154290\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3698 - bce_dice_loss: 0.3698\n",
      "Training: batch 8 begins at 22:28:41.157195\n",
      "\n",
      "Training: batch 8 ends at 22:28:41.951808\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3765 - bce_dice_loss: 0.3765\n",
      "Training: batch 9 begins at 22:28:41.955063\n",
      "\n",
      "Training: batch 9 ends at 22:28:42.766676\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3828 - bce_dice_loss: 0.3828\n",
      "Training: batch 10 begins at 22:28:42.774217\n",
      "\n",
      "Training: batch 10 ends at 22:28:43.572833\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3913 - bce_dice_loss: 0.3913\n",
      "Training: batch 11 begins at 22:28:43.575608\n",
      "\n",
      "Training: batch 11 ends at 22:28:44.367368\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3819 - bce_dice_loss: 0.3819\n",
      "Training: batch 12 begins at 22:28:44.370341\n",
      "\n",
      "Training: batch 12 ends at 22:28:45.186407\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3736 - bce_dice_loss: 0.3736\n",
      "Training: batch 13 begins at 22:28:45.193313\n",
      "\n",
      "Training: batch 13 ends at 22:28:45.988537\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3690 - bce_dice_loss: 0.3690\n",
      "Training: batch 14 begins at 22:28:45.992862\n",
      "\n",
      "Training: batch 14 ends at 22:28:46.781397\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3900 - bce_dice_loss: 0.3900\n",
      "Training: batch 15 begins at 22:28:46.784995\n",
      "\n",
      "Training: batch 15 ends at 22:28:47.575924\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3970 - bce_dice_loss: 0.3970\n",
      "Training: batch 16 begins at 22:28:47.578700\n",
      "\n",
      "Training: batch 16 ends at 22:28:48.373535\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3980 - bce_dice_loss: 0.3980\n",
      "Training: batch 17 begins at 22:28:48.377319\n",
      "\n",
      "Training: batch 17 ends at 22:28:49.173934\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3982 - bce_dice_loss: 0.3982\n",
      "Training: batch 18 begins at 22:28:49.178006\n",
      "\n",
      "Training: batch 18 ends at 22:28:49.974758\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3958 - bce_dice_loss: 0.3958\n",
      "Training: batch 19 begins at 22:28:49.978993\n",
      "\n",
      "Training: batch 19 ends at 22:28:50.773427\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3995 - bce_dice_loss: 0.3995\n",
      "Training: batch 20 begins at 22:28:50.777797\n",
      "\n",
      "Training: batch 20 ends at 22:28:51.576137\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3978 - bce_dice_loss: 0.3978\n",
      "Training: batch 21 begins at 22:28:51.578683\n",
      "\n",
      "Training: batch 21 ends at 22:28:52.382133\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.4072 - bce_dice_loss: 0.4072\n",
      "Training: batch 22 begins at 22:28:52.386830\n",
      "\n",
      "Training: batch 22 ends at 22:28:53.185486\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.4093 - bce_dice_loss: 0.4093\n",
      "Training: batch 23 begins at 22:28:53.188365\n",
      "\n",
      "Training: batch 23 ends at 22:28:53.988291\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4091 - bce_dice_loss: 0.4091\n",
      "Training: batch 24 begins at 22:28:53.992146\n",
      "\n",
      "Training: batch 24 ends at 22:28:54.781892\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4086 - bce_dice_loss: 0.4086\n",
      "Training: batch 25 begins at 22:28:54.786451\n",
      "\n",
      "Training: batch 25 ends at 22:28:55.580267\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4082 - bce_dice_loss: 0.4082\n",
      "Training: batch 26 begins at 22:28:55.583950\n",
      "\n",
      "Training: batch 26 ends at 22:28:56.376922\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4063 - bce_dice_loss: 0.4063\n",
      "Training: batch 27 begins at 22:28:56.381354\n",
      "\n",
      "Training: batch 27 ends at 22:28:57.176528\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.4030 - bce_dice_loss: 0.4030\n",
      "Training: batch 28 begins at 22:28:57.180385\n",
      "\n",
      "Training: batch 28 ends at 22:28:57.973042\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.4075 - bce_dice_loss: 0.4075\n",
      "Training: batch 29 begins at 22:28:57.977345\n",
      "\n",
      "Training: batch 29 ends at 22:28:58.779463\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4013 - bce_dice_loss: 0.4013\n",
      "Training: batch 30 begins at 22:28:58.786908\n",
      "\n",
      "Training: batch 30 ends at 22:28:59.580742\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3988 - bce_dice_loss: 0.3988\n",
      "Training: batch 31 begins at 22:28:59.584584\n",
      "\n",
      "Training: batch 31 ends at 22:29:00.395028\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3964 - bce_dice_loss: 0.3964\n",
      "Training: batch 32 begins at 22:29:00.398549\n",
      "\n",
      "Training: batch 32 ends at 22:29:01.198104\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3969 - bce_dice_loss: 0.3969\n",
      "Training: batch 33 begins at 22:29:01.202765\n",
      "\n",
      "Training: batch 33 ends at 22:29:01.999432\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3993 - bce_dice_loss: 0.3993\n",
      "Training: batch 34 begins at 22:29:02.004345\n",
      "\n",
      "Training: batch 34 ends at 22:29:02.832987\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3967 - bce_dice_loss: 0.3967\n",
      "Training: batch 35 begins at 22:29:02.837433\n",
      "\n",
      "Training: batch 35 ends at 22:29:03.640882\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3947 - bce_dice_loss: 0.3947\n",
      "Training: batch 36 begins at 22:29:03.645377\n",
      "\n",
      "Training: batch 36 ends at 22:29:04.442410\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3924 - bce_dice_loss: 0.3924\n",
      "Training: batch 37 begins at 22:29:04.446628\n",
      "\n",
      "Training: batch 37 ends at 22:29:05.237283\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3900 - bce_dice_loss: 0.3900\n",
      "Training: batch 38 begins at 22:29:05.241549\n",
      "\n",
      "Training: batch 38 ends at 22:29:06.055056\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3861 - bce_dice_loss: 0.3861\n",
      "Training: batch 39 begins at 22:29:06.058081\n",
      "\n",
      "Training: batch 39 ends at 22:29:06.859057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3845 - bce_dice_loss: 0.3845\n",
      "Training: batch 40 begins at 22:29:06.862383\n",
      "\n",
      "Training: batch 40 ends at 22:29:07.659709\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3857 - bce_dice_loss: 0.3857\n",
      "Training: batch 41 begins at 22:29:07.663845\n",
      "\n",
      "Training: batch 41 ends at 22:29:08.473689\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3868 - bce_dice_loss: 0.3868\n",
      "Training: batch 42 begins at 22:29:08.476254\n",
      "\n",
      "Training: batch 42 ends at 22:29:09.271791\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3840 - bce_dice_loss: 0.3840\n",
      "Training: batch 43 begins at 22:29:09.276200\n",
      "\n",
      "Training: batch 43 ends at 22:29:10.070745\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3807 - bce_dice_loss: 0.3807\n",
      "Training: batch 44 begins at 22:29:10.074106\n",
      "\n",
      "Training: batch 44 ends at 22:29:10.867560\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3778 - bce_dice_loss: 0.3778\n",
      "Training: batch 45 begins at 22:29:10.871330\n",
      "\n",
      "Training: batch 45 ends at 22:29:11.671650\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3780 - bce_dice_loss: 0.3780\n",
      "Training: batch 46 begins at 22:29:11.674820\n",
      "\n",
      "Training: batch 46 ends at 22:29:12.471537\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3764 - bce_dice_loss: 0.3764\n",
      "Training: batch 47 begins at 22:29:12.476543\n",
      "\n",
      "Training: batch 47 ends at 22:29:13.266408\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3742 - bce_dice_loss: 0.3742 \n",
      "Training: batch 48 begins at 22:29:13.269876\n",
      "\n",
      "Training: batch 48 ends at 22:29:14.067335\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3732 - bce_dice_loss: 0.3732\n",
      "Training: batch 49 begins at 22:29:14.071455\n",
      "\n",
      "Training: batch 49 ends at 22:29:14.866049\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3722 - bce_dice_loss: 0.3722\n",
      "Training: batch 50 begins at 22:29:14.870774\n",
      "\n",
      "Training: batch 50 ends at 22:29:15.690562\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3690 - bce_dice_loss: 0.3690\n",
      "Training: batch 51 begins at 22:29:15.694790\n",
      "\n",
      "Training: batch 51 ends at 22:29:16.489784\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3687 - bce_dice_loss: 0.3687\n",
      "Training: batch 52 begins at 22:29:16.492996\n",
      "\n",
      "Training: batch 52 ends at 22:29:17.292593\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3687 - bce_dice_loss: 0.3687\n",
      "Training: batch 53 begins at 22:29:17.296951\n",
      "\n",
      "Training: batch 53 ends at 22:29:18.091170\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3663 - bce_dice_loss: 0.3663\n",
      "Training: batch 54 begins at 22:29:18.095445\n",
      "\n",
      "Training: batch 54 ends at 22:29:18.914708\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3681 - bce_dice_loss: 0.3681\n",
      "Training: batch 55 begins at 22:29:18.917955\n",
      "\n",
      "Training: batch 55 ends at 22:29:19.714020\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3756 - bce_dice_loss: 0.3756\n",
      "Training: batch 56 begins at 22:29:19.718624\n",
      "\n",
      "Training: batch 56 ends at 22:29:20.512907\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3754 - bce_dice_loss: 0.3754\n",
      "Training: batch 57 begins at 22:29:20.516815\n",
      "\n",
      "Training: batch 57 ends at 22:29:21.313792\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3754 - bce_dice_loss: 0.3754\n",
      "Training: batch 58 begins at 22:29:21.317292\n",
      "\n",
      "Training: batch 58 ends at 22:29:22.108597\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3778 - bce_dice_loss: 0.3778\n",
      "Training: batch 59 begins at 22:29:22.111483\n",
      "\n",
      "Training: batch 59 ends at 22:29:22.921873\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3798 - bce_dice_loss: 0.3798\n",
      "Evaluating: batch 0 begins at 22:29:22.956784\n",
      "\n",
      "Evaluating: batch 0 ends at 22:29:23.224371\n",
      "\n",
      "Evaluating: batch 1 begins at 22:29:23.225861\n",
      "\n",
      "Evaluating: batch 1 ends at 22:29:23.441725\n",
      "\n",
      "Evaluating: batch 2 begins at 22:29:23.442942\n",
      "\n",
      "Evaluating: batch 2 ends at 22:29:23.665046\n",
      "\n",
      "Evaluating: batch 3 begins at 22:29:23.666588\n",
      "\n",
      "Evaluating: batch 3 ends at 22:29:23.887781\n",
      "\n",
      "Evaluating: batch 4 begins at 22:29:23.889958\n",
      "\n",
      "Evaluating: batch 4 ends at 22:29:24.115018\n",
      "\n",
      "Evaluating: batch 5 begins at 22:29:24.116762\n",
      "\n",
      "Evaluating: batch 5 ends at 22:29:24.338604\n",
      "\n",
      "Evaluating: batch 6 begins at 22:29:24.340461\n",
      "\n",
      "Evaluating: batch 6 ends at 22:29:24.571428\n",
      "\n",
      "Evaluating: batch 7 begins at 22:29:24.573488\n",
      "\n",
      "Evaluating: batch 7 ends at 22:29:24.795792\n",
      "\n",
      "Evaluating: batch 8 begins at 22:29:24.797087\n",
      "\n",
      "Evaluating: batch 8 ends at 22:29:25.019732\n",
      "\n",
      "Evaluating: batch 9 begins at 22:29:25.023225\n",
      "\n",
      "Evaluating: batch 9 ends at 22:29:25.247842\n",
      "\n",
      "Evaluating: batch 10 begins at 22:29:25.249192\n",
      "\n",
      "Evaluating: batch 10 ends at 22:29:25.471504\n",
      "\n",
      "Evaluating: batch 11 begins at 22:29:25.473006\n",
      "\n",
      "Evaluating: batch 11 ends at 22:29:25.690732\n",
      "\n",
      "Evaluating: batch 12 begins at 22:29:25.692235\n",
      "\n",
      "Evaluating: batch 12 ends at 22:29:25.914929\n",
      "\n",
      "Evaluating: batch 13 begins at 22:29:25.916261\n",
      "\n",
      "Evaluating: batch 13 ends at 22:29:26.137010\n",
      "\n",
      "Evaluating: batch 14 begins at 22:29:26.138715\n",
      "\n",
      "Evaluating: batch 14 ends at 22:29:26.357855\n",
      "\n",
      "Evaluating: batch 15 begins at 22:29:26.359557\n",
      "\n",
      "Evaluating: batch 15 ends at 22:29:26.583723\n",
      "\n",
      "Evaluating: batch 16 begins at 22:29:26.585106\n",
      "\n",
      "Evaluating: batch 16 ends at 22:29:26.810112\n",
      "\n",
      "Evaluating: batch 17 begins at 22:29:26.811446\n",
      "\n",
      "Evaluating: batch 17 ends at 22:29:27.035006\n",
      "\n",
      "Evaluating: batch 18 begins at 22:29:27.036764\n",
      "\n",
      "Evaluating: batch 18 ends at 22:29:27.260400\n",
      "\n",
      "Evaluating: batch 19 begins at 22:29:27.261832\n",
      "\n",
      "Evaluating: batch 19 ends at 22:29:27.481662\n",
      "\n",
      "Evaluating: batch 20 begins at 22:29:27.483868\n",
      "\n",
      "Evaluating: batch 20 ends at 22:29:27.698262\n",
      "\n",
      "Evaluating: batch 21 begins at 22:29:27.700069\n",
      "\n",
      "Evaluating: batch 21 ends at 22:29:27.916839\n",
      "\n",
      "Evaluating: batch 22 begins at 22:29:27.918398\n",
      "\n",
      "Evaluating: batch 22 ends at 22:29:28.135916\n",
      "\n",
      "Evaluating: batch 23 begins at 22:29:28.138541\n",
      "\n",
      "Evaluating: batch 23 ends at 22:29:28.357077\n",
      "\n",
      "Evaluating: batch 24 begins at 22:29:28.358398\n",
      "\n",
      "Evaluating: batch 24 ends at 22:29:28.575097\n",
      "\n",
      "Evaluating: batch 25 begins at 22:29:28.576688\n",
      "\n",
      "Evaluating: batch 25 ends at 22:29:28.796316\n",
      "\n",
      "Evaluating: batch 26 begins at 22:29:28.798004\n",
      "\n",
      "Evaluating: batch 26 ends at 22:29:29.016415\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.38795 to 0.38616, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 0.3798 - bce_dice_loss: 0.3798 - val_loss: 0.3862 - val_bce_dice_loss: 0.3862\n",
      "Epoch 14/25\n",
      "\n",
      "Training: batch 0 begins at 22:29:30.649426\n",
      "\n",
      "Training: batch 0 ends at 22:29:31.473494\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3475 - bce_dice_loss: 0.3475\n",
      "Training: batch 1 begins at 22:29:31.477492\n",
      "\n",
      "Training: batch 1 ends at 22:29:32.304645\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.3305 - bce_dice_loss: 0.3305\n",
      "Training: batch 2 begins at 22:29:32.308045\n",
      "\n",
      "Training: batch 2 ends at 22:29:33.119006\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3426 - bce_dice_loss: 0.3426\n",
      "Training: batch 3 begins at 22:29:33.122497\n",
      "\n",
      "Training: batch 3 ends at 22:29:33.922199\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3623 - bce_dice_loss: 0.3623\n",
      "Training: batch 4 begins at 22:29:33.926467\n",
      "\n",
      "Training: batch 4 ends at 22:29:34.722134\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3497 - bce_dice_loss: 0.3497\n",
      "Training: batch 5 begins at 22:29:34.726052\n",
      "\n",
      "Training: batch 5 ends at 22:29:35.522671\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3442 - bce_dice_loss: 0.3442\n",
      "Training: batch 6 begins at 22:29:35.527058\n",
      "\n",
      "Training: batch 6 ends at 22:29:36.324519\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 7 begins at 22:29:36.327078\n",
      "\n",
      "Training: batch 7 ends at 22:29:37.123518\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3704 - bce_dice_loss: 0.3704\n",
      "Training: batch 8 begins at 22:29:37.126630\n",
      "\n",
      "Training: batch 8 ends at 22:29:37.940043\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3540 - bce_dice_loss: 0.3540\n",
      "Training: batch 9 begins at 22:29:37.944279\n",
      "\n",
      "Training: batch 9 ends at 22:29:38.741659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3495 - bce_dice_loss: 0.3495\n",
      "Training: batch 10 begins at 22:29:38.745433\n",
      "\n",
      "Training: batch 10 ends at 22:29:39.544726\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3482 - bce_dice_loss: 0.3482\n",
      "Training: batch 11 begins at 22:29:39.547600\n",
      "\n",
      "Training: batch 11 ends at 22:29:40.340331\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3624 - bce_dice_loss: 0.3624\n",
      "Training: batch 12 begins at 22:29:40.344907\n",
      "\n",
      "Training: batch 12 ends at 22:29:41.139512\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3529 - bce_dice_loss: 0.3529\n",
      "Training: batch 13 begins at 22:29:41.142542\n",
      "\n",
      "Training: batch 13 ends at 22:29:41.938492\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3448 - bce_dice_loss: 0.3448\n",
      "Training: batch 14 begins at 22:29:41.941864\n",
      "\n",
      "Training: batch 14 ends at 22:29:42.775664\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3551 - bce_dice_loss: 0.3551\n",
      "Training: batch 15 begins at 22:29:42.780071\n",
      "\n",
      "Training: batch 15 ends at 22:29:43.581775\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3549 - bce_dice_loss: 0.3549\n",
      "Training: batch 16 begins at 22:29:43.590133\n",
      "\n",
      "Training: batch 16 ends at 22:29:44.375258\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3575 - bce_dice_loss: 0.3575\n",
      "Training: batch 17 begins at 22:29:44.379257\n",
      "\n",
      "Training: batch 17 ends at 22:29:45.175189\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3590 - bce_dice_loss: 0.3590\n",
      "Training: batch 18 begins at 22:29:45.178167\n",
      "\n",
      "Training: batch 18 ends at 22:29:45.977414\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3660 - bce_dice_loss: 0.3660\n",
      "Training: batch 19 begins at 22:29:45.981713\n",
      "\n",
      "Training: batch 19 ends at 22:29:46.767572\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3675 - bce_dice_loss: 0.3675\n",
      "Training: batch 20 begins at 22:29:46.770251\n",
      "\n",
      "Training: batch 20 ends at 22:29:47.563182\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3598 - bce_dice_loss: 0.3598\n",
      "Training: batch 21 begins at 22:29:47.567115\n",
      "\n",
      "Training: batch 21 ends at 22:29:48.360612\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3596 - bce_dice_loss: 0.3596\n",
      "Training: batch 22 begins at 22:29:48.363846\n",
      "\n",
      "Training: batch 22 ends at 22:29:49.151198\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3548 - bce_dice_loss: 0.3548\n",
      "Training: batch 23 begins at 22:29:49.153736\n",
      "\n",
      "Training: batch 23 ends at 22:29:49.950854\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3512 - bce_dice_loss: 0.3512\n",
      "Training: batch 24 begins at 22:29:49.955023\n",
      "\n",
      "Training: batch 24 ends at 22:29:50.752846\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3505 - bce_dice_loss: 0.3505\n",
      "Training: batch 25 begins at 22:29:50.757206\n",
      "\n",
      "Training: batch 25 ends at 22:29:51.554864\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3507 - bce_dice_loss: 0.3507\n",
      "Training: batch 26 begins at 22:29:51.557840\n",
      "\n",
      "Training: batch 26 ends at 22:29:52.362460\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3532 - bce_dice_loss: 0.3532\n",
      "Training: batch 27 begins at 22:29:52.365365\n",
      "\n",
      "Training: batch 27 ends at 22:29:53.153522\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3537 - bce_dice_loss: 0.3537\n",
      "Training: batch 28 begins at 22:29:53.157083\n",
      "\n",
      "Training: batch 28 ends at 22:29:53.947179\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3523 - bce_dice_loss: 0.3523\n",
      "Training: batch 29 begins at 22:29:53.951233\n",
      "\n",
      "Training: batch 29 ends at 22:29:54.748362\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 30 begins at 22:29:54.750444\n",
      "\n",
      "Training: batch 30 ends at 22:29:55.543254\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3582 - bce_dice_loss: 0.3582\n",
      "Training: batch 31 begins at 22:29:55.549292\n",
      "\n",
      "Training: batch 31 ends at 22:29:56.343263\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3553 - bce_dice_loss: 0.3553\n",
      "Training: batch 32 begins at 22:29:56.346965\n",
      "\n",
      "Training: batch 32 ends at 22:29:57.143390\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3529 - bce_dice_loss: 0.3529\n",
      "Training: batch 33 begins at 22:29:57.147999\n",
      "\n",
      "Training: batch 33 ends at 22:29:57.939444\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3625 - bce_dice_loss: 0.3625\n",
      "Training: batch 34 begins at 22:29:57.943641\n",
      "\n",
      "Training: batch 34 ends at 22:29:58.732907\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3576 - bce_dice_loss: 0.3576\n",
      "Training: batch 35 begins at 22:29:58.736323\n",
      "\n",
      "Training: batch 35 ends at 22:29:59.531181\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3521 - bce_dice_loss: 0.3521\n",
      "Training: batch 36 begins at 22:29:59.533837\n",
      "\n",
      "Training: batch 36 ends at 22:30:00.334750\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3567 - bce_dice_loss: 0.3567\n",
      "Training: batch 37 begins at 22:30:00.337637\n",
      "\n",
      "Training: batch 37 ends at 22:30:01.136955\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3642 - bce_dice_loss: 0.3642\n",
      "Training: batch 38 begins at 22:30:01.140398\n",
      "\n",
      "Training: batch 38 ends at 22:30:01.946642\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3645 - bce_dice_loss: 0.3645\n",
      "Training: batch 39 begins at 22:30:01.950949\n",
      "\n",
      "Training: batch 39 ends at 22:30:02.757172\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3672 - bce_dice_loss: 0.3672\n",
      "Training: batch 40 begins at 22:30:02.761611\n",
      "\n",
      "Training: batch 40 ends at 22:30:03.566581\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3679 - bce_dice_loss: 0.3679\n",
      "Training: batch 41 begins at 22:30:03.569666\n",
      "\n",
      "Training: batch 41 ends at 22:30:04.379661\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3695 - bce_dice_loss: 0.3695\n",
      "Training: batch 42 begins at 22:30:04.383813\n",
      "\n",
      "Training: batch 42 ends at 22:30:05.172260\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3670 - bce_dice_loss: 0.3670\n",
      "Training: batch 43 begins at 22:30:05.175551\n",
      "\n",
      "Training: batch 43 ends at 22:30:05.973627\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3653 - bce_dice_loss: 0.3653\n",
      "Training: batch 44 begins at 22:30:05.976617\n",
      "\n",
      "Training: batch 44 ends at 22:30:06.799134\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3613 - bce_dice_loss: 0.3613\n",
      "Training: batch 45 begins at 22:30:06.802231\n",
      "\n",
      "Training: batch 45 ends at 22:30:07.597738\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3623 - bce_dice_loss: 0.3623\n",
      "Training: batch 46 begins at 22:30:07.605313\n",
      "\n",
      "Training: batch 46 ends at 22:30:08.418891\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3628 - bce_dice_loss: 0.3628\n",
      "Training: batch 47 begins at 22:30:08.423552\n",
      "\n",
      "Training: batch 47 ends at 22:30:09.219677\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3602 - bce_dice_loss: 0.3602 \n",
      "Training: batch 48 begins at 22:30:09.222475\n",
      "\n",
      "Training: batch 48 ends at 22:30:10.021822\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3606 - bce_dice_loss: 0.3606\n",
      "Training: batch 49 begins at 22:30:10.024544\n",
      "\n",
      "Training: batch 49 ends at 22:30:10.821622\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3583 - bce_dice_loss: 0.3583\n",
      "Training: batch 50 begins at 22:30:10.824896\n",
      "\n",
      "Training: batch 50 ends at 22:30:11.621079\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3573 - bce_dice_loss: 0.3573\n",
      "Training: batch 51 begins at 22:30:11.624228\n",
      "\n",
      "Training: batch 51 ends at 22:30:12.429499\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3552 - bce_dice_loss: 0.3552\n",
      "Training: batch 52 begins at 22:30:12.435790\n",
      "\n",
      "Training: batch 52 ends at 22:30:13.252579\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3573 - bce_dice_loss: 0.3573\n",
      "Training: batch 53 begins at 22:30:13.256993\n",
      "\n",
      "Training: batch 53 ends at 22:30:14.047344\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3576 - bce_dice_loss: 0.3576\n",
      "Training: batch 54 begins at 22:30:14.051959\n",
      "\n",
      "Training: batch 54 ends at 22:30:14.843208\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3574 - bce_dice_loss: 0.3574\n",
      "Training: batch 55 begins at 22:30:14.847681\n",
      "\n",
      "Training: batch 55 ends at 22:30:15.669980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3562 - bce_dice_loss: 0.3562\n",
      "Training: batch 56 begins at 22:30:15.674414\n",
      "\n",
      "Training: batch 56 ends at 22:30:16.462888\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3544 - bce_dice_loss: 0.3544\n",
      "Training: batch 57 begins at 22:30:16.468250\n",
      "\n",
      "Training: batch 57 ends at 22:30:17.274677\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3527 - bce_dice_loss: 0.3527\n",
      "Training: batch 58 begins at 22:30:17.279233\n",
      "\n",
      "Training: batch 58 ends at 22:30:18.074809\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3509 - bce_dice_loss: 0.3509\n",
      "Training: batch 59 begins at 22:30:18.077246\n",
      "\n",
      "Training: batch 59 ends at 22:30:18.883263\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3493 - bce_dice_loss: 0.3493\n",
      "Evaluating: batch 0 begins at 22:30:18.917359\n",
      "\n",
      "Evaluating: batch 0 ends at 22:30:19.191568\n",
      "\n",
      "Evaluating: batch 1 begins at 22:30:19.192726\n",
      "\n",
      "Evaluating: batch 1 ends at 22:30:19.407752\n",
      "\n",
      "Evaluating: batch 2 begins at 22:30:19.409069\n",
      "\n",
      "Evaluating: batch 2 ends at 22:30:19.627438\n",
      "\n",
      "Evaluating: batch 3 begins at 22:30:19.628674\n",
      "\n",
      "Evaluating: batch 3 ends at 22:30:19.850268\n",
      "\n",
      "Evaluating: batch 4 begins at 22:30:19.851885\n",
      "\n",
      "Evaluating: batch 4 ends at 22:30:20.073686\n",
      "\n",
      "Evaluating: batch 5 begins at 22:30:20.074854\n",
      "\n",
      "Evaluating: batch 5 ends at 22:30:20.298297\n",
      "\n",
      "Evaluating: batch 6 begins at 22:30:20.300913\n",
      "\n",
      "Evaluating: batch 6 ends at 22:30:20.523682\n",
      "\n",
      "Evaluating: batch 7 begins at 22:30:20.525323\n",
      "\n",
      "Evaluating: batch 7 ends at 22:30:20.752809\n",
      "\n",
      "Evaluating: batch 8 begins at 22:30:20.754230\n",
      "\n",
      "Evaluating: batch 8 ends at 22:30:20.979040\n",
      "\n",
      "Evaluating: batch 9 begins at 22:30:20.980629\n",
      "\n",
      "Evaluating: batch 9 ends at 22:30:21.202870\n",
      "\n",
      "Evaluating: batch 10 begins at 22:30:21.204322\n",
      "\n",
      "Evaluating: batch 10 ends at 22:30:21.424675\n",
      "\n",
      "Evaluating: batch 11 begins at 22:30:21.426307\n",
      "\n",
      "Evaluating: batch 11 ends at 22:30:21.650934\n",
      "\n",
      "Evaluating: batch 12 begins at 22:30:21.653477\n",
      "\n",
      "Evaluating: batch 12 ends at 22:30:21.875735\n",
      "\n",
      "Evaluating: batch 13 begins at 22:30:21.877053\n",
      "\n",
      "Evaluating: batch 13 ends at 22:30:22.100538\n",
      "\n",
      "Evaluating: batch 14 begins at 22:30:22.101936\n",
      "\n",
      "Evaluating: batch 14 ends at 22:30:22.325238\n",
      "\n",
      "Evaluating: batch 15 begins at 22:30:22.326573\n",
      "\n",
      "Evaluating: batch 15 ends at 22:30:22.562955\n",
      "\n",
      "Evaluating: batch 16 begins at 22:30:22.565315\n",
      "\n",
      "Evaluating: batch 16 ends at 22:30:22.789812\n",
      "\n",
      "Evaluating: batch 17 begins at 22:30:22.791340\n",
      "\n",
      "Evaluating: batch 17 ends at 22:30:23.014941\n",
      "\n",
      "Evaluating: batch 18 begins at 22:30:23.016370\n",
      "\n",
      "Evaluating: batch 18 ends at 22:30:23.241012\n",
      "\n",
      "Evaluating: batch 19 begins at 22:30:23.242359\n",
      "\n",
      "Evaluating: batch 19 ends at 22:30:23.468167\n",
      "\n",
      "Evaluating: batch 20 begins at 22:30:23.469860\n",
      "\n",
      "Evaluating: batch 20 ends at 22:30:23.688206\n",
      "\n",
      "Evaluating: batch 21 begins at 22:30:23.689547\n",
      "\n",
      "Evaluating: batch 21 ends at 22:30:23.907778\n",
      "\n",
      "Evaluating: batch 22 begins at 22:30:23.909766\n",
      "\n",
      "Evaluating: batch 22 ends at 22:30:24.129227\n",
      "\n",
      "Evaluating: batch 23 begins at 22:30:24.131389\n",
      "\n",
      "Evaluating: batch 23 ends at 22:30:24.346985\n",
      "\n",
      "Evaluating: batch 24 begins at 22:30:24.348269\n",
      "\n",
      "Evaluating: batch 24 ends at 22:30:24.564688\n",
      "\n",
      "Evaluating: batch 25 begins at 22:30:24.565940\n",
      "\n",
      "Evaluating: batch 25 ends at 22:30:24.787760\n",
      "\n",
      "Evaluating: batch 26 begins at 22:30:24.789249\n",
      "\n",
      "Evaluating: batch 26 ends at 22:30:25.009546\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.38616 to 0.37467, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 0.3493 - bce_dice_loss: 0.3493 - val_loss: 0.3747 - val_bce_dice_loss: 0.3747\n",
      "Epoch 15/25\n",
      "\n",
      "Training: batch 0 begins at 22:30:26.668094\n",
      "\n",
      "Training: batch 0 ends at 22:30:27.494140\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3287 - bce_dice_loss: 0.3287\n",
      "Training: batch 1 begins at 22:30:27.500746\n",
      "\n",
      "Training: batch 1 ends at 22:30:28.315449\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3108 - bce_dice_loss: 0.3108\n",
      "Training: batch 2 begins at 22:30:28.319865\n",
      "\n",
      "Training: batch 2 ends at 22:30:29.120754\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3201 - bce_dice_loss: 0.3201\n",
      "Training: batch 3 begins at 22:30:29.125040\n",
      "\n",
      "Training: batch 3 ends at 22:30:29.924513\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2978 - bce_dice_loss: 0.2978\n",
      "Training: batch 4 begins at 22:30:29.927209\n",
      "\n",
      "Training: batch 4 ends at 22:30:30.722177\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3007 - bce_dice_loss: 0.3007\n",
      "Training: batch 5 begins at 22:30:30.724769\n",
      "\n",
      "Training: batch 5 ends at 22:30:31.518686\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3212 - bce_dice_loss: 0.3212\n",
      "Training: batch 6 begins at 22:30:31.521299\n",
      "\n",
      "Training: batch 6 ends at 22:30:32.314636\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3247 - bce_dice_loss: 0.3247\n",
      "Training: batch 7 begins at 22:30:32.319696\n",
      "\n",
      "Training: batch 7 ends at 22:30:33.148230\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3389 - bce_dice_loss: 0.3389\n",
      "Training: batch 8 begins at 22:30:33.151944\n",
      "\n",
      "Training: batch 8 ends at 22:30:33.950626\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3271 - bce_dice_loss: 0.3271\n",
      "Training: batch 9 begins at 22:30:33.955006\n",
      "\n",
      "Training: batch 9 ends at 22:30:34.745048\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3261 - bce_dice_loss: 0.3261\n",
      "Training: batch 10 begins at 22:30:34.755392\n",
      "\n",
      "Training: batch 10 ends at 22:30:35.546608\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3218 - bce_dice_loss: 0.3218\n",
      "Training: batch 11 begins at 22:30:35.551338\n",
      "\n",
      "Training: batch 11 ends at 22:30:36.359013\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3466 - bce_dice_loss: 0.3466\n",
      "Training: batch 12 begins at 22:30:36.363604\n",
      "\n",
      "Training: batch 12 ends at 22:30:37.164103\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3452 - bce_dice_loss: 0.3452\n",
      "Training: batch 13 begins at 22:30:37.168884\n",
      "\n",
      "Training: batch 13 ends at 22:30:37.964629\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3373 - bce_dice_loss: 0.3373\n",
      "Training: batch 14 begins at 22:30:37.970028\n",
      "\n",
      "Training: batch 14 ends at 22:30:38.765311\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3359 - bce_dice_loss: 0.3359\n",
      "Training: batch 15 begins at 22:30:38.769555\n",
      "\n",
      "Training: batch 15 ends at 22:30:39.564532\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3431 - bce_dice_loss: 0.3431\n",
      "Training: batch 16 begins at 22:30:39.568101\n",
      "\n",
      "Training: batch 16 ends at 22:30:40.386275\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3557 - bce_dice_loss: 0.3557\n",
      "Training: batch 17 begins at 22:30:40.389875\n",
      "\n",
      "Training: batch 17 ends at 22:30:41.181144\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3576 - bce_dice_loss: 0.3576\n",
      "Training: batch 18 begins at 22:30:41.185554\n",
      "\n",
      "Training: batch 18 ends at 22:30:41.983677\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3532 - bce_dice_loss: 0.3532\n",
      "Training: batch 19 begins at 22:30:41.988017\n",
      "\n",
      "Training: batch 19 ends at 22:30:42.788221\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3577 - bce_dice_loss: 0.3577\n",
      "Training: batch 20 begins at 22:30:42.792861\n",
      "\n",
      "Training: batch 20 ends at 22:30:43.590828\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3607 - bce_dice_loss: 0.3607\n",
      "Training: batch 21 begins at 22:30:43.594797\n",
      "\n",
      "Training: batch 21 ends at 22:30:44.396394\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3650 - bce_dice_loss: 0.3650\n",
      "Training: batch 22 begins at 22:30:44.401268\n",
      "\n",
      "Training: batch 22 ends at 22:30:45.208524\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3597 - bce_dice_loss: 0.3597\n",
      "Training: batch 23 begins at 22:30:45.213099\n",
      "\n",
      "Training: batch 23 ends at 22:30:46.011182\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3646 - bce_dice_loss: 0.3646\n",
      "Training: batch 24 begins at 22:30:46.013914\n",
      "\n",
      "Training: batch 24 ends at 22:30:46.810053\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3634 - bce_dice_loss: 0.3634\n",
      "Training: batch 25 begins at 22:30:46.814141\n",
      "\n",
      "Training: batch 25 ends at 22:30:47.608892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/60 [============>.................] - ETA: 27s - loss: 0.3572 - bce_dice_loss: 0.3572\n",
      "Training: batch 26 begins at 22:30:47.612052\n",
      "\n",
      "Training: batch 26 ends at 22:30:48.428893\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3572 - bce_dice_loss: 0.3572\n",
      "Training: batch 27 begins at 22:30:48.435909\n",
      "\n",
      "Training: batch 27 ends at 22:30:49.231977\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3625 - bce_dice_loss: 0.3625\n",
      "Training: batch 28 begins at 22:30:49.236397\n",
      "\n",
      "Training: batch 28 ends at 22:30:50.030105\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3580 - bce_dice_loss: 0.3580\n",
      "Training: batch 29 begins at 22:30:50.033693\n",
      "\n",
      "Training: batch 29 ends at 22:30:50.827072\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3564 - bce_dice_loss: 0.3564\n",
      "Training: batch 30 begins at 22:30:50.830388\n",
      "\n",
      "Training: batch 30 ends at 22:30:51.655187\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3544 - bce_dice_loss: 0.3544\n",
      "Training: batch 31 begins at 22:30:51.659631\n",
      "\n",
      "Training: batch 31 ends at 22:30:52.464119\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3557 - bce_dice_loss: 0.3557\n",
      "Training: batch 32 begins at 22:30:52.468282\n",
      "\n",
      "Training: batch 32 ends at 22:30:53.270188\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3524 - bce_dice_loss: 0.3524\n",
      "Training: batch 33 begins at 22:30:53.273908\n",
      "\n",
      "Training: batch 33 ends at 22:30:54.066930\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3545 - bce_dice_loss: 0.3545\n",
      "Training: batch 34 begins at 22:30:54.070304\n",
      "\n",
      "Training: batch 34 ends at 22:30:54.868481\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3537 - bce_dice_loss: 0.3537\n",
      "Training: batch 35 begins at 22:30:54.872976\n",
      "\n",
      "Training: batch 35 ends at 22:30:55.669884\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3593 - bce_dice_loss: 0.3593\n",
      "Training: batch 36 begins at 22:30:55.674580\n",
      "\n",
      "Training: batch 36 ends at 22:30:56.502443\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3566 - bce_dice_loss: 0.3566\n",
      "Training: batch 37 begins at 22:30:56.503392\n",
      "\n",
      "Training: batch 37 ends at 22:30:57.294587\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3535 - bce_dice_loss: 0.3535\n",
      "Training: batch 38 begins at 22:30:57.299889\n",
      "\n",
      "Training: batch 38 ends at 22:30:58.095674\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3532 - bce_dice_loss: 0.3532\n",
      "Training: batch 39 begins at 22:30:58.099181\n",
      "\n",
      "Training: batch 39 ends at 22:30:58.898095\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3510 - bce_dice_loss: 0.3510\n",
      "Training: batch 40 begins at 22:30:58.902667\n",
      "\n",
      "Training: batch 40 ends at 22:30:59.695598\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3527 - bce_dice_loss: 0.3527\n",
      "Training: batch 41 begins at 22:30:59.700415\n",
      "\n",
      "Training: batch 41 ends at 22:31:00.492817\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3608 - bce_dice_loss: 0.3608\n",
      "Training: batch 42 begins at 22:31:00.495950\n",
      "\n",
      "Training: batch 42 ends at 22:31:01.297816\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3588 - bce_dice_loss: 0.3588\n",
      "Training: batch 43 begins at 22:31:01.300535\n",
      "\n",
      "Training: batch 43 ends at 22:31:02.120614\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3617 - bce_dice_loss: 0.3617\n",
      "Training: batch 44 begins at 22:31:02.125143\n",
      "\n",
      "Training: batch 44 ends at 22:31:02.928198\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3584 - bce_dice_loss: 0.3584\n",
      "Training: batch 45 begins at 22:31:02.932418\n",
      "\n",
      "Training: batch 45 ends at 22:31:03.726298\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3583 - bce_dice_loss: 0.3583\n",
      "Training: batch 46 begins at 22:31:03.730326\n",
      "\n",
      "Training: batch 46 ends at 22:31:04.556231\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3571 - bce_dice_loss: 0.3571\n",
      "Training: batch 47 begins at 22:31:04.558658\n",
      "\n",
      "Training: batch 47 ends at 22:31:05.442649\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3533 - bce_dice_loss: 0.3533 \n",
      "Training: batch 48 begins at 22:31:05.444908\n",
      "\n",
      "Training: batch 48 ends at 22:31:06.373561\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3560 - bce_dice_loss: 0.3560\n",
      "Training: batch 49 begins at 22:31:06.377494\n",
      "\n",
      "Training: batch 49 ends at 22:31:07.261509\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3537 - bce_dice_loss: 0.3537\n",
      "Training: batch 50 begins at 22:31:07.265453\n",
      "\n",
      "Training: batch 50 ends at 22:31:08.139646\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3510 - bce_dice_loss: 0.3510\n",
      "Training: batch 51 begins at 22:31:08.143798\n",
      "\n",
      "Training: batch 51 ends at 22:31:08.957781\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3494 - bce_dice_loss: 0.3494\n",
      "Training: batch 52 begins at 22:31:08.962247\n",
      "\n",
      "Training: batch 52 ends at 22:31:09.757438\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3492 - bce_dice_loss: 0.3492\n",
      "Training: batch 53 begins at 22:31:09.761840\n",
      "\n",
      "Training: batch 53 ends at 22:31:10.557306\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3494 - bce_dice_loss: 0.3494\n",
      "Training: batch 54 begins at 22:31:10.560329\n",
      "\n",
      "Training: batch 54 ends at 22:31:11.355926\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3470 - bce_dice_loss: 0.3470\n",
      "Training: batch 55 begins at 22:31:11.358322\n",
      "\n",
      "Training: batch 55 ends at 22:31:12.149420\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3492 - bce_dice_loss: 0.3492\n",
      "Training: batch 56 begins at 22:31:12.155290\n",
      "\n",
      "Training: batch 56 ends at 22:31:12.958205\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3502 - bce_dice_loss: 0.3502\n",
      "Training: batch 57 begins at 22:31:12.962397\n",
      "\n",
      "Training: batch 57 ends at 22:31:13.776771\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3475 - bce_dice_loss: 0.3475\n",
      "Training: batch 58 begins at 22:31:13.780980\n",
      "\n",
      "Training: batch 58 ends at 22:31:14.601740\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3467 - bce_dice_loss: 0.3467\n",
      "Training: batch 59 begins at 22:31:14.605202\n",
      "\n",
      "Training: batch 59 ends at 22:31:15.402269\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3471 - bce_dice_loss: 0.3471\n",
      "Evaluating: batch 0 begins at 22:31:15.430066\n",
      "\n",
      "Evaluating: batch 0 ends at 22:31:15.713994\n",
      "\n",
      "Evaluating: batch 1 begins at 22:31:15.715270\n",
      "\n",
      "Evaluating: batch 1 ends at 22:31:15.936698\n",
      "\n",
      "Evaluating: batch 2 begins at 22:31:15.938392\n",
      "\n",
      "Evaluating: batch 2 ends at 22:31:16.156642\n",
      "\n",
      "Evaluating: batch 3 begins at 22:31:16.158787\n",
      "\n",
      "Evaluating: batch 3 ends at 22:31:16.380923\n",
      "\n",
      "Evaluating: batch 4 begins at 22:31:16.382984\n",
      "\n",
      "Evaluating: batch 4 ends at 22:31:16.603687\n",
      "\n",
      "Evaluating: batch 5 begins at 22:31:16.605360\n",
      "\n",
      "Evaluating: batch 5 ends at 22:31:16.828324\n",
      "\n",
      "Evaluating: batch 6 begins at 22:31:16.829646\n",
      "\n",
      "Evaluating: batch 6 ends at 22:31:17.050647\n",
      "\n",
      "Evaluating: batch 7 begins at 22:31:17.051964\n",
      "\n",
      "Evaluating: batch 7 ends at 22:31:17.273383\n",
      "\n",
      "Evaluating: batch 8 begins at 22:31:17.276143\n",
      "\n",
      "Evaluating: batch 8 ends at 22:31:17.492852\n",
      "\n",
      "Evaluating: batch 9 begins at 22:31:17.494093\n",
      "\n",
      "Evaluating: batch 9 ends at 22:31:17.716458\n",
      "\n",
      "Evaluating: batch 10 begins at 22:31:17.718395\n",
      "\n",
      "Evaluating: batch 10 ends at 22:31:17.939020\n",
      "\n",
      "Evaluating: batch 11 begins at 22:31:17.940369\n",
      "\n",
      "Evaluating: batch 11 ends at 22:31:18.161723\n",
      "\n",
      "Evaluating: batch 12 begins at 22:31:18.163024\n",
      "\n",
      "Evaluating: batch 12 ends at 22:31:18.381704\n",
      "\n",
      "Evaluating: batch 13 begins at 22:31:18.383409\n",
      "\n",
      "Evaluating: batch 13 ends at 22:31:18.604784\n",
      "\n",
      "Evaluating: batch 14 begins at 22:31:18.606348\n",
      "\n",
      "Evaluating: batch 14 ends at 22:31:18.825379\n",
      "\n",
      "Evaluating: batch 15 begins at 22:31:18.826809\n",
      "\n",
      "Evaluating: batch 15 ends at 22:31:19.044847\n",
      "\n",
      "Evaluating: batch 16 begins at 22:31:19.046082\n",
      "\n",
      "Evaluating: batch 16 ends at 22:31:19.270823\n",
      "\n",
      "Evaluating: batch 17 begins at 22:31:19.272503\n",
      "\n",
      "Evaluating: batch 17 ends at 22:31:19.494285\n",
      "\n",
      "Evaluating: batch 18 begins at 22:31:19.495651\n",
      "\n",
      "Evaluating: batch 18 ends at 22:31:19.714383\n",
      "\n",
      "Evaluating: batch 19 begins at 22:31:19.716035\n",
      "\n",
      "Evaluating: batch 19 ends at 22:31:19.938034\n",
      "\n",
      "Evaluating: batch 20 begins at 22:31:19.939770\n",
      "\n",
      "Evaluating: batch 20 ends at 22:31:20.157318\n",
      "\n",
      "Evaluating: batch 21 begins at 22:31:20.158635\n",
      "\n",
      "Evaluating: batch 21 ends at 22:31:20.374762\n",
      "\n",
      "Evaluating: batch 22 begins at 22:31:20.375947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 22 ends at 22:31:20.593289\n",
      "\n",
      "Evaluating: batch 23 begins at 22:31:20.594667\n",
      "\n",
      "Evaluating: batch 23 ends at 22:31:20.814493\n",
      "\n",
      "Evaluating: batch 24 begins at 22:31:20.816522\n",
      "\n",
      "Evaluating: batch 24 ends at 22:31:21.041191\n",
      "\n",
      "Evaluating: batch 25 begins at 22:31:21.042738\n",
      "\n",
      "Evaluating: batch 25 ends at 22:31:21.261660\n",
      "\n",
      "Evaluating: batch 26 begins at 22:31:21.262969\n",
      "\n",
      "Evaluating: batch 26 ends at 22:31:21.480922\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.37467\n",
      "60/60 [==============================] - 55s 915ms/step - loss: 0.3471 - bce_dice_loss: 0.3471 - val_loss: 0.4110 - val_bce_dice_loss: 0.4110\n",
      "Epoch 16/25\n",
      "\n",
      "Training: batch 0 begins at 22:31:21.513239\n",
      "\n",
      "Training: batch 0 ends at 22:31:22.335275\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2629 - bce_dice_loss: 0.2629\n",
      "Training: batch 1 begins at 22:31:22.337881\n",
      "\n",
      "Training: batch 1 ends at 22:31:23.158253\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3041 - bce_dice_loss: 0.3041\n",
      "Training: batch 2 begins at 22:31:23.160937\n",
      "\n",
      "Training: batch 2 ends at 22:31:23.972384\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2665 - bce_dice_loss: 0.2665\n",
      "Training: batch 3 begins at 22:31:23.974983\n",
      "\n",
      "Training: batch 3 ends at 22:31:24.774962\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2789 - bce_dice_loss: 0.2789\n",
      "Training: batch 4 begins at 22:31:24.779168\n",
      "\n",
      "Training: batch 4 ends at 22:31:25.574982\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2812 - bce_dice_loss: 0.2812\n",
      "Training: batch 5 begins at 22:31:25.577588\n",
      "\n",
      "Training: batch 5 ends at 22:31:26.381479\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2850 - bce_dice_loss: 0.2850\n",
      "Training: batch 6 begins at 22:31:26.385860\n",
      "\n",
      "Training: batch 6 ends at 22:31:27.182337\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2984 - bce_dice_loss: 0.2984\n",
      "Training: batch 7 begins at 22:31:27.186623\n",
      "\n",
      "Training: batch 7 ends at 22:31:27.977322\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2950 - bce_dice_loss: 0.2950\n",
      "Training: batch 8 begins at 22:31:27.981661\n",
      "\n",
      "Training: batch 8 ends at 22:31:28.792633\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3021 - bce_dice_loss: 0.3021\n",
      "Training: batch 9 begins at 22:31:28.796838\n",
      "\n",
      "Training: batch 9 ends at 22:31:29.592647\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3046 - bce_dice_loss: 0.3046\n",
      "Training: batch 10 begins at 22:31:29.595675\n",
      "\n",
      "Training: batch 10 ends at 22:31:30.416006\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2985 - bce_dice_loss: 0.2985\n",
      "Training: batch 11 begins at 22:31:30.419523\n",
      "\n",
      "Training: batch 11 ends at 22:31:31.211654\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3087 - bce_dice_loss: 0.3087\n",
      "Training: batch 12 begins at 22:31:31.215682\n",
      "\n",
      "Training: batch 12 ends at 22:31:32.013111\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3023 - bce_dice_loss: 0.3023\n",
      "Training: batch 13 begins at 22:31:32.016289\n",
      "\n",
      "Training: batch 13 ends at 22:31:32.824836\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2996 - bce_dice_loss: 0.2996\n",
      "Training: batch 14 begins at 22:31:32.828829\n",
      "\n",
      "Training: batch 14 ends at 22:31:33.632301\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3100 - bce_dice_loss: 0.3100\n",
      "Training: batch 15 begins at 22:31:33.634806\n",
      "\n",
      "Training: batch 15 ends at 22:31:34.432559\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3073 - bce_dice_loss: 0.3073\n",
      "Training: batch 16 begins at 22:31:34.435530\n",
      "\n",
      "Training: batch 16 ends at 22:31:35.264270\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3148 - bce_dice_loss: 0.3148\n",
      "Training: batch 17 begins at 22:31:35.269122\n",
      "\n",
      "Training: batch 17 ends at 22:31:36.064061\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3203 - bce_dice_loss: 0.3203\n",
      "Training: batch 18 begins at 22:31:36.068783\n",
      "\n",
      "Training: batch 18 ends at 22:31:36.872973\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3273 - bce_dice_loss: 0.3273\n",
      "Training: batch 19 begins at 22:31:36.875470\n",
      "\n",
      "Training: batch 19 ends at 22:31:37.668006\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3279 - bce_dice_loss: 0.3279\n",
      "Training: batch 20 begins at 22:31:37.672328\n",
      "\n",
      "Training: batch 20 ends at 22:31:38.466865\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3310 - bce_dice_loss: 0.3310\n",
      "Training: batch 21 begins at 22:31:38.471282\n",
      "\n",
      "Training: batch 21 ends at 22:31:39.267208\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3305 - bce_dice_loss: 0.3305\n",
      "Training: batch 22 begins at 22:31:39.270139\n",
      "\n",
      "Training: batch 22 ends at 22:31:40.059068\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3344 - bce_dice_loss: 0.3344\n",
      "Training: batch 23 begins at 22:31:40.062866\n",
      "\n",
      "Training: batch 23 ends at 22:31:40.847451\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3376 - bce_dice_loss: 0.3376\n",
      "Training: batch 24 begins at 22:31:40.851887\n",
      "\n",
      "Training: batch 24 ends at 22:31:41.657141\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3397 - bce_dice_loss: 0.3397\n",
      "Training: batch 25 begins at 22:31:41.662721\n",
      "\n",
      "Training: batch 25 ends at 22:31:42.456804\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3498 - bce_dice_loss: 0.3498\n",
      "Training: batch 26 begins at 22:31:42.459995\n",
      "\n",
      "Training: batch 26 ends at 22:31:43.274114\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3478 - bce_dice_loss: 0.3478\n",
      "Training: batch 27 begins at 22:31:43.277409\n",
      "\n",
      "Training: batch 27 ends at 22:31:44.095976\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3463 - bce_dice_loss: 0.3463\n",
      "Training: batch 28 begins at 22:31:44.098674\n",
      "\n",
      "Training: batch 28 ends at 22:31:44.895412\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3407 - bce_dice_loss: 0.3407\n",
      "Training: batch 29 begins at 22:31:44.898407\n",
      "\n",
      "Training: batch 29 ends at 22:31:45.690947\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3447 - bce_dice_loss: 0.3447\n",
      "Training: batch 30 begins at 22:31:45.693536\n",
      "\n",
      "Training: batch 30 ends at 22:31:46.485030\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3465 - bce_dice_loss: 0.3465\n",
      "Training: batch 31 begins at 22:31:46.487421\n",
      "\n",
      "Training: batch 31 ends at 22:31:47.287473\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3458 - bce_dice_loss: 0.3458\n",
      "Training: batch 32 begins at 22:31:47.291018\n",
      "\n",
      "Training: batch 32 ends at 22:31:48.076762\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3450 - bce_dice_loss: 0.3450\n",
      "Training: batch 33 begins at 22:31:48.079592\n",
      "\n",
      "Training: batch 33 ends at 22:31:48.880776\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3438 - bce_dice_loss: 0.3438\n",
      "Training: batch 34 begins at 22:31:48.884299\n",
      "\n",
      "Training: batch 34 ends at 22:31:49.676748\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3417 - bce_dice_loss: 0.3417\n",
      "Training: batch 35 begins at 22:31:49.681235\n",
      "\n",
      "Training: batch 35 ends at 22:31:50.482335\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3409 - bce_dice_loss: 0.3409\n",
      "Training: batch 36 begins at 22:31:50.485080\n",
      "\n",
      "Training: batch 36 ends at 22:31:51.285520\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3384 - bce_dice_loss: 0.3384\n",
      "Training: batch 37 begins at 22:31:51.290064\n",
      "\n",
      "Training: batch 37 ends at 22:31:52.087633\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3390 - bce_dice_loss: 0.3390\n",
      "Training: batch 38 begins at 22:31:52.091452\n",
      "\n",
      "Training: batch 38 ends at 22:31:52.902012\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3350 - bce_dice_loss: 0.3350\n",
      "Training: batch 39 begins at 22:31:52.906349\n",
      "\n",
      "Training: batch 39 ends at 22:31:53.701489\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3413 - bce_dice_loss: 0.3413\n",
      "Training: batch 40 begins at 22:31:53.704985\n",
      "\n",
      "Training: batch 40 ends at 22:31:54.503150\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3417 - bce_dice_loss: 0.3417\n",
      "Training: batch 41 begins at 22:31:54.505450\n",
      "\n",
      "Training: batch 41 ends at 22:31:55.323894\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3397 - bce_dice_loss: 0.3397\n",
      "Training: batch 42 begins at 22:31:55.327487\n",
      "\n",
      "Training: batch 42 ends at 22:31:56.122108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3381 - bce_dice_loss: 0.3381\n",
      "Training: batch 43 begins at 22:31:56.125083\n",
      "\n",
      "Training: batch 43 ends at 22:31:56.926771\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3376 - bce_dice_loss: 0.3376\n",
      "Training: batch 44 begins at 22:31:56.931078\n",
      "\n",
      "Training: batch 44 ends at 22:31:57.731899\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3412 - bce_dice_loss: 0.3412\n",
      "Training: batch 45 begins at 22:31:57.734718\n",
      "\n",
      "Training: batch 45 ends at 22:31:58.565540\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3440 - bce_dice_loss: 0.3440\n",
      "Training: batch 46 begins at 22:31:58.570044\n",
      "\n",
      "Training: batch 46 ends at 22:31:59.361029\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3426 - bce_dice_loss: 0.3426\n",
      "Training: batch 47 begins at 22:31:59.365646\n",
      "\n",
      "Training: batch 47 ends at 22:32:00.160222\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3414 - bce_dice_loss: 0.3414 \n",
      "Training: batch 48 begins at 22:32:00.163282\n",
      "\n",
      "Training: batch 48 ends at 22:32:00.956009\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 49 begins at 22:32:00.959085\n",
      "\n",
      "Training: batch 49 ends at 22:32:01.755738\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3401 - bce_dice_loss: 0.3401\n",
      "Training: batch 50 begins at 22:32:01.759599\n",
      "\n",
      "Training: batch 50 ends at 22:32:02.551726\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3454 - bce_dice_loss: 0.3454\n",
      "Training: batch 51 begins at 22:32:02.555190\n",
      "\n",
      "Training: batch 51 ends at 22:32:03.385986\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3429 - bce_dice_loss: 0.3429\n",
      "Training: batch 52 begins at 22:32:03.391298\n",
      "\n",
      "Training: batch 52 ends at 22:32:04.184241\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3411 - bce_dice_loss: 0.3411\n",
      "Training: batch 53 begins at 22:32:04.188091\n",
      "\n",
      "Training: batch 53 ends at 22:32:04.976988\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3413 - bce_dice_loss: 0.3413\n",
      "Training: batch 54 begins at 22:32:04.981222\n",
      "\n",
      "Training: batch 54 ends at 22:32:05.780517\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3420 - bce_dice_loss: 0.3420\n",
      "Training: batch 55 begins at 22:32:05.784811\n",
      "\n",
      "Training: batch 55 ends at 22:32:06.577260\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3416 - bce_dice_loss: 0.3416\n",
      "Training: batch 56 begins at 22:32:06.580704\n",
      "\n",
      "Training: batch 56 ends at 22:32:07.373819\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3406 - bce_dice_loss: 0.3406\n",
      "Training: batch 57 begins at 22:32:07.377162\n",
      "\n",
      "Training: batch 57 ends at 22:32:08.172297\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3399 - bce_dice_loss: 0.3399\n",
      "Training: batch 58 begins at 22:32:08.175355\n",
      "\n",
      "Training: batch 58 ends at 22:32:08.967645\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3425 - bce_dice_loss: 0.3425\n",
      "Training: batch 59 begins at 22:32:08.972419\n",
      "\n",
      "Training: batch 59 ends at 22:32:09.770618\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3387 - bce_dice_loss: 0.3387\n",
      "Evaluating: batch 0 begins at 22:32:09.804619\n",
      "\n",
      "Evaluating: batch 0 ends at 22:32:10.069351\n",
      "\n",
      "Evaluating: batch 1 begins at 22:32:10.070655\n",
      "\n",
      "Evaluating: batch 1 ends at 22:32:10.286320\n",
      "\n",
      "Evaluating: batch 2 begins at 22:32:10.287594\n",
      "\n",
      "Evaluating: batch 2 ends at 22:32:10.506102\n",
      "\n",
      "Evaluating: batch 3 begins at 22:32:10.507386\n",
      "\n",
      "Evaluating: batch 3 ends at 22:32:10.729887\n",
      "\n",
      "Evaluating: batch 4 begins at 22:32:10.731484\n",
      "\n",
      "Evaluating: batch 4 ends at 22:32:10.953275\n",
      "\n",
      "Evaluating: batch 5 begins at 22:32:10.954732\n",
      "\n",
      "Evaluating: batch 5 ends at 22:32:11.176908\n",
      "\n",
      "Evaluating: batch 6 begins at 22:32:11.178299\n",
      "\n",
      "Evaluating: batch 6 ends at 22:32:11.394903\n",
      "\n",
      "Evaluating: batch 7 begins at 22:32:11.396472\n",
      "\n",
      "Evaluating: batch 7 ends at 22:32:11.618439\n",
      "\n",
      "Evaluating: batch 8 begins at 22:32:11.620701\n",
      "\n",
      "Evaluating: batch 8 ends at 22:32:11.840575\n",
      "\n",
      "Evaluating: batch 9 begins at 22:32:11.842260\n",
      "\n",
      "Evaluating: batch 9 ends at 22:32:12.057337\n",
      "\n",
      "Evaluating: batch 10 begins at 22:32:12.058534\n",
      "\n",
      "Evaluating: batch 10 ends at 22:32:12.278973\n",
      "\n",
      "Evaluating: batch 11 begins at 22:32:12.280482\n",
      "\n",
      "Evaluating: batch 11 ends at 22:32:12.503729\n",
      "\n",
      "Evaluating: batch 12 begins at 22:32:12.505004\n",
      "\n",
      "Evaluating: batch 12 ends at 22:32:12.732647\n",
      "\n",
      "Evaluating: batch 13 begins at 22:32:12.733925\n",
      "\n",
      "Evaluating: batch 13 ends at 22:32:12.950393\n",
      "\n",
      "Evaluating: batch 14 begins at 22:32:12.953777\n",
      "\n",
      "Evaluating: batch 14 ends at 22:32:13.172530\n",
      "\n",
      "Evaluating: batch 15 begins at 22:32:13.173710\n",
      "\n",
      "Evaluating: batch 15 ends at 22:32:13.395263\n",
      "\n",
      "Evaluating: batch 16 begins at 22:32:13.396801\n",
      "\n",
      "Evaluating: batch 16 ends at 22:32:13.621717\n",
      "\n",
      "Evaluating: batch 17 begins at 22:32:13.623081\n",
      "\n",
      "Evaluating: batch 17 ends at 22:32:13.846136\n",
      "\n",
      "Evaluating: batch 18 begins at 22:32:13.848323\n",
      "\n",
      "Evaluating: batch 18 ends at 22:32:14.072384\n",
      "\n",
      "Evaluating: batch 19 begins at 22:32:14.074091\n",
      "\n",
      "Evaluating: batch 19 ends at 22:32:14.291006\n",
      "\n",
      "Evaluating: batch 20 begins at 22:32:14.292370\n",
      "\n",
      "Evaluating: batch 20 ends at 22:32:14.510709\n",
      "\n",
      "Evaluating: batch 21 begins at 22:32:14.511934\n",
      "\n",
      "Evaluating: batch 21 ends at 22:32:14.728896\n",
      "\n",
      "Evaluating: batch 22 begins at 22:32:14.730198\n",
      "\n",
      "Evaluating: batch 22 ends at 22:32:14.949971\n",
      "\n",
      "Evaluating: batch 23 begins at 22:32:14.952707\n",
      "\n",
      "Evaluating: batch 23 ends at 22:32:15.170705\n",
      "\n",
      "Evaluating: batch 24 begins at 22:32:15.173169\n",
      "\n",
      "Evaluating: batch 24 ends at 22:32:15.394999\n",
      "\n",
      "Evaluating: batch 25 begins at 22:32:15.396806\n",
      "\n",
      "Evaluating: batch 25 ends at 22:32:15.618671\n",
      "\n",
      "Evaluating: batch 26 begins at 22:32:15.620105\n",
      "\n",
      "Evaluating: batch 26 ends at 22:32:15.837695\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.37467 to 0.34637, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 0.3387 - bce_dice_loss: 0.3387 - val_loss: 0.3464 - val_bce_dice_loss: 0.3464\n",
      "Epoch 17/25\n",
      "\n",
      "Training: batch 0 begins at 22:32:17.485936\n",
      "\n",
      "Training: batch 0 ends at 22:32:18.308159\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2342 - bce_dice_loss: 0.2342\n",
      "Training: batch 1 begins at 22:32:18.312565\n",
      "\n",
      "Training: batch 1 ends at 22:32:19.126197\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3009 - bce_dice_loss: 0.3009\n",
      "Training: batch 2 begins at 22:32:19.129275\n",
      "\n",
      "Training: batch 2 ends at 22:32:19.937398\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2997 - bce_dice_loss: 0.2997\n",
      "Training: batch 3 begins at 22:32:19.943777\n",
      "\n",
      "Training: batch 3 ends at 22:32:20.739293\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3295 - bce_dice_loss: 0.3295\n",
      "Training: batch 4 begins at 22:32:20.743304\n",
      "\n",
      "Training: batch 4 ends at 22:32:21.549389\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3363 - bce_dice_loss: 0.3363\n",
      "Training: batch 5 begins at 22:32:21.553881\n",
      "\n",
      "Training: batch 5 ends at 22:32:22.339002\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4086 - bce_dice_loss: 0.4086\n",
      "Training: batch 6 begins at 22:32:22.341842\n",
      "\n",
      "Training: batch 6 ends at 22:32:23.149392\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3910 - bce_dice_loss: 0.3910\n",
      "Training: batch 7 begins at 22:32:23.151414\n",
      "\n",
      "Training: batch 7 ends at 22:32:23.951282\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3869 - bce_dice_loss: 0.3869\n",
      "Training: batch 8 begins at 22:32:23.955542\n",
      "\n",
      "Training: batch 8 ends at 22:32:24.776572\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.4161 - bce_dice_loss: 0.4161\n",
      "Training: batch 9 begins at 22:32:24.780227\n",
      "\n",
      "Training: batch 9 ends at 22:32:25.575377\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3969 - bce_dice_loss: 0.3969\n",
      "Training: batch 10 begins at 22:32:25.578461\n",
      "\n",
      "Training: batch 10 ends at 22:32:26.372028\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3823 - bce_dice_loss: 0.3823\n",
      "Training: batch 11 begins at 22:32:26.376231\n",
      "\n",
      "Training: batch 11 ends at 22:32:27.165053\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3751 - bce_dice_loss: 0.3751\n",
      "Training: batch 12 begins at 22:32:27.169816\n",
      "\n",
      "Training: batch 12 ends at 22:32:27.974232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3739 - bce_dice_loss: 0.3739\n",
      "Training: batch 13 begins at 22:32:27.976654\n",
      "\n",
      "Training: batch 13 ends at 22:32:28.773635\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3683 - bce_dice_loss: 0.3683\n",
      "Training: batch 14 begins at 22:32:28.777866\n",
      "\n",
      "Training: batch 14 ends at 22:32:29.594659\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3687 - bce_dice_loss: 0.3687\n",
      "Training: batch 15 begins at 22:32:29.599469\n",
      "\n",
      "Training: batch 15 ends at 22:32:30.395830\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3676 - bce_dice_loss: 0.3676\n",
      "Training: batch 16 begins at 22:32:30.399702\n",
      "\n",
      "Training: batch 16 ends at 22:32:31.195253\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3703 - bce_dice_loss: 0.3703\n",
      "Training: batch 17 begins at 22:32:31.199091\n",
      "\n",
      "Training: batch 17 ends at 22:32:31.989876\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3623 - bce_dice_loss: 0.3623\n",
      "Training: batch 18 begins at 22:32:31.994720\n",
      "\n",
      "Training: batch 18 ends at 22:32:32.805252\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3585 - bce_dice_loss: 0.3585\n",
      "Training: batch 19 begins at 22:32:32.808053\n",
      "\n",
      "Training: batch 19 ends at 22:32:33.602796\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3510 - bce_dice_loss: 0.3510\n",
      "Training: batch 20 begins at 22:32:33.605781\n",
      "\n",
      "Training: batch 20 ends at 22:32:34.402703\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3483 - bce_dice_loss: 0.3483\n",
      "Training: batch 21 begins at 22:32:34.407213\n",
      "\n",
      "Training: batch 21 ends at 22:32:35.203355\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3506 - bce_dice_loss: 0.3506\n",
      "Training: batch 22 begins at 22:32:35.206913\n",
      "\n",
      "Training: batch 22 ends at 22:32:36.022767\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3573 - bce_dice_loss: 0.3573\n",
      "Training: batch 23 begins at 22:32:36.026148\n",
      "\n",
      "Training: batch 23 ends at 22:32:36.817995\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3498 - bce_dice_loss: 0.3498\n",
      "Training: batch 24 begins at 22:32:36.823187\n",
      "\n",
      "Training: batch 24 ends at 22:32:37.609617\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3444 - bce_dice_loss: 0.3444\n",
      "Training: batch 25 begins at 22:32:37.612971\n",
      "\n",
      "Training: batch 25 ends at 22:32:38.420969\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3395 - bce_dice_loss: 0.3395\n",
      "Training: batch 26 begins at 22:32:38.424035\n",
      "\n",
      "Training: batch 26 ends at 22:32:39.219904\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3405 - bce_dice_loss: 0.3405\n",
      "Training: batch 27 begins at 22:32:39.223532\n",
      "\n",
      "Training: batch 27 ends at 22:32:40.021799\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3354 - bce_dice_loss: 0.3354\n",
      "Training: batch 28 begins at 22:32:40.024726\n",
      "\n",
      "Training: batch 28 ends at 22:32:40.817638\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3362 - bce_dice_loss: 0.3362\n",
      "Training: batch 29 begins at 22:32:40.822104\n",
      "\n",
      "Training: batch 29 ends at 22:32:41.642186\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3370 - bce_dice_loss: 0.3370\n",
      "Training: batch 30 begins at 22:32:41.646673\n",
      "\n",
      "Training: batch 30 ends at 22:32:42.449153\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3388 - bce_dice_loss: 0.3388\n",
      "Training: batch 31 begins at 22:32:42.453142\n",
      "\n",
      "Training: batch 31 ends at 22:32:43.258675\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3355 - bce_dice_loss: 0.3355\n",
      "Training: batch 32 begins at 22:32:43.262723\n",
      "\n",
      "Training: batch 32 ends at 22:32:44.078868\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3353 - bce_dice_loss: 0.3353\n",
      "Training: batch 33 begins at 22:32:44.084973\n",
      "\n",
      "Training: batch 33 ends at 22:32:44.888916\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3366 - bce_dice_loss: 0.3366\n",
      "Training: batch 34 begins at 22:32:44.892853\n",
      "\n",
      "Training: batch 34 ends at 22:32:45.689049\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3329 - bce_dice_loss: 0.3329\n",
      "Training: batch 35 begins at 22:32:45.693354\n",
      "\n",
      "Training: batch 35 ends at 22:32:46.494599\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3370 - bce_dice_loss: 0.3370\n",
      "Training: batch 36 begins at 22:32:46.500101\n",
      "\n",
      "Training: batch 36 ends at 22:32:47.291111\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3347 - bce_dice_loss: 0.3347\n",
      "Training: batch 37 begins at 22:32:47.294156\n",
      "\n",
      "Training: batch 37 ends at 22:32:48.092116\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3349 - bce_dice_loss: 0.3349\n",
      "Training: batch 38 begins at 22:32:48.095191\n",
      "\n",
      "Training: batch 38 ends at 22:32:48.910739\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3379 - bce_dice_loss: 0.3379\n",
      "Training: batch 39 begins at 22:32:48.913803\n",
      "\n",
      "Training: batch 39 ends at 22:32:49.709276\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3367 - bce_dice_loss: 0.3367\n",
      "Training: batch 40 begins at 22:32:49.713727\n",
      "\n",
      "Training: batch 40 ends at 22:32:50.505991\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3370 - bce_dice_loss: 0.3370\n",
      "Training: batch 41 begins at 22:32:50.510191\n",
      "\n",
      "Training: batch 41 ends at 22:32:51.330751\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3385 - bce_dice_loss: 0.3385\n",
      "Training: batch 42 begins at 22:32:51.333478\n",
      "\n",
      "Training: batch 42 ends at 22:32:52.124605\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3384 - bce_dice_loss: 0.3384\n",
      "Training: batch 43 begins at 22:32:52.128635\n",
      "\n",
      "Training: batch 43 ends at 22:32:52.948775\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3358 - bce_dice_loss: 0.3358\n",
      "Training: batch 44 begins at 22:32:52.954540\n",
      "\n",
      "Training: batch 44 ends at 22:32:53.750834\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3328 - bce_dice_loss: 0.3328\n",
      "Training: batch 45 begins at 22:32:53.755290\n",
      "\n",
      "Training: batch 45 ends at 22:32:54.548455\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3337 - bce_dice_loss: 0.3337\n",
      "Training: batch 46 begins at 22:32:54.551282\n",
      "\n",
      "Training: batch 46 ends at 22:32:55.372624\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3319 - bce_dice_loss: 0.3319\n",
      "Training: batch 47 begins at 22:32:55.375702\n",
      "\n",
      "Training: batch 47 ends at 22:32:56.204568\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3317 - bce_dice_loss: 0.3317 \n",
      "Training: batch 48 begins at 22:32:56.207573\n",
      "\n",
      "Training: batch 48 ends at 22:32:57.039111\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3308 - bce_dice_loss: 0.3308\n",
      "Training: batch 49 begins at 22:32:57.041968\n",
      "\n",
      "Training: batch 49 ends at 22:32:57.835418\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 50 begins at 22:32:57.838516\n",
      "\n",
      "Training: batch 50 ends at 22:32:58.632240\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3336 - bce_dice_loss: 0.3336\n",
      "Training: batch 51 begins at 22:32:58.636586\n",
      "\n",
      "Training: batch 51 ends at 22:32:59.432753\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3327 - bce_dice_loss: 0.3327\n",
      "Training: batch 52 begins at 22:32:59.436881\n",
      "\n",
      "Training: batch 52 ends at 22:33:00.233654\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3302 - bce_dice_loss: 0.3302\n",
      "Training: batch 53 begins at 22:33:00.236458\n",
      "\n",
      "Training: batch 53 ends at 22:33:01.029003\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3293 - bce_dice_loss: 0.3293\n",
      "Training: batch 54 begins at 22:33:01.032111\n",
      "\n",
      "Training: batch 54 ends at 22:33:01.836560\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3269 - bce_dice_loss: 0.3269\n",
      "Training: batch 55 begins at 22:33:01.839547\n",
      "\n",
      "Training: batch 55 ends at 22:33:02.634522\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3266 - bce_dice_loss: 0.3266\n",
      "Training: batch 56 begins at 22:33:02.638773\n",
      "\n",
      "Training: batch 56 ends at 22:33:03.447883\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3285 - bce_dice_loss: 0.3285\n",
      "Training: batch 57 begins at 22:33:03.450424\n",
      "\n",
      "Training: batch 57 ends at 22:33:04.247906\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 58 begins at 22:33:04.252367\n",
      "\n",
      "Training: batch 58 ends at 22:33:05.064492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.3298 - bce_dice_loss: 0.3298\n",
      "Training: batch 59 begins at 22:33:05.068168\n",
      "\n",
      "Training: batch 59 ends at 22:33:05.860808\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3277 - bce_dice_loss: 0.3277\n",
      "Evaluating: batch 0 begins at 22:33:05.891937\n",
      "\n",
      "Evaluating: batch 0 ends at 22:33:06.163172\n",
      "\n",
      "Evaluating: batch 1 begins at 22:33:06.164348\n",
      "\n",
      "Evaluating: batch 1 ends at 22:33:06.387771\n",
      "\n",
      "Evaluating: batch 2 begins at 22:33:06.389119\n",
      "\n",
      "Evaluating: batch 2 ends at 22:33:06.609253\n",
      "\n",
      "Evaluating: batch 3 begins at 22:33:06.610887\n",
      "\n",
      "Evaluating: batch 3 ends at 22:33:06.831729\n",
      "\n",
      "Evaluating: batch 4 begins at 22:33:06.837097\n",
      "\n",
      "Evaluating: batch 4 ends at 22:33:07.057341\n",
      "\n",
      "Evaluating: batch 5 begins at 22:33:07.058952\n",
      "\n",
      "Evaluating: batch 5 ends at 22:33:07.280856\n",
      "\n",
      "Evaluating: batch 6 begins at 22:33:07.282591\n",
      "\n",
      "Evaluating: batch 6 ends at 22:33:07.503201\n",
      "\n",
      "Evaluating: batch 7 begins at 22:33:07.504528\n",
      "\n",
      "Evaluating: batch 7 ends at 22:33:07.728584\n",
      "\n",
      "Evaluating: batch 8 begins at 22:33:07.729891\n",
      "\n",
      "Evaluating: batch 8 ends at 22:33:07.947650\n",
      "\n",
      "Evaluating: batch 9 begins at 22:33:07.949746\n",
      "\n",
      "Evaluating: batch 9 ends at 22:33:08.170403\n",
      "\n",
      "Evaluating: batch 10 begins at 22:33:08.171882\n",
      "\n",
      "Evaluating: batch 10 ends at 22:33:08.394478\n",
      "\n",
      "Evaluating: batch 11 begins at 22:33:08.395887\n",
      "\n",
      "Evaluating: batch 11 ends at 22:33:08.616665\n",
      "\n",
      "Evaluating: batch 12 begins at 22:33:08.619268\n",
      "\n",
      "Evaluating: batch 12 ends at 22:33:08.838578\n",
      "\n",
      "Evaluating: batch 13 begins at 22:33:08.839886\n",
      "\n",
      "Evaluating: batch 13 ends at 22:33:09.061805\n",
      "\n",
      "Evaluating: batch 14 begins at 22:33:09.063261\n",
      "\n",
      "Evaluating: batch 14 ends at 22:33:09.286054\n",
      "\n",
      "Evaluating: batch 15 begins at 22:33:09.287546\n",
      "\n",
      "Evaluating: batch 15 ends at 22:33:09.510801\n",
      "\n",
      "Evaluating: batch 16 begins at 22:33:09.512255\n",
      "\n",
      "Evaluating: batch 16 ends at 22:33:09.733748\n",
      "\n",
      "Evaluating: batch 17 begins at 22:33:09.734992\n",
      "\n",
      "Evaluating: batch 17 ends at 22:33:09.956812\n",
      "\n",
      "Evaluating: batch 18 begins at 22:33:09.958543\n",
      "\n",
      "Evaluating: batch 18 ends at 22:33:10.179973\n",
      "\n",
      "Evaluating: batch 19 begins at 22:33:10.181704\n",
      "\n",
      "Evaluating: batch 19 ends at 22:33:10.399222\n",
      "\n",
      "Evaluating: batch 20 begins at 22:33:10.400714\n",
      "\n",
      "Evaluating: batch 20 ends at 22:33:10.619075\n",
      "\n",
      "Evaluating: batch 21 begins at 22:33:10.620284\n",
      "\n",
      "Evaluating: batch 21 ends at 22:33:10.837689\n",
      "\n",
      "Evaluating: batch 22 begins at 22:33:10.839396\n",
      "\n",
      "Evaluating: batch 22 ends at 22:33:11.056567\n",
      "\n",
      "Evaluating: batch 23 begins at 22:33:11.057834\n",
      "\n",
      "Evaluating: batch 23 ends at 22:33:11.274500\n",
      "\n",
      "Evaluating: batch 24 begins at 22:33:11.275786\n",
      "\n",
      "Evaluating: batch 24 ends at 22:33:11.496806\n",
      "\n",
      "Evaluating: batch 25 begins at 22:33:11.498709\n",
      "\n",
      "Evaluating: batch 25 ends at 22:33:11.713763\n",
      "\n",
      "Evaluating: batch 26 begins at 22:33:11.714980\n",
      "\n",
      "Evaluating: batch 26 ends at 22:33:11.938587\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34637\n",
      "60/60 [==============================] - 54s 909ms/step - loss: 0.3277 - bce_dice_loss: 0.3277 - val_loss: 0.3614 - val_bce_dice_loss: 0.3614\n",
      "Epoch 18/25\n",
      "\n",
      "Training: batch 0 begins at 22:33:11.970642\n",
      "\n",
      "Training: batch 0 ends at 22:33:12.772560\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2266 - bce_dice_loss: 0.2266\n",
      "Training: batch 1 begins at 22:33:12.775918\n",
      "\n",
      "Training: batch 1 ends at 22:33:13.596737\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2257 - bce_dice_loss: 0.2257\n",
      "Training: batch 2 begins at 22:33:13.600492\n",
      "\n",
      "Training: batch 2 ends at 22:33:14.403588\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2162 - bce_dice_loss: 0.2162\n",
      "Training: batch 3 begins at 22:33:14.406508\n",
      "\n",
      "Training: batch 3 ends at 22:33:15.202364\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2123 - bce_dice_loss: 0.2123\n",
      "Training: batch 4 begins at 22:33:15.205449\n",
      "\n",
      "Training: batch 4 ends at 22:33:16.020720\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2719 - bce_dice_loss: 0.2719\n",
      "Training: batch 5 begins at 22:33:16.023688\n",
      "\n",
      "Training: batch 5 ends at 22:33:16.822231\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2931 - bce_dice_loss: 0.2931\n",
      "Training: batch 6 begins at 22:33:16.826162\n",
      "\n",
      "Training: batch 6 ends at 22:33:17.620037\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2999 - bce_dice_loss: 0.2999\n",
      "Training: batch 7 begins at 22:33:17.622957\n",
      "\n",
      "Training: batch 7 ends at 22:33:18.416837\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3034 - bce_dice_loss: 0.3034\n",
      "Training: batch 8 begins at 22:33:18.419420\n",
      "\n",
      "Training: batch 8 ends at 22:33:19.216237\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2908 - bce_dice_loss: 0.2908\n",
      "Training: batch 9 begins at 22:33:19.219099\n",
      "\n",
      "Training: batch 9 ends at 22:33:20.017363\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3131 - bce_dice_loss: 0.3131\n",
      "Training: batch 10 begins at 22:33:20.022588\n",
      "\n",
      "Training: batch 10 ends at 22:33:20.838581\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3147 - bce_dice_loss: 0.3147\n",
      "Training: batch 11 begins at 22:33:20.842521\n",
      "\n",
      "Training: batch 11 ends at 22:33:21.636693\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3124 - bce_dice_loss: 0.3124\n",
      "Training: batch 12 begins at 22:33:21.640928\n",
      "\n",
      "Training: batch 12 ends at 22:33:22.434504\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3129 - bce_dice_loss: 0.3129\n",
      "Training: batch 13 begins at 22:33:22.437681\n",
      "\n",
      "Training: batch 13 ends at 22:33:23.239385\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3223 - bce_dice_loss: 0.3223\n",
      "Training: batch 14 begins at 22:33:23.242312\n",
      "\n",
      "Training: batch 14 ends at 22:33:24.042618\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3154 - bce_dice_loss: 0.3154\n",
      "Training: batch 15 begins at 22:33:24.046773\n",
      "\n",
      "Training: batch 15 ends at 22:33:24.870795\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3213 - bce_dice_loss: 0.3213\n",
      "Training: batch 16 begins at 22:33:24.875473\n",
      "\n",
      "Training: batch 16 ends at 22:33:25.668612\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3360 - bce_dice_loss: 0.3360\n",
      "Training: batch 17 begins at 22:33:25.671245\n",
      "\n",
      "Training: batch 17 ends at 22:33:26.465683\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3295 - bce_dice_loss: 0.3295\n",
      "Training: batch 18 begins at 22:33:26.469721\n",
      "\n",
      "Training: batch 18 ends at 22:33:27.272509\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3220 - bce_dice_loss: 0.3220\n",
      "Training: batch 19 begins at 22:33:27.276653\n",
      "\n",
      "Training: batch 19 ends at 22:33:28.101267\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3260 - bce_dice_loss: 0.3260\n",
      "Training: batch 20 begins at 22:33:28.105310\n",
      "\n",
      "Training: batch 20 ends at 22:33:28.902150\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3209 - bce_dice_loss: 0.3209\n",
      "Training: batch 21 begins at 22:33:28.905664\n",
      "\n",
      "Training: batch 21 ends at 22:33:29.725285\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3213 - bce_dice_loss: 0.3213\n",
      "Training: batch 22 begins at 22:33:29.729548\n",
      "\n",
      "Training: batch 22 ends at 22:33:30.537066\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3229 - bce_dice_loss: 0.3229\n",
      "Training: batch 23 begins at 22:33:30.540317\n",
      "\n",
      "Training: batch 23 ends at 22:33:31.351202\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3228 - bce_dice_loss: 0.3228\n",
      "Training: batch 24 begins at 22:33:31.353999\n",
      "\n",
      "Training: batch 24 ends at 22:33:32.158514\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3257 - bce_dice_loss: 0.3257\n",
      "Training: batch 25 begins at 22:33:32.161504\n",
      "\n",
      "Training: batch 25 ends at 22:33:32.994127\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 26 begins at 22:33:33.000768\n",
      "\n",
      "Training: batch 26 ends at 22:33:33.811929\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3280 - bce_dice_loss: 0.3280\n",
      "Training: batch 27 begins at 22:33:33.820442\n",
      "\n",
      "Training: batch 27 ends at 22:33:34.638802\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3223 - bce_dice_loss: 0.3223\n",
      "Training: batch 28 begins at 22:33:34.641281\n",
      "\n",
      "Training: batch 28 ends at 22:33:35.449984\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3197 - bce_dice_loss: 0.3197\n",
      "Training: batch 29 begins at 22:33:35.453385\n",
      "\n",
      "Training: batch 29 ends at 22:33:36.287153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3222 - bce_dice_loss: 0.3222\n",
      "Training: batch 30 begins at 22:33:36.289933\n",
      "\n",
      "Training: batch 30 ends at 22:33:37.163090\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3173 - bce_dice_loss: 0.3173\n",
      "Training: batch 31 begins at 22:33:37.170489\n",
      "\n",
      "Training: batch 31 ends at 22:33:37.972737\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3182 - bce_dice_loss: 0.3182\n",
      "Training: batch 32 begins at 22:33:37.975591\n",
      "\n",
      "Training: batch 32 ends at 22:33:38.774165\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3177 - bce_dice_loss: 0.3177\n",
      "Training: batch 33 begins at 22:33:38.777662\n",
      "\n",
      "Training: batch 33 ends at 22:33:39.591058\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3146 - bce_dice_loss: 0.3146\n",
      "Training: batch 34 begins at 22:33:39.593757\n",
      "\n",
      "Training: batch 34 ends at 22:33:40.401112\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3144 - bce_dice_loss: 0.3144\n",
      "Training: batch 35 begins at 22:33:40.403842\n",
      "\n",
      "Training: batch 35 ends at 22:33:41.198976\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3138 - bce_dice_loss: 0.3138\n",
      "Training: batch 36 begins at 22:33:41.203784\n",
      "\n",
      "Training: batch 36 ends at 22:33:42.000709\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3140 - bce_dice_loss: 0.3140\n",
      "Training: batch 37 begins at 22:33:42.004157\n",
      "\n",
      "Training: batch 37 ends at 22:33:42.804982\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 38 begins at 22:33:42.809563\n",
      "\n",
      "Training: batch 38 ends at 22:33:43.635176\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3143 - bce_dice_loss: 0.3143\n",
      "Training: batch 39 begins at 22:33:43.639589\n",
      "\n",
      "Training: batch 39 ends at 22:33:44.437546\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3131 - bce_dice_loss: 0.3131\n",
      "Training: batch 40 begins at 22:33:44.441172\n",
      "\n",
      "Training: batch 40 ends at 22:33:45.242903\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3144 - bce_dice_loss: 0.3144\n",
      "Training: batch 41 begins at 22:33:45.246832\n",
      "\n",
      "Training: batch 41 ends at 22:33:46.069444\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3180 - bce_dice_loss: 0.3180\n",
      "Training: batch 42 begins at 22:33:46.073631\n",
      "\n",
      "Training: batch 42 ends at 22:33:46.873037\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3168 - bce_dice_loss: 0.3168\n",
      "Training: batch 43 begins at 22:33:46.877084\n",
      "\n",
      "Training: batch 43 ends at 22:33:47.669900\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3149 - bce_dice_loss: 0.3149\n",
      "Training: batch 44 begins at 22:33:47.672474\n",
      "\n",
      "Training: batch 44 ends at 22:33:48.465635\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3145 - bce_dice_loss: 0.3145\n",
      "Training: batch 45 begins at 22:33:48.469601\n",
      "\n",
      "Training: batch 45 ends at 22:33:49.295539\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3163 - bce_dice_loss: 0.3163\n",
      "Training: batch 46 begins at 22:33:49.300535\n",
      "\n",
      "Training: batch 46 ends at 22:33:50.103745\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3185 - bce_dice_loss: 0.3185\n",
      "Training: batch 47 begins at 22:33:50.107235\n",
      "\n",
      "Training: batch 47 ends at 22:33:50.931538\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3164 - bce_dice_loss: 0.3164 \n",
      "Training: batch 48 begins at 22:33:50.937790\n",
      "\n",
      "Training: batch 48 ends at 22:33:51.732638\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3177 - bce_dice_loss: 0.3177\n",
      "Training: batch 49 begins at 22:33:51.736867\n",
      "\n",
      "Training: batch 49 ends at 22:33:52.527851\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3204 - bce_dice_loss: 0.3204\n",
      "Training: batch 50 begins at 22:33:52.532544\n",
      "\n",
      "Training: batch 50 ends at 22:33:53.367680\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3189 - bce_dice_loss: 0.3189\n",
      "Training: batch 51 begins at 22:33:53.371910\n",
      "\n",
      "Training: batch 51 ends at 22:33:54.174179\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3198 - bce_dice_loss: 0.3198\n",
      "Training: batch 52 begins at 22:33:54.178868\n",
      "\n",
      "Training: batch 52 ends at 22:33:55.008836\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3231 - bce_dice_loss: 0.3231\n",
      "Training: batch 53 begins at 22:33:55.013449\n",
      "\n",
      "Training: batch 53 ends at 22:33:55.815171\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3212 - bce_dice_loss: 0.3212\n",
      "Training: batch 54 begins at 22:33:55.819679\n",
      "\n",
      "Training: batch 54 ends at 22:33:56.618417\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3203 - bce_dice_loss: 0.3203\n",
      "Training: batch 55 begins at 22:33:56.620574\n",
      "\n",
      "Training: batch 55 ends at 22:33:57.422315\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3196 - bce_dice_loss: 0.3196\n",
      "Training: batch 56 begins at 22:33:57.426771\n",
      "\n",
      "Training: batch 56 ends at 22:33:58.222959\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3216 - bce_dice_loss: 0.3216\n",
      "Training: batch 57 begins at 22:33:58.226361\n",
      "\n",
      "Training: batch 57 ends at 22:33:59.030961\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3204 - bce_dice_loss: 0.3204\n",
      "Training: batch 58 begins at 22:33:59.034285\n",
      "\n",
      "Training: batch 58 ends at 22:33:59.833900\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3176 - bce_dice_loss: 0.3176\n",
      "Training: batch 59 begins at 22:33:59.837670\n",
      "\n",
      "Training: batch 59 ends at 22:34:00.646145\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3186 - bce_dice_loss: 0.3186\n",
      "Evaluating: batch 0 begins at 22:34:00.679589\n",
      "\n",
      "Evaluating: batch 0 ends at 22:34:00.950960\n",
      "\n",
      "Evaluating: batch 1 begins at 22:34:00.952579\n",
      "\n",
      "Evaluating: batch 1 ends at 22:34:01.170627\n",
      "\n",
      "Evaluating: batch 2 begins at 22:34:01.172326\n",
      "\n",
      "Evaluating: batch 2 ends at 22:34:01.396479\n",
      "\n",
      "Evaluating: batch 3 begins at 22:34:01.401562\n",
      "\n",
      "Evaluating: batch 3 ends at 22:34:01.623295\n",
      "\n",
      "Evaluating: batch 4 begins at 22:34:01.624863\n",
      "\n",
      "Evaluating: batch 4 ends at 22:34:01.849898\n",
      "\n",
      "Evaluating: batch 5 begins at 22:34:01.851434\n",
      "\n",
      "Evaluating: batch 5 ends at 22:34:02.070673\n",
      "\n",
      "Evaluating: batch 6 begins at 22:34:02.072786\n",
      "\n",
      "Evaluating: batch 6 ends at 22:34:02.295879\n",
      "\n",
      "Evaluating: batch 7 begins at 22:34:02.297807\n",
      "\n",
      "Evaluating: batch 7 ends at 22:34:02.518180\n",
      "\n",
      "Evaluating: batch 8 begins at 22:34:02.520619\n",
      "\n",
      "Evaluating: batch 8 ends at 22:34:02.744520\n",
      "\n",
      "Evaluating: batch 9 begins at 22:34:02.747247\n",
      "\n",
      "Evaluating: batch 9 ends at 22:34:02.979399\n",
      "\n",
      "Evaluating: batch 10 begins at 22:34:02.982370\n",
      "\n",
      "Evaluating: batch 10 ends at 22:34:03.201600\n",
      "\n",
      "Evaluating: batch 11 begins at 22:34:03.204155\n",
      "\n",
      "Evaluating: batch 11 ends at 22:34:03.425120\n",
      "\n",
      "Evaluating: batch 12 begins at 22:34:03.427715\n",
      "\n",
      "Evaluating: batch 12 ends at 22:34:03.649402\n",
      "\n",
      "Evaluating: batch 13 begins at 22:34:03.651441\n",
      "\n",
      "Evaluating: batch 13 ends at 22:34:03.874646\n",
      "\n",
      "Evaluating: batch 14 begins at 22:34:03.877456\n",
      "\n",
      "Evaluating: batch 14 ends at 22:34:04.098462\n",
      "\n",
      "Evaluating: batch 15 begins at 22:34:04.100861\n",
      "\n",
      "Evaluating: batch 15 ends at 22:34:04.321171\n",
      "\n",
      "Evaluating: batch 16 begins at 22:34:04.323342\n",
      "\n",
      "Evaluating: batch 16 ends at 22:34:04.546736\n",
      "\n",
      "Evaluating: batch 17 begins at 22:34:04.548535\n",
      "\n",
      "Evaluating: batch 17 ends at 22:34:04.772381\n",
      "\n",
      "Evaluating: batch 18 begins at 22:34:04.773995\n",
      "\n",
      "Evaluating: batch 18 ends at 22:34:04.994243\n",
      "\n",
      "Evaluating: batch 19 begins at 22:34:04.995785\n",
      "\n",
      "Evaluating: batch 19 ends at 22:34:05.214795\n",
      "\n",
      "Evaluating: batch 20 begins at 22:34:05.216037\n",
      "\n",
      "Evaluating: batch 20 ends at 22:34:05.435107\n",
      "\n",
      "Evaluating: batch 21 begins at 22:34:05.436846\n",
      "\n",
      "Evaluating: batch 21 ends at 22:34:05.654830\n",
      "\n",
      "Evaluating: batch 22 begins at 22:34:05.656324\n",
      "\n",
      "Evaluating: batch 22 ends at 22:34:05.880022\n",
      "\n",
      "Evaluating: batch 23 begins at 22:34:05.882241\n",
      "\n",
      "Evaluating: batch 23 ends at 22:34:06.100558\n",
      "\n",
      "Evaluating: batch 24 begins at 22:34:06.101872\n",
      "\n",
      "Evaluating: batch 24 ends at 22:34:06.319282\n",
      "\n",
      "Evaluating: batch 25 begins at 22:34:06.320479\n",
      "\n",
      "Evaluating: batch 25 ends at 22:34:06.541613\n",
      "\n",
      "Evaluating: batch 26 begins at 22:34:06.544206\n",
      "\n",
      "Evaluating: batch 26 ends at 22:34:06.766105\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.34637 to 0.34501, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 943ms/step - loss: 0.3186 - bce_dice_loss: 0.3186 - val_loss: 0.3450 - val_bce_dice_loss: 0.3450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25\n",
      "\n",
      "Training: batch 0 begins at 22:34:08.415345\n",
      "\n",
      "Training: batch 0 ends at 22:34:09.237347\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2203 - bce_dice_loss: 0.2203\n",
      "Training: batch 1 begins at 22:34:09.239959\n",
      "\n",
      "Training: batch 1 ends at 22:34:10.048212\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3067 - bce_dice_loss: 0.3067\n",
      "Training: batch 2 begins at 22:34:10.052668\n",
      "\n",
      "Training: batch 2 ends at 22:34:10.848689\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.3448 - bce_dice_loss: 0.3448\n",
      "Training: batch 3 begins at 22:34:10.853729\n",
      "\n",
      "Training: batch 3 ends at 22:34:11.653145\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2983 - bce_dice_loss: 0.2983\n",
      "Training: batch 4 begins at 22:34:11.656055\n",
      "\n",
      "Training: batch 4 ends at 22:34:12.452973\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2846 - bce_dice_loss: 0.2846\n",
      "Training: batch 5 begins at 22:34:12.455623\n",
      "\n",
      "Training: batch 5 ends at 22:34:13.263802\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2809 - bce_dice_loss: 0.2809\n",
      "Training: batch 6 begins at 22:34:13.268125\n",
      "\n",
      "Training: batch 6 ends at 22:34:14.069699\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2698 - bce_dice_loss: 0.2698\n",
      "Training: batch 7 begins at 22:34:14.072799\n",
      "\n",
      "Training: batch 7 ends at 22:34:14.866037\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2726 - bce_dice_loss: 0.2726\n",
      "Training: batch 8 begins at 22:34:14.870627\n",
      "\n",
      "Training: batch 8 ends at 22:34:15.653411\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2781 - bce_dice_loss: 0.2781\n",
      "Training: batch 9 begins at 22:34:15.655990\n",
      "\n",
      "Training: batch 9 ends at 22:34:16.449905\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2733 - bce_dice_loss: 0.2733\n",
      "Training: batch 10 begins at 22:34:16.453526\n",
      "\n",
      "Training: batch 10 ends at 22:34:17.251265\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2872 - bce_dice_loss: 0.2872\n",
      "Training: batch 11 begins at 22:34:17.254137\n",
      "\n",
      "Training: batch 11 ends at 22:34:18.051089\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2908 - bce_dice_loss: 0.2908\n",
      "Training: batch 12 begins at 22:34:18.053660\n",
      "\n",
      "Training: batch 12 ends at 22:34:18.846435\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2910 - bce_dice_loss: 0.2910\n",
      "Training: batch 13 begins at 22:34:18.849332\n",
      "\n",
      "Training: batch 13 ends at 22:34:19.644690\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2816 - bce_dice_loss: 0.2816\n",
      "Training: batch 14 begins at 22:34:19.649155\n",
      "\n",
      "Training: batch 14 ends at 22:34:20.457078\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2734 - bce_dice_loss: 0.2734\n",
      "Training: batch 15 begins at 22:34:20.460264\n",
      "\n",
      "Training: batch 15 ends at 22:34:21.256437\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2787 - bce_dice_loss: 0.2787\n",
      "Training: batch 16 begins at 22:34:21.259004\n",
      "\n",
      "Training: batch 16 ends at 22:34:22.072656\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2750 - bce_dice_loss: 0.2750\n",
      "Training: batch 17 begins at 22:34:22.076710\n",
      "\n",
      "Training: batch 17 ends at 22:34:22.868901\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2804 - bce_dice_loss: 0.2804\n",
      "Training: batch 18 begins at 22:34:22.871763\n",
      "\n",
      "Training: batch 18 ends at 22:34:23.679130\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2950 - bce_dice_loss: 0.2950\n",
      "Training: batch 19 begins at 22:34:23.682540\n",
      "\n",
      "Training: batch 19 ends at 22:34:24.503892\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2906 - bce_dice_loss: 0.2906\n",
      "Training: batch 20 begins at 22:34:24.507568\n",
      "\n",
      "Training: batch 20 ends at 22:34:25.306009\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2902 - bce_dice_loss: 0.2902\n",
      "Training: batch 21 begins at 22:34:25.310245\n",
      "\n",
      "Training: batch 21 ends at 22:34:26.125282\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2889 - bce_dice_loss: 0.2889\n",
      "Training: batch 22 begins at 22:34:26.129654\n",
      "\n",
      "Training: batch 22 ends at 22:34:26.921728\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2904 - bce_dice_loss: 0.2904\n",
      "Training: batch 23 begins at 22:34:26.926170\n",
      "\n",
      "Training: batch 23 ends at 22:34:27.717140\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2922 - bce_dice_loss: 0.2922\n",
      "Training: batch 24 begins at 22:34:27.721742\n",
      "\n",
      "Training: batch 24 ends at 22:34:28.520585\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2873 - bce_dice_loss: 0.2873\n",
      "Training: batch 25 begins at 22:34:28.524794\n",
      "\n",
      "Training: batch 25 ends at 22:34:29.321730\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2849 - bce_dice_loss: 0.2849\n",
      "Training: batch 26 begins at 22:34:29.326535\n",
      "\n",
      "Training: batch 26 ends at 22:34:30.138026\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2885 - bce_dice_loss: 0.2885\n",
      "Training: batch 27 begins at 22:34:30.141005\n",
      "\n",
      "Training: batch 27 ends at 22:34:30.936761\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2923 - bce_dice_loss: 0.2923\n",
      "Training: batch 28 begins at 22:34:30.941132\n",
      "\n",
      "Training: batch 28 ends at 22:34:31.733113\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2894 - bce_dice_loss: 0.2894\n",
      "Training: batch 29 begins at 22:34:31.736046\n",
      "\n",
      "Training: batch 29 ends at 22:34:32.532059\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2953 - bce_dice_loss: 0.2953\n",
      "Training: batch 30 begins at 22:34:32.536240\n",
      "\n",
      "Training: batch 30 ends at 22:34:33.351189\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2999 - bce_dice_loss: 0.2999\n",
      "Training: batch 31 begins at 22:34:33.354733\n",
      "\n",
      "Training: batch 31 ends at 22:34:34.149027\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3000 - bce_dice_loss: 0.3000\n",
      "Training: batch 32 begins at 22:34:34.153894\n",
      "\n",
      "Training: batch 32 ends at 22:34:34.952450\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2995 - bce_dice_loss: 0.2995\n",
      "Training: batch 33 begins at 22:34:34.955424\n",
      "\n",
      "Training: batch 33 ends at 22:34:35.757216\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3028 - bce_dice_loss: 0.3028\n",
      "Training: batch 34 begins at 22:34:35.761290\n",
      "\n",
      "Training: batch 34 ends at 22:34:36.555285\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3132 - bce_dice_loss: 0.3132\n",
      "Training: batch 35 begins at 22:34:36.559345\n",
      "\n",
      "Training: batch 35 ends at 22:34:37.356201\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3190 - bce_dice_loss: 0.3190\n",
      "Training: batch 36 begins at 22:34:37.359429\n",
      "\n",
      "Training: batch 36 ends at 22:34:38.152254\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3185 - bce_dice_loss: 0.3185\n",
      "Training: batch 37 begins at 22:34:38.155829\n",
      "\n",
      "Training: batch 37 ends at 22:34:38.951301\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3126 - bce_dice_loss: 0.3126\n",
      "Training: batch 38 begins at 22:34:38.954316\n",
      "\n",
      "Training: batch 38 ends at 22:34:39.746254\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3091 - bce_dice_loss: 0.3091\n",
      "Training: batch 39 begins at 22:34:39.749428\n",
      "\n",
      "Training: batch 39 ends at 22:34:40.549622\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3076 - bce_dice_loss: 0.3076\n",
      "Training: batch 40 begins at 22:34:40.553790\n",
      "\n",
      "Training: batch 40 ends at 22:34:41.352450\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3091 - bce_dice_loss: 0.3091\n",
      "Training: batch 41 begins at 22:34:41.355235\n",
      "\n",
      "Training: batch 41 ends at 22:34:42.153221\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3084 - bce_dice_loss: 0.3084\n",
      "Training: batch 42 begins at 22:34:42.156842\n",
      "\n",
      "Training: batch 42 ends at 22:34:42.949311\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3063 - bce_dice_loss: 0.3063\n",
      "Training: batch 43 begins at 22:34:42.954461\n",
      "\n",
      "Training: batch 43 ends at 22:34:43.774325\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3067 - bce_dice_loss: 0.3067\n",
      "Training: batch 44 begins at 22:34:43.778425\n",
      "\n",
      "Training: batch 44 ends at 22:34:44.575920\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3032 - bce_dice_loss: 0.3032\n",
      "Training: batch 45 begins at 22:34:44.580002\n",
      "\n",
      "Training: batch 45 ends at 22:34:45.388048\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3024 - bce_dice_loss: 0.3024\n",
      "Training: batch 46 begins at 22:34:45.390642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 46 ends at 22:34:46.187330\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3037 - bce_dice_loss: 0.3037\n",
      "Training: batch 47 begins at 22:34:46.190179\n",
      "\n",
      "Training: batch 47 ends at 22:34:46.982978\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3071 - bce_dice_loss: 0.3071 \n",
      "Training: batch 48 begins at 22:34:46.987400\n",
      "\n",
      "Training: batch 48 ends at 22:34:47.774525\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3114 - bce_dice_loss: 0.3114\n",
      "Training: batch 49 begins at 22:34:47.779287\n",
      "\n",
      "Training: batch 49 ends at 22:34:48.569459\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3122 - bce_dice_loss: 0.3122\n",
      "Training: batch 50 begins at 22:34:48.572028\n",
      "\n",
      "Training: batch 50 ends at 22:34:49.367100\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3102 - bce_dice_loss: 0.3102\n",
      "Training: batch 51 begins at 22:34:49.371731\n",
      "\n",
      "Training: batch 51 ends at 22:34:50.160096\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3106 - bce_dice_loss: 0.3106\n",
      "Training: batch 52 begins at 22:34:50.164084\n",
      "\n",
      "Training: batch 52 ends at 22:34:50.957798\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3123 - bce_dice_loss: 0.3123\n",
      "Training: batch 53 begins at 22:34:50.961802\n",
      "\n",
      "Training: batch 53 ends at 22:34:51.744655\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3121 - bce_dice_loss: 0.3121\n",
      "Training: batch 54 begins at 22:34:51.748307\n",
      "\n",
      "Training: batch 54 ends at 22:34:52.543430\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3150 - bce_dice_loss: 0.3150\n",
      "Training: batch 55 begins at 22:34:52.548506\n",
      "\n",
      "Training: batch 55 ends at 22:34:53.345879\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 56 begins at 22:34:53.349221\n",
      "\n",
      "Training: batch 56 ends at 22:34:54.145348\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3136 - bce_dice_loss: 0.3136\n",
      "Training: batch 57 begins at 22:34:54.152197\n",
      "\n",
      "Training: batch 57 ends at 22:34:54.942824\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3118 - bce_dice_loss: 0.3118\n",
      "Training: batch 58 begins at 22:34:54.945911\n",
      "\n",
      "Training: batch 58 ends at 22:34:55.745462\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3152 - bce_dice_loss: 0.3152\n",
      "Training: batch 59 begins at 22:34:55.748989\n",
      "\n",
      "Training: batch 59 ends at 22:34:56.552694\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3168 - bce_dice_loss: 0.3168\n",
      "Evaluating: batch 0 begins at 22:34:56.586739\n",
      "\n",
      "Evaluating: batch 0 ends at 22:34:56.867137\n",
      "\n",
      "Evaluating: batch 1 begins at 22:34:56.868488\n",
      "\n",
      "Evaluating: batch 1 ends at 22:34:57.086532\n",
      "\n",
      "Evaluating: batch 2 begins at 22:34:57.087947\n",
      "\n",
      "Evaluating: batch 2 ends at 22:34:57.307771\n",
      "\n",
      "Evaluating: batch 3 begins at 22:34:57.310064\n",
      "\n",
      "Evaluating: batch 3 ends at 22:34:57.530124\n",
      "\n",
      "Evaluating: batch 4 begins at 22:34:57.531746\n",
      "\n",
      "Evaluating: batch 4 ends at 22:34:57.759186\n",
      "\n",
      "Evaluating: batch 5 begins at 22:34:57.760524\n",
      "\n",
      "Evaluating: batch 5 ends at 22:34:57.981916\n",
      "\n",
      "Evaluating: batch 6 begins at 22:34:57.983062\n",
      "\n",
      "Evaluating: batch 6 ends at 22:34:58.206021\n",
      "\n",
      "Evaluating: batch 7 begins at 22:34:58.207434\n",
      "\n",
      "Evaluating: batch 7 ends at 22:34:58.429839\n",
      "\n",
      "Evaluating: batch 8 begins at 22:34:58.431276\n",
      "\n",
      "Evaluating: batch 8 ends at 22:34:58.649854\n",
      "\n",
      "Evaluating: batch 9 begins at 22:34:58.651500\n",
      "\n",
      "Evaluating: batch 9 ends at 22:34:58.875512\n",
      "\n",
      "Evaluating: batch 10 begins at 22:34:58.876897\n",
      "\n",
      "Evaluating: batch 10 ends at 22:34:59.105394\n",
      "\n",
      "Evaluating: batch 11 begins at 22:34:59.106993\n",
      "\n",
      "Evaluating: batch 11 ends at 22:34:59.331181\n",
      "\n",
      "Evaluating: batch 12 begins at 22:34:59.334995\n",
      "\n",
      "Evaluating: batch 12 ends at 22:34:59.553026\n",
      "\n",
      "Evaluating: batch 13 begins at 22:34:59.554676\n",
      "\n",
      "Evaluating: batch 13 ends at 22:34:59.774896\n",
      "\n",
      "Evaluating: batch 14 begins at 22:34:59.776505\n",
      "\n",
      "Evaluating: batch 14 ends at 22:34:59.996960\n",
      "\n",
      "Evaluating: batch 15 begins at 22:35:00.003181\n",
      "\n",
      "Evaluating: batch 15 ends at 22:35:00.223430\n",
      "\n",
      "Evaluating: batch 16 begins at 22:35:00.224824\n",
      "\n",
      "Evaluating: batch 16 ends at 22:35:00.446781\n",
      "\n",
      "Evaluating: batch 17 begins at 22:35:00.450222\n",
      "\n",
      "Evaluating: batch 17 ends at 22:35:00.670518\n",
      "\n",
      "Evaluating: batch 18 begins at 22:35:00.671678\n",
      "\n",
      "Evaluating: batch 18 ends at 22:35:00.892128\n",
      "\n",
      "Evaluating: batch 19 begins at 22:35:00.893921\n",
      "\n",
      "Evaluating: batch 19 ends at 22:35:01.108363\n",
      "\n",
      "Evaluating: batch 20 begins at 22:35:01.109535\n",
      "\n",
      "Evaluating: batch 20 ends at 22:35:01.336369\n",
      "\n",
      "Evaluating: batch 21 begins at 22:35:01.339627\n",
      "\n",
      "Evaluating: batch 21 ends at 22:35:01.557288\n",
      "\n",
      "Evaluating: batch 22 begins at 22:35:01.558903\n",
      "\n",
      "Evaluating: batch 22 ends at 22:35:01.774055\n",
      "\n",
      "Evaluating: batch 23 begins at 22:35:01.775296\n",
      "\n",
      "Evaluating: batch 23 ends at 22:35:01.999409\n",
      "\n",
      "Evaluating: batch 24 begins at 22:35:02.000637\n",
      "\n",
      "Evaluating: batch 24 ends at 22:35:02.217544\n",
      "\n",
      "Evaluating: batch 25 begins at 22:35:02.219307\n",
      "\n",
      "Evaluating: batch 25 ends at 22:35:02.436268\n",
      "\n",
      "Evaluating: batch 26 begins at 22:35:02.437561\n",
      "\n",
      "Evaluating: batch 26 ends at 22:35:02.657352\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.34501 to 0.34323, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 933ms/step - loss: 0.3168 - bce_dice_loss: 0.3168 - val_loss: 0.3432 - val_bce_dice_loss: 0.3432\n",
      "Epoch 20/25\n",
      "\n",
      "Training: batch 0 begins at 22:35:04.316255\n",
      "\n",
      "Training: batch 0 ends at 22:35:05.198924\n",
      " 1/60 [..............................] - ETA: 52s - loss: 0.3694 - bce_dice_loss: 0.3694\n",
      "Training: batch 1 begins at 22:35:05.201856\n",
      "\n",
      "Training: batch 1 ends at 22:35:06.095799\n",
      " 2/60 [>.............................] - ETA: 52s - loss: 0.3384 - bce_dice_loss: 0.3384\n",
      "Training: batch 2 begins at 22:35:06.098505\n",
      "\n",
      "Training: batch 2 ends at 22:35:06.973851\n",
      " 3/60 [>.............................] - ETA: 50s - loss: 0.3033 - bce_dice_loss: 0.3033\n",
      "Training: batch 3 begins at 22:35:06.976162\n",
      "\n",
      "Training: batch 3 ends at 22:35:07.846008\n",
      " 4/60 [=>............................] - ETA: 49s - loss: 0.3359 - bce_dice_loss: 0.3359\n",
      "Training: batch 4 begins at 22:35:07.848394\n",
      "\n",
      "Training: batch 4 ends at 22:35:08.676513\n",
      " 5/60 [=>............................] - ETA: 47s - loss: 0.3429 - bce_dice_loss: 0.3429\n",
      "Training: batch 5 begins at 22:35:08.680864\n",
      "\n",
      "Training: batch 5 ends at 22:35:09.497923\n",
      " 6/60 [==>...........................] - ETA: 46s - loss: 0.3547 - bce_dice_loss: 0.3547\n",
      "Training: batch 6 begins at 22:35:09.502249\n",
      "\n",
      "Training: batch 6 ends at 22:35:10.302749\n",
      " 7/60 [==>...........................] - ETA: 45s - loss: 0.3353 - bce_dice_loss: 0.3353\n",
      "Training: batch 7 begins at 22:35:10.306179\n",
      "\n",
      "Training: batch 7 ends at 22:35:11.129094\n",
      " 8/60 [===>..........................] - ETA: 44s - loss: 0.3329 - bce_dice_loss: 0.3329\n",
      "Training: batch 8 begins at 22:35:11.134582\n",
      "\n",
      "Training: batch 8 ends at 22:35:11.925751\n",
      " 9/60 [===>..........................] - ETA: 42s - loss: 0.3204 - bce_dice_loss: 0.3204\n",
      "Training: batch 9 begins at 22:35:11.930188\n",
      "\n",
      "Training: batch 9 ends at 22:35:12.732879\n",
      "10/60 [====>.........................] - ETA: 41s - loss: 0.3088 - bce_dice_loss: 0.3088\n",
      "Training: batch 10 begins at 22:35:12.735407\n",
      "\n",
      "Training: batch 10 ends at 22:35:13.548078\n",
      "11/60 [====>.........................] - ETA: 40s - loss: 0.3110 - bce_dice_loss: 0.3110\n",
      "Training: batch 11 begins at 22:35:13.550582\n",
      "\n",
      "Training: batch 11 ends at 22:35:14.347219\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.3036 - bce_dice_loss: 0.3036\n",
      "Training: batch 12 begins at 22:35:14.351971\n",
      "\n",
      "Training: batch 12 ends at 22:35:15.151192\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3006 - bce_dice_loss: 0.3006\n",
      "Training: batch 13 begins at 22:35:15.155316\n",
      "\n",
      "Training: batch 13 ends at 22:35:15.973650\n",
      "14/60 [======>.......................] - ETA: 38s - loss: 0.3089 - bce_dice_loss: 0.3089\n",
      "Training: batch 14 begins at 22:35:15.977801\n",
      "\n",
      "Training: batch 14 ends at 22:35:16.768588\n",
      "15/60 [======>.......................] - ETA: 37s - loss: 0.3023 - bce_dice_loss: 0.3023\n",
      "Training: batch 15 begins at 22:35:16.771412\n",
      "\n",
      "Training: batch 15 ends at 22:35:17.566333\n",
      "16/60 [=======>......................] - ETA: 36s - loss: 0.2953 - bce_dice_loss: 0.2953\n",
      "Training: batch 16 begins at 22:35:17.570664\n",
      "\n",
      "Training: batch 16 ends at 22:35:18.392226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/60 [=======>......................] - ETA: 35s - loss: 0.2996 - bce_dice_loss: 0.2996\n",
      "Training: batch 17 begins at 22:35:18.397095\n",
      "\n",
      "Training: batch 17 ends at 22:35:19.192715\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.2961 - bce_dice_loss: 0.2961\n",
      "Training: batch 18 begins at 22:35:19.197566\n",
      "\n",
      "Training: batch 18 ends at 22:35:20.011170\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3014 - bce_dice_loss: 0.3014\n",
      "Training: batch 19 begins at 22:35:20.015835\n",
      "\n",
      "Training: batch 19 ends at 22:35:20.810851\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2950 - bce_dice_loss: 0.2950\n",
      "Training: batch 20 begins at 22:35:20.815698\n",
      "\n",
      "Training: batch 20 ends at 22:35:21.615746\n",
      "21/60 [=========>....................] - ETA: 32s - loss: 0.2915 - bce_dice_loss: 0.2915\n",
      "Training: batch 21 begins at 22:35:21.618845\n",
      "\n",
      "Training: batch 21 ends at 22:35:22.412720\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.2882 - bce_dice_loss: 0.2882\n",
      "Training: batch 22 begins at 22:35:22.417172\n",
      "\n",
      "Training: batch 22 ends at 22:35:23.219478\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.2903 - bce_dice_loss: 0.2903\n",
      "Training: batch 23 begins at 22:35:23.223902\n",
      "\n",
      "Training: batch 23 ends at 22:35:24.043761\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2911 - bce_dice_loss: 0.2911\n",
      "Training: batch 24 begins at 22:35:24.047483\n",
      "\n",
      "Training: batch 24 ends at 22:35:24.839169\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2941 - bce_dice_loss: 0.2941\n",
      "Training: batch 25 begins at 22:35:24.843493\n",
      "\n",
      "Training: batch 25 ends at 22:35:25.649276\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2939 - bce_dice_loss: 0.2939\n",
      "Training: batch 26 begins at 22:35:25.654127\n",
      "\n",
      "Training: batch 26 ends at 22:35:26.454636\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2942 - bce_dice_loss: 0.2942\n",
      "Training: batch 27 begins at 22:35:26.460608\n",
      "\n",
      "Training: batch 27 ends at 22:35:27.261515\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.2983 - bce_dice_loss: 0.2983\n",
      "Training: batch 28 begins at 22:35:27.265654\n",
      "\n",
      "Training: batch 28 ends at 22:35:28.063911\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3049 - bce_dice_loss: 0.3049\n",
      "Training: batch 29 begins at 22:35:28.068554\n",
      "\n",
      "Training: batch 29 ends at 22:35:28.874284\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3061 - bce_dice_loss: 0.3061\n",
      "Training: batch 30 begins at 22:35:28.879977\n",
      "\n",
      "Training: batch 30 ends at 22:35:29.675737\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3092 - bce_dice_loss: 0.3092\n",
      "Training: batch 31 begins at 22:35:29.684375\n",
      "\n",
      "Training: batch 31 ends at 22:35:30.477174\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3121 - bce_dice_loss: 0.3121\n",
      "Training: batch 32 begins at 22:35:30.482290\n",
      "\n",
      "Training: batch 32 ends at 22:35:31.277970\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.3131 - bce_dice_loss: 0.3131\n",
      "Training: batch 33 begins at 22:35:31.280976\n",
      "\n",
      "Training: batch 33 ends at 22:35:32.106161\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3122 - bce_dice_loss: 0.3122\n",
      "Training: batch 34 begins at 22:35:32.110526\n",
      "\n",
      "Training: batch 34 ends at 22:35:32.903661\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3096 - bce_dice_loss: 0.3096\n",
      "Training: batch 35 begins at 22:35:32.907994\n",
      "\n",
      "Training: batch 35 ends at 22:35:33.712806\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3154 - bce_dice_loss: 0.3154\n",
      "Training: batch 36 begins at 22:35:33.716221\n",
      "\n",
      "Training: batch 36 ends at 22:35:34.517465\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3244 - bce_dice_loss: 0.3244\n",
      "Training: batch 37 begins at 22:35:34.521512\n",
      "\n",
      "Training: batch 37 ends at 22:35:35.320405\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3237 - bce_dice_loss: 0.3237\n",
      "Training: batch 38 begins at 22:35:35.324914\n",
      "\n",
      "Training: batch 38 ends at 22:35:36.142497\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3208 - bce_dice_loss: 0.3208\n",
      "Training: batch 39 begins at 22:35:36.147169\n",
      "\n",
      "Training: batch 39 ends at 22:35:36.940830\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3211 - bce_dice_loss: 0.3211\n",
      "Training: batch 40 begins at 22:35:36.945103\n",
      "\n",
      "Training: batch 40 ends at 22:35:37.741022\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3247 - bce_dice_loss: 0.3247\n",
      "Training: batch 41 begins at 22:35:37.745833\n",
      "\n",
      "Training: batch 41 ends at 22:35:38.556065\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3264 - bce_dice_loss: 0.3264\n",
      "Training: batch 42 begins at 22:35:38.560151\n",
      "\n",
      "Training: batch 42 ends at 22:35:39.354949\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3265 - bce_dice_loss: 0.3265\n",
      "Training: batch 43 begins at 22:35:39.359075\n",
      "\n",
      "Training: batch 43 ends at 22:35:40.151975\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3303 - bce_dice_loss: 0.3303\n",
      "Training: batch 44 begins at 22:35:40.155693\n",
      "\n",
      "Training: batch 44 ends at 22:35:40.981069\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3287 - bce_dice_loss: 0.3287\n",
      "Training: batch 45 begins at 22:35:40.983519\n",
      "\n",
      "Training: batch 45 ends at 22:35:41.775168\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3263 - bce_dice_loss: 0.3263\n",
      "Training: batch 46 begins at 22:35:41.784507\n",
      "\n",
      "Training: batch 46 ends at 22:35:42.575879\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3254 - bce_dice_loss: 0.3254\n",
      "Training: batch 47 begins at 22:35:42.580132\n",
      "\n",
      "Training: batch 47 ends at 22:35:43.401340\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3260 - bce_dice_loss: 0.3260 \n",
      "Training: batch 48 begins at 22:35:43.405854\n",
      "\n",
      "Training: batch 48 ends at 22:35:44.200540\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3233 - bce_dice_loss: 0.3233\n",
      "Training: batch 49 begins at 22:35:44.204289\n",
      "\n",
      "Training: batch 49 ends at 22:35:45.001970\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3288 - bce_dice_loss: 0.3288\n",
      "Training: batch 50 begins at 22:35:45.006331\n",
      "\n",
      "Training: batch 50 ends at 22:35:45.811650\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3270 - bce_dice_loss: 0.3270\n",
      "Training: batch 51 begins at 22:35:45.814424\n",
      "\n",
      "Training: batch 51 ends at 22:35:46.615802\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3270 - bce_dice_loss: 0.3270\n",
      "Training: batch 52 begins at 22:35:46.619368\n",
      "\n",
      "Training: batch 52 ends at 22:35:47.417638\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3271 - bce_dice_loss: 0.3271\n",
      "Training: batch 53 begins at 22:35:47.421935\n",
      "\n",
      "Training: batch 53 ends at 22:35:48.239664\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3283 - bce_dice_loss: 0.3283\n",
      "Training: batch 54 begins at 22:35:48.244250\n",
      "\n",
      "Training: batch 54 ends at 22:35:49.028076\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3271 - bce_dice_loss: 0.3271\n",
      "Training: batch 55 begins at 22:35:49.032263\n",
      "\n",
      "Training: batch 55 ends at 22:35:49.819798\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3265 - bce_dice_loss: 0.3265\n",
      "Training: batch 56 begins at 22:35:49.823788\n",
      "\n",
      "Training: batch 56 ends at 22:35:50.617419\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3249 - bce_dice_loss: 0.3249\n",
      "Training: batch 57 begins at 22:35:50.621470\n",
      "\n",
      "Training: batch 57 ends at 22:35:51.423796\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3233 - bce_dice_loss: 0.3233\n",
      "Training: batch 58 begins at 22:35:51.427512\n",
      "\n",
      "Training: batch 58 ends at 22:35:52.215939\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3254 - bce_dice_loss: 0.3254\n",
      "Training: batch 59 begins at 22:35:52.220677\n",
      "\n",
      "Training: batch 59 ends at 22:35:53.019357\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3239 - bce_dice_loss: 0.3239\n",
      "Evaluating: batch 0 begins at 22:35:53.052952\n",
      "\n",
      "Evaluating: batch 0 ends at 22:35:53.346765\n",
      "\n",
      "Evaluating: batch 1 begins at 22:35:53.348740\n",
      "\n",
      "Evaluating: batch 1 ends at 22:35:53.569758\n",
      "\n",
      "Evaluating: batch 2 begins at 22:35:53.571011\n",
      "\n",
      "Evaluating: batch 2 ends at 22:35:53.791868\n",
      "\n",
      "Evaluating: batch 3 begins at 22:35:53.794366\n",
      "\n",
      "Evaluating: batch 3 ends at 22:35:54.017319\n",
      "\n",
      "Evaluating: batch 4 begins at 22:35:54.018600\n",
      "\n",
      "Evaluating: batch 4 ends at 22:35:54.239972\n",
      "\n",
      "Evaluating: batch 5 begins at 22:35:54.242367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 5 ends at 22:35:54.466077\n",
      "\n",
      "Evaluating: batch 6 begins at 22:35:54.467418\n",
      "\n",
      "Evaluating: batch 6 ends at 22:35:54.697149\n",
      "\n",
      "Evaluating: batch 7 begins at 22:35:54.698701\n",
      "\n",
      "Evaluating: batch 7 ends at 22:35:54.914035\n",
      "\n",
      "Evaluating: batch 8 begins at 22:35:54.915267\n",
      "\n",
      "Evaluating: batch 8 ends at 22:35:55.131669\n",
      "\n",
      "Evaluating: batch 9 begins at 22:35:55.132961\n",
      "\n",
      "Evaluating: batch 9 ends at 22:35:55.350283\n",
      "\n",
      "Evaluating: batch 10 begins at 22:35:55.351871\n",
      "\n",
      "Evaluating: batch 10 ends at 22:35:55.569707\n",
      "\n",
      "Evaluating: batch 11 begins at 22:35:55.570947\n",
      "\n",
      "Evaluating: batch 11 ends at 22:35:55.801084\n",
      "\n",
      "Evaluating: batch 12 begins at 22:35:55.802512\n",
      "\n",
      "Evaluating: batch 12 ends at 22:35:56.030218\n",
      "\n",
      "Evaluating: batch 13 begins at 22:35:56.031462\n",
      "\n",
      "Evaluating: batch 13 ends at 22:35:56.253142\n",
      "\n",
      "Evaluating: batch 14 begins at 22:35:56.254524\n",
      "\n",
      "Evaluating: batch 14 ends at 22:35:56.474970\n",
      "\n",
      "Evaluating: batch 15 begins at 22:35:56.476310\n",
      "\n",
      "Evaluating: batch 15 ends at 22:35:56.702526\n",
      "\n",
      "Evaluating: batch 16 begins at 22:35:56.703978\n",
      "\n",
      "Evaluating: batch 16 ends at 22:35:56.927792\n",
      "\n",
      "Evaluating: batch 17 begins at 22:35:56.929682\n",
      "\n",
      "Evaluating: batch 17 ends at 22:35:57.152002\n",
      "\n",
      "Evaluating: batch 18 begins at 22:35:57.153299\n",
      "\n",
      "Evaluating: batch 18 ends at 22:35:57.373436\n",
      "\n",
      "Evaluating: batch 19 begins at 22:35:57.374976\n",
      "\n",
      "Evaluating: batch 19 ends at 22:35:57.589973\n",
      "\n",
      "Evaluating: batch 20 begins at 22:35:57.591601\n",
      "\n",
      "Evaluating: batch 20 ends at 22:35:57.814089\n",
      "\n",
      "Evaluating: batch 21 begins at 22:35:57.815309\n",
      "\n",
      "Evaluating: batch 21 ends at 22:35:58.033056\n",
      "\n",
      "Evaluating: batch 22 begins at 22:35:58.034570\n",
      "\n",
      "Evaluating: batch 22 ends at 22:35:58.251625\n",
      "\n",
      "Evaluating: batch 23 begins at 22:35:58.253142\n",
      "\n",
      "Evaluating: batch 23 ends at 22:35:58.468349\n",
      "\n",
      "Evaluating: batch 24 begins at 22:35:58.470785\n",
      "\n",
      "Evaluating: batch 24 ends at 22:35:58.690916\n",
      "\n",
      "Evaluating: batch 25 begins at 22:35:58.692582\n",
      "\n",
      "Evaluating: batch 25 ends at 22:35:58.913171\n",
      "\n",
      "Evaluating: batch 26 begins at 22:35:58.915416\n",
      "\n",
      "Evaluating: batch 26 ends at 22:35:59.133178\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34323\n",
      "60/60 [==============================] - 55s 915ms/step - loss: 0.3239 - bce_dice_loss: 0.3239 - val_loss: 0.3634 - val_bce_dice_loss: 0.3634\n",
      "Epoch 21/25\n",
      "\n",
      "Training: batch 0 begins at 22:35:59.163224\n",
      "\n",
      "Training: batch 0 ends at 22:35:59.968653\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3125 - bce_dice_loss: 0.3125\n",
      "Training: batch 1 begins at 22:35:59.971206\n",
      "\n",
      "Training: batch 1 ends at 22:36:00.788463\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4032 - bce_dice_loss: 0.4032\n",
      "Training: batch 2 begins at 22:36:00.792874\n",
      "\n",
      "Training: batch 2 ends at 22:36:01.617186\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 3 begins at 22:36:01.620797\n",
      "\n",
      "Training: batch 3 ends at 22:36:02.419958\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3225 - bce_dice_loss: 0.3225\n",
      "Training: batch 4 begins at 22:36:02.424296\n",
      "\n",
      "Training: batch 4 ends at 22:36:03.226578\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 5 begins at 22:36:03.229709\n",
      "\n",
      "Training: batch 5 ends at 22:36:04.022349\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3802 - bce_dice_loss: 0.3802\n",
      "Training: batch 6 begins at 22:36:04.025336\n",
      "\n",
      "Training: batch 6 ends at 22:36:04.847897\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.3845 - bce_dice_loss: 0.3845\n",
      "Training: batch 7 begins at 22:36:04.851432\n",
      "\n",
      "Training: batch 7 ends at 22:36:05.647777\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3706 - bce_dice_loss: 0.3706\n",
      "Training: batch 8 begins at 22:36:05.652103\n",
      "\n",
      "Training: batch 8 ends at 22:36:06.449290\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3580 - bce_dice_loss: 0.3580\n",
      "Training: batch 9 begins at 22:36:06.451825\n",
      "\n",
      "Training: batch 9 ends at 22:36:07.252532\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3477 - bce_dice_loss: 0.3477\n",
      "Training: batch 10 begins at 22:36:07.256893\n",
      "\n",
      "Training: batch 10 ends at 22:36:08.052534\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3440 - bce_dice_loss: 0.3440\n",
      "Training: batch 11 begins at 22:36:08.056804\n",
      "\n",
      "Training: batch 11 ends at 22:36:08.857262\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3293 - bce_dice_loss: 0.3293\n",
      "Training: batch 12 begins at 22:36:08.861516\n",
      "\n",
      "Training: batch 12 ends at 22:36:09.669383\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 13 begins at 22:36:09.674044\n",
      "\n",
      "Training: batch 13 ends at 22:36:10.472214\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3285 - bce_dice_loss: 0.3285\n",
      "Training: batch 14 begins at 22:36:10.476881\n",
      "\n",
      "Training: batch 14 ends at 22:36:11.275583\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3236 - bce_dice_loss: 0.3236\n",
      "Training: batch 15 begins at 22:36:11.279809\n",
      "\n",
      "Training: batch 15 ends at 22:36:12.074419\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3214 - bce_dice_loss: 0.3214\n",
      "Training: batch 16 begins at 22:36:12.077178\n",
      "\n",
      "Training: batch 16 ends at 22:36:12.871969\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3188 - bce_dice_loss: 0.3188\n",
      "Training: batch 17 begins at 22:36:12.875460\n",
      "\n",
      "Training: batch 17 ends at 22:36:13.669594\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3254 - bce_dice_loss: 0.3254\n",
      "Training: batch 18 begins at 22:36:13.673710\n",
      "\n",
      "Training: batch 18 ends at 22:36:14.472003\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3222 - bce_dice_loss: 0.3222\n",
      "Training: batch 19 begins at 22:36:14.476429\n",
      "\n",
      "Training: batch 19 ends at 22:36:15.276565\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3248 - bce_dice_loss: 0.3248\n",
      "Training: batch 20 begins at 22:36:15.279806\n",
      "\n",
      "Training: batch 20 ends at 22:36:16.067296\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3205 - bce_dice_loss: 0.3205\n",
      "Training: batch 21 begins at 22:36:16.070212\n",
      "\n",
      "Training: batch 21 ends at 22:36:16.868901\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3174 - bce_dice_loss: 0.3174\n",
      "Training: batch 22 begins at 22:36:16.872316\n",
      "\n",
      "Training: batch 22 ends at 22:36:17.649317\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3162 - bce_dice_loss: 0.3162\n",
      "Training: batch 23 begins at 22:36:17.651725\n",
      "\n",
      "Training: batch 23 ends at 22:36:18.446821\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3121 - bce_dice_loss: 0.3121\n",
      "Training: batch 24 begins at 22:36:18.452262\n",
      "\n",
      "Training: batch 24 ends at 22:36:19.239790\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3133 - bce_dice_loss: 0.3133\n",
      "Training: batch 25 begins at 22:36:19.243607\n",
      "\n",
      "Training: batch 25 ends at 22:36:20.036771\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3125 - bce_dice_loss: 0.3125\n",
      "Training: batch 26 begins at 22:36:20.039688\n",
      "\n",
      "Training: batch 26 ends at 22:36:20.837149\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3163 - bce_dice_loss: 0.3163\n",
      "Training: batch 27 begins at 22:36:20.841715\n",
      "\n",
      "Training: batch 27 ends at 22:36:21.635551\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3182 - bce_dice_loss: 0.3182\n",
      "Training: batch 28 begins at 22:36:21.640225\n",
      "\n",
      "Training: batch 28 ends at 22:36:22.463551\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3221 - bce_dice_loss: 0.3221\n",
      "Training: batch 29 begins at 22:36:22.466602\n",
      "\n",
      "Training: batch 29 ends at 22:36:23.264212\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3218 - bce_dice_loss: 0.3218\n",
      "Training: batch 30 begins at 22:36:23.268448\n",
      "\n",
      "Training: batch 30 ends at 22:36:24.094410\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3182 - bce_dice_loss: 0.3182\n",
      "Training: batch 31 begins at 22:36:24.097440\n",
      "\n",
      "Training: batch 31 ends at 22:36:24.901316\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3160 - bce_dice_loss: 0.3160\n",
      "Training: batch 32 begins at 22:36:24.905230\n",
      "\n",
      "Training: batch 32 ends at 22:36:25.702764\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 33 begins at 22:36:25.706330\n",
      "\n",
      "Training: batch 33 ends at 22:36:26.521826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/60 [================>.............] - ETA: 20s - loss: 0.3098 - bce_dice_loss: 0.3098\n",
      "Training: batch 34 begins at 22:36:26.526325\n",
      "\n",
      "Training: batch 34 ends at 22:36:27.352414\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3116 - bce_dice_loss: 0.3116\n",
      "Training: batch 35 begins at 22:36:27.356456\n",
      "\n",
      "Training: batch 35 ends at 22:36:28.151902\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3150 - bce_dice_loss: 0.3150\n",
      "Training: batch 36 begins at 22:36:28.156224\n",
      "\n",
      "Training: batch 36 ends at 22:36:28.966492\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 37 begins at 22:36:28.970902\n",
      "\n",
      "Training: batch 37 ends at 22:36:29.787079\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3130 - bce_dice_loss: 0.3130\n",
      "Training: batch 38 begins at 22:36:29.790860\n",
      "\n",
      "Training: batch 38 ends at 22:36:30.575598\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3122 - bce_dice_loss: 0.3122\n",
      "Training: batch 39 begins at 22:36:30.579929\n",
      "\n",
      "Training: batch 39 ends at 22:36:31.403608\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3082 - bce_dice_loss: 0.3082\n",
      "Training: batch 40 begins at 22:36:31.408096\n",
      "\n",
      "Training: batch 40 ends at 22:36:32.203664\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3050 - bce_dice_loss: 0.3050\n",
      "Training: batch 41 begins at 22:36:32.208174\n",
      "\n",
      "Training: batch 41 ends at 22:36:33.000629\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3041 - bce_dice_loss: 0.3041\n",
      "Training: batch 42 begins at 22:36:33.005056\n",
      "\n",
      "Training: batch 42 ends at 22:36:33.807956\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3049 - bce_dice_loss: 0.3049\n",
      "Training: batch 43 begins at 22:36:33.812380\n",
      "\n",
      "Training: batch 43 ends at 22:36:34.612546\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3073 - bce_dice_loss: 0.3073\n",
      "Training: batch 44 begins at 22:36:34.616959\n",
      "\n",
      "Training: batch 44 ends at 22:36:35.415177\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3042 - bce_dice_loss: 0.3042\n",
      "Training: batch 45 begins at 22:36:35.419643\n",
      "\n",
      "Training: batch 45 ends at 22:36:36.212708\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3078 - bce_dice_loss: 0.3078\n",
      "Training: batch 46 begins at 22:36:36.217434\n",
      "\n",
      "Training: batch 46 ends at 22:36:37.007853\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3039 - bce_dice_loss: 0.3039\n",
      "Training: batch 47 begins at 22:36:37.012552\n",
      "\n",
      "Training: batch 47 ends at 22:36:37.809598\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3049 - bce_dice_loss: 0.3049 \n",
      "Training: batch 48 begins at 22:36:37.814164\n",
      "\n",
      "Training: batch 48 ends at 22:36:38.611265\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3053 - bce_dice_loss: 0.3053\n",
      "Training: batch 49 begins at 22:36:38.616038\n",
      "\n",
      "Training: batch 49 ends at 22:36:39.414217\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3068 - bce_dice_loss: 0.3068\n",
      "Training: batch 50 begins at 22:36:39.418669\n",
      "\n",
      "Training: batch 50 ends at 22:36:40.214338\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3064 - bce_dice_loss: 0.3064\n",
      "Training: batch 51 begins at 22:36:40.217161\n",
      "\n",
      "Training: batch 51 ends at 22:36:41.013606\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3059 - bce_dice_loss: 0.3059\n",
      "Training: batch 52 begins at 22:36:41.017516\n",
      "\n",
      "Training: batch 52 ends at 22:36:41.829780\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3091 - bce_dice_loss: 0.3091\n",
      "Training: batch 53 begins at 22:36:41.835551\n",
      "\n",
      "Training: batch 53 ends at 22:36:42.630652\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3084 - bce_dice_loss: 0.3084\n",
      "Training: batch 54 begins at 22:36:42.633325\n",
      "\n",
      "Training: batch 54 ends at 22:36:43.430981\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3082 - bce_dice_loss: 0.3082\n",
      "Training: batch 55 begins at 22:36:43.434349\n",
      "\n",
      "Training: batch 55 ends at 22:36:44.228016\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3087 - bce_dice_loss: 0.3087\n",
      "Training: batch 56 begins at 22:36:44.231899\n",
      "\n",
      "Training: batch 56 ends at 22:36:45.030372\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3089 - bce_dice_loss: 0.3089\n",
      "Training: batch 57 begins at 22:36:45.033828\n",
      "\n",
      "Training: batch 57 ends at 22:36:45.833618\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3098 - bce_dice_loss: 0.3098\n",
      "Training: batch 58 begins at 22:36:45.838212\n",
      "\n",
      "Training: batch 58 ends at 22:36:46.635068\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3103 - bce_dice_loss: 0.3103\n",
      "Training: batch 59 begins at 22:36:46.637754\n",
      "\n",
      "Training: batch 59 ends at 22:36:47.456909\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3135 - bce_dice_loss: 0.3135\n",
      "Evaluating: batch 0 begins at 22:36:47.488919\n",
      "\n",
      "Evaluating: batch 0 ends at 22:36:47.766307\n",
      "\n",
      "Evaluating: batch 1 begins at 22:36:47.767784\n",
      "\n",
      "Evaluating: batch 1 ends at 22:36:47.986676\n",
      "\n",
      "Evaluating: batch 2 begins at 22:36:47.987955\n",
      "\n",
      "Evaluating: batch 2 ends at 22:36:48.206014\n",
      "\n",
      "Evaluating: batch 3 begins at 22:36:48.207286\n",
      "\n",
      "Evaluating: batch 3 ends at 22:36:48.427007\n",
      "\n",
      "Evaluating: batch 4 begins at 22:36:48.429967\n",
      "\n",
      "Evaluating: batch 4 ends at 22:36:48.648651\n",
      "\n",
      "Evaluating: batch 5 begins at 22:36:48.650354\n",
      "\n",
      "Evaluating: batch 5 ends at 22:36:48.876093\n",
      "\n",
      "Evaluating: batch 6 begins at 22:36:48.877634\n",
      "\n",
      "Evaluating: batch 6 ends at 22:36:49.097447\n",
      "\n",
      "Evaluating: batch 7 begins at 22:36:49.098726\n",
      "\n",
      "Evaluating: batch 7 ends at 22:36:49.319178\n",
      "\n",
      "Evaluating: batch 8 begins at 22:36:49.321515\n",
      "\n",
      "Evaluating: batch 8 ends at 22:36:49.541475\n",
      "\n",
      "Evaluating: batch 9 begins at 22:36:49.544184\n",
      "\n",
      "Evaluating: batch 9 ends at 22:36:49.768029\n",
      "\n",
      "Evaluating: batch 10 begins at 22:36:49.769407\n",
      "\n",
      "Evaluating: batch 10 ends at 22:36:49.987932\n",
      "\n",
      "Evaluating: batch 11 begins at 22:36:49.990591\n",
      "\n",
      "Evaluating: batch 11 ends at 22:36:50.212155\n",
      "\n",
      "Evaluating: batch 12 begins at 22:36:50.213691\n",
      "\n",
      "Evaluating: batch 12 ends at 22:36:50.432692\n",
      "\n",
      "Evaluating: batch 13 begins at 22:36:50.434021\n",
      "\n",
      "Evaluating: batch 13 ends at 22:36:50.652251\n",
      "\n",
      "Evaluating: batch 14 begins at 22:36:50.653524\n",
      "\n",
      "Evaluating: batch 14 ends at 22:36:50.874871\n",
      "\n",
      "Evaluating: batch 15 begins at 22:36:50.877417\n",
      "\n",
      "Evaluating: batch 15 ends at 22:36:51.097988\n",
      "\n",
      "Evaluating: batch 16 begins at 22:36:51.100108\n",
      "\n",
      "Evaluating: batch 16 ends at 22:36:51.319017\n",
      "\n",
      "Evaluating: batch 17 begins at 22:36:51.320282\n",
      "\n",
      "Evaluating: batch 17 ends at 22:36:51.540866\n",
      "\n",
      "Evaluating: batch 18 begins at 22:36:51.543541\n",
      "\n",
      "Evaluating: batch 18 ends at 22:36:51.767668\n",
      "\n",
      "Evaluating: batch 19 begins at 22:36:51.769303\n",
      "\n",
      "Evaluating: batch 19 ends at 22:36:51.987410\n",
      "\n",
      "Evaluating: batch 20 begins at 22:36:51.988681\n",
      "\n",
      "Evaluating: batch 20 ends at 22:36:52.205235\n",
      "\n",
      "Evaluating: batch 21 begins at 22:36:52.206859\n",
      "\n",
      "Evaluating: batch 21 ends at 22:36:52.425176\n",
      "\n",
      "Evaluating: batch 22 begins at 22:36:52.428074\n",
      "\n",
      "Evaluating: batch 22 ends at 22:36:52.646647\n",
      "\n",
      "Evaluating: batch 23 begins at 22:36:52.648277\n",
      "\n",
      "Evaluating: batch 23 ends at 22:36:52.869855\n",
      "\n",
      "Evaluating: batch 24 begins at 22:36:52.872456\n",
      "\n",
      "Evaluating: batch 24 ends at 22:36:53.089424\n",
      "\n",
      "Evaluating: batch 25 begins at 22:36:53.090788\n",
      "\n",
      "Evaluating: batch 25 ends at 22:36:53.308914\n",
      "\n",
      "Evaluating: batch 26 begins at 22:36:53.310188\n",
      "\n",
      "Evaluating: batch 26 ends at 22:36:53.537684\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34323\n",
      "60/60 [==============================] - 54s 908ms/step - loss: 0.3135 - bce_dice_loss: 0.3135 - val_loss: 0.3827 - val_bce_dice_loss: 0.3827\n",
      "Epoch 22/25\n",
      "\n",
      "Training: batch 0 begins at 22:36:53.563484\n",
      "\n",
      "Training: batch 0 ends at 22:36:54.373385\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2416 - bce_dice_loss: 0.2416\n",
      "Training: batch 1 begins at 22:36:54.377846\n",
      "\n",
      "Training: batch 1 ends at 22:36:55.180628\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3298 - bce_dice_loss: 0.3298\n",
      "Training: batch 2 begins at 22:36:55.183989\n",
      "\n",
      "Training: batch 2 ends at 22:36:55.990016\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2893 - bce_dice_loss: 0.2893\n",
      "Training: batch 3 begins at 22:36:55.996078\n",
      "\n",
      "Training: batch 3 ends at 22:36:56.797914\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3196 - bce_dice_loss: 0.3196\n",
      "Training: batch 4 begins at 22:36:56.802503\n",
      "\n",
      "Training: batch 4 ends at 22:36:57.592961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3488 - bce_dice_loss: 0.3488\n",
      "Training: batch 5 begins at 22:36:57.597375\n",
      "\n",
      "Training: batch 5 ends at 22:36:58.410776\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3253 - bce_dice_loss: 0.3253\n",
      "Training: batch 6 begins at 22:36:58.413443\n",
      "\n",
      "Training: batch 6 ends at 22:36:59.210902\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3149 - bce_dice_loss: 0.3149\n",
      "Training: batch 7 begins at 22:36:59.216566\n",
      "\n",
      "Training: batch 7 ends at 22:37:00.011319\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3431 - bce_dice_loss: 0.3431\n",
      "Training: batch 8 begins at 22:37:00.014320\n",
      "\n",
      "Training: batch 8 ends at 22:37:00.832750\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3232 - bce_dice_loss: 0.3232\n",
      "Training: batch 9 begins at 22:37:00.836796\n",
      "\n",
      "Training: batch 9 ends at 22:37:01.636786\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3523 - bce_dice_loss: 0.3523\n",
      "Training: batch 10 begins at 22:37:01.641173\n",
      "\n",
      "Training: batch 10 ends at 22:37:02.445354\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3476 - bce_dice_loss: 0.3476\n",
      "Training: batch 11 begins at 22:37:02.449291\n",
      "\n",
      "Training: batch 11 ends at 22:37:03.247141\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3527 - bce_dice_loss: 0.3527\n",
      "Training: batch 12 begins at 22:37:03.250553\n",
      "\n",
      "Training: batch 12 ends at 22:37:04.086065\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3454 - bce_dice_loss: 0.3454\n",
      "Training: batch 13 begins at 22:37:04.090481\n",
      "\n",
      "Training: batch 13 ends at 22:37:04.889436\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3397 - bce_dice_loss: 0.3397\n",
      "Training: batch 14 begins at 22:37:04.898581\n",
      "\n",
      "Training: batch 14 ends at 22:37:05.694417\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3423 - bce_dice_loss: 0.3423\n",
      "Training: batch 15 begins at 22:37:05.698024\n",
      "\n",
      "Training: batch 15 ends at 22:37:06.492072\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3414 - bce_dice_loss: 0.3414\n",
      "Training: batch 16 begins at 22:37:06.495173\n",
      "\n",
      "Training: batch 16 ends at 22:37:07.289780\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3361 - bce_dice_loss: 0.3361\n",
      "Training: batch 17 begins at 22:37:07.293189\n",
      "\n",
      "Training: batch 17 ends at 22:37:08.112834\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3355 - bce_dice_loss: 0.3355\n",
      "Training: batch 18 begins at 22:37:08.117413\n",
      "\n",
      "Training: batch 18 ends at 22:37:08.913121\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 19 begins at 22:37:08.916504\n",
      "\n",
      "Training: batch 19 ends at 22:37:09.708671\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3405 - bce_dice_loss: 0.3405\n",
      "Training: batch 20 begins at 22:37:09.711962\n",
      "\n",
      "Training: batch 20 ends at 22:37:10.510823\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3477 - bce_dice_loss: 0.3477\n",
      "Training: batch 21 begins at 22:37:10.513735\n",
      "\n",
      "Training: batch 21 ends at 22:37:11.311764\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3467 - bce_dice_loss: 0.3467\n",
      "Training: batch 22 begins at 22:37:11.316042\n",
      "\n",
      "Training: batch 22 ends at 22:37:12.114655\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3426 - bce_dice_loss: 0.3426\n",
      "Training: batch 23 begins at 22:37:12.117761\n",
      "\n",
      "Training: batch 23 ends at 22:37:12.943464\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3390 - bce_dice_loss: 0.3390\n",
      "Training: batch 24 begins at 22:37:12.946596\n",
      "\n",
      "Training: batch 24 ends at 22:37:13.751292\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3394 - bce_dice_loss: 0.3394\n",
      "Training: batch 25 begins at 22:37:13.754837\n",
      "\n",
      "Training: batch 25 ends at 22:37:14.565214\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3359 - bce_dice_loss: 0.3359\n",
      "Training: batch 26 begins at 22:37:14.569648\n",
      "\n",
      "Training: batch 26 ends at 22:37:15.362280\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3445 - bce_dice_loss: 0.3445\n",
      "Training: batch 27 begins at 22:37:15.366489\n",
      "\n",
      "Training: batch 27 ends at 22:37:16.184346\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3397 - bce_dice_loss: 0.3397\n",
      "Training: batch 28 begins at 22:37:16.187760\n",
      "\n",
      "Training: batch 28 ends at 22:37:16.983516\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3473 - bce_dice_loss: 0.3473\n",
      "Training: batch 29 begins at 22:37:16.987991\n",
      "\n",
      "Training: batch 29 ends at 22:37:17.785391\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3484 - bce_dice_loss: 0.3484\n",
      "Training: batch 30 begins at 22:37:17.789716\n",
      "\n",
      "Training: batch 30 ends at 22:37:18.612081\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3443 - bce_dice_loss: 0.3443\n",
      "Training: batch 31 begins at 22:37:18.616424\n",
      "\n",
      "Training: batch 31 ends at 22:37:19.414870\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3468 - bce_dice_loss: 0.3468\n",
      "Training: batch 32 begins at 22:37:19.419005\n",
      "\n",
      "Training: batch 32 ends at 22:37:20.209862\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3452 - bce_dice_loss: 0.3452\n",
      "Training: batch 33 begins at 22:37:20.215756\n",
      "\n",
      "Training: batch 33 ends at 22:37:21.011076\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3468 - bce_dice_loss: 0.3468\n",
      "Training: batch 34 begins at 22:37:21.013716\n",
      "\n",
      "Training: batch 34 ends at 22:37:21.809941\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3477 - bce_dice_loss: 0.3477\n",
      "Training: batch 35 begins at 22:37:21.813933\n",
      "\n",
      "Training: batch 35 ends at 22:37:22.613078\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3464 - bce_dice_loss: 0.3464\n",
      "Training: batch 36 begins at 22:37:22.616255\n",
      "\n",
      "Training: batch 36 ends at 22:37:23.413479\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3476 - bce_dice_loss: 0.3476\n",
      "Training: batch 37 begins at 22:37:23.415837\n",
      "\n",
      "Training: batch 37 ends at 22:37:24.223703\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3449 - bce_dice_loss: 0.3449\n",
      "Training: batch 38 begins at 22:37:24.227737\n",
      "\n",
      "Training: batch 38 ends at 22:37:25.023244\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3450 - bce_dice_loss: 0.3450\n",
      "Training: batch 39 begins at 22:37:25.027524\n",
      "\n",
      "Training: batch 39 ends at 22:37:25.819271\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3430 - bce_dice_loss: 0.3430\n",
      "Training: batch 40 begins at 22:37:25.823620\n",
      "\n",
      "Training: batch 40 ends at 22:37:26.620364\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3435 - bce_dice_loss: 0.3435\n",
      "Training: batch 41 begins at 22:37:26.624653\n",
      "\n",
      "Training: batch 41 ends at 22:37:27.433743\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3420 - bce_dice_loss: 0.3420\n",
      "Training: batch 42 begins at 22:37:27.437261\n",
      "\n",
      "Training: batch 42 ends at 22:37:28.238055\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3483 - bce_dice_loss: 0.3483\n",
      "Training: batch 43 begins at 22:37:28.242342\n",
      "\n",
      "Training: batch 43 ends at 22:37:29.039387\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3497 - bce_dice_loss: 0.3497\n",
      "Training: batch 44 begins at 22:37:29.043446\n",
      "\n",
      "Training: batch 44 ends at 22:37:29.852443\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3493 - bce_dice_loss: 0.3493\n",
      "Training: batch 45 begins at 22:37:29.854956\n",
      "\n",
      "Training: batch 45 ends at 22:37:30.655365\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3474 - bce_dice_loss: 0.3474\n",
      "Training: batch 46 begins at 22:37:30.658244\n",
      "\n",
      "Training: batch 46 ends at 22:37:31.470230\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3455 - bce_dice_loss: 0.3455\n",
      "Training: batch 47 begins at 22:37:31.474432\n",
      "\n",
      "Training: batch 47 ends at 22:37:32.277170\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3443 - bce_dice_loss: 0.3443 \n",
      "Training: batch 48 begins at 22:37:32.279726\n",
      "\n",
      "Training: batch 48 ends at 22:37:33.066387\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3421 - bce_dice_loss: 0.3421\n",
      "Training: batch 49 begins at 22:37:33.069122\n",
      "\n",
      "Training: batch 49 ends at 22:37:33.875454\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3386 - bce_dice_loss: 0.3386\n",
      "Training: batch 50 begins at 22:37:33.879633\n",
      "\n",
      "Training: batch 50 ends at 22:37:34.681121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3369 - bce_dice_loss: 0.3369\n",
      "Training: batch 51 begins at 22:37:34.684319\n",
      "\n",
      "Training: batch 51 ends at 22:37:35.482268\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3373 - bce_dice_loss: 0.3373\n",
      "Training: batch 52 begins at 22:37:35.484794\n",
      "\n",
      "Training: batch 52 ends at 22:37:36.287626\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3371 - bce_dice_loss: 0.3371\n",
      "Training: batch 53 begins at 22:37:36.292383\n",
      "\n",
      "Training: batch 53 ends at 22:37:37.090944\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3385 - bce_dice_loss: 0.3385\n",
      "Training: batch 54 begins at 22:37:37.095363\n",
      "\n",
      "Training: batch 54 ends at 22:37:37.889440\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3375 - bce_dice_loss: 0.3375\n",
      "Training: batch 55 begins at 22:37:37.897117\n",
      "\n",
      "Training: batch 55 ends at 22:37:38.710621\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3409 - bce_dice_loss: 0.3409\n",
      "Training: batch 56 begins at 22:37:38.714644\n",
      "\n",
      "Training: batch 56 ends at 22:37:39.509927\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3391 - bce_dice_loss: 0.3391\n",
      "Training: batch 57 begins at 22:37:39.513328\n",
      "\n",
      "Training: batch 57 ends at 22:37:40.310053\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3401 - bce_dice_loss: 0.3401\n",
      "Training: batch 58 begins at 22:37:40.313094\n",
      "\n",
      "Training: batch 58 ends at 22:37:41.107993\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3417 - bce_dice_loss: 0.3417\n",
      "Training: batch 59 begins at 22:37:41.114890\n",
      "\n",
      "Training: batch 59 ends at 22:37:41.912278\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3485 - bce_dice_loss: 0.3485\n",
      "Evaluating: batch 0 begins at 22:37:41.942704\n",
      "\n",
      "Evaluating: batch 0 ends at 22:37:42.212795\n",
      "\n",
      "Evaluating: batch 1 begins at 22:37:42.214074\n",
      "\n",
      "Evaluating: batch 1 ends at 22:37:42.428722\n",
      "\n",
      "Evaluating: batch 2 begins at 22:37:42.429996\n",
      "\n",
      "Evaluating: batch 2 ends at 22:37:42.643520\n",
      "\n",
      "Evaluating: batch 3 begins at 22:37:42.645078\n",
      "\n",
      "Evaluating: batch 3 ends at 22:37:42.866646\n",
      "\n",
      "Evaluating: batch 4 begins at 22:37:42.868174\n",
      "\n",
      "Evaluating: batch 4 ends at 22:37:43.089244\n",
      "\n",
      "Evaluating: batch 5 begins at 22:37:43.091548\n",
      "\n",
      "Evaluating: batch 5 ends at 22:37:43.312038\n",
      "\n",
      "Evaluating: batch 6 begins at 22:37:43.313525\n",
      "\n",
      "Evaluating: batch 6 ends at 22:37:43.536549\n",
      "\n",
      "Evaluating: batch 7 begins at 22:37:43.537853\n",
      "\n",
      "Evaluating: batch 7 ends at 22:37:43.761935\n",
      "\n",
      "Evaluating: batch 8 begins at 22:37:43.763412\n",
      "\n",
      "Evaluating: batch 8 ends at 22:37:43.985142\n",
      "\n",
      "Evaluating: batch 9 begins at 22:37:43.986974\n",
      "\n",
      "Evaluating: batch 9 ends at 22:37:44.208313\n",
      "\n",
      "Evaluating: batch 10 begins at 22:37:44.209588\n",
      "\n",
      "Evaluating: batch 10 ends at 22:37:44.430558\n",
      "\n",
      "Evaluating: batch 11 begins at 22:37:44.432251\n",
      "\n",
      "Evaluating: batch 11 ends at 22:37:44.655086\n",
      "\n",
      "Evaluating: batch 12 begins at 22:37:44.656423\n",
      "\n",
      "Evaluating: batch 12 ends at 22:37:44.881733\n",
      "\n",
      "Evaluating: batch 13 begins at 22:37:44.883111\n",
      "\n",
      "Evaluating: batch 13 ends at 22:37:45.105232\n",
      "\n",
      "Evaluating: batch 14 begins at 22:37:45.106579\n",
      "\n",
      "Evaluating: batch 14 ends at 22:37:45.325780\n",
      "\n",
      "Evaluating: batch 15 begins at 22:37:45.327877\n",
      "\n",
      "Evaluating: batch 15 ends at 22:37:45.544407\n",
      "\n",
      "Evaluating: batch 16 begins at 22:37:45.545665\n",
      "\n",
      "Evaluating: batch 16 ends at 22:37:45.771557\n",
      "\n",
      "Evaluating: batch 17 begins at 22:37:45.773213\n",
      "\n",
      "Evaluating: batch 17 ends at 22:37:45.993464\n",
      "\n",
      "Evaluating: batch 18 begins at 22:37:45.995127\n",
      "\n",
      "Evaluating: batch 18 ends at 22:37:46.216301\n",
      "\n",
      "Evaluating: batch 19 begins at 22:37:46.218443\n",
      "\n",
      "Evaluating: batch 19 ends at 22:37:46.434935\n",
      "\n",
      "Evaluating: batch 20 begins at 22:37:46.436179\n",
      "\n",
      "Evaluating: batch 20 ends at 22:37:46.655560\n",
      "\n",
      "Evaluating: batch 21 begins at 22:37:46.657511\n",
      "\n",
      "Evaluating: batch 21 ends at 22:37:46.876770\n",
      "\n",
      "Evaluating: batch 22 begins at 22:37:46.878063\n",
      "\n",
      "Evaluating: batch 22 ends at 22:37:47.093949\n",
      "\n",
      "Evaluating: batch 23 begins at 22:37:47.095172\n",
      "\n",
      "Evaluating: batch 23 ends at 22:37:47.311856\n",
      "\n",
      "Evaluating: batch 24 begins at 22:37:47.313390\n",
      "\n",
      "Evaluating: batch 24 ends at 22:37:47.531451\n",
      "\n",
      "Evaluating: batch 25 begins at 22:37:47.533765\n",
      "\n",
      "Evaluating: batch 25 ends at 22:37:47.755787\n",
      "\n",
      "Evaluating: batch 26 begins at 22:37:47.757218\n",
      "\n",
      "Evaluating: batch 26 ends at 22:37:47.974022\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34323\n",
      "60/60 [==============================] - 54s 909ms/step - loss: 0.3485 - bce_dice_loss: 0.3485 - val_loss: 0.4060 - val_bce_dice_loss: 0.4060\n",
      "Epoch 23/25\n",
      "\n",
      "Training: batch 0 begins at 22:37:48.007941\n",
      "\n",
      "Training: batch 0 ends at 22:37:48.817024\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2168 - bce_dice_loss: 0.2168\n",
      "Training: batch 1 begins at 22:37:48.823393\n",
      "\n",
      "Training: batch 1 ends at 22:37:49.648686\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.2551 - bce_dice_loss: 0.2551\n",
      "Training: batch 2 begins at 22:37:49.652780\n",
      "\n",
      "Training: batch 2 ends at 22:37:50.452931\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2460 - bce_dice_loss: 0.2460\n",
      "Training: batch 3 begins at 22:37:50.456391\n",
      "\n",
      "Training: batch 3 ends at 22:37:51.253463\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2587 - bce_dice_loss: 0.2587\n",
      "Training: batch 4 begins at 22:37:51.257653\n",
      "\n",
      "Training: batch 4 ends at 22:37:52.055062\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2693 - bce_dice_loss: 0.2693\n",
      "Training: batch 5 begins at 22:37:52.059776\n",
      "\n",
      "Training: batch 5 ends at 22:37:52.856554\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2727 - bce_dice_loss: 0.2727\n",
      "Training: batch 6 begins at 22:37:52.864199\n",
      "\n",
      "Training: batch 6 ends at 22:37:53.670073\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2569 - bce_dice_loss: 0.2569\n",
      "Training: batch 7 begins at 22:37:53.674012\n",
      "\n",
      "Training: batch 7 ends at 22:37:54.484438\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2721 - bce_dice_loss: 0.2721\n",
      "Training: batch 8 begins at 22:37:54.488885\n",
      "\n",
      "Training: batch 8 ends at 22:37:55.306569\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2853 - bce_dice_loss: 0.2853\n",
      "Training: batch 9 begins at 22:37:55.311342\n",
      "\n",
      "Training: batch 9 ends at 22:37:56.110499\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3041 - bce_dice_loss: 0.3041\n",
      "Training: batch 10 begins at 22:37:56.114650\n",
      "\n",
      "Training: batch 10 ends at 22:37:56.917054\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3037 - bce_dice_loss: 0.3037\n",
      "Training: batch 11 begins at 22:37:56.920601\n",
      "\n",
      "Training: batch 11 ends at 22:37:57.719034\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3040 - bce_dice_loss: 0.3040\n",
      "Training: batch 12 begins at 22:37:57.722665\n",
      "\n",
      "Training: batch 12 ends at 22:37:58.510277\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3042 - bce_dice_loss: 0.3042\n",
      "Training: batch 13 begins at 22:37:58.514605\n",
      "\n",
      "Training: batch 13 ends at 22:37:59.311195\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3025 - bce_dice_loss: 0.3025\n",
      "Training: batch 14 begins at 22:37:59.315382\n",
      "\n",
      "Training: batch 14 ends at 22:38:00.114935\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2995 - bce_dice_loss: 0.2995\n",
      "Training: batch 15 begins at 22:38:00.118692\n",
      "\n",
      "Training: batch 15 ends at 22:38:00.933763\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3120 - bce_dice_loss: 0.3120\n",
      "Training: batch 16 begins at 22:38:00.937359\n",
      "\n",
      "Training: batch 16 ends at 22:38:01.732719\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3214 - bce_dice_loss: 0.3214\n",
      "Training: batch 17 begins at 22:38:01.737192\n",
      "\n",
      "Training: batch 17 ends at 22:38:02.532135\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3211 - bce_dice_loss: 0.3211\n",
      "Training: batch 18 begins at 22:38:02.536453\n",
      "\n",
      "Training: batch 18 ends at 22:38:03.345466\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3200 - bce_dice_loss: 0.3200\n",
      "Training: batch 19 begins at 22:38:03.348553\n",
      "\n",
      "Training: batch 19 ends at 22:38:04.156742\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3263 - bce_dice_loss: 0.3263\n",
      "Training: batch 20 begins at 22:38:04.161701\n",
      "\n",
      "Training: batch 20 ends at 22:38:04.962955\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3215 - bce_dice_loss: 0.3215\n",
      "Training: batch 21 begins at 22:38:04.965406\n",
      "\n",
      "Training: batch 21 ends at 22:38:05.766221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3238 - bce_dice_loss: 0.3238\n",
      "Training: batch 22 begins at 22:38:05.770196\n",
      "\n",
      "Training: batch 22 ends at 22:38:06.565223\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3219 - bce_dice_loss: 0.3219\n",
      "Training: batch 23 begins at 22:38:06.569992\n",
      "\n",
      "Training: batch 23 ends at 22:38:07.385341\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3183 - bce_dice_loss: 0.3183\n",
      "Training: batch 24 begins at 22:38:07.389639\n",
      "\n",
      "Training: batch 24 ends at 22:38:08.182982\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3174 - bce_dice_loss: 0.3174\n",
      "Training: batch 25 begins at 22:38:08.185911\n",
      "\n",
      "Training: batch 25 ends at 22:38:08.983439\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3168 - bce_dice_loss: 0.3168\n",
      "Training: batch 26 begins at 22:38:08.987218\n",
      "\n",
      "Training: batch 26 ends at 22:38:09.800969\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3114 - bce_dice_loss: 0.3114\n",
      "Training: batch 27 begins at 22:38:09.803650\n",
      "\n",
      "Training: batch 27 ends at 22:38:10.599144\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3107 - bce_dice_loss: 0.3107\n",
      "Training: batch 28 begins at 22:38:10.602183\n",
      "\n",
      "Training: batch 28 ends at 22:38:11.398168\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3128 - bce_dice_loss: 0.3128\n",
      "Training: batch 29 begins at 22:38:11.402592\n",
      "\n",
      "Training: batch 29 ends at 22:38:12.226594\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3153 - bce_dice_loss: 0.3153\n",
      "Training: batch 30 begins at 22:38:12.231664\n",
      "\n",
      "Training: batch 30 ends at 22:38:13.034792\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3121 - bce_dice_loss: 0.3121\n",
      "Training: batch 31 begins at 22:38:13.039680\n",
      "\n",
      "Training: batch 31 ends at 22:38:13.848873\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3107 - bce_dice_loss: 0.3107\n",
      "Training: batch 32 begins at 22:38:13.853405\n",
      "\n",
      "Training: batch 32 ends at 22:38:14.656693\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3112 - bce_dice_loss: 0.3112\n",
      "Training: batch 33 begins at 22:38:14.661044\n",
      "\n",
      "Training: batch 33 ends at 22:38:15.464763\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3095 - bce_dice_loss: 0.3095\n",
      "Training: batch 34 begins at 22:38:15.469066\n",
      "\n",
      "Training: batch 34 ends at 22:38:16.263569\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3090 - bce_dice_loss: 0.3090\n",
      "Training: batch 35 begins at 22:38:16.266917\n",
      "\n",
      "Training: batch 35 ends at 22:38:17.062835\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3059 - bce_dice_loss: 0.3059\n",
      "Training: batch 36 begins at 22:38:17.067145\n",
      "\n",
      "Training: batch 36 ends at 22:38:17.869181\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3088 - bce_dice_loss: 0.3088\n",
      "Training: batch 37 begins at 22:38:17.873616\n",
      "\n",
      "Training: batch 37 ends at 22:38:18.670952\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3073 - bce_dice_loss: 0.3073\n",
      "Training: batch 38 begins at 22:38:18.675945\n",
      "\n",
      "Training: batch 38 ends at 22:38:19.474276\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3080 - bce_dice_loss: 0.3080\n",
      "Training: batch 39 begins at 22:38:19.479007\n",
      "\n",
      "Training: batch 39 ends at 22:38:20.277885\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3115 - bce_dice_loss: 0.3115\n",
      "Training: batch 40 begins at 22:38:20.282163\n",
      "\n",
      "Training: batch 40 ends at 22:38:21.075390\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3068 - bce_dice_loss: 0.3068\n",
      "Training: batch 41 begins at 22:38:21.077877\n",
      "\n",
      "Training: batch 41 ends at 22:38:21.875899\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3074 - bce_dice_loss: 0.3074\n",
      "Training: batch 42 begins at 22:38:21.878538\n",
      "\n",
      "Training: batch 42 ends at 22:38:22.672887\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3073 - bce_dice_loss: 0.3073\n",
      "Training: batch 43 begins at 22:38:22.678184\n",
      "\n",
      "Training: batch 43 ends at 22:38:23.477526\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3054 - bce_dice_loss: 0.3054\n",
      "Training: batch 44 begins at 22:38:23.481467\n",
      "\n",
      "Training: batch 44 ends at 22:38:24.291361\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3045 - bce_dice_loss: 0.3045\n",
      "Training: batch 45 begins at 22:38:24.294635\n",
      "\n",
      "Training: batch 45 ends at 22:38:25.103445\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3046 - bce_dice_loss: 0.3046\n",
      "Training: batch 46 begins at 22:38:25.109482\n",
      "\n",
      "Training: batch 46 ends at 22:38:25.907343\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3019 - bce_dice_loss: 0.3019\n",
      "Training: batch 47 begins at 22:38:25.911052\n",
      "\n",
      "Training: batch 47 ends at 22:38:26.701261\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3057 - bce_dice_loss: 0.3057 \n",
      "Training: batch 48 begins at 22:38:26.705523\n",
      "\n",
      "Training: batch 48 ends at 22:38:27.494875\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3085 - bce_dice_loss: 0.3085\n",
      "Training: batch 49 begins at 22:38:27.498345\n",
      "\n",
      "Training: batch 49 ends at 22:38:28.297115\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3089 - bce_dice_loss: 0.3089\n",
      "Training: batch 50 begins at 22:38:28.301125\n",
      "\n",
      "Training: batch 50 ends at 22:38:29.097353\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3084 - bce_dice_loss: 0.3084\n",
      "Training: batch 51 begins at 22:38:29.100970\n",
      "\n",
      "Training: batch 51 ends at 22:38:29.887122\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3116 - bce_dice_loss: 0.3116\n",
      "Training: batch 52 begins at 22:38:29.892240\n",
      "\n",
      "Training: batch 52 ends at 22:38:30.688015\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3135 - bce_dice_loss: 0.3135\n",
      "Training: batch 53 begins at 22:38:30.697210\n",
      "\n",
      "Training: batch 53 ends at 22:38:31.486686\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3137 - bce_dice_loss: 0.3137\n",
      "Training: batch 54 begins at 22:38:31.490999\n",
      "\n",
      "Training: batch 54 ends at 22:38:32.288303\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3147 - bce_dice_loss: 0.3147\n",
      "Training: batch 55 begins at 22:38:32.292576\n",
      "\n",
      "Training: batch 55 ends at 22:38:33.083558\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3148 - bce_dice_loss: 0.3148\n",
      "Training: batch 56 begins at 22:38:33.087930\n",
      "\n",
      "Training: batch 56 ends at 22:38:33.896393\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3147 - bce_dice_loss: 0.3147\n",
      "Training: batch 57 begins at 22:38:33.900807\n",
      "\n",
      "Training: batch 57 ends at 22:38:34.700726\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3172 - bce_dice_loss: 0.3172\n",
      "Training: batch 58 begins at 22:38:34.704047\n",
      "\n",
      "Training: batch 58 ends at 22:38:35.501172\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3171 - bce_dice_loss: 0.3171\n",
      "Training: batch 59 begins at 22:38:35.504844\n",
      "\n",
      "Training: batch 59 ends at 22:38:36.303204\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3172 - bce_dice_loss: 0.3172\n",
      "Evaluating: batch 0 begins at 22:38:36.335320\n",
      "\n",
      "Evaluating: batch 0 ends at 22:38:36.604318\n",
      "\n",
      "Evaluating: batch 1 begins at 22:38:36.605761\n",
      "\n",
      "Evaluating: batch 1 ends at 22:38:36.821897\n",
      "\n",
      "Evaluating: batch 2 begins at 22:38:36.823131\n",
      "\n",
      "Evaluating: batch 2 ends at 22:38:37.045889\n",
      "\n",
      "Evaluating: batch 3 begins at 22:38:37.047721\n",
      "\n",
      "Evaluating: batch 3 ends at 22:38:37.265459\n",
      "\n",
      "Evaluating: batch 4 begins at 22:38:37.268057\n",
      "\n",
      "Evaluating: batch 4 ends at 22:38:37.488973\n",
      "\n",
      "Evaluating: batch 5 begins at 22:38:37.490668\n",
      "\n",
      "Evaluating: batch 5 ends at 22:38:37.712998\n",
      "\n",
      "Evaluating: batch 6 begins at 22:38:37.716038\n",
      "\n",
      "Evaluating: batch 6 ends at 22:38:37.935382\n",
      "\n",
      "Evaluating: batch 7 begins at 22:38:37.937820\n",
      "\n",
      "Evaluating: batch 7 ends at 22:38:38.156789\n",
      "\n",
      "Evaluating: batch 8 begins at 22:38:38.162571\n",
      "\n",
      "Evaluating: batch 8 ends at 22:38:38.380811\n",
      "\n",
      "Evaluating: batch 9 begins at 22:38:38.382990\n",
      "\n",
      "Evaluating: batch 9 ends at 22:38:38.602754\n",
      "\n",
      "Evaluating: batch 10 begins at 22:38:38.605060\n",
      "\n",
      "Evaluating: batch 10 ends at 22:38:38.831399\n",
      "\n",
      "Evaluating: batch 11 begins at 22:38:38.832745\n",
      "\n",
      "Evaluating: batch 11 ends at 22:38:39.055731\n",
      "\n",
      "Evaluating: batch 12 begins at 22:38:39.057694\n",
      "\n",
      "Evaluating: batch 12 ends at 22:38:39.280256\n",
      "\n",
      "Evaluating: batch 13 begins at 22:38:39.282432\n",
      "\n",
      "Evaluating: batch 13 ends at 22:38:39.505661\n",
      "\n",
      "Evaluating: batch 14 begins at 22:38:39.507405\n",
      "\n",
      "Evaluating: batch 14 ends at 22:38:39.726486\n",
      "\n",
      "Evaluating: batch 15 begins at 22:38:39.728311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 15 ends at 22:38:39.949602\n",
      "\n",
      "Evaluating: batch 16 begins at 22:38:39.952070\n",
      "\n",
      "Evaluating: batch 16 ends at 22:38:40.174484\n",
      "\n",
      "Evaluating: batch 17 begins at 22:38:40.175793\n",
      "\n",
      "Evaluating: batch 17 ends at 22:38:40.396608\n",
      "\n",
      "Evaluating: batch 18 begins at 22:38:40.397710\n",
      "\n",
      "Evaluating: batch 18 ends at 22:38:40.618960\n",
      "\n",
      "Evaluating: batch 19 begins at 22:38:40.620598\n",
      "\n",
      "Evaluating: batch 19 ends at 22:38:40.841777\n",
      "\n",
      "Evaluating: batch 20 begins at 22:38:40.843245\n",
      "\n",
      "Evaluating: batch 20 ends at 22:38:41.061050\n",
      "\n",
      "Evaluating: batch 21 begins at 22:38:41.063606\n",
      "\n",
      "Evaluating: batch 21 ends at 22:38:41.282884\n",
      "\n",
      "Evaluating: batch 22 begins at 22:38:41.284771\n",
      "\n",
      "Evaluating: batch 22 ends at 22:38:41.503354\n",
      "\n",
      "Evaluating: batch 23 begins at 22:38:41.504684\n",
      "\n",
      "Evaluating: batch 23 ends at 22:38:41.721463\n",
      "\n",
      "Evaluating: batch 24 begins at 22:38:41.722739\n",
      "\n",
      "Evaluating: batch 24 ends at 22:38:41.942478\n",
      "\n",
      "Evaluating: batch 25 begins at 22:38:41.945527\n",
      "\n",
      "Evaluating: batch 25 ends at 22:38:42.163681\n",
      "\n",
      "Evaluating: batch 26 begins at 22:38:42.165389\n",
      "\n",
      "Evaluating: batch 26 ends at 22:38:42.385687\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34323\n",
      "60/60 [==============================] - 54s 908ms/step - loss: 0.3172 - bce_dice_loss: 0.3172 - val_loss: 0.3475 - val_bce_dice_loss: 0.3475\n",
      "Epoch 24/25\n",
      "\n",
      "Training: batch 0 begins at 22:38:42.418282\n",
      "\n",
      "Training: batch 0 ends at 22:38:43.227011\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3385 - bce_dice_loss: 0.3385\n",
      "Training: batch 1 begins at 22:38:43.231465\n",
      "\n",
      "Training: batch 1 ends at 22:38:44.041470\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2870 - bce_dice_loss: 0.2870\n",
      "Training: batch 2 begins at 22:38:44.047006\n",
      "\n",
      "Training: batch 2 ends at 22:38:44.853269\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2904 - bce_dice_loss: 0.2904\n",
      "Training: batch 3 begins at 22:38:44.858218\n",
      "\n",
      "Training: batch 3 ends at 22:38:45.657540\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2738 - bce_dice_loss: 0.2738\n",
      "Training: batch 4 begins at 22:38:45.662045\n",
      "\n",
      "Training: batch 4 ends at 22:38:46.461134\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3160 - bce_dice_loss: 0.3160\n",
      "Training: batch 5 begins at 22:38:46.463619\n",
      "\n",
      "Training: batch 5 ends at 22:38:47.259776\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3014 - bce_dice_loss: 0.3014\n",
      "Training: batch 6 begins at 22:38:47.264276\n",
      "\n",
      "Training: batch 6 ends at 22:38:48.059794\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2921 - bce_dice_loss: 0.2921\n",
      "Training: batch 7 begins at 22:38:48.064364\n",
      "\n",
      "Training: batch 7 ends at 22:38:48.867916\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2882 - bce_dice_loss: 0.2882\n",
      "Training: batch 8 begins at 22:38:48.872289\n",
      "\n",
      "Training: batch 8 ends at 22:38:49.667159\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2882 - bce_dice_loss: 0.2882\n",
      "Training: batch 9 begins at 22:38:49.671472\n",
      "\n",
      "Training: batch 9 ends at 22:38:50.468819\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2911 - bce_dice_loss: 0.2911\n",
      "Training: batch 10 begins at 22:38:50.473208\n",
      "\n",
      "Training: batch 10 ends at 22:38:51.270151\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2915 - bce_dice_loss: 0.2915\n",
      "Training: batch 11 begins at 22:38:51.274309\n",
      "\n",
      "Training: batch 11 ends at 22:38:52.068132\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3012 - bce_dice_loss: 0.3012\n",
      "Training: batch 12 begins at 22:38:52.072581\n",
      "\n",
      "Training: batch 12 ends at 22:38:52.874515\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2951 - bce_dice_loss: 0.2951\n",
      "Training: batch 13 begins at 22:38:52.878112\n",
      "\n",
      "Training: batch 13 ends at 22:38:53.674339\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2995 - bce_dice_loss: 0.2995\n",
      "Training: batch 14 begins at 22:38:53.678536\n",
      "\n",
      "Training: batch 14 ends at 22:38:54.477368\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2979 - bce_dice_loss: 0.2979\n",
      "Training: batch 15 begins at 22:38:54.481654\n",
      "\n",
      "Training: batch 15 ends at 22:38:55.306216\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3012 - bce_dice_loss: 0.3012\n",
      "Training: batch 16 begins at 22:38:55.311587\n",
      "\n",
      "Training: batch 16 ends at 22:38:56.118819\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2995 - bce_dice_loss: 0.2995\n",
      "Training: batch 17 begins at 22:38:56.123118\n",
      "\n",
      "Training: batch 17 ends at 22:38:56.920850\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3018 - bce_dice_loss: 0.3018\n",
      "Training: batch 18 begins at 22:38:56.926742\n",
      "\n",
      "Training: batch 18 ends at 22:38:57.745656\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2991 - bce_dice_loss: 0.2991\n",
      "Training: batch 19 begins at 22:38:57.749302\n",
      "\n",
      "Training: batch 19 ends at 22:38:58.541122\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2994 - bce_dice_loss: 0.2994\n",
      "Training: batch 20 begins at 22:38:58.545376\n",
      "\n",
      "Training: batch 20 ends at 22:38:59.348942\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3103 - bce_dice_loss: 0.3103\n",
      "Training: batch 21 begins at 22:38:59.353280\n",
      "\n",
      "Training: batch 21 ends at 22:39:00.180583\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3046 - bce_dice_loss: 0.3046\n",
      "Training: batch 22 begins at 22:39:00.183141\n",
      "\n",
      "Training: batch 22 ends at 22:39:00.984340\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3039 - bce_dice_loss: 0.3039\n",
      "Training: batch 23 begins at 22:39:00.989004\n",
      "\n",
      "Training: batch 23 ends at 22:39:01.788158\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3141 - bce_dice_loss: 0.3141\n",
      "Training: batch 24 begins at 22:39:01.792840\n",
      "\n",
      "Training: batch 24 ends at 22:39:02.617823\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3225 - bce_dice_loss: 0.3225\n",
      "Training: batch 25 begins at 22:39:02.621601\n",
      "\n",
      "Training: batch 25 ends at 22:39:03.520097\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3194 - bce_dice_loss: 0.3194\n",
      "Training: batch 26 begins at 22:39:03.525297\n",
      "\n",
      "Training: batch 26 ends at 22:39:04.423697\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3277 - bce_dice_loss: 0.3277\n",
      "Training: batch 27 begins at 22:39:04.427574\n",
      "\n",
      "Training: batch 27 ends at 22:39:05.317767\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.3326 - bce_dice_loss: 0.3326\n",
      "Training: batch 28 begins at 22:39:05.321838\n",
      "\n",
      "Training: batch 28 ends at 22:39:06.204823\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3343 - bce_dice_loss: 0.3343\n",
      "Training: batch 29 begins at 22:39:06.209024\n",
      "\n",
      "Training: batch 29 ends at 22:39:07.012301\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3313 - bce_dice_loss: 0.3313\n",
      "Training: batch 30 begins at 22:39:07.016683\n",
      "\n",
      "Training: batch 30 ends at 22:39:07.824031\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3330 - bce_dice_loss: 0.3330\n",
      "Training: batch 31 begins at 22:39:07.828251\n",
      "\n",
      "Training: batch 31 ends at 22:39:08.645557\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3288 - bce_dice_loss: 0.3288\n",
      "Training: batch 32 begins at 22:39:08.649355\n",
      "\n",
      "Training: batch 32 ends at 22:39:09.447848\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.3279 - bce_dice_loss: 0.3279\n",
      "Training: batch 33 begins at 22:39:09.451373\n",
      "\n",
      "Training: batch 33 ends at 22:39:10.274492\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3322 - bce_dice_loss: 0.3322\n",
      "Training: batch 34 begins at 22:39:10.279111\n",
      "\n",
      "Training: batch 34 ends at 22:39:11.074619\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3330 - bce_dice_loss: 0.3330\n",
      "Training: batch 35 begins at 22:39:11.078932\n",
      "\n",
      "Training: batch 35 ends at 22:39:11.877779\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3345 - bce_dice_loss: 0.3345\n",
      "Training: batch 36 begins at 22:39:11.882319\n",
      "\n",
      "Training: batch 36 ends at 22:39:12.705343\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3387 - bce_dice_loss: 0.3387\n",
      "Training: batch 37 begins at 22:39:12.709772\n",
      "\n",
      "Training: batch 37 ends at 22:39:13.494554\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3422 - bce_dice_loss: 0.3422\n",
      "Training: batch 38 begins at 22:39:13.498596\n",
      "\n",
      "Training: batch 38 ends at 22:39:14.306995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 39 begins at 22:39:14.310446\n",
      "\n",
      "Training: batch 39 ends at 22:39:15.109128\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3371 - bce_dice_loss: 0.3371\n",
      "Training: batch 40 begins at 22:39:15.113272\n",
      "\n",
      "Training: batch 40 ends at 22:39:15.932746\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3387 - bce_dice_loss: 0.3387\n",
      "Training: batch 41 begins at 22:39:15.936386\n",
      "\n",
      "Training: batch 41 ends at 22:39:16.732421\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3358 - bce_dice_loss: 0.3358\n",
      "Training: batch 42 begins at 22:39:16.735118\n",
      "\n",
      "Training: batch 42 ends at 22:39:17.529038\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3330 - bce_dice_loss: 0.3330\n",
      "Training: batch 43 begins at 22:39:17.532494\n",
      "\n",
      "Training: batch 43 ends at 22:39:18.355260\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3333 - bce_dice_loss: 0.3333\n",
      "Training: batch 44 begins at 22:39:18.359328\n",
      "\n",
      "Training: batch 44 ends at 22:39:19.161122\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3338 - bce_dice_loss: 0.3338\n",
      "Training: batch 45 begins at 22:39:19.165319\n",
      "\n",
      "Training: batch 45 ends at 22:39:19.963133\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3333 - bce_dice_loss: 0.3333\n",
      "Training: batch 46 begins at 22:39:19.967786\n",
      "\n",
      "Training: batch 46 ends at 22:39:20.768715\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3316 - bce_dice_loss: 0.3316\n",
      "Training: batch 47 begins at 22:39:20.772746\n",
      "\n",
      "Training: batch 47 ends at 22:39:21.573820\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3293 - bce_dice_loss: 0.3293 \n",
      "Training: batch 48 begins at 22:39:21.578213\n",
      "\n",
      "Training: batch 48 ends at 22:39:22.373723\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3307 - bce_dice_loss: 0.3307\n",
      "Training: batch 49 begins at 22:39:22.378567\n",
      "\n",
      "Training: batch 49 ends at 22:39:23.189641\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3296 - bce_dice_loss: 0.3296\n",
      "Training: batch 50 begins at 22:39:23.194842\n",
      "\n",
      "Training: batch 50 ends at 22:39:24.004381\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3271 - bce_dice_loss: 0.3271\n",
      "Training: batch 51 begins at 22:39:24.010386\n",
      "\n",
      "Training: batch 51 ends at 22:39:24.811661\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3264 - bce_dice_loss: 0.3264\n",
      "Training: batch 52 begins at 22:39:24.815103\n",
      "\n",
      "Training: batch 52 ends at 22:39:25.610359\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3304 - bce_dice_loss: 0.3304\n",
      "Training: batch 53 begins at 22:39:25.613845\n",
      "\n",
      "Training: batch 53 ends at 22:39:26.421660\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3274 - bce_dice_loss: 0.3274\n",
      "Training: batch 54 begins at 22:39:26.427671\n",
      "\n",
      "Training: batch 54 ends at 22:39:27.225747\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3268 - bce_dice_loss: 0.3268\n",
      "Training: batch 55 begins at 22:39:27.230125\n",
      "\n",
      "Training: batch 55 ends at 22:39:28.028862\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3260 - bce_dice_loss: 0.3260\n",
      "Training: batch 56 begins at 22:39:28.032765\n",
      "\n",
      "Training: batch 56 ends at 22:39:28.829116\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3266 - bce_dice_loss: 0.3266\n",
      "Training: batch 57 begins at 22:39:28.833577\n",
      "\n",
      "Training: batch 57 ends at 22:39:29.639719\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3260 - bce_dice_loss: 0.3260\n",
      "Training: batch 58 begins at 22:39:29.643692\n",
      "\n",
      "Training: batch 58 ends at 22:39:30.440275\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3251 - bce_dice_loss: 0.3251\n",
      "Training: batch 59 begins at 22:39:30.445050\n",
      "\n",
      "Training: batch 59 ends at 22:39:31.235208\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3241 - bce_dice_loss: 0.3241\n",
      "Evaluating: batch 0 begins at 22:39:31.267579\n",
      "\n",
      "Evaluating: batch 0 ends at 22:39:31.538670\n",
      "\n",
      "Evaluating: batch 1 begins at 22:39:31.539812\n",
      "\n",
      "Evaluating: batch 1 ends at 22:39:31.758977\n",
      "\n",
      "Evaluating: batch 2 begins at 22:39:31.760966\n",
      "\n",
      "Evaluating: batch 2 ends at 22:39:31.977788\n",
      "\n",
      "Evaluating: batch 3 begins at 22:39:31.979324\n",
      "\n",
      "Evaluating: batch 3 ends at 22:39:32.201324\n",
      "\n",
      "Evaluating: batch 4 begins at 22:39:32.203724\n",
      "\n",
      "Evaluating: batch 4 ends at 22:39:32.425117\n",
      "\n",
      "Evaluating: batch 5 begins at 22:39:32.426796\n",
      "\n",
      "Evaluating: batch 5 ends at 22:39:32.647567\n",
      "\n",
      "Evaluating: batch 6 begins at 22:39:32.649047\n",
      "\n",
      "Evaluating: batch 6 ends at 22:39:32.869875\n",
      "\n",
      "Evaluating: batch 7 begins at 22:39:32.872410\n",
      "\n",
      "Evaluating: batch 7 ends at 22:39:33.093232\n",
      "\n",
      "Evaluating: batch 8 begins at 22:39:33.094495\n",
      "\n",
      "Evaluating: batch 8 ends at 22:39:33.314517\n",
      "\n",
      "Evaluating: batch 9 begins at 22:39:33.315813\n",
      "\n",
      "Evaluating: batch 9 ends at 22:39:33.541101\n",
      "\n",
      "Evaluating: batch 10 begins at 22:39:33.542605\n",
      "\n",
      "Evaluating: batch 10 ends at 22:39:33.773759\n",
      "\n",
      "Evaluating: batch 11 begins at 22:39:33.776191\n",
      "\n",
      "Evaluating: batch 11 ends at 22:39:33.996603\n",
      "\n",
      "Evaluating: batch 12 begins at 22:39:33.999385\n",
      "\n",
      "Evaluating: batch 12 ends at 22:39:34.222701\n",
      "\n",
      "Evaluating: batch 13 begins at 22:39:34.223993\n",
      "\n",
      "Evaluating: batch 13 ends at 22:39:34.445290\n",
      "\n",
      "Evaluating: batch 14 begins at 22:39:34.446666\n",
      "\n",
      "Evaluating: batch 14 ends at 22:39:34.664027\n",
      "\n",
      "Evaluating: batch 15 begins at 22:39:34.666339\n",
      "\n",
      "Evaluating: batch 15 ends at 22:39:34.893169\n",
      "\n",
      "Evaluating: batch 16 begins at 22:39:34.894703\n",
      "\n",
      "Evaluating: batch 16 ends at 22:39:35.111923\n",
      "\n",
      "Evaluating: batch 17 begins at 22:39:35.113780\n",
      "\n",
      "Evaluating: batch 17 ends at 22:39:35.337694\n",
      "\n",
      "Evaluating: batch 18 begins at 22:39:35.339227\n",
      "\n",
      "Evaluating: batch 18 ends at 22:39:35.558078\n",
      "\n",
      "Evaluating: batch 19 begins at 22:39:35.560588\n",
      "\n",
      "Evaluating: batch 19 ends at 22:39:35.781369\n",
      "\n",
      "Evaluating: batch 20 begins at 22:39:35.782872\n",
      "\n",
      "Evaluating: batch 20 ends at 22:39:36.000225\n",
      "\n",
      "Evaluating: batch 21 begins at 22:39:36.002641\n",
      "\n",
      "Evaluating: batch 21 ends at 22:39:36.220348\n",
      "\n",
      "Evaluating: batch 22 begins at 22:39:36.221992\n",
      "\n",
      "Evaluating: batch 22 ends at 22:39:36.441354\n",
      "\n",
      "Evaluating: batch 23 begins at 22:39:36.442771\n",
      "\n",
      "Evaluating: batch 23 ends at 22:39:36.659155\n",
      "\n",
      "Evaluating: batch 24 begins at 22:39:36.660565\n",
      "\n",
      "Evaluating: batch 24 ends at 22:39:36.880783\n",
      "\n",
      "Evaluating: batch 25 begins at 22:39:36.883152\n",
      "\n",
      "Evaluating: batch 25 ends at 22:39:37.102416\n",
      "\n",
      "Evaluating: batch 26 begins at 22:39:37.103902\n",
      "\n",
      "Evaluating: batch 26 ends at 22:39:37.320640\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34323\n",
      "60/60 [==============================] - 55s 917ms/step - loss: 0.3241 - bce_dice_loss: 0.3241 - val_loss: 0.3454 - val_bce_dice_loss: 0.3454\n",
      "Epoch 25/25\n",
      "\n",
      "Training: batch 0 begins at 22:39:37.350682\n",
      "\n",
      "Training: batch 0 ends at 22:39:38.157723\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.4253 - bce_dice_loss: 0.4253\n",
      "Training: batch 1 begins at 22:39:38.162243\n",
      "\n",
      "Training: batch 1 ends at 22:39:38.968849\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3842 - bce_dice_loss: 0.3842\n",
      "Training: batch 2 begins at 22:39:38.978800\n",
      "\n",
      "Training: batch 2 ends at 22:39:39.799764\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3591 - bce_dice_loss: 0.3591\n",
      "Training: batch 3 begins at 22:39:39.803582\n",
      "\n",
      "Training: batch 3 ends at 22:39:40.603819\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3337 - bce_dice_loss: 0.3337\n",
      "Training: batch 4 begins at 22:39:40.607906\n",
      "\n",
      "Training: batch 4 ends at 22:39:41.428140\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3768 - bce_dice_loss: 0.3768\n",
      "Training: batch 5 begins at 22:39:41.432100\n",
      "\n",
      "Training: batch 5 ends at 22:39:42.234416\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.3653 - bce_dice_loss: 0.3653\n",
      "Training: batch 6 begins at 22:39:42.238423\n",
      "\n",
      "Training: batch 6 ends at 22:39:43.031784\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.3668 - bce_dice_loss: 0.3668\n",
      "Training: batch 7 begins at 22:39:43.036069\n",
      "\n",
      "Training: batch 7 ends at 22:39:43.843143\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3570 - bce_dice_loss: 0.3570\n",
      "Training: batch 8 begins at 22:39:43.847286\n",
      "\n",
      "Training: batch 8 ends at 22:39:44.648608\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3519 - bce_dice_loss: 0.3519\n",
      "Training: batch 9 begins at 22:39:44.652880\n",
      "\n",
      "Training: batch 9 ends at 22:39:45.451739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3315 - bce_dice_loss: 0.3315\n",
      "Training: batch 10 begins at 22:39:45.456518\n",
      "\n",
      "Training: batch 10 ends at 22:39:46.266118\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3250 - bce_dice_loss: 0.3250\n",
      "Training: batch 11 begins at 22:39:46.270114\n",
      "\n",
      "Training: batch 11 ends at 22:39:47.065017\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3182 - bce_dice_loss: 0.3182\n",
      "Training: batch 12 begins at 22:39:47.069369\n",
      "\n",
      "Training: batch 12 ends at 22:39:47.870856\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3215 - bce_dice_loss: 0.3215\n",
      "Training: batch 13 begins at 22:39:47.875810\n",
      "\n",
      "Training: batch 13 ends at 22:39:48.669448\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3242 - bce_dice_loss: 0.3242\n",
      "Training: batch 14 begins at 22:39:48.674731\n",
      "\n",
      "Training: batch 14 ends at 22:39:49.472210\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3170 - bce_dice_loss: 0.3170\n",
      "Training: batch 15 begins at 22:39:49.476142\n",
      "\n",
      "Training: batch 15 ends at 22:39:50.297991\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3208 - bce_dice_loss: 0.3208\n",
      "Training: batch 16 begins at 22:39:50.301509\n",
      "\n",
      "Training: batch 16 ends at 22:39:51.100289\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3179 - bce_dice_loss: 0.3179\n",
      "Training: batch 17 begins at 22:39:51.103411\n",
      "\n",
      "Training: batch 17 ends at 22:39:51.904212\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3162 - bce_dice_loss: 0.3162\n",
      "Training: batch 18 begins at 22:39:51.908919\n",
      "\n",
      "Training: batch 18 ends at 22:39:52.728687\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3072 - bce_dice_loss: 0.3072\n",
      "Training: batch 19 begins at 22:39:52.732066\n",
      "\n",
      "Training: batch 19 ends at 22:39:53.525695\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3135 - bce_dice_loss: 0.3135\n",
      "Training: batch 20 begins at 22:39:53.529265\n",
      "\n",
      "Training: batch 20 ends at 22:39:54.344638\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3188 - bce_dice_loss: 0.3188\n",
      "Training: batch 21 begins at 22:39:54.348005\n",
      "\n",
      "Training: batch 21 ends at 22:39:55.165831\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3228 - bce_dice_loss: 0.3228\n",
      "Training: batch 22 begins at 22:39:55.170196\n",
      "\n",
      "Training: batch 22 ends at 22:39:55.967549\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3238 - bce_dice_loss: 0.3238\n",
      "Training: batch 23 begins at 22:39:55.970292\n",
      "\n",
      "Training: batch 23 ends at 22:39:56.764090\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3284 - bce_dice_loss: 0.3284\n",
      "Training: batch 24 begins at 22:39:56.766244\n",
      "\n",
      "Training: batch 24 ends at 22:39:57.557465\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3227 - bce_dice_loss: 0.3227\n",
      "Training: batch 25 begins at 22:39:57.561350\n",
      "\n",
      "Training: batch 25 ends at 22:39:58.357867\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3246 - bce_dice_loss: 0.3246\n",
      "Training: batch 26 begins at 22:39:58.362628\n",
      "\n",
      "Training: batch 26 ends at 22:39:59.172167\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3339 - bce_dice_loss: 0.3339\n",
      "Training: batch 27 begins at 22:39:59.177931\n",
      "\n",
      "Training: batch 27 ends at 22:39:59.976799\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3378 - bce_dice_loss: 0.3378\n",
      "Training: batch 28 begins at 22:39:59.980454\n",
      "\n",
      "Training: batch 28 ends at 22:40:00.806363\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3361 - bce_dice_loss: 0.3361\n",
      "Training: batch 29 begins at 22:40:00.809981\n",
      "\n",
      "Training: batch 29 ends at 22:40:01.607432\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3355 - bce_dice_loss: 0.3355\n",
      "Training: batch 30 begins at 22:40:01.611842\n",
      "\n",
      "Training: batch 30 ends at 22:40:02.438635\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3303 - bce_dice_loss: 0.3303\n",
      "Training: batch 31 begins at 22:40:02.443953\n",
      "\n",
      "Training: batch 31 ends at 22:40:03.244445\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3275 - bce_dice_loss: 0.3275\n",
      "Training: batch 32 begins at 22:40:03.248036\n",
      "\n",
      "Training: batch 32 ends at 22:40:04.050801\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3277 - bce_dice_loss: 0.3277\n",
      "Training: batch 33 begins at 22:40:04.056634\n",
      "\n",
      "Training: batch 33 ends at 22:40:04.883561\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3239 - bce_dice_loss: 0.3239\n",
      "Training: batch 34 begins at 22:40:04.887646\n",
      "\n",
      "Training: batch 34 ends at 22:40:05.683169\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3239 - bce_dice_loss: 0.3239\n",
      "Training: batch 35 begins at 22:40:05.685952\n",
      "\n",
      "Training: batch 35 ends at 22:40:06.480429\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3214 - bce_dice_loss: 0.3214\n",
      "Training: batch 36 begins at 22:40:06.484628\n",
      "\n",
      "Training: batch 36 ends at 22:40:07.295260\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3198 - bce_dice_loss: 0.3198\n",
      "Training: batch 37 begins at 22:40:07.298626\n",
      "\n",
      "Training: batch 37 ends at 22:40:08.096523\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3180 - bce_dice_loss: 0.3180\n",
      "Training: batch 38 begins at 22:40:08.100889\n",
      "\n",
      "Training: batch 38 ends at 22:40:08.901287\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3172 - bce_dice_loss: 0.3172\n",
      "Training: batch 39 begins at 22:40:08.905636\n",
      "\n",
      "Training: batch 39 ends at 22:40:09.730744\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3175 - bce_dice_loss: 0.3175\n",
      "Training: batch 40 begins at 22:40:09.733814\n",
      "\n",
      "Training: batch 40 ends at 22:40:10.535965\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3195 - bce_dice_loss: 0.3195\n",
      "Training: batch 41 begins at 22:40:10.539909\n",
      "\n",
      "Training: batch 41 ends at 22:40:11.334983\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3233 - bce_dice_loss: 0.3233\n",
      "Training: batch 42 begins at 22:40:11.337891\n",
      "\n",
      "Training: batch 42 ends at 22:40:12.156541\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3192 - bce_dice_loss: 0.3192\n",
      "Training: batch 43 begins at 22:40:12.161049\n",
      "\n",
      "Training: batch 43 ends at 22:40:12.960593\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3149 - bce_dice_loss: 0.3149\n",
      "Training: batch 44 begins at 22:40:12.964939\n",
      "\n",
      "Training: batch 44 ends at 22:40:13.765966\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3129 - bce_dice_loss: 0.3129\n",
      "Training: batch 45 begins at 22:40:13.769512\n",
      "\n",
      "Training: batch 45 ends at 22:40:14.585854\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 46 begins at 22:40:14.590720\n",
      "\n",
      "Training: batch 46 ends at 22:40:15.381589\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3130 - bce_dice_loss: 0.3130\n",
      "Training: batch 47 begins at 22:40:15.386136\n",
      "\n",
      "Training: batch 47 ends at 22:40:16.192558\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3104 - bce_dice_loss: 0.3104 \n",
      "Training: batch 48 begins at 22:40:16.197195\n",
      "\n",
      "Training: batch 48 ends at 22:40:16.990792\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3109 - bce_dice_loss: 0.3109\n",
      "Training: batch 49 begins at 22:40:16.995147\n",
      "\n",
      "Training: batch 49 ends at 22:40:17.785766\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3104 - bce_dice_loss: 0.3104\n",
      "Training: batch 50 begins at 22:40:17.790394\n",
      "\n",
      "Training: batch 50 ends at 22:40:18.610440\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3115 - bce_dice_loss: 0.3115\n",
      "Training: batch 51 begins at 22:40:18.613320\n",
      "\n",
      "Training: batch 51 ends at 22:40:19.410713\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3129 - bce_dice_loss: 0.3129\n",
      "Training: batch 52 begins at 22:40:19.413895\n",
      "\n",
      "Training: batch 52 ends at 22:40:20.210788\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3119 - bce_dice_loss: 0.3119\n",
      "Training: batch 53 begins at 22:40:20.213878\n",
      "\n",
      "Training: batch 53 ends at 22:40:21.010150\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3087 - bce_dice_loss: 0.3087\n",
      "Training: batch 54 begins at 22:40:21.015547\n",
      "\n",
      "Training: batch 54 ends at 22:40:21.799588\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3074 - bce_dice_loss: 0.3074\n",
      "Training: batch 55 begins at 22:40:21.803982\n",
      "\n",
      "Training: batch 55 ends at 22:40:22.598369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3054 - bce_dice_loss: 0.3054\n",
      "Training: batch 56 begins at 22:40:22.602741\n",
      "\n",
      "Training: batch 56 ends at 22:40:23.400683\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3036 - bce_dice_loss: 0.3036\n",
      "Training: batch 57 begins at 22:40:23.406916\n",
      "\n",
      "Training: batch 57 ends at 22:40:24.221989\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3025 - bce_dice_loss: 0.3025\n",
      "Training: batch 58 begins at 22:40:24.226310\n",
      "\n",
      "Training: batch 58 ends at 22:40:25.016851\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3010 - bce_dice_loss: 0.3010\n",
      "Training: batch 59 begins at 22:40:25.021523\n",
      "\n",
      "Training: batch 59 ends at 22:40:25.813354\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3004 - bce_dice_loss: 0.3004\n",
      "Evaluating: batch 0 begins at 22:40:25.846419\n",
      "\n",
      "Evaluating: batch 0 ends at 22:40:26.111208\n",
      "\n",
      "Evaluating: batch 1 begins at 22:40:26.112723\n",
      "\n",
      "Evaluating: batch 1 ends at 22:40:26.326045\n",
      "\n",
      "Evaluating: batch 2 begins at 22:40:26.327747\n",
      "\n",
      "Evaluating: batch 2 ends at 22:40:26.545848\n",
      "\n",
      "Evaluating: batch 3 begins at 22:40:26.548194\n",
      "\n",
      "Evaluating: batch 3 ends at 22:40:26.770884\n",
      "\n",
      "Evaluating: batch 4 begins at 22:40:26.772620\n",
      "\n",
      "Evaluating: batch 4 ends at 22:40:26.991335\n",
      "\n",
      "Evaluating: batch 5 begins at 22:40:26.992626\n",
      "\n",
      "Evaluating: batch 5 ends at 22:40:27.211141\n",
      "\n",
      "Evaluating: batch 6 begins at 22:40:27.213003\n",
      "\n",
      "Evaluating: batch 6 ends at 22:40:27.434114\n",
      "\n",
      "Evaluating: batch 7 begins at 22:40:27.435461\n",
      "\n",
      "Evaluating: batch 7 ends at 22:40:27.653831\n",
      "\n",
      "Evaluating: batch 8 begins at 22:40:27.655101\n",
      "\n",
      "Evaluating: batch 8 ends at 22:40:27.879202\n",
      "\n",
      "Evaluating: batch 9 begins at 22:40:27.880491\n",
      "\n",
      "Evaluating: batch 9 ends at 22:40:28.103617\n",
      "\n",
      "Evaluating: batch 10 begins at 22:40:28.105089\n",
      "\n",
      "Evaluating: batch 10 ends at 22:40:28.326462\n",
      "\n",
      "Evaluating: batch 11 begins at 22:40:28.327572\n",
      "\n",
      "Evaluating: batch 11 ends at 22:40:28.546064\n",
      "\n",
      "Evaluating: batch 12 begins at 22:40:28.548612\n",
      "\n",
      "Evaluating: batch 12 ends at 22:40:28.769789\n",
      "\n",
      "Evaluating: batch 13 begins at 22:40:28.771844\n",
      "\n",
      "Evaluating: batch 13 ends at 22:40:28.994067\n",
      "\n",
      "Evaluating: batch 14 begins at 22:40:28.995832\n",
      "\n",
      "Evaluating: batch 14 ends at 22:40:29.211397\n",
      "\n",
      "Evaluating: batch 15 begins at 22:40:29.212631\n",
      "\n",
      "Evaluating: batch 15 ends at 22:40:29.433686\n",
      "\n",
      "Evaluating: batch 16 begins at 22:40:29.435476\n",
      "\n",
      "Evaluating: batch 16 ends at 22:40:29.654849\n",
      "\n",
      "Evaluating: batch 17 begins at 22:40:29.656475\n",
      "\n",
      "Evaluating: batch 17 ends at 22:40:29.880411\n",
      "\n",
      "Evaluating: batch 18 begins at 22:40:29.881820\n",
      "\n",
      "Evaluating: batch 18 ends at 22:40:30.104808\n",
      "\n",
      "Evaluating: batch 19 begins at 22:40:30.106251\n",
      "\n",
      "Evaluating: batch 19 ends at 22:40:30.325921\n",
      "\n",
      "Evaluating: batch 20 begins at 22:40:30.327897\n",
      "\n",
      "Evaluating: batch 20 ends at 22:40:30.545828\n",
      "\n",
      "Evaluating: batch 21 begins at 22:40:30.548298\n",
      "\n",
      "Evaluating: batch 21 ends at 22:40:30.770232\n",
      "\n",
      "Evaluating: batch 22 begins at 22:40:30.771545\n",
      "\n",
      "Evaluating: batch 22 ends at 22:40:30.992296\n",
      "\n",
      "Evaluating: batch 23 begins at 22:40:30.993715\n",
      "\n",
      "Evaluating: batch 23 ends at 22:40:31.217318\n",
      "\n",
      "Evaluating: batch 24 begins at 22:40:31.218700\n",
      "\n",
      "Evaluating: batch 24 ends at 22:40:31.444120\n",
      "\n",
      "Evaluating: batch 25 begins at 22:40:31.445514\n",
      "\n",
      "Evaluating: batch 25 ends at 22:40:31.669394\n",
      "\n",
      "Evaluating: batch 26 begins at 22:40:31.670707\n",
      "\n",
      "Evaluating: batch 26 ends at 22:40:31.901109\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.34323 to 0.33645, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 937ms/step - loss: 0.3004 - bce_dice_loss: 0.3004 - val_loss: 0.3364 - val_bce_dice_loss: 0.3364\n",
      "Train hist recorded for batch -  1\n",
      "Saving the model\n",
      "Saved the model at saved_models/training_25/batch_1/\n",
      "Reading train data\n",
      "x train ----  120\n",
      "x val ----  54\n",
      "Train and Validation data created\n",
      "Epoch 1/25\n",
      "\n",
      "Training: batch 0 begins at 22:40:54.223561\n",
      "\n",
      "Training: batch 0 ends at 22:40:55.456161\n",
      " 1/60 [..............................] - ETA: 1:15 - loss: 0.3499 - bce_dice_loss: 0.3499\n",
      "Training: batch 1 begins at 22:40:55.496432\n",
      "\n",
      "Training: batch 1 ends at 22:40:56.334194\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.3423 - bce_dice_loss: 0.3423 \n",
      "Training: batch 2 begins at 22:40:56.336484\n",
      "\n",
      "Training: batch 2 ends at 22:40:57.152133\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.4454 - bce_dice_loss: 0.4454\n",
      "Training: batch 3 begins at 22:40:57.155638\n",
      "\n",
      "Training: batch 3 ends at 22:40:57.961060\n",
      " 4/60 [=>............................] - ETA: 46s - loss: 0.4323 - bce_dice_loss: 0.4323\n",
      "Training: batch 4 begins at 22:40:57.964721\n",
      "\n",
      "Training: batch 4 ends at 22:40:58.776835\n",
      " 5/60 [=>............................] - ETA: 45s - loss: 0.4062 - bce_dice_loss: 0.4062\n",
      "Training: batch 5 begins at 22:40:58.780265\n",
      "\n",
      "Training: batch 5 ends at 22:40:59.595105\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.3568 - bce_dice_loss: 0.3568WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1574s vs `on_train_batch_end` time: 0.6910s). Check your callbacks.\n",
      "\n",
      "Training: batch 6 begins at 22:40:59.600334\n",
      "\n",
      "Training: batch 6 ends at 22:41:00.432194\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.3473 - bce_dice_loss: 0.3473\n",
      "Training: batch 7 begins at 22:41:00.436153\n",
      "\n",
      "Training: batch 7 ends at 22:41:01.249466\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3381 - bce_dice_loss: 0.3381\n",
      "Training: batch 8 begins at 22:41:01.253550\n",
      "\n",
      "Training: batch 8 ends at 22:41:02.050540\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3468 - bce_dice_loss: 0.3468\n",
      "Training: batch 9 begins at 22:41:02.054481\n",
      "\n",
      "Training: batch 9 ends at 22:41:02.858610\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3410 - bce_dice_loss: 0.3410\n",
      "Training: batch 10 begins at 22:41:02.861534\n",
      "\n",
      "Training: batch 10 ends at 22:41:03.688977\n",
      "11/60 [====>.........................] - ETA: 40s - loss: 0.3517 - bce_dice_loss: 0.3517\n",
      "Training: batch 11 begins at 22:41:03.693288\n",
      "\n",
      "Training: batch 11 ends at 22:41:04.504425\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.3430 - bce_dice_loss: 0.3430\n",
      "Training: batch 12 begins at 22:41:04.507541\n",
      "\n",
      "Training: batch 12 ends at 22:41:05.315075\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3576 - bce_dice_loss: 0.3576\n",
      "Training: batch 13 begins at 22:41:05.319213\n",
      "\n",
      "Training: batch 13 ends at 22:41:06.136999\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3854 - bce_dice_loss: 0.3854\n",
      "Training: batch 14 begins at 22:41:06.140388\n",
      "\n",
      "Training: batch 14 ends at 22:41:06.944654\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3876 - bce_dice_loss: 0.3876\n",
      "Training: batch 15 begins at 22:41:06.947537\n",
      "\n",
      "Training: batch 15 ends at 22:41:07.753233\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3828 - bce_dice_loss: 0.3828\n",
      "Training: batch 16 begins at 22:41:07.758010\n",
      "\n",
      "Training: batch 16 ends at 22:41:08.553495\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.3740 - bce_dice_loss: 0.3740\n",
      "Training: batch 17 begins at 22:41:08.557578\n",
      "\n",
      "Training: batch 17 ends at 22:41:09.386221\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.3802 - bce_dice_loss: 0.3802\n",
      "Training: batch 18 begins at 22:41:09.391655\n",
      "\n",
      "Training: batch 18 ends at 22:41:10.189598\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3829 - bce_dice_loss: 0.3829\n",
      "Training: batch 19 begins at 22:41:10.192145\n",
      "\n",
      "Training: batch 19 ends at 22:41:10.993487\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3870 - bce_dice_loss: 0.3870\n",
      "Training: batch 20 begins at 22:41:10.996819\n",
      "\n",
      "Training: batch 20 ends at 22:41:11.818627\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3983 - bce_dice_loss: 0.3983\n",
      "Training: batch 21 begins at 22:41:11.823881\n",
      "\n",
      "Training: batch 21 ends at 22:41:12.626364\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.3954 - bce_dice_loss: 0.3954\n",
      "Training: batch 22 begins at 22:41:12.629744\n",
      "\n",
      "Training: batch 22 ends at 22:41:13.436286\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.4023 - bce_dice_loss: 0.4023\n",
      "Training: batch 23 begins at 22:41:13.440838\n",
      "\n",
      "Training: batch 23 ends at 22:41:14.256573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4004 - bce_dice_loss: 0.4004\n",
      "Training: batch 24 begins at 22:41:14.261169\n",
      "\n",
      "Training: batch 24 ends at 22:41:15.069119\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4004 - bce_dice_loss: 0.4004\n",
      "Training: batch 25 begins at 22:41:15.073958\n",
      "\n",
      "Training: batch 25 ends at 22:41:15.887295\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4021 - bce_dice_loss: 0.4021\n",
      "Training: batch 26 begins at 22:41:15.892484\n",
      "\n",
      "Training: batch 26 ends at 22:41:16.714248\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.4100 - bce_dice_loss: 0.4100\n",
      "Training: batch 27 begins at 22:41:16.719643\n",
      "\n",
      "Training: batch 27 ends at 22:41:17.517664\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.4212 - bce_dice_loss: 0.4212\n",
      "Training: batch 28 begins at 22:41:17.525136\n",
      "\n",
      "Training: batch 28 ends at 22:41:18.323654\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.4228 - bce_dice_loss: 0.4228\n",
      "Training: batch 29 begins at 22:41:18.326070\n",
      "\n",
      "Training: batch 29 ends at 22:41:19.125485\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.4214 - bce_dice_loss: 0.4214\n",
      "Training: batch 30 begins at 22:41:19.128327\n",
      "\n",
      "Training: batch 30 ends at 22:41:19.930846\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.4229 - bce_dice_loss: 0.4229\n",
      "Training: batch 31 begins at 22:41:19.933303\n",
      "\n",
      "Training: batch 31 ends at 22:41:20.732149\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.4213 - bce_dice_loss: 0.4213\n",
      "Training: batch 32 begins at 22:41:20.736259\n",
      "\n",
      "Training: batch 32 ends at 22:41:21.538745\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.4227 - bce_dice_loss: 0.4227\n",
      "Training: batch 33 begins at 22:41:21.541493\n",
      "\n",
      "Training: batch 33 ends at 22:41:22.362766\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.4190 - bce_dice_loss: 0.4190\n",
      "Training: batch 34 begins at 22:41:22.367469\n",
      "\n",
      "Training: batch 34 ends at 22:41:23.165209\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.4167 - bce_dice_loss: 0.4167\n",
      "Training: batch 35 begins at 22:41:23.169198\n",
      "\n",
      "Training: batch 35 ends at 22:41:23.970454\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.4142 - bce_dice_loss: 0.4142\n",
      "Training: batch 36 begins at 22:41:23.972819\n",
      "\n",
      "Training: batch 36 ends at 22:41:24.800966\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.4110 - bce_dice_loss: 0.4110\n",
      "Training: batch 37 begins at 22:41:24.803956\n",
      "\n",
      "Training: batch 37 ends at 22:41:25.609778\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.4124 - bce_dice_loss: 0.4124\n",
      "Training: batch 38 begins at 22:41:25.614291\n",
      "\n",
      "Training: batch 38 ends at 22:41:26.427331\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.4104 - bce_dice_loss: 0.4104\n",
      "Training: batch 39 begins at 22:41:26.430088\n",
      "\n",
      "Training: batch 39 ends at 22:41:27.251530\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.4138 - bce_dice_loss: 0.4138\n",
      "Training: batch 40 begins at 22:41:27.254163\n",
      "\n",
      "Training: batch 40 ends at 22:41:28.075054\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.4133 - bce_dice_loss: 0.4133\n",
      "Training: batch 41 begins at 22:41:28.078456\n",
      "\n",
      "Training: batch 41 ends at 22:41:28.879882\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.4157 - bce_dice_loss: 0.4157\n",
      "Training: batch 42 begins at 22:41:28.882648\n",
      "\n",
      "Training: batch 42 ends at 22:41:29.716445\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.4137 - bce_dice_loss: 0.4137\n",
      "Training: batch 43 begins at 22:41:29.720605\n",
      "\n",
      "Training: batch 43 ends at 22:41:30.521593\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.4133 - bce_dice_loss: 0.4133\n",
      "Training: batch 44 begins at 22:41:30.525284\n",
      "\n",
      "Training: batch 44 ends at 22:41:31.324235\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.4119 - bce_dice_loss: 0.4119\n",
      "Training: batch 45 begins at 22:41:31.326796\n",
      "\n",
      "Training: batch 45 ends at 22:41:32.128064\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.4125 - bce_dice_loss: 0.4125\n",
      "Training: batch 46 begins at 22:41:32.130816\n",
      "\n",
      "Training: batch 46 ends at 22:41:32.942080\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.4140 - bce_dice_loss: 0.4140\n",
      "Training: batch 47 begins at 22:41:32.945707\n",
      "\n",
      "Training: batch 47 ends at 22:41:33.774517\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.4144 - bce_dice_loss: 0.4144 \n",
      "Training: batch 48 begins at 22:41:33.777808\n",
      "\n",
      "Training: batch 48 ends at 22:41:34.597169\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.4160 - bce_dice_loss: 0.4160\n",
      "Training: batch 49 begins at 22:41:34.601262\n",
      "\n",
      "Training: batch 49 ends at 22:41:35.419186\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.4145 - bce_dice_loss: 0.4145\n",
      "Training: batch 50 begins at 22:41:35.422086\n",
      "\n",
      "Training: batch 50 ends at 22:41:36.256762\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.4156 - bce_dice_loss: 0.4156\n",
      "Training: batch 51 begins at 22:41:36.261144\n",
      "\n",
      "Training: batch 51 ends at 22:41:37.064286\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.4132 - bce_dice_loss: 0.4132\n",
      "Training: batch 52 begins at 22:41:37.068735\n",
      "\n",
      "Training: batch 52 ends at 22:41:37.898762\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.4125 - bce_dice_loss: 0.4125\n",
      "Training: batch 53 begins at 22:41:37.902151\n",
      "\n",
      "Training: batch 53 ends at 22:41:38.704042\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.4116 - bce_dice_loss: 0.4116\n",
      "Training: batch 54 begins at 22:41:38.708794\n",
      "\n",
      "Training: batch 54 ends at 22:41:39.515308\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.4166 - bce_dice_loss: 0.4166\n",
      "Training: batch 55 begins at 22:41:39.519742\n",
      "\n",
      "Training: batch 55 ends at 22:41:40.325005\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.4138 - bce_dice_loss: 0.4138\n",
      "Training: batch 56 begins at 22:41:40.328118\n",
      "\n",
      "Training: batch 56 ends at 22:41:41.126933\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.4123 - bce_dice_loss: 0.4123\n",
      "Training: batch 57 begins at 22:41:41.129479\n",
      "\n",
      "Training: batch 57 ends at 22:41:41.942240\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.4123 - bce_dice_loss: 0.4123\n",
      "Training: batch 58 begins at 22:41:41.945244\n",
      "\n",
      "Training: batch 58 ends at 22:41:42.757713\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4094 - bce_dice_loss: 0.4094\n",
      "Training: batch 59 begins at 22:41:42.761730\n",
      "\n",
      "Training: batch 59 ends at 22:41:43.571723\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4102 - bce_dice_loss: 0.4102\n",
      "Evaluating: batch 0 begins at 22:41:44.560053\n",
      "\n",
      "Evaluating: batch 0 ends at 22:41:44.844316\n",
      "\n",
      "Evaluating: batch 1 begins at 22:41:44.846157\n",
      "\n",
      "Evaluating: batch 1 ends at 22:41:45.070193\n",
      "\n",
      "Evaluating: batch 2 begins at 22:41:45.071525\n",
      "\n",
      "Evaluating: batch 2 ends at 22:41:45.300679\n",
      "\n",
      "Evaluating: batch 3 begins at 22:41:45.301822\n",
      "\n",
      "Evaluating: batch 3 ends at 22:41:45.529856\n",
      "\n",
      "Evaluating: batch 4 begins at 22:41:45.530872\n",
      "\n",
      "Evaluating: batch 4 ends at 22:41:45.763086\n",
      "\n",
      "Evaluating: batch 5 begins at 22:41:45.764230\n",
      "\n",
      "Evaluating: batch 5 ends at 22:41:45.996241\n",
      "\n",
      "Evaluating: batch 6 begins at 22:41:45.997371\n",
      "\n",
      "Evaluating: batch 6 ends at 22:41:46.223636\n",
      "\n",
      "Evaluating: batch 7 begins at 22:41:46.224765\n",
      "\n",
      "Evaluating: batch 7 ends at 22:41:46.451131\n",
      "\n",
      "Evaluating: batch 8 begins at 22:41:46.452181\n",
      "\n",
      "Evaluating: batch 8 ends at 22:41:46.682024\n",
      "\n",
      "Evaluating: batch 9 begins at 22:41:46.683306\n",
      "\n",
      "Evaluating: batch 9 ends at 22:41:46.914384\n",
      "\n",
      "Evaluating: batch 10 begins at 22:41:46.915407\n",
      "\n",
      "Evaluating: batch 10 ends at 22:41:47.145088\n",
      "\n",
      "Evaluating: batch 11 begins at 22:41:47.146226\n",
      "\n",
      "Evaluating: batch 11 ends at 22:41:47.373532\n",
      "\n",
      "Evaluating: batch 12 begins at 22:41:47.374750\n",
      "\n",
      "Evaluating: batch 12 ends at 22:41:47.600263\n",
      "\n",
      "Evaluating: batch 13 begins at 22:41:47.601466\n",
      "\n",
      "Evaluating: batch 13 ends at 22:41:47.831834\n",
      "\n",
      "Evaluating: batch 14 begins at 22:41:47.832959\n",
      "\n",
      "Evaluating: batch 14 ends at 22:41:48.061909\n",
      "\n",
      "Evaluating: batch 15 begins at 22:41:48.063308\n",
      "\n",
      "Evaluating: batch 15 ends at 22:41:48.291293\n",
      "\n",
      "Evaluating: batch 16 begins at 22:41:48.292267\n",
      "\n",
      "Evaluating: batch 16 ends at 22:41:48.519218\n",
      "\n",
      "Evaluating: batch 17 begins at 22:41:48.520482\n",
      "\n",
      "Evaluating: batch 17 ends at 22:41:48.749551\n",
      "\n",
      "Evaluating: batch 18 begins at 22:41:48.750562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 18 ends at 22:41:48.974535\n",
      "\n",
      "Evaluating: batch 19 begins at 22:41:48.975702\n",
      "\n",
      "Evaluating: batch 19 ends at 22:41:49.201573\n",
      "\n",
      "Evaluating: batch 20 begins at 22:41:49.202722\n",
      "\n",
      "Evaluating: batch 20 ends at 22:41:49.428468\n",
      "\n",
      "Evaluating: batch 21 begins at 22:41:49.429639\n",
      "\n",
      "Evaluating: batch 21 ends at 22:41:49.652748\n",
      "\n",
      "Evaluating: batch 22 begins at 22:41:49.654136\n",
      "\n",
      "Evaluating: batch 22 ends at 22:41:49.884346\n",
      "\n",
      "Evaluating: batch 23 begins at 22:41:49.885496\n",
      "\n",
      "Evaluating: batch 23 ends at 22:41:50.112712\n",
      "\n",
      "Evaluating: batch 24 begins at 22:41:50.113805\n",
      "\n",
      "Evaluating: batch 24 ends at 22:41:50.335066\n",
      "\n",
      "Evaluating: batch 25 begins at 22:41:50.336251\n",
      "\n",
      "Evaluating: batch 25 ends at 22:41:50.562018\n",
      "\n",
      "Evaluating: batch 26 begins at 22:41:50.563182\n",
      "\n",
      "Evaluating: batch 26 ends at 22:41:50.791984\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33645\n",
      "60/60 [==============================] - 57s 938ms/step - loss: 0.4102 - bce_dice_loss: 0.4102 - val_loss: 0.3460 - val_bce_dice_loss: 0.3460\n",
      "Epoch 2/25\n",
      "\n",
      "Training: batch 0 begins at 22:41:50.811151\n",
      "\n",
      "Training: batch 0 ends at 22:41:51.622157\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.3815 - bce_dice_loss: 0.3815\n",
      "Training: batch 1 begins at 22:41:51.625050\n",
      "\n",
      "Training: batch 1 ends at 22:41:52.435688\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.5237 - bce_dice_loss: 0.5237\n",
      "Training: batch 2 begins at 22:41:52.439204\n",
      "\n",
      "Training: batch 2 ends at 22:41:53.245041\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.4792 - bce_dice_loss: 0.4792\n",
      "Training: batch 3 begins at 22:41:53.247902\n",
      "\n",
      "Training: batch 3 ends at 22:41:54.046272\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.4333 - bce_dice_loss: 0.4333\n",
      "Training: batch 4 begins at 22:41:54.049638\n",
      "\n",
      "Training: batch 4 ends at 22:41:54.891343\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.4151 - bce_dice_loss: 0.4151\n",
      "Training: batch 5 begins at 22:41:54.895767\n",
      "\n",
      "Training: batch 5 ends at 22:41:55.701334\n",
      " 6/60 [==>...........................] - ETA: 44s - loss: 0.3875 - bce_dice_loss: 0.3875\n",
      "Training: batch 6 begins at 22:41:55.705867\n",
      "\n",
      "Training: batch 6 ends at 22:41:56.511940\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.4205 - bce_dice_loss: 0.4205\n",
      "Training: batch 7 begins at 22:41:56.516130\n",
      "\n",
      "Training: batch 7 ends at 22:41:57.330411\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.4176 - bce_dice_loss: 0.4176\n",
      "Training: batch 8 begins at 22:41:57.333299\n",
      "\n",
      "Training: batch 8 ends at 22:41:58.138208\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.4252 - bce_dice_loss: 0.4252\n",
      "Training: batch 9 begins at 22:41:58.141372\n",
      "\n",
      "Training: batch 9 ends at 22:41:58.977435\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.4469 - bce_dice_loss: 0.4469\n",
      "Training: batch 10 begins at 22:41:58.980384\n",
      "\n",
      "Training: batch 10 ends at 22:41:59.784489\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.4304 - bce_dice_loss: 0.4304\n",
      "Training: batch 11 begins at 22:41:59.789537\n",
      "\n",
      "Training: batch 11 ends at 22:42:00.626922\n",
      "12/60 [=====>........................] - ETA: 39s - loss: 0.4242 - bce_dice_loss: 0.4242\n",
      "Training: batch 12 begins at 22:42:00.630381\n",
      "\n",
      "Training: batch 12 ends at 22:42:01.441143\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.4156 - bce_dice_loss: 0.4156\n",
      "Training: batch 13 begins at 22:42:01.445144\n",
      "\n",
      "Training: batch 13 ends at 22:42:02.247283\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.4147 - bce_dice_loss: 0.4147\n",
      "Training: batch 14 begins at 22:42:02.251464\n",
      "\n",
      "Training: batch 14 ends at 22:42:03.051860\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.4085 - bce_dice_loss: 0.4085\n",
      "Training: batch 15 begins at 22:42:03.055939\n",
      "\n",
      "Training: batch 15 ends at 22:42:03.856767\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3968 - bce_dice_loss: 0.3968\n",
      "Training: batch 16 begins at 22:42:03.859378\n",
      "\n",
      "Training: batch 16 ends at 22:42:04.688555\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.3900 - bce_dice_loss: 0.3900\n",
      "Training: batch 17 begins at 22:42:04.694230\n",
      "\n",
      "Training: batch 17 ends at 22:42:05.505823\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.3981 - bce_dice_loss: 0.3981\n",
      "Training: batch 18 begins at 22:42:05.509571\n",
      "\n",
      "Training: batch 18 ends at 22:42:06.313684\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3917 - bce_dice_loss: 0.3917\n",
      "Training: batch 19 begins at 22:42:06.318181\n",
      "\n",
      "Training: batch 19 ends at 22:42:07.145917\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.4066 - bce_dice_loss: 0.4066\n",
      "Training: batch 20 begins at 22:42:07.149092\n",
      "\n",
      "Training: batch 20 ends at 22:42:07.949153\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.4013 - bce_dice_loss: 0.4013\n",
      "Training: batch 21 begins at 22:42:07.953543\n",
      "\n",
      "Training: batch 21 ends at 22:42:08.759698\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.3975 - bce_dice_loss: 0.3975\n",
      "Training: batch 22 begins at 22:42:08.763747\n",
      "\n",
      "Training: batch 22 ends at 22:42:09.563137\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.4068 - bce_dice_loss: 0.4068\n",
      "Training: batch 23 begins at 22:42:09.567398\n",
      "\n",
      "Training: batch 23 ends at 22:42:10.373882\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.4018 - bce_dice_loss: 0.4018\n",
      "Training: batch 24 begins at 22:42:10.378416\n",
      "\n",
      "Training: batch 24 ends at 22:42:11.213299\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.4080 - bce_dice_loss: 0.4080\n",
      "Training: batch 25 begins at 22:42:11.217743\n",
      "\n",
      "Training: batch 25 ends at 22:42:12.017809\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.4017 - bce_dice_loss: 0.4017\n",
      "Training: batch 26 begins at 22:42:12.022384\n",
      "\n",
      "Training: batch 26 ends at 22:42:12.826285\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3990 - bce_dice_loss: 0.3990\n",
      "Training: batch 27 begins at 22:42:12.829562\n",
      "\n",
      "Training: batch 27 ends at 22:42:13.657183\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.3976 - bce_dice_loss: 0.3976\n",
      "Training: batch 28 begins at 22:42:13.661782\n",
      "\n",
      "Training: batch 28 ends at 22:42:14.472561\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3920 - bce_dice_loss: 0.3920\n",
      "Training: batch 29 begins at 22:42:14.476588\n",
      "\n",
      "Training: batch 29 ends at 22:42:15.288569\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3912 - bce_dice_loss: 0.3912\n",
      "Training: batch 30 begins at 22:42:15.294319\n",
      "\n",
      "Training: batch 30 ends at 22:42:16.115696\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3919 - bce_dice_loss: 0.3919\n",
      "Training: batch 31 begins at 22:42:16.120852\n",
      "\n",
      "Training: batch 31 ends at 22:42:16.928789\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3927 - bce_dice_loss: 0.3927\n",
      "Training: batch 32 begins at 22:42:16.933188\n",
      "\n",
      "Training: batch 32 ends at 22:42:17.739972\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.3986 - bce_dice_loss: 0.3986\n",
      "Training: batch 33 begins at 22:42:17.744307\n",
      "\n",
      "Training: batch 33 ends at 22:42:18.545933\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3946 - bce_dice_loss: 0.3946\n",
      "Training: batch 34 begins at 22:42:18.550397\n",
      "\n",
      "Training: batch 34 ends at 22:42:19.354254\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3933 - bce_dice_loss: 0.3933\n",
      "Training: batch 35 begins at 22:42:19.358554\n",
      "\n",
      "Training: batch 35 ends at 22:42:20.160264\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3891 - bce_dice_loss: 0.3891\n",
      "Training: batch 36 begins at 22:42:20.164637\n",
      "\n",
      "Training: batch 36 ends at 22:42:20.982230\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3939 - bce_dice_loss: 0.3939\n",
      "Training: batch 37 begins at 22:42:20.986406\n",
      "\n",
      "Training: batch 37 ends at 22:42:21.791042\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3931 - bce_dice_loss: 0.3931\n",
      "Training: batch 38 begins at 22:42:21.794022\n",
      "\n",
      "Training: batch 38 ends at 22:42:22.599829\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3964 - bce_dice_loss: 0.3964\n",
      "Training: batch 39 begins at 22:42:22.604588\n",
      "\n",
      "Training: batch 39 ends at 22:42:23.431842\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3937 - bce_dice_loss: 0.3937\n",
      "Training: batch 40 begins at 22:42:23.434797\n",
      "\n",
      "Training: batch 40 ends at 22:42:24.251831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3929 - bce_dice_loss: 0.3929\n",
      "Training: batch 41 begins at 22:42:24.255151\n",
      "\n",
      "Training: batch 41 ends at 22:42:25.078975\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3898 - bce_dice_loss: 0.3898\n",
      "Training: batch 42 begins at 22:42:25.083404\n",
      "\n",
      "Training: batch 42 ends at 22:42:25.892489\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3937 - bce_dice_loss: 0.3937\n",
      "Training: batch 43 begins at 22:42:25.896156\n",
      "\n",
      "Training: batch 43 ends at 22:42:26.701949\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3934 - bce_dice_loss: 0.3934\n",
      "Training: batch 44 begins at 22:42:26.706848\n",
      "\n",
      "Training: batch 44 ends at 22:42:27.513137\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3914 - bce_dice_loss: 0.3914\n",
      "Training: batch 45 begins at 22:42:27.517265\n",
      "\n",
      "Training: batch 45 ends at 22:42:28.327610\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3914 - bce_dice_loss: 0.3914\n",
      "Training: batch 46 begins at 22:42:28.330123\n",
      "\n",
      "Training: batch 46 ends at 22:42:29.131025\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3879 - bce_dice_loss: 0.3879\n",
      "Training: batch 47 begins at 22:42:29.134468\n",
      "\n",
      "Training: batch 47 ends at 22:42:29.941346\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3882 - bce_dice_loss: 0.3882 \n",
      "Training: batch 48 begins at 22:42:29.944865\n",
      "\n",
      "Training: batch 48 ends at 22:42:30.746487\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3892 - bce_dice_loss: 0.3892\n",
      "Training: batch 49 begins at 22:42:30.749229\n",
      "\n",
      "Training: batch 49 ends at 22:42:31.583383\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3862 - bce_dice_loss: 0.3862\n",
      "Training: batch 50 begins at 22:42:31.586632\n",
      "\n",
      "Training: batch 50 ends at 22:42:32.388915\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3865 - bce_dice_loss: 0.3865\n",
      "Training: batch 51 begins at 22:42:32.391668\n",
      "\n",
      "Training: batch 51 ends at 22:42:33.189424\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3867 - bce_dice_loss: 0.3867\n",
      "Training: batch 52 begins at 22:42:33.192436\n",
      "\n",
      "Training: batch 52 ends at 22:42:33.994689\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3847 - bce_dice_loss: 0.3847\n",
      "Training: batch 53 begins at 22:42:33.998998\n",
      "\n",
      "Training: batch 53 ends at 22:42:34.809048\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3833 - bce_dice_loss: 0.3833\n",
      "Training: batch 54 begins at 22:42:34.812109\n",
      "\n",
      "Training: batch 54 ends at 22:42:35.617036\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3905 - bce_dice_loss: 0.3905\n",
      "Training: batch 55 begins at 22:42:35.620059\n",
      "\n",
      "Training: batch 55 ends at 22:42:36.443924\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3892 - bce_dice_loss: 0.3892\n",
      "Training: batch 56 begins at 22:42:36.446418\n",
      "\n",
      "Training: batch 56 ends at 22:42:37.245291\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3874 - bce_dice_loss: 0.3874\n",
      "Training: batch 57 begins at 22:42:37.249185\n",
      "\n",
      "Training: batch 57 ends at 22:42:38.079087\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3851 - bce_dice_loss: 0.3851\n",
      "Training: batch 58 begins at 22:42:38.084805\n",
      "\n",
      "Training: batch 58 ends at 22:42:38.883513\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3833 - bce_dice_loss: 0.3833\n",
      "Training: batch 59 begins at 22:42:38.888264\n",
      "\n",
      "Training: batch 59 ends at 22:42:39.688790\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3827 - bce_dice_loss: 0.3827\n",
      "Evaluating: batch 0 begins at 22:42:39.724115\n",
      "\n",
      "Evaluating: batch 0 ends at 22:42:40.004330\n",
      "\n",
      "Evaluating: batch 1 begins at 22:42:40.005697\n",
      "\n",
      "Evaluating: batch 1 ends at 22:42:40.232968\n",
      "\n",
      "Evaluating: batch 2 begins at 22:42:40.234162\n",
      "\n",
      "Evaluating: batch 2 ends at 22:42:40.464241\n",
      "\n",
      "Evaluating: batch 3 begins at 22:42:40.465409\n",
      "\n",
      "Evaluating: batch 3 ends at 22:42:40.693105\n",
      "\n",
      "Evaluating: batch 4 begins at 22:42:40.694246\n",
      "\n",
      "Evaluating: batch 4 ends at 22:42:40.925518\n",
      "\n",
      "Evaluating: batch 5 begins at 22:42:40.926752\n",
      "\n",
      "Evaluating: batch 5 ends at 22:42:41.159146\n",
      "\n",
      "Evaluating: batch 6 begins at 22:42:41.160377\n",
      "\n",
      "Evaluating: batch 6 ends at 22:42:41.387813\n",
      "\n",
      "Evaluating: batch 7 begins at 22:42:41.389095\n",
      "\n",
      "Evaluating: batch 7 ends at 22:42:41.616150\n",
      "\n",
      "Evaluating: batch 8 begins at 22:42:41.617389\n",
      "\n",
      "Evaluating: batch 8 ends at 22:42:41.848323\n",
      "\n",
      "Evaluating: batch 9 begins at 22:42:41.849484\n",
      "\n",
      "Evaluating: batch 9 ends at 22:42:42.074810\n",
      "\n",
      "Evaluating: batch 10 begins at 22:42:42.076201\n",
      "\n",
      "Evaluating: batch 10 ends at 22:42:42.306641\n",
      "\n",
      "Evaluating: batch 11 begins at 22:42:42.307804\n",
      "\n",
      "Evaluating: batch 11 ends at 22:42:42.535980\n",
      "\n",
      "Evaluating: batch 12 begins at 22:42:42.537157\n",
      "\n",
      "Evaluating: batch 12 ends at 22:42:42.766521\n",
      "\n",
      "Evaluating: batch 13 begins at 22:42:42.767700\n",
      "\n",
      "Evaluating: batch 13 ends at 22:42:42.997008\n",
      "\n",
      "Evaluating: batch 14 begins at 22:42:42.998047\n",
      "\n",
      "Evaluating: batch 14 ends at 22:42:43.227035\n",
      "\n",
      "Evaluating: batch 15 begins at 22:42:43.228218\n",
      "\n",
      "Evaluating: batch 15 ends at 22:42:43.455409\n",
      "\n",
      "Evaluating: batch 16 begins at 22:42:43.456407\n",
      "\n",
      "Evaluating: batch 16 ends at 22:42:43.684092\n",
      "\n",
      "Evaluating: batch 17 begins at 22:42:43.685111\n",
      "\n",
      "Evaluating: batch 17 ends at 22:42:43.919573\n",
      "\n",
      "Evaluating: batch 18 begins at 22:42:43.920725\n",
      "\n",
      "Evaluating: batch 18 ends at 22:42:44.153751\n",
      "\n",
      "Evaluating: batch 19 begins at 22:42:44.155097\n",
      "\n",
      "Evaluating: batch 19 ends at 22:42:44.392349\n",
      "\n",
      "Evaluating: batch 20 begins at 22:42:44.393539\n",
      "\n",
      "Evaluating: batch 20 ends at 22:42:44.620786\n",
      "\n",
      "Evaluating: batch 21 begins at 22:42:44.621815\n",
      "\n",
      "Evaluating: batch 21 ends at 22:42:44.850560\n",
      "\n",
      "Evaluating: batch 22 begins at 22:42:44.851714\n",
      "\n",
      "Evaluating: batch 22 ends at 22:42:45.078715\n",
      "\n",
      "Evaluating: batch 23 begins at 22:42:45.079770\n",
      "\n",
      "Evaluating: batch 23 ends at 22:42:45.306938\n",
      "\n",
      "Evaluating: batch 24 begins at 22:42:45.308046\n",
      "\n",
      "Evaluating: batch 24 ends at 22:42:45.535787\n",
      "\n",
      "Evaluating: batch 25 begins at 22:42:45.536951\n",
      "\n",
      "Evaluating: batch 25 ends at 22:42:45.763790\n",
      "\n",
      "Evaluating: batch 26 begins at 22:42:45.764902\n",
      "\n",
      "Evaluating: batch 26 ends at 22:42:45.992152\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33645\n",
      "60/60 [==============================] - 55s 922ms/step - loss: 0.3827 - bce_dice_loss: 0.3827 - val_loss: 0.3659 - val_bce_dice_loss: 0.3659\n",
      "Epoch 3/25\n",
      "\n",
      "Training: batch 0 begins at 22:42:46.012456\n",
      "\n",
      "Training: batch 0 ends at 22:42:46.823343\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.4546 - bce_dice_loss: 0.4546\n",
      "Training: batch 1 begins at 22:42:46.827824\n",
      "\n",
      "Training: batch 1 ends at 22:42:47.640070\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3292 - bce_dice_loss: 0.3292\n",
      "Training: batch 2 begins at 22:42:47.643625\n",
      "\n",
      "Training: batch 2 ends at 22:42:48.441665\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3634 - bce_dice_loss: 0.3634\n",
      "Training: batch 3 begins at 22:42:48.444613\n",
      "\n",
      "Training: batch 3 ends at 22:42:49.251622\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3376 - bce_dice_loss: 0.3376\n",
      "Training: batch 4 begins at 22:42:49.256698\n",
      "\n",
      "Training: batch 4 ends at 22:42:50.061144\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3144 - bce_dice_loss: 0.3144\n",
      "Training: batch 5 begins at 22:42:50.065589\n",
      "\n",
      "Training: batch 5 ends at 22:42:50.865266\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3113 - bce_dice_loss: 0.3113\n",
      "Training: batch 6 begins at 22:42:50.869991\n",
      "\n",
      "Training: batch 6 ends at 22:42:51.672444\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3197 - bce_dice_loss: 0.3197\n",
      "Training: batch 7 begins at 22:42:51.675750\n",
      "\n",
      "Training: batch 7 ends at 22:42:52.481233\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3464 - bce_dice_loss: 0.3464\n",
      "Training: batch 8 begins at 22:42:52.485465\n",
      "\n",
      "Training: batch 8 ends at 22:42:53.292074\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3463 - bce_dice_loss: 0.3463\n",
      "Training: batch 9 begins at 22:42:53.295058\n",
      "\n",
      "Training: batch 9 ends at 22:42:54.102717\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3430 - bce_dice_loss: 0.3430\n",
      "Training: batch 10 begins at 22:42:54.106819\n",
      "\n",
      "Training: batch 10 ends at 22:42:54.928009\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3360 - bce_dice_loss: 0.3360\n",
      "Training: batch 11 begins at 22:42:54.930669\n",
      "\n",
      "Training: batch 11 ends at 22:42:55.730580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3360 - bce_dice_loss: 0.3360\n",
      "Training: batch 12 begins at 22:42:55.734621\n",
      "\n",
      "Training: batch 12 ends at 22:42:56.549814\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3317 - bce_dice_loss: 0.3317\n",
      "Training: batch 13 begins at 22:42:56.554874\n",
      "\n",
      "Training: batch 13 ends at 22:42:57.354486\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3303 - bce_dice_loss: 0.3303\n",
      "Training: batch 14 begins at 22:42:57.357529\n",
      "\n",
      "Training: batch 14 ends at 22:42:58.157965\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3326 - bce_dice_loss: 0.3326\n",
      "Training: batch 15 begins at 22:42:58.160649\n",
      "\n",
      "Training: batch 15 ends at 22:42:58.962997\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3368 - bce_dice_loss: 0.3368\n",
      "Training: batch 16 begins at 22:42:58.966511\n",
      "\n",
      "Training: batch 16 ends at 22:42:59.806352\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3543 - bce_dice_loss: 0.3543\n",
      "Training: batch 17 begins at 22:42:59.809656\n",
      "\n",
      "Training: batch 17 ends at 22:43:00.610067\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.3587 - bce_dice_loss: 0.3587\n",
      "Training: batch 18 begins at 22:43:00.614221\n",
      "\n",
      "Training: batch 18 ends at 22:43:01.434428\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3557 - bce_dice_loss: 0.3557\n",
      "Training: batch 19 begins at 22:43:01.440792\n",
      "\n",
      "Training: batch 19 ends at 22:43:02.242726\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3586 - bce_dice_loss: 0.3586\n",
      "Training: batch 20 begins at 22:43:02.246170\n",
      "\n",
      "Training: batch 20 ends at 22:43:03.054091\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3556 - bce_dice_loss: 0.3556\n",
      "Training: batch 21 begins at 22:43:03.058400\n",
      "\n",
      "Training: batch 21 ends at 22:43:03.867948\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3517 - bce_dice_loss: 0.3517\n",
      "Training: batch 22 begins at 22:43:03.872414\n",
      "\n",
      "Training: batch 22 ends at 22:43:04.691493\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.3479 - bce_dice_loss: 0.3479\n",
      "Training: batch 23 begins at 22:43:04.695675\n",
      "\n",
      "Training: batch 23 ends at 22:43:05.498383\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3493 - bce_dice_loss: 0.3493\n",
      "Training: batch 24 begins at 22:43:05.502581\n",
      "\n",
      "Training: batch 24 ends at 22:43:06.309177\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3544 - bce_dice_loss: 0.3544\n",
      "Training: batch 25 begins at 22:43:06.313151\n",
      "\n",
      "Training: batch 25 ends at 22:43:07.152028\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3534 - bce_dice_loss: 0.3534\n",
      "Training: batch 26 begins at 22:43:07.157224\n",
      "\n",
      "Training: batch 26 ends at 22:43:07.955650\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3620 - bce_dice_loss: 0.3620\n",
      "Training: batch 27 begins at 22:43:07.958332\n",
      "\n",
      "Training: batch 27 ends at 22:43:08.760090\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3595 - bce_dice_loss: 0.3595\n",
      "Training: batch 28 begins at 22:43:08.764495\n",
      "\n",
      "Training: batch 28 ends at 22:43:09.568970\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3560 - bce_dice_loss: 0.3560\n",
      "Training: batch 29 begins at 22:43:09.573829\n",
      "\n",
      "Training: batch 29 ends at 22:43:10.376742\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3562 - bce_dice_loss: 0.3562\n",
      "Training: batch 30 begins at 22:43:10.379683\n",
      "\n",
      "Training: batch 30 ends at 22:43:11.182881\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3557 - bce_dice_loss: 0.3557\n",
      "Training: batch 31 begins at 22:43:11.186162\n",
      "\n",
      "Training: batch 31 ends at 22:43:12.017226\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3537 - bce_dice_loss: 0.3537\n",
      "Training: batch 32 begins at 22:43:12.023649\n",
      "\n",
      "Training: batch 32 ends at 22:43:12.825921\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3643 - bce_dice_loss: 0.3643\n",
      "Training: batch 33 begins at 22:43:12.829395\n",
      "\n",
      "Training: batch 33 ends at 22:43:13.634929\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3610 - bce_dice_loss: 0.3610\n",
      "Training: batch 34 begins at 22:43:13.642000\n",
      "\n",
      "Training: batch 34 ends at 22:43:14.453400\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3650 - bce_dice_loss: 0.3650\n",
      "Training: batch 35 begins at 22:43:14.456528\n",
      "\n",
      "Training: batch 35 ends at 22:43:15.264800\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3620 - bce_dice_loss: 0.3620\n",
      "Training: batch 36 begins at 22:43:15.269291\n",
      "\n",
      "Training: batch 36 ends at 22:43:16.108140\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3636 - bce_dice_loss: 0.3636\n",
      "Training: batch 37 begins at 22:43:16.111487\n",
      "\n",
      "Training: batch 37 ends at 22:43:16.918339\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3602 - bce_dice_loss: 0.3602\n",
      "Training: batch 38 begins at 22:43:16.922178\n",
      "\n",
      "Training: batch 38 ends at 22:43:17.723424\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3637 - bce_dice_loss: 0.3637\n",
      "Training: batch 39 begins at 22:43:17.727626\n",
      "\n",
      "Training: batch 39 ends at 22:43:18.536777\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3717 - bce_dice_loss: 0.3717\n",
      "Training: batch 40 begins at 22:43:18.541057\n",
      "\n",
      "Training: batch 40 ends at 22:43:19.376240\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3710 - bce_dice_loss: 0.3710\n",
      "Training: batch 41 begins at 22:43:19.379687\n",
      "\n",
      "Training: batch 41 ends at 22:43:20.185644\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3700 - bce_dice_loss: 0.3700\n",
      "Training: batch 42 begins at 22:43:20.189869\n",
      "\n",
      "Training: batch 42 ends at 22:43:21.004431\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3697 - bce_dice_loss: 0.3697\n",
      "Training: batch 43 begins at 22:43:21.008698\n",
      "\n",
      "Training: batch 43 ends at 22:43:21.812491\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3696 - bce_dice_loss: 0.3696\n",
      "Training: batch 44 begins at 22:43:21.815744\n",
      "\n",
      "Training: batch 44 ends at 22:43:22.614164\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3681 - bce_dice_loss: 0.3681\n",
      "Training: batch 45 begins at 22:43:22.618076\n",
      "\n",
      "Training: batch 45 ends at 22:43:23.422998\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3689 - bce_dice_loss: 0.3689\n",
      "Training: batch 46 begins at 22:43:23.426543\n",
      "\n",
      "Training: batch 46 ends at 22:43:24.232153\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3705 - bce_dice_loss: 0.3705\n",
      "Training: batch 47 begins at 22:43:24.236975\n",
      "\n",
      "Training: batch 47 ends at 22:43:25.053566\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3712 - bce_dice_loss: 0.3712 \n",
      "Training: batch 48 begins at 22:43:25.058219\n",
      "\n",
      "Training: batch 48 ends at 22:43:25.860097\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3726 - bce_dice_loss: 0.3726\n",
      "Training: batch 49 begins at 22:43:25.865476\n",
      "\n",
      "Training: batch 49 ends at 22:43:26.666877\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3727 - bce_dice_loss: 0.3727\n",
      "Training: batch 50 begins at 22:43:26.671585\n",
      "\n",
      "Training: batch 50 ends at 22:43:27.481664\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3694 - bce_dice_loss: 0.3694\n",
      "Training: batch 51 begins at 22:43:27.486133\n",
      "\n",
      "Training: batch 51 ends at 22:43:28.303629\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3685 - bce_dice_loss: 0.3685\n",
      "Training: batch 52 begins at 22:43:28.306854\n",
      "\n",
      "Training: batch 52 ends at 22:43:29.107614\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3665 - bce_dice_loss: 0.3665\n",
      "Training: batch 53 begins at 22:43:29.113048\n",
      "\n",
      "Training: batch 53 ends at 22:43:29.931041\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3662 - bce_dice_loss: 0.3662\n",
      "Training: batch 54 begins at 22:43:29.935380\n",
      "\n",
      "Training: batch 54 ends at 22:43:30.740308\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3649 - bce_dice_loss: 0.3649\n",
      "Training: batch 55 begins at 22:43:30.743923\n",
      "\n",
      "Training: batch 55 ends at 22:43:31.551637\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3630 - bce_dice_loss: 0.3630\n",
      "Training: batch 56 begins at 22:43:31.555971\n",
      "\n",
      "Training: batch 56 ends at 22:43:32.354701\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3598 - bce_dice_loss: 0.3598\n",
      "Training: batch 57 begins at 22:43:32.358174\n",
      "\n",
      "Training: batch 57 ends at 22:43:33.184484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/60 [============================>.] - ETA: 1s - loss: 0.3595 - bce_dice_loss: 0.3595\n",
      "Training: batch 58 begins at 22:43:33.188561\n",
      "\n",
      "Training: batch 58 ends at 22:43:33.983838\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3587 - bce_dice_loss: 0.3587\n",
      "Training: batch 59 begins at 22:43:33.988367\n",
      "\n",
      "Training: batch 59 ends at 22:43:34.787035\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3580 - bce_dice_loss: 0.3580\n",
      "Evaluating: batch 0 begins at 22:43:34.820700\n",
      "\n",
      "Evaluating: batch 0 ends at 22:43:35.100841\n",
      "\n",
      "Evaluating: batch 1 begins at 22:43:35.102286\n",
      "\n",
      "Evaluating: batch 1 ends at 22:43:35.329857\n",
      "\n",
      "Evaluating: batch 2 begins at 22:43:35.331041\n",
      "\n",
      "Evaluating: batch 2 ends at 22:43:35.561806\n",
      "\n",
      "Evaluating: batch 3 begins at 22:43:35.562846\n",
      "\n",
      "Evaluating: batch 3 ends at 22:43:35.793306\n",
      "\n",
      "Evaluating: batch 4 begins at 22:43:35.794677\n",
      "\n",
      "Evaluating: batch 4 ends at 22:43:36.023255\n",
      "\n",
      "Evaluating: batch 5 begins at 22:43:36.024413\n",
      "\n",
      "Evaluating: batch 5 ends at 22:43:36.251861\n",
      "\n",
      "Evaluating: batch 6 begins at 22:43:36.252973\n",
      "\n",
      "Evaluating: batch 6 ends at 22:43:36.480659\n",
      "\n",
      "Evaluating: batch 7 begins at 22:43:36.481776\n",
      "\n",
      "Evaluating: batch 7 ends at 22:43:36.709185\n",
      "\n",
      "Evaluating: batch 8 begins at 22:43:36.710302\n",
      "\n",
      "Evaluating: batch 8 ends at 22:43:36.938732\n",
      "\n",
      "Evaluating: batch 9 begins at 22:43:36.939870\n",
      "\n",
      "Evaluating: batch 9 ends at 22:43:37.163950\n",
      "\n",
      "Evaluating: batch 10 begins at 22:43:37.165172\n",
      "\n",
      "Evaluating: batch 10 ends at 22:43:37.397425\n",
      "\n",
      "Evaluating: batch 11 begins at 22:43:37.398572\n",
      "\n",
      "Evaluating: batch 11 ends at 22:43:37.626912\n",
      "\n",
      "Evaluating: batch 12 begins at 22:43:37.628140\n",
      "\n",
      "Evaluating: batch 12 ends at 22:43:37.855622\n",
      "\n",
      "Evaluating: batch 13 begins at 22:43:37.856762\n",
      "\n",
      "Evaluating: batch 13 ends at 22:43:38.084834\n",
      "\n",
      "Evaluating: batch 14 begins at 22:43:38.086016\n",
      "\n",
      "Evaluating: batch 14 ends at 22:43:38.313281\n",
      "\n",
      "Evaluating: batch 15 begins at 22:43:38.314431\n",
      "\n",
      "Evaluating: batch 15 ends at 22:43:38.543559\n",
      "\n",
      "Evaluating: batch 16 begins at 22:43:38.544690\n",
      "\n",
      "Evaluating: batch 16 ends at 22:43:38.776319\n",
      "\n",
      "Evaluating: batch 17 begins at 22:43:38.777495\n",
      "\n",
      "Evaluating: batch 17 ends at 22:43:39.006803\n",
      "\n",
      "Evaluating: batch 18 begins at 22:43:39.007955\n",
      "\n",
      "Evaluating: batch 18 ends at 22:43:39.237474\n",
      "\n",
      "Evaluating: batch 19 begins at 22:43:39.238617\n",
      "\n",
      "Evaluating: batch 19 ends at 22:43:39.464626\n",
      "\n",
      "Evaluating: batch 20 begins at 22:43:39.465773\n",
      "\n",
      "Evaluating: batch 20 ends at 22:43:39.692203\n",
      "\n",
      "Evaluating: batch 21 begins at 22:43:39.693390\n",
      "\n",
      "Evaluating: batch 21 ends at 22:43:39.926368\n",
      "\n",
      "Evaluating: batch 22 begins at 22:43:39.927586\n",
      "\n",
      "Evaluating: batch 22 ends at 22:43:40.153415\n",
      "\n",
      "Evaluating: batch 23 begins at 22:43:40.154428\n",
      "\n",
      "Evaluating: batch 23 ends at 22:43:40.379694\n",
      "\n",
      "Evaluating: batch 24 begins at 22:43:40.380881\n",
      "\n",
      "Evaluating: batch 24 ends at 22:43:40.607476\n",
      "\n",
      "Evaluating: batch 25 begins at 22:43:40.608615\n",
      "\n",
      "Evaluating: batch 25 ends at 22:43:40.835843\n",
      "\n",
      "Evaluating: batch 26 begins at 22:43:40.836966\n",
      "\n",
      "Evaluating: batch 26 ends at 22:43:41.057387\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33645 to 0.33485, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 57s 948ms/step - loss: 0.3580 - bce_dice_loss: 0.3580 - val_loss: 0.3348 - val_bce_dice_loss: 0.3348\n",
      "Epoch 4/25\n",
      "\n",
      "Training: batch 0 begins at 22:43:42.812463\n",
      "\n",
      "Training: batch 0 ends at 22:43:43.654914\n",
      " 1/60 [..............................] - ETA: 49s - loss: 0.6886 - bce_dice_loss: 0.6886\n",
      "Training: batch 1 begins at 22:43:43.659615\n",
      "\n",
      "Training: batch 1 ends at 22:43:44.479882\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4954 - bce_dice_loss: 0.4954\n",
      "Training: batch 2 begins at 22:43:44.484235\n",
      "\n",
      "Training: batch 2 ends at 22:43:45.292582\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3998 - bce_dice_loss: 0.3998\n",
      "Training: batch 3 begins at 22:43:45.298187\n",
      "\n",
      "Training: batch 3 ends at 22:43:46.107575\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3750 - bce_dice_loss: 0.3750\n",
      "Training: batch 4 begins at 22:43:46.112287\n",
      "\n",
      "Training: batch 4 ends at 22:43:46.902488\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3595 - bce_dice_loss: 0.3595\n",
      "Training: batch 5 begins at 22:43:46.907064\n",
      "\n",
      "Training: batch 5 ends at 22:43:47.698461\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3328 - bce_dice_loss: 0.3328\n",
      "Training: batch 6 begins at 22:43:47.703188\n",
      "\n",
      "Training: batch 6 ends at 22:43:48.496748\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3490 - bce_dice_loss: 0.3490\n",
      "Training: batch 7 begins at 22:43:48.501677\n",
      "\n",
      "Training: batch 7 ends at 22:43:49.297395\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3398 - bce_dice_loss: 0.3398\n",
      "Training: batch 8 begins at 22:43:49.301580\n",
      "\n",
      "Training: batch 8 ends at 22:43:50.124248\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3329 - bce_dice_loss: 0.3329\n",
      "Training: batch 9 begins at 22:43:50.126848\n",
      "\n",
      "Training: batch 9 ends at 22:43:50.938119\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 10 begins at 22:43:50.941593\n",
      "\n",
      "Training: batch 10 ends at 22:43:51.736718\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3325 - bce_dice_loss: 0.3325\n",
      "Training: batch 11 begins at 22:43:51.740700\n",
      "\n",
      "Training: batch 11 ends at 22:43:52.571676\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3268 - bce_dice_loss: 0.3268\n",
      "Training: batch 12 begins at 22:43:52.575055\n",
      "\n",
      "Training: batch 12 ends at 22:43:53.480536\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.3267 - bce_dice_loss: 0.3267\n",
      "Training: batch 13 begins at 22:43:53.483631\n",
      "\n",
      "Training: batch 13 ends at 22:43:54.381484\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3225 - bce_dice_loss: 0.3225\n",
      "Training: batch 14 begins at 22:43:54.386414\n",
      "\n",
      "Training: batch 14 ends at 22:43:55.323007\n",
      "15/60 [======>.......................] - ETA: 37s - loss: 0.3127 - bce_dice_loss: 0.3127\n",
      "Training: batch 15 begins at 22:43:55.326476\n",
      "\n",
      "Training: batch 15 ends at 22:43:56.201283\n",
      "16/60 [=======>......................] - ETA: 36s - loss: 0.3085 - bce_dice_loss: 0.3085\n",
      "Training: batch 16 begins at 22:43:56.204724\n",
      "\n",
      "Training: batch 16 ends at 22:43:57.026239\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.3218 - bce_dice_loss: 0.3218\n",
      "Training: batch 17 begins at 22:43:57.028979\n",
      "\n",
      "Training: batch 17 ends at 22:43:57.852447\n",
      "18/60 [========>.....................] - ETA: 35s - loss: 0.3103 - bce_dice_loss: 0.3103\n",
      "Training: batch 18 begins at 22:43:57.856000\n",
      "\n",
      "Training: batch 18 ends at 22:43:58.677939\n",
      "19/60 [========>.....................] - ETA: 34s - loss: 0.3066 - bce_dice_loss: 0.3066\n",
      "Training: batch 19 begins at 22:43:58.681236\n",
      "\n",
      "Training: batch 19 ends at 22:43:59.483892\n",
      "20/60 [=========>....................] - ETA: 33s - loss: 0.3117 - bce_dice_loss: 0.3117\n",
      "Training: batch 20 begins at 22:43:59.487390\n",
      "\n",
      "Training: batch 20 ends at 22:44:00.312771\n",
      "21/60 [=========>....................] - ETA: 32s - loss: 0.3078 - bce_dice_loss: 0.3078\n",
      "Training: batch 21 begins at 22:44:00.316437\n",
      "\n",
      "Training: batch 21 ends at 22:44:01.118778\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.3095 - bce_dice_loss: 0.3095\n",
      "Training: batch 22 begins at 22:44:01.122275\n",
      "\n",
      "Training: batch 22 ends at 22:44:01.936689\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.3159 - bce_dice_loss: 0.3159\n",
      "Training: batch 23 begins at 22:44:01.940908\n",
      "\n",
      "Training: batch 23 ends at 22:44:02.773678\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3132 - bce_dice_loss: 0.3132\n",
      "Training: batch 24 begins at 22:44:02.776583\n",
      "\n",
      "Training: batch 24 ends at 22:44:03.578046\n",
      "25/60 [===========>..................] - ETA: 29s - loss: 0.3116 - bce_dice_loss: 0.3116\n",
      "Training: batch 25 begins at 22:44:03.581387\n",
      "\n",
      "Training: batch 25 ends at 22:44:04.384595\n",
      "26/60 [============>.................] - ETA: 28s - loss: 0.3164 - bce_dice_loss: 0.3164\n",
      "Training: batch 26 begins at 22:44:04.387333\n",
      "\n",
      "Training: batch 26 ends at 22:44:05.198891\n",
      "27/60 [============>.................] - ETA: 27s - loss: 0.3145 - bce_dice_loss: 0.3145\n",
      "Training: batch 27 begins at 22:44:05.202268\n",
      "\n",
      "Training: batch 27 ends at 22:44:06.020034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/60 [=============>................] - ETA: 26s - loss: 0.3162 - bce_dice_loss: 0.3162\n",
      "Training: batch 28 begins at 22:44:06.024196\n",
      "\n",
      "Training: batch 28 ends at 22:44:06.838286\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3163 - bce_dice_loss: 0.3163\n",
      "Training: batch 29 begins at 22:44:06.841540\n",
      "\n",
      "Training: batch 29 ends at 22:44:07.658407\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3167 - bce_dice_loss: 0.3167\n",
      "Training: batch 30 begins at 22:44:07.661998\n",
      "\n",
      "Training: batch 30 ends at 22:44:08.466459\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3165 - bce_dice_loss: 0.3165\n",
      "Training: batch 31 begins at 22:44:08.470350\n",
      "\n",
      "Training: batch 31 ends at 22:44:09.290317\n",
      "32/60 [===============>..............] - ETA: 23s - loss: 0.3159 - bce_dice_loss: 0.3159\n",
      "Training: batch 32 begins at 22:44:09.294296\n",
      "\n",
      "Training: batch 32 ends at 22:44:10.099797\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.3220 - bce_dice_loss: 0.3220\n",
      "Training: batch 33 begins at 22:44:10.103955\n",
      "\n",
      "Training: batch 33 ends at 22:44:10.915092\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3226 - bce_dice_loss: 0.3226\n",
      "Training: batch 34 begins at 22:44:10.919316\n",
      "\n",
      "Training: batch 34 ends at 22:44:11.722388\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3276 - bce_dice_loss: 0.3276\n",
      "Training: batch 35 begins at 22:44:11.726485\n",
      "\n",
      "Training: batch 35 ends at 22:44:12.530261\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3250 - bce_dice_loss: 0.3250\n",
      "Training: batch 36 begins at 22:44:12.533522\n",
      "\n",
      "Training: batch 36 ends at 22:44:13.338408\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3223 - bce_dice_loss: 0.3223\n",
      "Training: batch 37 begins at 22:44:13.341465\n",
      "\n",
      "Training: batch 37 ends at 22:44:14.147096\n",
      "38/60 [==================>...........] - ETA: 18s - loss: 0.3218 - bce_dice_loss: 0.3218\n",
      "Training: batch 38 begins at 22:44:14.150008\n",
      "\n",
      "Training: batch 38 ends at 22:44:14.960092\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3219 - bce_dice_loss: 0.3219\n",
      "Training: batch 39 begins at 22:44:14.962837\n",
      "\n",
      "Training: batch 39 ends at 22:44:15.770297\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3260 - bce_dice_loss: 0.3260\n",
      "Training: batch 40 begins at 22:44:15.773522\n",
      "\n",
      "Training: batch 40 ends at 22:44:16.572213\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3247 - bce_dice_loss: 0.3247\n",
      "Training: batch 41 begins at 22:44:16.575172\n",
      "\n",
      "Training: batch 41 ends at 22:44:17.379179\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3248 - bce_dice_loss: 0.3248\n",
      "Training: batch 42 begins at 22:44:17.382130\n",
      "\n",
      "Training: batch 42 ends at 22:44:18.197232\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 43 begins at 22:44:18.199964\n",
      "\n",
      "Training: batch 43 ends at 22:44:19.008933\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3268 - bce_dice_loss: 0.3268\n",
      "Training: batch 44 begins at 22:44:19.011712\n",
      "\n",
      "Training: batch 44 ends at 22:44:19.820510\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3293 - bce_dice_loss: 0.3293\n",
      "Training: batch 45 begins at 22:44:19.823275\n",
      "\n",
      "Training: batch 45 ends at 22:44:20.625110\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3280 - bce_dice_loss: 0.3280\n",
      "Training: batch 46 begins at 22:44:20.629577\n",
      "\n",
      "Training: batch 46 ends at 22:44:21.432763\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3282 - bce_dice_loss: 0.3282\n",
      "Training: batch 47 begins at 22:44:21.439078\n",
      "\n",
      "Training: batch 47 ends at 22:44:22.261209\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3273 - bce_dice_loss: 0.3273 \n",
      "Training: batch 48 begins at 22:44:22.265672\n",
      "\n",
      "Training: batch 48 ends at 22:44:23.069389\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.3255 - bce_dice_loss: 0.3255\n",
      "Training: batch 49 begins at 22:44:23.072527\n",
      "\n",
      "Training: batch 49 ends at 22:44:23.876792\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3273 - bce_dice_loss: 0.3273\n",
      "Training: batch 50 begins at 22:44:23.881201\n",
      "\n",
      "Training: batch 50 ends at 22:44:24.692873\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3298 - bce_dice_loss: 0.3298\n",
      "Training: batch 51 begins at 22:44:24.698842\n",
      "\n",
      "Training: batch 51 ends at 22:44:25.525726\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3315 - bce_dice_loss: 0.3315\n",
      "Training: batch 52 begins at 22:44:25.529863\n",
      "\n",
      "Training: batch 52 ends at 22:44:26.335162\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3376 - bce_dice_loss: 0.3376\n",
      "Training: batch 53 begins at 22:44:26.339396\n",
      "\n",
      "Training: batch 53 ends at 22:44:27.137542\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3362 - bce_dice_loss: 0.3362\n",
      "Training: batch 54 begins at 22:44:27.141954\n",
      "\n",
      "Training: batch 54 ends at 22:44:27.940821\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3383 - bce_dice_loss: 0.3383\n",
      "Training: batch 55 begins at 22:44:27.945451\n",
      "\n",
      "Training: batch 55 ends at 22:44:28.739968\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3381 - bce_dice_loss: 0.3381\n",
      "Training: batch 56 begins at 22:44:28.744433\n",
      "\n",
      "Training: batch 56 ends at 22:44:29.550438\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3418 - bce_dice_loss: 0.3418\n",
      "Training: batch 57 begins at 22:44:29.555156\n",
      "\n",
      "Training: batch 57 ends at 22:44:30.353087\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3388 - bce_dice_loss: 0.3388\n",
      "Training: batch 58 begins at 22:44:30.355355\n",
      "\n",
      "Training: batch 58 ends at 22:44:31.146888\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3444 - bce_dice_loss: 0.3444\n",
      "Training: batch 59 begins at 22:44:31.153466\n",
      "\n",
      "Training: batch 59 ends at 22:44:31.962749\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3461 - bce_dice_loss: 0.3461\n",
      "Evaluating: batch 0 begins at 22:44:31.995848\n",
      "\n",
      "Evaluating: batch 0 ends at 22:44:32.267158\n",
      "\n",
      "Evaluating: batch 1 begins at 22:44:32.268323\n",
      "\n",
      "Evaluating: batch 1 ends at 22:44:32.483369\n",
      "\n",
      "Evaluating: batch 2 begins at 22:44:32.484811\n",
      "\n",
      "Evaluating: batch 2 ends at 22:44:32.698583\n",
      "\n",
      "Evaluating: batch 3 begins at 22:44:32.699825\n",
      "\n",
      "Evaluating: batch 3 ends at 22:44:32.923140\n",
      "\n",
      "Evaluating: batch 4 begins at 22:44:32.924802\n",
      "\n",
      "Evaluating: batch 4 ends at 22:44:33.145075\n",
      "\n",
      "Evaluating: batch 5 begins at 22:44:33.147632\n",
      "\n",
      "Evaluating: batch 5 ends at 22:44:33.369941\n",
      "\n",
      "Evaluating: batch 6 begins at 22:44:33.372011\n",
      "\n",
      "Evaluating: batch 6 ends at 22:44:33.591735\n",
      "\n",
      "Evaluating: batch 7 begins at 22:44:33.594104\n",
      "\n",
      "Evaluating: batch 7 ends at 22:44:33.817829\n",
      "\n",
      "Evaluating: batch 8 begins at 22:44:33.819411\n",
      "\n",
      "Evaluating: batch 8 ends at 22:44:34.039956\n",
      "\n",
      "Evaluating: batch 9 begins at 22:44:34.041971\n",
      "\n",
      "Evaluating: batch 9 ends at 22:44:34.262814\n",
      "\n",
      "Evaluating: batch 10 begins at 22:44:34.265138\n",
      "\n",
      "Evaluating: batch 10 ends at 22:44:34.484547\n",
      "\n",
      "Evaluating: batch 11 begins at 22:44:34.488048\n",
      "\n",
      "Evaluating: batch 11 ends at 22:44:34.718159\n",
      "\n",
      "Evaluating: batch 12 begins at 22:44:34.719599\n",
      "\n",
      "Evaluating: batch 12 ends at 22:44:34.941621\n",
      "\n",
      "Evaluating: batch 13 begins at 22:44:34.943927\n",
      "\n",
      "Evaluating: batch 13 ends at 22:44:35.167257\n",
      "\n",
      "Evaluating: batch 14 begins at 22:44:35.169125\n",
      "\n",
      "Evaluating: batch 14 ends at 22:44:35.388919\n",
      "\n",
      "Evaluating: batch 15 begins at 22:44:35.390286\n",
      "\n",
      "Evaluating: batch 15 ends at 22:44:35.608287\n",
      "\n",
      "Evaluating: batch 16 begins at 22:44:35.610615\n",
      "\n",
      "Evaluating: batch 16 ends at 22:44:35.834219\n",
      "\n",
      "Evaluating: batch 17 begins at 22:44:35.836701\n",
      "\n",
      "Evaluating: batch 17 ends at 22:44:36.056944\n",
      "\n",
      "Evaluating: batch 18 begins at 22:44:36.059346\n",
      "\n",
      "Evaluating: batch 18 ends at 22:44:36.281875\n",
      "\n",
      "Evaluating: batch 19 begins at 22:44:36.283263\n",
      "\n",
      "Evaluating: batch 19 ends at 22:44:36.499012\n",
      "\n",
      "Evaluating: batch 20 begins at 22:44:36.500245\n",
      "\n",
      "Evaluating: batch 20 ends at 22:44:36.715953\n",
      "\n",
      "Evaluating: batch 21 begins at 22:44:36.717255\n",
      "\n",
      "Evaluating: batch 21 ends at 22:44:36.936335\n",
      "\n",
      "Evaluating: batch 22 begins at 22:44:36.938413\n",
      "\n",
      "Evaluating: batch 22 ends at 22:44:37.155975\n",
      "\n",
      "Evaluating: batch 23 begins at 22:44:37.158291\n",
      "\n",
      "Evaluating: batch 23 ends at 22:44:37.375762\n",
      "\n",
      "Evaluating: batch 24 begins at 22:44:37.377686\n",
      "\n",
      "Evaluating: batch 24 ends at 22:44:37.598968\n",
      "\n",
      "Evaluating: batch 25 begins at 22:44:37.601502\n",
      "\n",
      "Evaluating: batch 25 ends at 22:44:37.822461\n",
      "\n",
      "Evaluating: batch 26 begins at 22:44:37.824951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 26 ends at 22:44:38.040477\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33485\n",
      "60/60 [==============================] - 55s 922ms/step - loss: 0.3461 - bce_dice_loss: 0.3461 - val_loss: 0.3830 - val_bce_dice_loss: 0.3830\n",
      "Epoch 5/25\n",
      "\n",
      "Training: batch 0 begins at 22:44:38.065360\n",
      "\n",
      "Training: batch 0 ends at 22:44:38.873730\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.4290 - bce_dice_loss: 0.4290\n",
      "Training: batch 1 begins at 22:44:38.878016\n",
      "\n",
      "Training: batch 1 ends at 22:44:39.675647\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3480 - bce_dice_loss: 0.3480\n",
      "Training: batch 2 begins at 22:44:39.678438\n",
      "\n",
      "Training: batch 2 ends at 22:44:40.478765\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.3518 - bce_dice_loss: 0.3518\n",
      "Training: batch 3 begins at 22:44:40.481706\n",
      "\n",
      "Training: batch 3 ends at 22:44:41.276379\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.3711 - bce_dice_loss: 0.3711\n",
      "Training: batch 4 begins at 22:44:41.278946\n",
      "\n",
      "Training: batch 4 ends at 22:44:42.083599\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3928 - bce_dice_loss: 0.3928\n",
      "Training: batch 5 begins at 22:44:42.086566\n",
      "\n",
      "Training: batch 5 ends at 22:44:42.887348\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.4020 - bce_dice_loss: 0.4020\n",
      "Training: batch 6 begins at 22:44:42.892321\n",
      "\n",
      "Training: batch 6 ends at 22:44:43.693368\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3950 - bce_dice_loss: 0.3950\n",
      "Training: batch 7 begins at 22:44:43.697440\n",
      "\n",
      "Training: batch 7 ends at 22:44:44.489385\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3866 - bce_dice_loss: 0.3866\n",
      "Training: batch 8 begins at 22:44:44.493829\n",
      "\n",
      "Training: batch 8 ends at 22:44:45.312448\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3746 - bce_dice_loss: 0.3746\n",
      "Training: batch 9 begins at 22:44:45.315848\n",
      "\n",
      "Training: batch 9 ends at 22:44:46.131749\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3731 - bce_dice_loss: 0.3731\n",
      "Training: batch 10 begins at 22:44:46.136538\n",
      "\n",
      "Training: batch 10 ends at 22:44:46.947913\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3728 - bce_dice_loss: 0.3728\n",
      "Training: batch 11 begins at 22:44:46.956271\n",
      "\n",
      "Training: batch 11 ends at 22:44:47.744363\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3819 - bce_dice_loss: 0.3819\n",
      "Training: batch 12 begins at 22:44:47.748535\n",
      "\n",
      "Training: batch 12 ends at 22:44:48.548848\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3804 - bce_dice_loss: 0.3804\n",
      "Training: batch 13 begins at 22:44:48.553177\n",
      "\n",
      "Training: batch 13 ends at 22:44:49.346799\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3722 - bce_dice_loss: 0.3722\n",
      "Training: batch 14 begins at 22:44:49.351268\n",
      "\n",
      "Training: batch 14 ends at 22:44:50.155463\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3690 - bce_dice_loss: 0.3690\n",
      "Training: batch 15 begins at 22:44:50.159061\n",
      "\n",
      "Training: batch 15 ends at 22:44:50.979084\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3642 - bce_dice_loss: 0.3642\n",
      "Training: batch 16 begins at 22:44:50.985893\n",
      "\n",
      "Training: batch 16 ends at 22:44:51.778452\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3530 - bce_dice_loss: 0.3530\n",
      "Training: batch 17 begins at 22:44:51.782945\n",
      "\n",
      "Training: batch 17 ends at 22:44:52.580391\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3495 - bce_dice_loss: 0.3495\n",
      "Training: batch 18 begins at 22:44:52.585234\n",
      "\n",
      "Training: batch 18 ends at 22:44:53.375207\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3505 - bce_dice_loss: 0.3505\n",
      "Training: batch 19 begins at 22:44:53.379073\n",
      "\n",
      "Training: batch 19 ends at 22:44:54.178472\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3448 - bce_dice_loss: 0.3448\n",
      "Training: batch 20 begins at 22:44:54.182735\n",
      "\n",
      "Training: batch 20 ends at 22:44:54.978000\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3501 - bce_dice_loss: 0.3501\n",
      "Training: batch 21 begins at 22:44:54.982153\n",
      "\n",
      "Training: batch 21 ends at 22:44:55.783868\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3670 - bce_dice_loss: 0.3670\n",
      "Training: batch 22 begins at 22:44:55.787947\n",
      "\n",
      "Training: batch 22 ends at 22:44:56.580021\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3684 - bce_dice_loss: 0.3684\n",
      "Training: batch 23 begins at 22:44:56.583060\n",
      "\n",
      "Training: batch 23 ends at 22:44:57.379218\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3758 - bce_dice_loss: 0.3758\n",
      "Training: batch 24 begins at 22:44:57.383295\n",
      "\n",
      "Training: batch 24 ends at 22:44:58.188941\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3733 - bce_dice_loss: 0.3733\n",
      "Training: batch 25 begins at 22:44:58.191464\n",
      "\n",
      "Training: batch 25 ends at 22:44:58.987458\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3694 - bce_dice_loss: 0.3694\n",
      "Training: batch 26 begins at 22:44:58.989855\n",
      "\n",
      "Training: batch 26 ends at 22:44:59.790637\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3714 - bce_dice_loss: 0.3714\n",
      "Training: batch 27 begins at 22:44:59.793246\n",
      "\n",
      "Training: batch 27 ends at 22:45:00.621551\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3694 - bce_dice_loss: 0.3694\n",
      "Training: batch 28 begins at 22:45:00.625880\n",
      "\n",
      "Training: batch 28 ends at 22:45:01.427942\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3660 - bce_dice_loss: 0.3660\n",
      "Training: batch 29 begins at 22:45:01.432182\n",
      "\n",
      "Training: batch 29 ends at 22:45:02.236590\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3626 - bce_dice_loss: 0.3626\n",
      "Training: batch 30 begins at 22:45:02.241630\n",
      "\n",
      "Training: batch 30 ends at 22:45:03.040037\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3571 - bce_dice_loss: 0.3571\n",
      "Training: batch 31 begins at 22:45:03.042712\n",
      "\n",
      "Training: batch 31 ends at 22:45:03.851456\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3538 - bce_dice_loss: 0.3538\n",
      "Training: batch 32 begins at 22:45:03.854844\n",
      "\n",
      "Training: batch 32 ends at 22:45:04.656764\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3476 - bce_dice_loss: 0.3476\n",
      "Training: batch 33 begins at 22:45:04.660087\n",
      "\n",
      "Training: batch 33 ends at 22:45:05.452729\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3435 - bce_dice_loss: 0.3435\n",
      "Training: batch 34 begins at 22:45:05.457014\n",
      "\n",
      "Training: batch 34 ends at 22:45:06.278679\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3487 - bce_dice_loss: 0.3487\n",
      "Training: batch 35 begins at 22:45:06.281485\n",
      "\n",
      "Training: batch 35 ends at 22:45:07.080568\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3479 - bce_dice_loss: 0.3479\n",
      "Training: batch 36 begins at 22:45:07.085432\n",
      "\n",
      "Training: batch 36 ends at 22:45:07.883134\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3480 - bce_dice_loss: 0.3480\n",
      "Training: batch 37 begins at 22:45:07.886959\n",
      "\n",
      "Training: batch 37 ends at 22:45:08.679510\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3470 - bce_dice_loss: 0.3470\n",
      "Training: batch 38 begins at 22:45:08.683949\n",
      "\n",
      "Training: batch 38 ends at 22:45:09.479407\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3463 - bce_dice_loss: 0.3463\n",
      "Training: batch 39 begins at 22:45:09.483660\n",
      "\n",
      "Training: batch 39 ends at 22:45:10.278558\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3426 - bce_dice_loss: 0.3426\n",
      "Training: batch 40 begins at 22:45:10.280959\n",
      "\n",
      "Training: batch 40 ends at 22:45:11.070596\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3426 - bce_dice_loss: 0.3426\n",
      "Training: batch 41 begins at 22:45:11.074329\n",
      "\n",
      "Training: batch 41 ends at 22:45:11.871315\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3498 - bce_dice_loss: 0.3498\n",
      "Training: batch 42 begins at 22:45:11.874315\n",
      "\n",
      "Training: batch 42 ends at 22:45:12.670003\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3462 - bce_dice_loss: 0.3462\n",
      "Training: batch 43 begins at 22:45:12.675339\n",
      "\n",
      "Training: batch 43 ends at 22:45:13.487406\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3461 - bce_dice_loss: 0.3461\n",
      "Training: batch 44 begins at 22:45:13.490832\n",
      "\n",
      "Training: batch 44 ends at 22:45:14.286286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3473 - bce_dice_loss: 0.3473\n",
      "Training: batch 45 begins at 22:45:14.289125\n",
      "\n",
      "Training: batch 45 ends at 22:45:15.097999\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3442 - bce_dice_loss: 0.3442\n",
      "Training: batch 46 begins at 22:45:15.103538\n",
      "\n",
      "Training: batch 46 ends at 22:45:15.904969\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3441 - bce_dice_loss: 0.3441\n",
      "Training: batch 47 begins at 22:45:15.907429\n",
      "\n",
      "Training: batch 47 ends at 22:45:16.709650\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3536 - bce_dice_loss: 0.3536 \n",
      "Training: batch 48 begins at 22:45:16.714218\n",
      "\n",
      "Training: batch 48 ends at 22:45:17.540150\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3522 - bce_dice_loss: 0.3522\n",
      "Training: batch 49 begins at 22:45:17.544271\n",
      "\n",
      "Training: batch 49 ends at 22:45:18.338849\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3515 - bce_dice_loss: 0.3515\n",
      "Training: batch 50 begins at 22:45:18.341633\n",
      "\n",
      "Training: batch 50 ends at 22:45:19.140946\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3508 - bce_dice_loss: 0.3508\n",
      "Training: batch 51 begins at 22:45:19.145028\n",
      "\n",
      "Training: batch 51 ends at 22:45:19.952694\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3519 - bce_dice_loss: 0.3519\n",
      "Training: batch 52 begins at 22:45:19.957189\n",
      "\n",
      "Training: batch 52 ends at 22:45:20.748438\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3512 - bce_dice_loss: 0.3512\n",
      "Training: batch 53 begins at 22:45:20.757000\n",
      "\n",
      "Training: batch 53 ends at 22:45:21.550051\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3520 - bce_dice_loss: 0.3520\n",
      "Training: batch 54 begins at 22:45:21.554817\n",
      "\n",
      "Training: batch 54 ends at 22:45:22.347805\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3506 - bce_dice_loss: 0.3506\n",
      "Training: batch 55 begins at 22:45:22.353331\n",
      "\n",
      "Training: batch 55 ends at 22:45:23.155289\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3504 - bce_dice_loss: 0.3504\n",
      "Training: batch 56 begins at 22:45:23.159003\n",
      "\n",
      "Training: batch 56 ends at 22:45:23.952009\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3484 - bce_dice_loss: 0.3484\n",
      "Training: batch 57 begins at 22:45:23.956350\n",
      "\n",
      "Training: batch 57 ends at 22:45:24.751756\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3524 - bce_dice_loss: 0.3524\n",
      "Training: batch 58 begins at 22:45:24.756337\n",
      "\n",
      "Training: batch 58 ends at 22:45:25.549839\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3511 - bce_dice_loss: 0.3511\n",
      "Training: batch 59 begins at 22:45:25.555425\n",
      "\n",
      "Training: batch 59 ends at 22:45:26.343473\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3503 - bce_dice_loss: 0.3503\n",
      "Evaluating: batch 0 begins at 22:45:26.373070\n",
      "\n",
      "Evaluating: batch 0 ends at 22:45:26.636858\n",
      "\n",
      "Evaluating: batch 1 begins at 22:45:26.638055\n",
      "\n",
      "Evaluating: batch 1 ends at 22:45:26.854260\n",
      "\n",
      "Evaluating: batch 2 begins at 22:45:26.855526\n",
      "\n",
      "Evaluating: batch 2 ends at 22:45:27.076018\n",
      "\n",
      "Evaluating: batch 3 begins at 22:45:27.078133\n",
      "\n",
      "Evaluating: batch 3 ends at 22:45:27.299297\n",
      "\n",
      "Evaluating: batch 4 begins at 22:45:27.301732\n",
      "\n",
      "Evaluating: batch 4 ends at 22:45:27.523589\n",
      "\n",
      "Evaluating: batch 5 begins at 22:45:27.525385\n",
      "\n",
      "Evaluating: batch 5 ends at 22:45:27.743651\n",
      "\n",
      "Evaluating: batch 6 begins at 22:45:27.746091\n",
      "\n",
      "Evaluating: batch 6 ends at 22:45:27.967953\n",
      "\n",
      "Evaluating: batch 7 begins at 22:45:27.969273\n",
      "\n",
      "Evaluating: batch 7 ends at 22:45:28.188094\n",
      "\n",
      "Evaluating: batch 8 begins at 22:45:28.190412\n",
      "\n",
      "Evaluating: batch 8 ends at 22:45:28.414092\n",
      "\n",
      "Evaluating: batch 9 begins at 22:45:28.415587\n",
      "\n",
      "Evaluating: batch 9 ends at 22:45:28.636942\n",
      "\n",
      "Evaluating: batch 10 begins at 22:45:28.639119\n",
      "\n",
      "Evaluating: batch 10 ends at 22:45:28.861195\n",
      "\n",
      "Evaluating: batch 11 begins at 22:45:28.862465\n",
      "\n",
      "Evaluating: batch 11 ends at 22:45:29.083826\n",
      "\n",
      "Evaluating: batch 12 begins at 22:45:29.085281\n",
      "\n",
      "Evaluating: batch 12 ends at 22:45:29.308230\n",
      "\n",
      "Evaluating: batch 13 begins at 22:45:29.310157\n",
      "\n",
      "Evaluating: batch 13 ends at 22:45:29.531974\n",
      "\n",
      "Evaluating: batch 14 begins at 22:45:29.533349\n",
      "\n",
      "Evaluating: batch 14 ends at 22:45:29.757545\n",
      "\n",
      "Evaluating: batch 15 begins at 22:45:29.758811\n",
      "\n",
      "Evaluating: batch 15 ends at 22:45:29.978352\n",
      "\n",
      "Evaluating: batch 16 begins at 22:45:29.980205\n",
      "\n",
      "Evaluating: batch 16 ends at 22:45:30.200973\n",
      "\n",
      "Evaluating: batch 17 begins at 22:45:30.202932\n",
      "\n",
      "Evaluating: batch 17 ends at 22:45:30.423634\n",
      "\n",
      "Evaluating: batch 18 begins at 22:45:30.425855\n",
      "\n",
      "Evaluating: batch 18 ends at 22:45:30.647157\n",
      "\n",
      "Evaluating: batch 19 begins at 22:45:30.649167\n",
      "\n",
      "Evaluating: batch 19 ends at 22:45:30.872272\n",
      "\n",
      "Evaluating: batch 20 begins at 22:45:30.873727\n",
      "\n",
      "Evaluating: batch 20 ends at 22:45:31.094334\n",
      "\n",
      "Evaluating: batch 21 begins at 22:45:31.096879\n",
      "\n",
      "Evaluating: batch 21 ends at 22:45:31.315536\n",
      "\n",
      "Evaluating: batch 22 begins at 22:45:31.316975\n",
      "\n",
      "Evaluating: batch 22 ends at 22:45:31.532217\n",
      "\n",
      "Evaluating: batch 23 begins at 22:45:31.533609\n",
      "\n",
      "Evaluating: batch 23 ends at 22:45:31.754431\n",
      "\n",
      "Evaluating: batch 24 begins at 22:45:31.756739\n",
      "\n",
      "Evaluating: batch 24 ends at 22:45:31.972522\n",
      "\n",
      "Evaluating: batch 25 begins at 22:45:31.974865\n",
      "\n",
      "Evaluating: batch 25 ends at 22:45:32.191613\n",
      "\n",
      "Evaluating: batch 26 begins at 22:45:32.193327\n",
      "\n",
      "Evaluating: batch 26 ends at 22:45:32.414917\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33485 to 0.32433, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 0.3503 - bce_dice_loss: 0.3503 - val_loss: 0.3243 - val_bce_dice_loss: 0.3243\n",
      "Epoch 6/25\n",
      "\n",
      "Training: batch 0 begins at 22:45:34.046523\n",
      "\n",
      "Training: batch 0 ends at 22:45:34.879350\n",
      " 1/60 [..............................] - ETA: 49s - loss: 0.2376 - bce_dice_loss: 0.2376\n",
      "Training: batch 1 begins at 22:45:34.885353\n",
      "\n",
      "Training: batch 1 ends at 22:45:35.698281\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2424 - bce_dice_loss: 0.2424\n",
      "Training: batch 2 begins at 22:45:35.706389\n",
      "\n",
      "Training: batch 2 ends at 22:45:36.534339\n",
      " 3/60 [>.............................] - ETA: 47s - loss: 0.2668 - bce_dice_loss: 0.2668\n",
      "Training: batch 3 begins at 22:45:36.538752\n",
      "\n",
      "Training: batch 3 ends at 22:45:37.344865\n",
      " 4/60 [=>............................] - ETA: 46s - loss: 0.2671 - bce_dice_loss: 0.2671\n",
      "Training: batch 4 begins at 22:45:37.349196\n",
      "\n",
      "Training: batch 4 ends at 22:45:38.144709\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3305 - bce_dice_loss: 0.3305\n",
      "Training: batch 5 begins at 22:45:38.148918\n",
      "\n",
      "Training: batch 5 ends at 22:45:38.943176\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3182 - bce_dice_loss: 0.3182\n",
      "Training: batch 6 begins at 22:45:38.947308\n",
      "\n",
      "Training: batch 6 ends at 22:45:39.742387\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3462 - bce_dice_loss: 0.3462\n",
      "Training: batch 7 begins at 22:45:39.746613\n",
      "\n",
      "Training: batch 7 ends at 22:45:40.562259\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3340 - bce_dice_loss: 0.3340\n",
      "Training: batch 8 begins at 22:45:40.565247\n",
      "\n",
      "Training: batch 8 ends at 22:45:41.360453\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3370 - bce_dice_loss: 0.3370\n",
      "Training: batch 9 begins at 22:45:41.365084\n",
      "\n",
      "Training: batch 9 ends at 22:45:42.156805\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3291 - bce_dice_loss: 0.3291\n",
      "Training: batch 10 begins at 22:45:42.159942\n",
      "\n",
      "Training: batch 10 ends at 22:45:42.957946\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3261 - bce_dice_loss: 0.3261\n",
      "Training: batch 11 begins at 22:45:42.961116\n",
      "\n",
      "Training: batch 11 ends at 22:45:43.758297\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3399 - bce_dice_loss: 0.3399\n",
      "Training: batch 12 begins at 22:45:43.761055\n",
      "\n",
      "Training: batch 12 ends at 22:45:44.555418\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3486 - bce_dice_loss: 0.3486\n",
      "Training: batch 13 begins at 22:45:44.558326\n",
      "\n",
      "Training: batch 13 ends at 22:45:45.394396\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3500 - bce_dice_loss: 0.3500\n",
      "Training: batch 14 begins at 22:45:45.398764\n",
      "\n",
      "Training: batch 14 ends at 22:45:46.196965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3664 - bce_dice_loss: 0.3664\n",
      "Training: batch 15 begins at 22:45:46.200978\n",
      "\n",
      "Training: batch 15 ends at 22:45:46.994317\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3655 - bce_dice_loss: 0.3655\n",
      "Training: batch 16 begins at 22:45:46.998466\n",
      "\n",
      "Training: batch 16 ends at 22:45:47.823262\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3648 - bce_dice_loss: 0.3648\n",
      "Training: batch 17 begins at 22:45:47.827556\n",
      "\n",
      "Training: batch 17 ends at 22:45:48.621316\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3581 - bce_dice_loss: 0.3581\n",
      "Training: batch 18 begins at 22:45:48.625429\n",
      "\n",
      "Training: batch 18 ends at 22:45:49.418426\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3589 - bce_dice_loss: 0.3589\n",
      "Training: batch 19 begins at 22:45:49.422747\n",
      "\n",
      "Training: batch 19 ends at 22:45:50.226211\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3581 - bce_dice_loss: 0.3581\n",
      "Training: batch 20 begins at 22:45:50.230614\n",
      "\n",
      "Training: batch 20 ends at 22:45:51.028518\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3646 - bce_dice_loss: 0.3646\n",
      "Training: batch 21 begins at 22:45:51.033294\n",
      "\n",
      "Training: batch 21 ends at 22:45:51.853723\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3575 - bce_dice_loss: 0.3575\n",
      "Training: batch 22 begins at 22:45:51.857887\n",
      "\n",
      "Training: batch 22 ends at 22:45:52.655029\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3675 - bce_dice_loss: 0.3675\n",
      "Training: batch 23 begins at 22:45:52.659344\n",
      "\n",
      "Training: batch 23 ends at 22:45:53.449355\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3680 - bce_dice_loss: 0.3680\n",
      "Training: batch 24 begins at 22:45:53.451888\n",
      "\n",
      "Training: batch 24 ends at 22:45:54.258118\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3583 - bce_dice_loss: 0.3583\n",
      "Training: batch 25 begins at 22:45:54.261523\n",
      "\n",
      "Training: batch 25 ends at 22:45:55.074534\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3566 - bce_dice_loss: 0.3566\n",
      "Training: batch 26 begins at 22:45:55.078564\n",
      "\n",
      "Training: batch 26 ends at 22:45:55.883557\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3526 - bce_dice_loss: 0.3526\n",
      "Training: batch 27 begins at 22:45:55.885870\n",
      "\n",
      "Training: batch 27 ends at 22:45:56.707910\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3501 - bce_dice_loss: 0.3501\n",
      "Training: batch 28 begins at 22:45:56.710029\n",
      "\n",
      "Training: batch 28 ends at 22:45:57.502244\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.3432 - bce_dice_loss: 0.3432\n",
      "Training: batch 29 begins at 22:45:57.506344\n",
      "\n",
      "Training: batch 29 ends at 22:45:58.323799\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3381 - bce_dice_loss: 0.3381\n",
      "Training: batch 30 begins at 22:45:58.327359\n",
      "\n",
      "Training: batch 30 ends at 22:45:59.128398\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3367 - bce_dice_loss: 0.3367\n",
      "Training: batch 31 begins at 22:45:59.132643\n",
      "\n",
      "Training: batch 31 ends at 22:45:59.927339\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3330 - bce_dice_loss: 0.3330\n",
      "Training: batch 32 begins at 22:45:59.931575\n",
      "\n",
      "Training: batch 32 ends at 22:46:00.755134\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3318 - bce_dice_loss: 0.3318\n",
      "Training: batch 33 begins at 22:46:00.760853\n",
      "\n",
      "Training: batch 33 ends at 22:46:01.559621\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3276 - bce_dice_loss: 0.3276\n",
      "Training: batch 34 begins at 22:46:01.564154\n",
      "\n",
      "Training: batch 34 ends at 22:46:02.370756\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3263 - bce_dice_loss: 0.3263\n",
      "Training: batch 35 begins at 22:46:02.374320\n",
      "\n",
      "Training: batch 35 ends at 22:46:03.190399\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3297 - bce_dice_loss: 0.3297\n",
      "Training: batch 36 begins at 22:46:03.194861\n",
      "\n",
      "Training: batch 36 ends at 22:46:03.989432\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3287 - bce_dice_loss: 0.3287\n",
      "Training: batch 37 begins at 22:46:03.992275\n",
      "\n",
      "Training: batch 37 ends at 22:46:04.785143\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3282 - bce_dice_loss: 0.3282\n",
      "Training: batch 38 begins at 22:46:04.787690\n",
      "\n",
      "Training: batch 38 ends at 22:46:05.613239\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3253 - bce_dice_loss: 0.3253\n",
      "Training: batch 39 begins at 22:46:05.617128\n",
      "\n",
      "Training: batch 39 ends at 22:46:06.417757\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3239 - bce_dice_loss: 0.3239\n",
      "Training: batch 40 begins at 22:46:06.421987\n",
      "\n",
      "Training: batch 40 ends at 22:46:07.218918\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3241 - bce_dice_loss: 0.3241\n",
      "Training: batch 41 begins at 22:46:07.223113\n",
      "\n",
      "Training: batch 41 ends at 22:46:08.044159\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3289 - bce_dice_loss: 0.3289\n",
      "Training: batch 42 begins at 22:46:08.048411\n",
      "\n",
      "Training: batch 42 ends at 22:46:08.846464\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3349 - bce_dice_loss: 0.3349\n",
      "Training: batch 43 begins at 22:46:08.851033\n",
      "\n",
      "Training: batch 43 ends at 22:46:09.650798\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3329 - bce_dice_loss: 0.3329\n",
      "Training: batch 44 begins at 22:46:09.655051\n",
      "\n",
      "Training: batch 44 ends at 22:46:10.452674\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 45 begins at 22:46:10.456496\n",
      "\n",
      "Training: batch 45 ends at 22:46:11.250280\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3312 - bce_dice_loss: 0.3312\n",
      "Training: batch 46 begins at 22:46:11.253341\n",
      "\n",
      "Training: batch 46 ends at 22:46:12.049435\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3324 - bce_dice_loss: 0.3324\n",
      "Training: batch 47 begins at 22:46:12.052160\n",
      "\n",
      "Training: batch 47 ends at 22:46:12.850658\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3313 - bce_dice_loss: 0.3313 \n",
      "Training: batch 48 begins at 22:46:12.854102\n",
      "\n",
      "Training: batch 48 ends at 22:46:13.642374\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3299 - bce_dice_loss: 0.3299\n",
      "Training: batch 49 begins at 22:46:13.647102\n",
      "\n",
      "Training: batch 49 ends at 22:46:14.452758\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3286 - bce_dice_loss: 0.3286\n",
      "Training: batch 50 begins at 22:46:14.456306\n",
      "\n",
      "Training: batch 50 ends at 22:46:15.258505\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3290 - bce_dice_loss: 0.3290\n",
      "Training: batch 51 begins at 22:46:15.261567\n",
      "\n",
      "Training: batch 51 ends at 22:46:16.063951\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 52 begins at 22:46:16.068386\n",
      "\n",
      "Training: batch 52 ends at 22:46:16.885141\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 53 begins at 22:46:16.888629\n",
      "\n",
      "Training: batch 53 ends at 22:46:17.680587\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3307 - bce_dice_loss: 0.3307\n",
      "Training: batch 54 begins at 22:46:17.685145\n",
      "\n",
      "Training: batch 54 ends at 22:46:18.477426\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3303 - bce_dice_loss: 0.3303\n",
      "Training: batch 55 begins at 22:46:18.480037\n",
      "\n",
      "Training: batch 55 ends at 22:46:19.263611\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3287 - bce_dice_loss: 0.3287\n",
      "Training: batch 56 begins at 22:46:19.267440\n",
      "\n",
      "Training: batch 56 ends at 22:46:20.062552\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3316 - bce_dice_loss: 0.3316\n",
      "Training: batch 57 begins at 22:46:20.068614\n",
      "\n",
      "Training: batch 57 ends at 22:46:20.860684\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 58 begins at 22:46:20.865081\n",
      "\n",
      "Training: batch 58 ends at 22:46:21.682546\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3304 - bce_dice_loss: 0.3304\n",
      "Training: batch 59 begins at 22:46:21.686086\n",
      "\n",
      "Training: batch 59 ends at 22:46:22.484188\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3361 - bce_dice_loss: 0.3361\n",
      "Evaluating: batch 0 begins at 22:46:22.512146\n",
      "\n",
      "Evaluating: batch 0 ends at 22:46:22.798212\n",
      "\n",
      "Evaluating: batch 1 begins at 22:46:22.800297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 1 ends at 22:46:23.014226\n",
      "\n",
      "Evaluating: batch 2 begins at 22:46:23.015484\n",
      "\n",
      "Evaluating: batch 2 ends at 22:46:23.236265\n",
      "\n",
      "Evaluating: batch 3 begins at 22:46:23.238540\n",
      "\n",
      "Evaluating: batch 3 ends at 22:46:23.456659\n",
      "\n",
      "Evaluating: batch 4 begins at 22:46:23.459079\n",
      "\n",
      "Evaluating: batch 4 ends at 22:46:23.679939\n",
      "\n",
      "Evaluating: batch 5 begins at 22:46:23.681138\n",
      "\n",
      "Evaluating: batch 5 ends at 22:46:23.905333\n",
      "\n",
      "Evaluating: batch 6 begins at 22:46:23.906922\n",
      "\n",
      "Evaluating: batch 6 ends at 22:46:24.128007\n",
      "\n",
      "Evaluating: batch 7 begins at 22:46:24.129720\n",
      "\n",
      "Evaluating: batch 7 ends at 22:46:24.355313\n",
      "\n",
      "Evaluating: batch 8 begins at 22:46:24.357905\n",
      "\n",
      "Evaluating: batch 8 ends at 22:46:24.576825\n",
      "\n",
      "Evaluating: batch 9 begins at 22:46:24.579377\n",
      "\n",
      "Evaluating: batch 9 ends at 22:46:24.804890\n",
      "\n",
      "Evaluating: batch 10 begins at 22:46:24.806506\n",
      "\n",
      "Evaluating: batch 10 ends at 22:46:25.038097\n",
      "\n",
      "Evaluating: batch 11 begins at 22:46:25.039856\n",
      "\n",
      "Evaluating: batch 11 ends at 22:46:25.258115\n",
      "\n",
      "Evaluating: batch 12 begins at 22:46:25.260511\n",
      "\n",
      "Evaluating: batch 12 ends at 22:46:25.486004\n",
      "\n",
      "Evaluating: batch 13 begins at 22:46:25.487630\n",
      "\n",
      "Evaluating: batch 13 ends at 22:46:25.713747\n",
      "\n",
      "Evaluating: batch 14 begins at 22:46:25.714982\n",
      "\n",
      "Evaluating: batch 14 ends at 22:46:25.939027\n",
      "\n",
      "Evaluating: batch 15 begins at 22:46:25.940410\n",
      "\n",
      "Evaluating: batch 15 ends at 22:46:26.166758\n",
      "\n",
      "Evaluating: batch 16 begins at 22:46:26.168651\n",
      "\n",
      "Evaluating: batch 16 ends at 22:46:26.387253\n",
      "\n",
      "Evaluating: batch 17 begins at 22:46:26.388726\n",
      "\n",
      "Evaluating: batch 17 ends at 22:46:26.607177\n",
      "\n",
      "Evaluating: batch 18 begins at 22:46:26.609590\n",
      "\n",
      "Evaluating: batch 18 ends at 22:46:26.832054\n",
      "\n",
      "Evaluating: batch 19 begins at 22:46:26.833278\n",
      "\n",
      "Evaluating: batch 19 ends at 22:46:27.051731\n",
      "\n",
      "Evaluating: batch 20 begins at 22:46:27.053403\n",
      "\n",
      "Evaluating: batch 20 ends at 22:46:27.275130\n",
      "\n",
      "Evaluating: batch 21 begins at 22:46:27.276767\n",
      "\n",
      "Evaluating: batch 21 ends at 22:46:27.494975\n",
      "\n",
      "Evaluating: batch 22 begins at 22:46:27.497459\n",
      "\n",
      "Evaluating: batch 22 ends at 22:46:27.714238\n",
      "\n",
      "Evaluating: batch 23 begins at 22:46:27.716200\n",
      "\n",
      "Evaluating: batch 23 ends at 22:46:27.935494\n",
      "\n",
      "Evaluating: batch 24 begins at 22:46:27.937875\n",
      "\n",
      "Evaluating: batch 24 ends at 22:46:28.160818\n",
      "\n",
      "Evaluating: batch 25 begins at 22:46:28.162200\n",
      "\n",
      "Evaluating: batch 25 ends at 22:46:28.381884\n",
      "\n",
      "Evaluating: batch 26 begins at 22:46:28.385267\n",
      "\n",
      "Evaluating: batch 26 ends at 22:46:28.602393\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32433\n",
      "60/60 [==============================] - 55s 911ms/step - loss: 0.3361 - bce_dice_loss: 0.3361 - val_loss: 0.3330 - val_bce_dice_loss: 0.3330\n",
      "Epoch 7/25\n",
      "\n",
      "Training: batch 0 begins at 22:46:28.631539\n",
      "\n",
      "Training: batch 0 ends at 22:46:29.448051\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2509 - bce_dice_loss: 0.2509\n",
      "Training: batch 1 begins at 22:46:29.453041\n",
      "\n",
      "Training: batch 1 ends at 22:46:30.256957\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.2770 - bce_dice_loss: 0.2770\n",
      "Training: batch 2 begins at 22:46:30.260669\n",
      "\n",
      "Training: batch 2 ends at 22:46:31.060511\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2593 - bce_dice_loss: 0.2593\n",
      "Training: batch 3 begins at 22:46:31.064910\n",
      "\n",
      "Training: batch 3 ends at 22:46:31.885589\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2931 - bce_dice_loss: 0.2931\n",
      "Training: batch 4 begins at 22:46:31.888903\n",
      "\n",
      "Training: batch 4 ends at 22:46:32.683777\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3095 - bce_dice_loss: 0.3095\n",
      "Training: batch 5 begins at 22:46:32.686623\n",
      "\n",
      "Training: batch 5 ends at 22:46:33.481808\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3297 - bce_dice_loss: 0.3297\n",
      "Training: batch 6 begins at 22:46:33.486386\n",
      "\n",
      "Training: batch 6 ends at 22:46:34.279405\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3278 - bce_dice_loss: 0.3278\n",
      "Training: batch 7 begins at 22:46:34.281938\n",
      "\n",
      "Training: batch 7 ends at 22:46:35.084832\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3564 - bce_dice_loss: 0.3564\n",
      "Training: batch 8 begins at 22:46:35.088788\n",
      "\n",
      "Training: batch 8 ends at 22:46:35.878018\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.3484 - bce_dice_loss: 0.3484\n",
      "Training: batch 9 begins at 22:46:35.882086\n",
      "\n",
      "Training: batch 9 ends at 22:46:36.678958\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3406 - bce_dice_loss: 0.3406\n",
      "Training: batch 10 begins at 22:46:36.683668\n",
      "\n",
      "Training: batch 10 ends at 22:46:37.479717\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3570 - bce_dice_loss: 0.3570\n",
      "Training: batch 11 begins at 22:46:37.483531\n",
      "\n",
      "Training: batch 11 ends at 22:46:38.272691\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3546 - bce_dice_loss: 0.3546\n",
      "Training: batch 12 begins at 22:46:38.275135\n",
      "\n",
      "Training: batch 12 ends at 22:46:39.071330\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3401 - bce_dice_loss: 0.3401\n",
      "Training: batch 13 begins at 22:46:39.073794\n",
      "\n",
      "Training: batch 13 ends at 22:46:39.870836\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.3343 - bce_dice_loss: 0.3343\n",
      "Training: batch 14 begins at 22:46:39.873972\n",
      "\n",
      "Training: batch 14 ends at 22:46:40.685035\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3364 - bce_dice_loss: 0.3364\n",
      "Training: batch 15 begins at 22:46:40.687788\n",
      "\n",
      "Training: batch 15 ends at 22:46:41.476503\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3576 - bce_dice_loss: 0.3576\n",
      "Training: batch 16 begins at 22:46:41.479720\n",
      "\n",
      "Training: batch 16 ends at 22:46:42.274278\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3584 - bce_dice_loss: 0.3584\n",
      "Training: batch 17 begins at 22:46:42.277021\n",
      "\n",
      "Training: batch 17 ends at 22:46:43.097728\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3519 - bce_dice_loss: 0.3519\n",
      "Training: batch 18 begins at 22:46:43.101562\n",
      "\n",
      "Training: batch 18 ends at 22:46:43.894920\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.3518 - bce_dice_loss: 0.3518\n",
      "Training: batch 19 begins at 22:46:43.898949\n",
      "\n",
      "Training: batch 19 ends at 22:46:44.698109\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3456 - bce_dice_loss: 0.3456\n",
      "Training: batch 20 begins at 22:46:44.701405\n",
      "\n",
      "Training: batch 20 ends at 22:46:45.504384\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3452 - bce_dice_loss: 0.3452\n",
      "Training: batch 21 begins at 22:46:45.508426\n",
      "\n",
      "Training: batch 21 ends at 22:46:46.307044\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3503 - bce_dice_loss: 0.3503\n",
      "Training: batch 22 begins at 22:46:46.311065\n",
      "\n",
      "Training: batch 22 ends at 22:46:47.103662\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3612 - bce_dice_loss: 0.3612\n",
      "Training: batch 23 begins at 22:46:47.106763\n",
      "\n",
      "Training: batch 23 ends at 22:46:47.917404\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.3581 - bce_dice_loss: 0.3581\n",
      "Training: batch 24 begins at 22:46:47.921438\n",
      "\n",
      "Training: batch 24 ends at 22:46:48.721344\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3642 - bce_dice_loss: 0.3642\n",
      "Training: batch 25 begins at 22:46:48.725361\n",
      "\n",
      "Training: batch 25 ends at 22:46:49.523405\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3682 - bce_dice_loss: 0.3682\n",
      "Training: batch 26 begins at 22:46:49.526405\n",
      "\n",
      "Training: batch 26 ends at 22:46:50.332385\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3617 - bce_dice_loss: 0.3617\n",
      "Training: batch 27 begins at 22:46:50.336842\n",
      "\n",
      "Training: batch 27 ends at 22:46:51.133539\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3599 - bce_dice_loss: 0.3599\n",
      "Training: batch 28 begins at 22:46:51.136054\n",
      "\n",
      "Training: batch 28 ends at 22:46:51.931000\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3561 - bce_dice_loss: 0.3561\n",
      "Training: batch 29 begins at 22:46:51.934069\n",
      "\n",
      "Training: batch 29 ends at 22:46:52.719558\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3519 - bce_dice_loss: 0.3519\n",
      "Training: batch 30 begins at 22:46:52.723511\n",
      "\n",
      "Training: batch 30 ends at 22:46:53.517296\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3466 - bce_dice_loss: 0.3466\n",
      "Training: batch 31 begins at 22:46:53.519862\n",
      "\n",
      "Training: batch 31 ends at 22:46:54.323655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3493 - bce_dice_loss: 0.3493\n",
      "Training: batch 32 begins at 22:46:54.327996\n",
      "\n",
      "Training: batch 32 ends at 22:46:55.129899\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3593 - bce_dice_loss: 0.3593\n",
      "Training: batch 33 begins at 22:46:55.135230\n",
      "\n",
      "Training: batch 33 ends at 22:46:55.928210\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3595 - bce_dice_loss: 0.3595\n",
      "Training: batch 34 begins at 22:46:55.933095\n",
      "\n",
      "Training: batch 34 ends at 22:46:56.730525\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3614 - bce_dice_loss: 0.3614\n",
      "Training: batch 35 begins at 22:46:56.737243\n",
      "\n",
      "Training: batch 35 ends at 22:46:57.527840\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3590 - bce_dice_loss: 0.3590\n",
      "Training: batch 36 begins at 22:46:57.532553\n",
      "\n",
      "Training: batch 36 ends at 22:46:58.353045\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3581 - bce_dice_loss: 0.3581\n",
      "Training: batch 37 begins at 22:46:58.357819\n",
      "\n",
      "Training: batch 37 ends at 22:46:59.163249\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3583 - bce_dice_loss: 0.3583\n",
      "Training: batch 38 begins at 22:46:59.167093\n",
      "\n",
      "Training: batch 38 ends at 22:46:59.969107\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3566 - bce_dice_loss: 0.3566\n",
      "Training: batch 39 begins at 22:46:59.972664\n",
      "\n",
      "Training: batch 39 ends at 22:47:00.762215\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3528 - bce_dice_loss: 0.3528\n",
      "Training: batch 40 begins at 22:47:00.766488\n",
      "\n",
      "Training: batch 40 ends at 22:47:01.584724\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3534 - bce_dice_loss: 0.3534\n",
      "Training: batch 41 begins at 22:47:01.588951\n",
      "\n",
      "Training: batch 41 ends at 22:47:02.391742\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3516 - bce_dice_loss: 0.3516\n",
      "Training: batch 42 begins at 22:47:02.395300\n",
      "\n",
      "Training: batch 42 ends at 22:47:03.188266\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3464 - bce_dice_loss: 0.3464\n",
      "Training: batch 43 begins at 22:47:03.190801\n",
      "\n",
      "Training: batch 43 ends at 22:47:03.989283\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3511 - bce_dice_loss: 0.3511\n",
      "Training: batch 44 begins at 22:47:03.992162\n",
      "\n",
      "Training: batch 44 ends at 22:47:04.787673\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3492 - bce_dice_loss: 0.3492\n",
      "Training: batch 45 begins at 22:47:04.790893\n",
      "\n",
      "Training: batch 45 ends at 22:47:05.597020\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3460 - bce_dice_loss: 0.3460\n",
      "Training: batch 46 begins at 22:47:05.599888\n",
      "\n",
      "Training: batch 46 ends at 22:47:06.410761\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3451 - bce_dice_loss: 0.3451\n",
      "Training: batch 47 begins at 22:47:06.415111\n",
      "\n",
      "Training: batch 47 ends at 22:47:07.211028\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3416 - bce_dice_loss: 0.3416 \n",
      "Training: batch 48 begins at 22:47:07.215957\n",
      "\n",
      "Training: batch 48 ends at 22:47:08.002410\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3401 - bce_dice_loss: 0.3401\n",
      "Training: batch 49 begins at 22:47:08.005369\n",
      "\n",
      "Training: batch 49 ends at 22:47:08.796710\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3380 - bce_dice_loss: 0.3380\n",
      "Training: batch 50 begins at 22:47:08.800247\n",
      "\n",
      "Training: batch 50 ends at 22:47:09.595986\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3357 - bce_dice_loss: 0.3357\n",
      "Training: batch 51 begins at 22:47:09.600595\n",
      "\n",
      "Training: batch 51 ends at 22:47:10.401347\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3371 - bce_dice_loss: 0.3371\n",
      "Training: batch 52 begins at 22:47:10.403844\n",
      "\n",
      "Training: batch 52 ends at 22:47:11.198014\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3324 - bce_dice_loss: 0.3324\n",
      "Training: batch 53 begins at 22:47:11.201085\n",
      "\n",
      "Training: batch 53 ends at 22:47:12.018634\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3346 - bce_dice_loss: 0.3346\n",
      "Training: batch 54 begins at 22:47:12.022917\n",
      "\n",
      "Training: batch 54 ends at 22:47:12.816321\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3331 - bce_dice_loss: 0.3331\n",
      "Training: batch 55 begins at 22:47:12.821173\n",
      "\n",
      "Training: batch 55 ends at 22:47:13.640629\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3320 - bce_dice_loss: 0.3320\n",
      "Training: batch 56 begins at 22:47:13.645022\n",
      "\n",
      "Training: batch 56 ends at 22:47:14.439439\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3301 - bce_dice_loss: 0.3301\n",
      "Training: batch 57 begins at 22:47:14.443760\n",
      "\n",
      "Training: batch 57 ends at 22:47:15.242753\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3308 - bce_dice_loss: 0.3308\n",
      "Training: batch 58 begins at 22:47:15.245336\n",
      "\n",
      "Training: batch 58 ends at 22:47:16.041584\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3271 - bce_dice_loss: 0.3271\n",
      "Training: batch 59 begins at 22:47:16.044857\n",
      "\n",
      "Training: batch 59 ends at 22:47:16.871801\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3254 - bce_dice_loss: 0.3254\n",
      "Evaluating: batch 0 begins at 22:47:16.901740\n",
      "\n",
      "Evaluating: batch 0 ends at 22:47:17.173326\n",
      "\n",
      "Evaluating: batch 1 begins at 22:47:17.174686\n",
      "\n",
      "Evaluating: batch 1 ends at 22:47:17.389958\n",
      "\n",
      "Evaluating: batch 2 begins at 22:47:17.391591\n",
      "\n",
      "Evaluating: batch 2 ends at 22:47:17.608440\n",
      "\n",
      "Evaluating: batch 3 begins at 22:47:17.610093\n",
      "\n",
      "Evaluating: batch 3 ends at 22:47:17.831415\n",
      "\n",
      "Evaluating: batch 4 begins at 22:47:17.833586\n",
      "\n",
      "Evaluating: batch 4 ends at 22:47:18.052713\n",
      "\n",
      "Evaluating: batch 5 begins at 22:47:18.055122\n",
      "\n",
      "Evaluating: batch 5 ends at 22:47:18.273703\n",
      "\n",
      "Evaluating: batch 6 begins at 22:47:18.275081\n",
      "\n",
      "Evaluating: batch 6 ends at 22:47:18.496486\n",
      "\n",
      "Evaluating: batch 7 begins at 22:47:18.497899\n",
      "\n",
      "Evaluating: batch 7 ends at 22:47:18.718487\n",
      "\n",
      "Evaluating: batch 8 begins at 22:47:18.719804\n",
      "\n",
      "Evaluating: batch 8 ends at 22:47:18.941496\n",
      "\n",
      "Evaluating: batch 9 begins at 22:47:18.943714\n",
      "\n",
      "Evaluating: batch 9 ends at 22:47:19.167953\n",
      "\n",
      "Evaluating: batch 10 begins at 22:47:19.169425\n",
      "\n",
      "Evaluating: batch 10 ends at 22:47:19.390473\n",
      "\n",
      "Evaluating: batch 11 begins at 22:47:19.392185\n",
      "\n",
      "Evaluating: batch 11 ends at 22:47:19.614346\n",
      "\n",
      "Evaluating: batch 12 begins at 22:47:19.615913\n",
      "\n",
      "Evaluating: batch 12 ends at 22:47:19.836907\n",
      "\n",
      "Evaluating: batch 13 begins at 22:47:19.838142\n",
      "\n",
      "Evaluating: batch 13 ends at 22:47:20.055050\n",
      "\n",
      "Evaluating: batch 14 begins at 22:47:20.057403\n",
      "\n",
      "Evaluating: batch 14 ends at 22:47:20.283570\n",
      "\n",
      "Evaluating: batch 15 begins at 22:47:20.284821\n",
      "\n",
      "Evaluating: batch 15 ends at 22:47:20.503313\n",
      "\n",
      "Evaluating: batch 16 begins at 22:47:20.504574\n",
      "\n",
      "Evaluating: batch 16 ends at 22:47:20.721978\n",
      "\n",
      "Evaluating: batch 17 begins at 22:47:20.723334\n",
      "\n",
      "Evaluating: batch 17 ends at 22:47:20.948151\n",
      "\n",
      "Evaluating: batch 18 begins at 22:47:20.952492\n",
      "\n",
      "Evaluating: batch 18 ends at 22:47:21.173883\n",
      "\n",
      "Evaluating: batch 19 begins at 22:47:21.174890\n",
      "\n",
      "Evaluating: batch 19 ends at 22:47:21.391671\n",
      "\n",
      "Evaluating: batch 20 begins at 22:47:21.393130\n",
      "\n",
      "Evaluating: batch 20 ends at 22:47:21.613888\n",
      "\n",
      "Evaluating: batch 21 begins at 22:47:21.616296\n",
      "\n",
      "Evaluating: batch 21 ends at 22:47:21.838930\n",
      "\n",
      "Evaluating: batch 22 begins at 22:47:21.840232\n",
      "\n",
      "Evaluating: batch 22 ends at 22:47:22.055338\n",
      "\n",
      "Evaluating: batch 23 begins at 22:47:22.056911\n",
      "\n",
      "Evaluating: batch 23 ends at 22:47:22.275875\n",
      "\n",
      "Evaluating: batch 24 begins at 22:47:22.278301\n",
      "\n",
      "Evaluating: batch 24 ends at 22:47:22.495277\n",
      "\n",
      "Evaluating: batch 25 begins at 22:47:22.497584\n",
      "\n",
      "Evaluating: batch 25 ends at 22:47:22.716982\n",
      "\n",
      "Evaluating: batch 26 begins at 22:47:22.718625\n",
      "\n",
      "Evaluating: batch 26 ends at 22:47:22.938572\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.32433\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.3254 - bce_dice_loss: 0.3254 - val_loss: 0.3266 - val_bce_dice_loss: 0.3266\n",
      "Epoch 8/25\n",
      "\n",
      "Training: batch 0 begins at 22:47:22.969563\n",
      "\n",
      "Training: batch 0 ends at 22:47:23.768359\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2169 - bce_dice_loss: 0.2169\n",
      "Training: batch 1 begins at 22:47:23.773721\n",
      "\n",
      "Training: batch 1 ends at 22:47:24.574468\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.2945 - bce_dice_loss: 0.2945\n",
      "Training: batch 2 begins at 22:47:24.578890\n",
      "\n",
      "Training: batch 2 ends at 22:47:25.384779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2622 - bce_dice_loss: 0.2622\n",
      "Training: batch 3 begins at 22:47:25.387367\n",
      "\n",
      "Training: batch 3 ends at 22:47:26.189269\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2505 - bce_dice_loss: 0.2505\n",
      "Training: batch 4 begins at 22:47:26.191765\n",
      "\n",
      "Training: batch 4 ends at 22:47:26.990084\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2879 - bce_dice_loss: 0.2879\n",
      "Training: batch 5 begins at 22:47:26.993114\n",
      "\n",
      "Training: batch 5 ends at 22:47:27.790586\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3249 - bce_dice_loss: 0.3249\n",
      "Training: batch 6 begins at 22:47:27.794396\n",
      "\n",
      "Training: batch 6 ends at 22:47:28.592126\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3303 - bce_dice_loss: 0.3303\n",
      "Training: batch 7 begins at 22:47:28.596261\n",
      "\n",
      "Training: batch 7 ends at 22:47:29.391162\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3121 - bce_dice_loss: 0.3121\n",
      "Training: batch 8 begins at 22:47:29.394770\n",
      "\n",
      "Training: batch 8 ends at 22:47:30.190748\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2943 - bce_dice_loss: 0.2943\n",
      "Training: batch 9 begins at 22:47:30.194998\n",
      "\n",
      "Training: batch 9 ends at 22:47:30.993086\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3041 - bce_dice_loss: 0.3041\n",
      "Training: batch 10 begins at 22:47:30.996211\n",
      "\n",
      "Training: batch 10 ends at 22:47:31.797932\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3185 - bce_dice_loss: 0.3185\n",
      "Training: batch 11 begins at 22:47:31.801295\n",
      "\n",
      "Training: batch 11 ends at 22:47:32.597010\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3174 - bce_dice_loss: 0.3174\n",
      "Training: batch 12 begins at 22:47:32.601268\n",
      "\n",
      "Training: batch 12 ends at 22:47:33.413334\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3294 - bce_dice_loss: 0.3294\n",
      "Training: batch 13 begins at 22:47:33.418036\n",
      "\n",
      "Training: batch 13 ends at 22:47:34.235257\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3302 - bce_dice_loss: 0.3302\n",
      "Training: batch 14 begins at 22:47:34.239595\n",
      "\n",
      "Training: batch 14 ends at 22:47:35.031954\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3496 - bce_dice_loss: 0.3496\n",
      "Training: batch 15 begins at 22:47:35.034773\n",
      "\n",
      "Training: batch 15 ends at 22:47:35.845455\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3521 - bce_dice_loss: 0.3521\n",
      "Training: batch 16 begins at 22:47:35.849885\n",
      "\n",
      "Training: batch 16 ends at 22:47:36.674433\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3450 - bce_dice_loss: 0.3450\n",
      "Training: batch 17 begins at 22:47:36.678649\n",
      "\n",
      "Training: batch 17 ends at 22:47:37.477158\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3391 - bce_dice_loss: 0.3391\n",
      "Training: batch 18 begins at 22:47:37.481899\n",
      "\n",
      "Training: batch 18 ends at 22:47:38.282004\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3408 - bce_dice_loss: 0.3408\n",
      "Training: batch 19 begins at 22:47:38.285296\n",
      "\n",
      "Training: batch 19 ends at 22:47:39.119093\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3517 - bce_dice_loss: 0.3517\n",
      "Training: batch 20 begins at 22:47:39.123476\n",
      "\n",
      "Training: batch 20 ends at 22:47:39.926202\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3495 - bce_dice_loss: 0.3495\n",
      "Training: batch 21 begins at 22:47:39.930637\n",
      "\n",
      "Training: batch 21 ends at 22:47:40.725609\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3460 - bce_dice_loss: 0.3460\n",
      "Training: batch 22 begins at 22:47:40.729787\n",
      "\n",
      "Training: batch 22 ends at 22:47:41.533799\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3514 - bce_dice_loss: 0.3514\n",
      "Training: batch 23 begins at 22:47:41.538233\n",
      "\n",
      "Training: batch 23 ends at 22:47:42.336194\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3522 - bce_dice_loss: 0.3522\n",
      "Training: batch 24 begins at 22:47:42.340563\n",
      "\n",
      "Training: batch 24 ends at 22:47:43.132412\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3479 - bce_dice_loss: 0.3479\n",
      "Training: batch 25 begins at 22:47:43.136666\n",
      "\n",
      "Training: batch 25 ends at 22:47:43.927688\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3455 - bce_dice_loss: 0.3455\n",
      "Training: batch 26 begins at 22:47:43.932294\n",
      "\n",
      "Training: batch 26 ends at 22:47:44.722346\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3369 - bce_dice_loss: 0.3369\n",
      "Training: batch 27 begins at 22:47:44.726513\n",
      "\n",
      "Training: batch 27 ends at 22:47:45.543268\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3356 - bce_dice_loss: 0.3356\n",
      "Training: batch 28 begins at 22:47:45.547646\n",
      "\n",
      "Training: batch 28 ends at 22:47:46.347337\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3480 - bce_dice_loss: 0.3480\n",
      "Training: batch 29 begins at 22:47:46.351798\n",
      "\n",
      "Training: batch 29 ends at 22:47:47.144212\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3415 - bce_dice_loss: 0.3415\n",
      "Training: batch 30 begins at 22:47:47.149753\n",
      "\n",
      "Training: batch 30 ends at 22:47:47.983544\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3393 - bce_dice_loss: 0.3393\n",
      "Training: batch 31 begins at 22:47:47.986225\n",
      "\n",
      "Training: batch 31 ends at 22:47:48.874431\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3366 - bce_dice_loss: 0.3366\n",
      "Training: batch 32 begins at 22:47:48.879109\n",
      "\n",
      "Training: batch 32 ends at 22:47:49.760234\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3412 - bce_dice_loss: 0.3412\n",
      "Training: batch 33 begins at 22:47:49.764275\n",
      "\n",
      "Training: batch 33 ends at 22:47:50.647979\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.3399 - bce_dice_loss: 0.3399\n",
      "Training: batch 34 begins at 22:47:50.651110\n",
      "\n",
      "Training: batch 34 ends at 22:47:51.502991\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3353 - bce_dice_loss: 0.3353\n",
      "Training: batch 35 begins at 22:47:51.505596\n",
      "\n",
      "Training: batch 35 ends at 22:47:52.325381\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3338 - bce_dice_loss: 0.3338\n",
      "Training: batch 36 begins at 22:47:52.329455\n",
      "\n",
      "Training: batch 36 ends at 22:47:53.127023\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3311 - bce_dice_loss: 0.3311\n",
      "Training: batch 37 begins at 22:47:53.130480\n",
      "\n",
      "Training: batch 37 ends at 22:47:53.922542\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3274 - bce_dice_loss: 0.3274\n",
      "Training: batch 38 begins at 22:47:53.926454\n",
      "\n",
      "Training: batch 38 ends at 22:47:54.729436\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.3299 - bce_dice_loss: 0.3299\n",
      "Training: batch 39 begins at 22:47:54.731052\n",
      "\n",
      "Training: batch 39 ends at 22:47:55.526040\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3319 - bce_dice_loss: 0.3319\n",
      "Training: batch 40 begins at 22:47:55.530402\n",
      "\n",
      "Training: batch 40 ends at 22:47:56.373082\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3293 - bce_dice_loss: 0.3293\n",
      "Training: batch 41 begins at 22:47:56.377237\n",
      "\n",
      "Training: batch 41 ends at 22:47:57.172468\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3284 - bce_dice_loss: 0.3284\n",
      "Training: batch 42 begins at 22:47:57.175855\n",
      "\n",
      "Training: batch 42 ends at 22:47:57.983517\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3320 - bce_dice_loss: 0.3320\n",
      "Training: batch 43 begins at 22:47:57.986170\n",
      "\n",
      "Training: batch 43 ends at 22:47:58.808406\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.3295 - bce_dice_loss: 0.3295\n",
      "Training: batch 44 begins at 22:47:58.812777\n",
      "\n",
      "Training: batch 44 ends at 22:47:59.612328\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3266 - bce_dice_loss: 0.3266\n",
      "Training: batch 45 begins at 22:47:59.617035\n",
      "\n",
      "Training: batch 45 ends at 22:48:00.408253\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3291 - bce_dice_loss: 0.3291\n",
      "Training: batch 46 begins at 22:48:00.412401\n",
      "\n",
      "Training: batch 46 ends at 22:48:01.219847\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3292 - bce_dice_loss: 0.3292\n",
      "Training: batch 47 begins at 22:48:01.222518\n",
      "\n",
      "Training: batch 47 ends at 22:48:02.016508\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3269 - bce_dice_loss: 0.3269 \n",
      "Training: batch 48 begins at 22:48:02.019872\n",
      "\n",
      "Training: batch 48 ends at 22:48:02.828062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3252 - bce_dice_loss: 0.3252\n",
      "Training: batch 49 begins at 22:48:02.830671\n",
      "\n",
      "Training: batch 49 ends at 22:48:03.630697\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3229 - bce_dice_loss: 0.3229\n",
      "Training: batch 50 begins at 22:48:03.633908\n",
      "\n",
      "Training: batch 50 ends at 22:48:04.446939\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3207 - bce_dice_loss: 0.3207\n",
      "Training: batch 51 begins at 22:48:04.452018\n",
      "\n",
      "Training: batch 51 ends at 22:48:05.251622\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3206 - bce_dice_loss: 0.3206\n",
      "Training: batch 52 begins at 22:48:05.254978\n",
      "\n",
      "Training: batch 52 ends at 22:48:06.053932\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3253 - bce_dice_loss: 0.3253\n",
      "Training: batch 53 begins at 22:48:06.057619\n",
      "\n",
      "Training: batch 53 ends at 22:48:06.852098\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3289 - bce_dice_loss: 0.3289\n",
      "Training: batch 54 begins at 22:48:06.855349\n",
      "\n",
      "Training: batch 54 ends at 22:48:07.647840\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3284 - bce_dice_loss: 0.3284\n",
      "Training: batch 55 begins at 22:48:07.652164\n",
      "\n",
      "Training: batch 55 ends at 22:48:08.448371\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3249 - bce_dice_loss: 0.3249\n",
      "Training: batch 56 begins at 22:48:08.453652\n",
      "\n",
      "Training: batch 56 ends at 22:48:09.269554\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3228 - bce_dice_loss: 0.3228\n",
      "Training: batch 57 begins at 22:48:09.272943\n",
      "\n",
      "Training: batch 57 ends at 22:48:10.067567\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3219 - bce_dice_loss: 0.3219\n",
      "Training: batch 58 begins at 22:48:10.071983\n",
      "\n",
      "Training: batch 58 ends at 22:48:10.870093\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3235 - bce_dice_loss: 0.3235\n",
      "Training: batch 59 begins at 22:48:10.872661\n",
      "\n",
      "Training: batch 59 ends at 22:48:11.665270\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3222 - bce_dice_loss: 0.3222\n",
      "Evaluating: batch 0 begins at 22:48:11.699387\n",
      "\n",
      "Evaluating: batch 0 ends at 22:48:11.969996\n",
      "\n",
      "Evaluating: batch 1 begins at 22:48:11.971504\n",
      "\n",
      "Evaluating: batch 1 ends at 22:48:12.186291\n",
      "\n",
      "Evaluating: batch 2 begins at 22:48:12.187567\n",
      "\n",
      "Evaluating: batch 2 ends at 22:48:12.408412\n",
      "\n",
      "Evaluating: batch 3 begins at 22:48:12.410641\n",
      "\n",
      "Evaluating: batch 3 ends at 22:48:12.635866\n",
      "\n",
      "Evaluating: batch 4 begins at 22:48:12.637049\n",
      "\n",
      "Evaluating: batch 4 ends at 22:48:12.860162\n",
      "\n",
      "Evaluating: batch 5 begins at 22:48:12.862357\n",
      "\n",
      "Evaluating: batch 5 ends at 22:48:13.081222\n",
      "\n",
      "Evaluating: batch 6 begins at 22:48:13.084613\n",
      "\n",
      "Evaluating: batch 6 ends at 22:48:13.303794\n",
      "\n",
      "Evaluating: batch 7 begins at 22:48:13.305477\n",
      "\n",
      "Evaluating: batch 7 ends at 22:48:13.529170\n",
      "\n",
      "Evaluating: batch 8 begins at 22:48:13.530495\n",
      "\n",
      "Evaluating: batch 8 ends at 22:48:13.752220\n",
      "\n",
      "Evaluating: batch 9 begins at 22:48:13.754487\n",
      "\n",
      "Evaluating: batch 9 ends at 22:48:13.973381\n",
      "\n",
      "Evaluating: batch 10 begins at 22:48:13.976706\n",
      "\n",
      "Evaluating: batch 10 ends at 22:48:14.199718\n",
      "\n",
      "Evaluating: batch 11 begins at 22:48:14.201046\n",
      "\n",
      "Evaluating: batch 11 ends at 22:48:14.420278\n",
      "\n",
      "Evaluating: batch 12 begins at 22:48:14.422839\n",
      "\n",
      "Evaluating: batch 12 ends at 22:48:14.642925\n",
      "\n",
      "Evaluating: batch 13 begins at 22:48:14.644591\n",
      "\n",
      "Evaluating: batch 13 ends at 22:48:14.867764\n",
      "\n",
      "Evaluating: batch 14 begins at 22:48:14.869502\n",
      "\n",
      "Evaluating: batch 14 ends at 22:48:15.086958\n",
      "\n",
      "Evaluating: batch 15 begins at 22:48:15.089325\n",
      "\n",
      "Evaluating: batch 15 ends at 22:48:15.320306\n",
      "\n",
      "Evaluating: batch 16 begins at 22:48:15.322077\n",
      "\n",
      "Evaluating: batch 16 ends at 22:48:15.546333\n",
      "\n",
      "Evaluating: batch 17 begins at 22:48:15.547881\n",
      "\n",
      "Evaluating: batch 17 ends at 22:48:15.772008\n",
      "\n",
      "Evaluating: batch 18 begins at 22:48:15.773506\n",
      "\n",
      "Evaluating: batch 18 ends at 22:48:15.997055\n",
      "\n",
      "Evaluating: batch 19 begins at 22:48:15.999121\n",
      "\n",
      "Evaluating: batch 19 ends at 22:48:16.223042\n",
      "\n",
      "Evaluating: batch 20 begins at 22:48:16.224793\n",
      "\n",
      "Evaluating: batch 20 ends at 22:48:16.444568\n",
      "\n",
      "Evaluating: batch 21 begins at 22:48:16.447092\n",
      "\n",
      "Evaluating: batch 21 ends at 22:48:16.664471\n",
      "\n",
      "Evaluating: batch 22 begins at 22:48:16.667008\n",
      "\n",
      "Evaluating: batch 22 ends at 22:48:16.887516\n",
      "\n",
      "Evaluating: batch 23 begins at 22:48:16.889857\n",
      "\n",
      "Evaluating: batch 23 ends at 22:48:17.106638\n",
      "\n",
      "Evaluating: batch 24 begins at 22:48:17.108489\n",
      "\n",
      "Evaluating: batch 24 ends at 22:48:17.326538\n",
      "\n",
      "Evaluating: batch 25 begins at 22:48:17.328906\n",
      "\n",
      "Evaluating: batch 25 ends at 22:48:17.545309\n",
      "\n",
      "Evaluating: batch 26 begins at 22:48:17.546510\n",
      "\n",
      "Evaluating: batch 26 ends at 22:48:17.769411\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32433\n",
      "60/60 [==============================] - 55s 916ms/step - loss: 0.3222 - bce_dice_loss: 0.3222 - val_loss: 0.3312 - val_bce_dice_loss: 0.3312\n",
      "Epoch 9/25\n",
      "\n",
      "Training: batch 0 begins at 22:48:17.799205\n",
      "\n",
      "Training: batch 0 ends at 22:48:18.593594\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.1542 - bce_dice_loss: 0.1542\n",
      "Training: batch 1 begins at 22:48:18.598565\n",
      "\n",
      "Training: batch 1 ends at 22:48:19.397872\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.2044 - bce_dice_loss: 0.2044\n",
      "Training: batch 2 begins at 22:48:19.402220\n",
      "\n",
      "Training: batch 2 ends at 22:48:20.200374\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2282 - bce_dice_loss: 0.2282\n",
      "Training: batch 3 begins at 22:48:20.203494\n",
      "\n",
      "Training: batch 3 ends at 22:48:21.020902\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2781 - bce_dice_loss: 0.2781\n",
      "Training: batch 4 begins at 22:48:21.024805\n",
      "\n",
      "Training: batch 4 ends at 22:48:21.822102\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3083 - bce_dice_loss: 0.3083\n",
      "Training: batch 5 begins at 22:48:21.826457\n",
      "\n",
      "Training: batch 5 ends at 22:48:22.624164\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2955 - bce_dice_loss: 0.2955\n",
      "Training: batch 6 begins at 22:48:22.626836\n",
      "\n",
      "Training: batch 6 ends at 22:48:23.441505\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3381 - bce_dice_loss: 0.3381\n",
      "Training: batch 7 begins at 22:48:23.444143\n",
      "\n",
      "Training: batch 7 ends at 22:48:24.259424\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3407 - bce_dice_loss: 0.3407\n",
      "Training: batch 8 begins at 22:48:24.263485\n",
      "\n",
      "Training: batch 8 ends at 22:48:25.060026\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3359 - bce_dice_loss: 0.3359\n",
      "Training: batch 9 begins at 22:48:25.064628\n",
      "\n",
      "Training: batch 9 ends at 22:48:25.879061\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3240 - bce_dice_loss: 0.3240\n",
      "Training: batch 10 begins at 22:48:25.883274\n",
      "\n",
      "Training: batch 10 ends at 22:48:26.689530\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3078 - bce_dice_loss: 0.3078\n",
      "Training: batch 11 begins at 22:48:26.693558\n",
      "\n",
      "Training: batch 11 ends at 22:48:27.487208\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3079 - bce_dice_loss: 0.3079\n",
      "Training: batch 12 begins at 22:48:27.491678\n",
      "\n",
      "Training: batch 12 ends at 22:48:28.290629\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3143 - bce_dice_loss: 0.3143\n",
      "Training: batch 13 begins at 22:48:28.294706\n",
      "\n",
      "Training: batch 13 ends at 22:48:29.092951\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3117 - bce_dice_loss: 0.3117\n",
      "Training: batch 14 begins at 22:48:29.096563\n",
      "\n",
      "Training: batch 14 ends at 22:48:29.906803\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3170 - bce_dice_loss: 0.3170\n",
      "Training: batch 15 begins at 22:48:29.909481\n",
      "\n",
      "Training: batch 15 ends at 22:48:30.704585\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3136 - bce_dice_loss: 0.3136\n",
      "Training: batch 16 begins at 22:48:30.708953\n",
      "\n",
      "Training: batch 16 ends at 22:48:31.505897\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3175 - bce_dice_loss: 0.3175\n",
      "Training: batch 17 begins at 22:48:31.509975\n",
      "\n",
      "Training: batch 17 ends at 22:48:32.307812\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3148 - bce_dice_loss: 0.3148\n",
      "Training: batch 18 begins at 22:48:32.312027\n",
      "\n",
      "Training: batch 18 ends at 22:48:33.105042\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3214 - bce_dice_loss: 0.3214\n",
      "Training: batch 19 begins at 22:48:33.108741\n",
      "\n",
      "Training: batch 19 ends at 22:48:33.930380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3141 - bce_dice_loss: 0.3141\n",
      "Training: batch 20 begins at 22:48:33.937219\n",
      "\n",
      "Training: batch 20 ends at 22:48:34.729154\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3136 - bce_dice_loss: 0.3136\n",
      "Training: batch 21 begins at 22:48:34.734163\n",
      "\n",
      "Training: batch 21 ends at 22:48:35.552891\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3081 - bce_dice_loss: 0.3081\n",
      "Training: batch 22 begins at 22:48:35.557179\n",
      "\n",
      "Training: batch 22 ends at 22:48:36.358178\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3055 - bce_dice_loss: 0.3055\n",
      "Training: batch 23 begins at 22:48:36.361595\n",
      "\n",
      "Training: batch 23 ends at 22:48:37.155624\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3056 - bce_dice_loss: 0.3056\n",
      "Training: batch 24 begins at 22:48:37.162046\n",
      "\n",
      "Training: batch 24 ends at 22:48:37.960296\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.3070 - bce_dice_loss: 0.3070\n",
      "Training: batch 25 begins at 22:48:37.964156\n",
      "\n",
      "Training: batch 25 ends at 22:48:38.771385\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3102 - bce_dice_loss: 0.3102\n",
      "Training: batch 26 begins at 22:48:38.776244\n",
      "\n",
      "Training: batch 26 ends at 22:48:39.565094\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3124 - bce_dice_loss: 0.3124\n",
      "Training: batch 27 begins at 22:48:39.570537\n",
      "\n",
      "Training: batch 27 ends at 22:48:40.367853\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3117 - bce_dice_loss: 0.3117\n",
      "Training: batch 28 begins at 22:48:40.370899\n",
      "\n",
      "Training: batch 28 ends at 22:48:41.168624\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.3097 - bce_dice_loss: 0.3097\n",
      "Training: batch 29 begins at 22:48:41.173626\n",
      "\n",
      "Training: batch 29 ends at 22:48:41.986196\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.3099 - bce_dice_loss: 0.3099\n",
      "Training: batch 30 begins at 22:48:41.991287\n",
      "\n",
      "Training: batch 30 ends at 22:48:42.787319\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.3119 - bce_dice_loss: 0.3119\n",
      "Training: batch 31 begins at 22:48:42.791804\n",
      "\n",
      "Training: batch 31 ends at 22:48:43.589012\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.3063 - bce_dice_loss: 0.3063\n",
      "Training: batch 32 begins at 22:48:43.593348\n",
      "\n",
      "Training: batch 32 ends at 22:48:44.415103\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.3047 - bce_dice_loss: 0.3047\n",
      "Training: batch 33 begins at 22:48:44.419455\n",
      "\n",
      "Training: batch 33 ends at 22:48:45.213383\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3039 - bce_dice_loss: 0.3039\n",
      "Training: batch 34 begins at 22:48:45.218069\n",
      "\n",
      "Training: batch 34 ends at 22:48:46.034193\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.3023 - bce_dice_loss: 0.3023\n",
      "Training: batch 35 begins at 22:48:46.038317\n",
      "\n",
      "Training: batch 35 ends at 22:48:46.838307\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.3124 - bce_dice_loss: 0.3124\n",
      "Training: batch 36 begins at 22:48:46.842466\n",
      "\n",
      "Training: batch 36 ends at 22:48:47.667487\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.3084 - bce_dice_loss: 0.3084\n",
      "Training: batch 37 begins at 22:48:47.671942\n",
      "\n",
      "Training: batch 37 ends at 22:48:48.469466\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.3129 - bce_dice_loss: 0.3129\n",
      "Training: batch 38 begins at 22:48:48.473018\n",
      "\n",
      "Training: batch 38 ends at 22:48:49.269138\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.3094 - bce_dice_loss: 0.3094\n",
      "Training: batch 39 begins at 22:48:49.271909\n",
      "\n",
      "Training: batch 39 ends at 22:48:50.069839\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.3073 - bce_dice_loss: 0.3073\n",
      "Training: batch 40 begins at 22:48:50.073790\n",
      "\n",
      "Training: batch 40 ends at 22:48:50.871574\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.3083 - bce_dice_loss: 0.3083\n",
      "Training: batch 41 begins at 22:48:50.874676\n",
      "\n",
      "Training: batch 41 ends at 22:48:51.668704\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.3087 - bce_dice_loss: 0.3087\n",
      "Training: batch 42 begins at 22:48:51.672897\n",
      "\n",
      "Training: batch 42 ends at 22:48:52.466113\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.3098 - bce_dice_loss: 0.3098\n",
      "Training: batch 43 begins at 22:48:52.470157\n",
      "\n",
      "Training: batch 43 ends at 22:48:53.268465\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.3094 - bce_dice_loss: 0.3094\n",
      "Training: batch 44 begins at 22:48:53.272155\n",
      "\n",
      "Training: batch 44 ends at 22:48:54.068562\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.3066 - bce_dice_loss: 0.3066\n",
      "Training: batch 45 begins at 22:48:54.071937\n",
      "\n",
      "Training: batch 45 ends at 22:48:54.888642\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.3066 - bce_dice_loss: 0.3066\n",
      "Training: batch 46 begins at 22:48:54.893026\n",
      "\n",
      "Training: batch 46 ends at 22:48:55.694136\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.3063 - bce_dice_loss: 0.3063\n",
      "Training: batch 47 begins at 22:48:55.699666\n",
      "\n",
      "Training: batch 47 ends at 22:48:56.503884\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.3048 - bce_dice_loss: 0.3048 \n",
      "Training: batch 48 begins at 22:48:56.506847\n",
      "\n",
      "Training: batch 48 ends at 22:48:57.306073\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3017 - bce_dice_loss: 0.3017\n",
      "Training: batch 49 begins at 22:48:57.310063\n",
      "\n",
      "Training: batch 49 ends at 22:48:58.103504\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3045 - bce_dice_loss: 0.3045\n",
      "Training: batch 50 begins at 22:48:58.108265\n",
      "\n",
      "Training: batch 50 ends at 22:48:58.904939\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3074 - bce_dice_loss: 0.3074\n",
      "Training: batch 51 begins at 22:48:58.909347\n",
      "\n",
      "Training: batch 51 ends at 22:48:59.710602\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.3054 - bce_dice_loss: 0.3054\n",
      "Training: batch 52 begins at 22:48:59.714710\n",
      "\n",
      "Training: batch 52 ends at 22:49:00.550119\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.3030 - bce_dice_loss: 0.3030\n",
      "Training: batch 53 begins at 22:49:00.557006\n",
      "\n",
      "Training: batch 53 ends at 22:49:01.351375\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.3013 - bce_dice_loss: 0.3013\n",
      "Training: batch 54 begins at 22:49:01.354320\n",
      "\n",
      "Training: batch 54 ends at 22:49:02.165947\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.3023 - bce_dice_loss: 0.3023\n",
      "Training: batch 55 begins at 22:49:02.170112\n",
      "\n",
      "Training: batch 55 ends at 22:49:02.969640\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.3022 - bce_dice_loss: 0.3022\n",
      "Training: batch 56 begins at 22:49:02.973318\n",
      "\n",
      "Training: batch 56 ends at 22:49:03.757381\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.3051 - bce_dice_loss: 0.3051\n",
      "Training: batch 57 begins at 22:49:03.761282\n",
      "\n",
      "Training: batch 57 ends at 22:49:04.554881\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3062 - bce_dice_loss: 0.3062\n",
      "Training: batch 58 begins at 22:49:04.557763\n",
      "\n",
      "Training: batch 58 ends at 22:49:05.374401\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3049 - bce_dice_loss: 0.3049\n",
      "Training: batch 59 begins at 22:49:05.378663\n",
      "\n",
      "Training: batch 59 ends at 22:49:06.173503\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3058 - bce_dice_loss: 0.3058\n",
      "Evaluating: batch 0 begins at 22:49:06.202341\n",
      "\n",
      "Evaluating: batch 0 ends at 22:49:06.486589\n",
      "\n",
      "Evaluating: batch 1 begins at 22:49:06.488545\n",
      "\n",
      "Evaluating: batch 1 ends at 22:49:06.708643\n",
      "\n",
      "Evaluating: batch 2 begins at 22:49:06.710306\n",
      "\n",
      "Evaluating: batch 2 ends at 22:49:06.933821\n",
      "\n",
      "Evaluating: batch 3 begins at 22:49:06.935771\n",
      "\n",
      "Evaluating: batch 3 ends at 22:49:07.154532\n",
      "\n",
      "Evaluating: batch 4 begins at 22:49:07.156939\n",
      "\n",
      "Evaluating: batch 4 ends at 22:49:07.378423\n",
      "\n",
      "Evaluating: batch 5 begins at 22:49:07.380923\n",
      "\n",
      "Evaluating: batch 5 ends at 22:49:07.600881\n",
      "\n",
      "Evaluating: batch 6 begins at 22:49:07.602916\n",
      "\n",
      "Evaluating: batch 6 ends at 22:49:07.827022\n",
      "\n",
      "Evaluating: batch 7 begins at 22:49:07.828913\n",
      "\n",
      "Evaluating: batch 7 ends at 22:49:08.048632\n",
      "\n",
      "Evaluating: batch 8 begins at 22:49:08.055436\n",
      "\n",
      "Evaluating: batch 8 ends at 22:49:08.276400\n",
      "\n",
      "Evaluating: batch 9 begins at 22:49:08.278231\n",
      "\n",
      "Evaluating: batch 9 ends at 22:49:08.500148\n",
      "\n",
      "Evaluating: batch 10 begins at 22:49:08.502585\n",
      "\n",
      "Evaluating: batch 10 ends at 22:49:08.717924\n",
      "\n",
      "Evaluating: batch 11 begins at 22:49:08.719435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 11 ends at 22:49:08.941342\n",
      "\n",
      "Evaluating: batch 12 begins at 22:49:08.943583\n",
      "\n",
      "Evaluating: batch 12 ends at 22:49:09.158676\n",
      "\n",
      "Evaluating: batch 13 begins at 22:49:09.159993\n",
      "\n",
      "Evaluating: batch 13 ends at 22:49:09.383184\n",
      "\n",
      "Evaluating: batch 14 begins at 22:49:09.384862\n",
      "\n",
      "Evaluating: batch 14 ends at 22:49:09.605504\n",
      "\n",
      "Evaluating: batch 15 begins at 22:49:09.606821\n",
      "\n",
      "Evaluating: batch 15 ends at 22:49:09.841599\n",
      "\n",
      "Evaluating: batch 16 begins at 22:49:09.844049\n",
      "\n",
      "Evaluating: batch 16 ends at 22:49:10.067702\n",
      "\n",
      "Evaluating: batch 17 begins at 22:49:10.068940\n",
      "\n",
      "Evaluating: batch 17 ends at 22:49:10.289709\n",
      "\n",
      "Evaluating: batch 18 begins at 22:49:10.292246\n",
      "\n",
      "Evaluating: batch 18 ends at 22:49:10.512914\n",
      "\n",
      "Evaluating: batch 19 begins at 22:49:10.514271\n",
      "\n",
      "Evaluating: batch 19 ends at 22:49:10.731498\n",
      "\n",
      "Evaluating: batch 20 begins at 22:49:10.737445\n",
      "\n",
      "Evaluating: batch 20 ends at 22:49:10.956720\n",
      "\n",
      "Evaluating: batch 21 begins at 22:49:10.958582\n",
      "\n",
      "Evaluating: batch 21 ends at 22:49:11.176759\n",
      "\n",
      "Evaluating: batch 22 begins at 22:49:11.178182\n",
      "\n",
      "Evaluating: batch 22 ends at 22:49:11.397575\n",
      "\n",
      "Evaluating: batch 23 begins at 22:49:11.402358\n",
      "\n",
      "Evaluating: batch 23 ends at 22:49:11.618613\n",
      "\n",
      "Evaluating: batch 24 begins at 22:49:11.620522\n",
      "\n",
      "Evaluating: batch 24 ends at 22:49:11.844049\n",
      "\n",
      "Evaluating: batch 25 begins at 22:49:11.845553\n",
      "\n",
      "Evaluating: batch 25 ends at 22:49:12.063155\n",
      "\n",
      "Evaluating: batch 26 begins at 22:49:12.064429\n",
      "\n",
      "Evaluating: batch 26 ends at 22:49:12.282664\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.32433 to 0.29889, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 938ms/step - loss: 0.3058 - bce_dice_loss: 0.3058 - val_loss: 0.2989 - val_bce_dice_loss: 0.2989\n",
      "Epoch 10/25\n",
      "\n",
      "Training: batch 0 begins at 22:49:13.914479\n",
      "\n",
      "Training: batch 0 ends at 22:49:14.739338\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.5275 - bce_dice_loss: 0.5275\n",
      "Training: batch 1 begins at 22:49:14.742350\n",
      "\n",
      "Training: batch 1 ends at 22:49:15.558164\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4446 - bce_dice_loss: 0.4446\n",
      "Training: batch 2 begins at 22:49:15.562343\n",
      "\n",
      "Training: batch 2 ends at 22:49:16.366985\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3916 - bce_dice_loss: 0.3916\n",
      "Training: batch 3 begins at 22:49:16.371848\n",
      "\n",
      "Training: batch 3 ends at 22:49:17.166686\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3536 - bce_dice_loss: 0.3536\n",
      "Training: batch 4 begins at 22:49:17.169248\n",
      "\n",
      "Training: batch 4 ends at 22:49:17.967805\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3428 - bce_dice_loss: 0.3428\n",
      "Training: batch 5 begins at 22:49:17.972508\n",
      "\n",
      "Training: batch 5 ends at 22:49:18.798852\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3323 - bce_dice_loss: 0.3323\n",
      "Training: batch 6 begins at 22:49:18.803689\n",
      "\n",
      "Training: batch 6 ends at 22:49:19.601289\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3139 - bce_dice_loss: 0.3139\n",
      "Training: batch 7 begins at 22:49:19.605457\n",
      "\n",
      "Training: batch 7 ends at 22:49:20.404310\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.3108 - bce_dice_loss: 0.3108\n",
      "Training: batch 8 begins at 22:49:20.407787\n",
      "\n",
      "Training: batch 8 ends at 22:49:21.215908\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.3005 - bce_dice_loss: 0.3005\n",
      "Training: batch 9 begins at 22:49:21.218730\n",
      "\n",
      "Training: batch 9 ends at 22:49:22.015902\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.3364 - bce_dice_loss: 0.3364\n",
      "Training: batch 10 begins at 22:49:22.020834\n",
      "\n",
      "Training: batch 10 ends at 22:49:22.803900\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.3202 - bce_dice_loss: 0.3202\n",
      "Training: batch 11 begins at 22:49:22.807687\n",
      "\n",
      "Training: batch 11 ends at 22:49:23.608017\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3143 - bce_dice_loss: 0.3143\n",
      "Training: batch 12 begins at 22:49:23.612353\n",
      "\n",
      "Training: batch 12 ends at 22:49:24.420629\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.3081 - bce_dice_loss: 0.3081\n",
      "Training: batch 13 begins at 22:49:24.424784\n",
      "\n",
      "Training: batch 13 ends at 22:49:25.220597\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3240 - bce_dice_loss: 0.3240\n",
      "Training: batch 14 begins at 22:49:25.223839\n",
      "\n",
      "Training: batch 14 ends at 22:49:26.033424\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.3118 - bce_dice_loss: 0.3118\n",
      "Training: batch 15 begins at 22:49:26.037574\n",
      "\n",
      "Training: batch 15 ends at 22:49:26.840445\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3079 - bce_dice_loss: 0.3079\n",
      "Training: batch 16 begins at 22:49:26.845226\n",
      "\n",
      "Training: batch 16 ends at 22:49:27.659493\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3108 - bce_dice_loss: 0.3108\n",
      "Training: batch 17 begins at 22:49:27.662402\n",
      "\n",
      "Training: batch 17 ends at 22:49:28.462042\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3094 - bce_dice_loss: 0.3094\n",
      "Training: batch 18 begins at 22:49:28.466089\n",
      "\n",
      "Training: batch 18 ends at 22:49:29.257128\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.3125 - bce_dice_loss: 0.3125\n",
      "Training: batch 19 begins at 22:49:29.261303\n",
      "\n",
      "Training: batch 19 ends at 22:49:30.060214\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3067 - bce_dice_loss: 0.3067\n",
      "Training: batch 20 begins at 22:49:30.064581\n",
      "\n",
      "Training: batch 20 ends at 22:49:30.851276\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.3081 - bce_dice_loss: 0.3081\n",
      "Training: batch 21 begins at 22:49:30.855957\n",
      "\n",
      "Training: batch 21 ends at 22:49:31.677967\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.3004 - bce_dice_loss: 0.3004\n",
      "Training: batch 22 begins at 22:49:31.682959\n",
      "\n",
      "Training: batch 22 ends at 22:49:32.482912\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.3065 - bce_dice_loss: 0.3065\n",
      "Training: batch 23 begins at 22:49:32.486994\n",
      "\n",
      "Training: batch 23 ends at 22:49:33.278206\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.3044 - bce_dice_loss: 0.3044\n",
      "Training: batch 24 begins at 22:49:33.281577\n",
      "\n",
      "Training: batch 24 ends at 22:49:34.104102\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2999 - bce_dice_loss: 0.2999\n",
      "Training: batch 25 begins at 22:49:34.108248\n",
      "\n",
      "Training: batch 25 ends at 22:49:34.896849\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.3032 - bce_dice_loss: 0.3032\n",
      "Training: batch 26 begins at 22:49:34.901128\n",
      "\n",
      "Training: batch 26 ends at 22:49:35.712326\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.3018 - bce_dice_loss: 0.3018\n",
      "Training: batch 27 begins at 22:49:35.717011\n",
      "\n",
      "Training: batch 27 ends at 22:49:36.525657\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.3003 - bce_dice_loss: 0.3003\n",
      "Training: batch 28 begins at 22:49:36.529019\n",
      "\n",
      "Training: batch 28 ends at 22:49:37.333159\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.2955 - bce_dice_loss: 0.2955\n",
      "Training: batch 29 begins at 22:49:37.338233\n",
      "\n",
      "Training: batch 29 ends at 22:49:38.137385\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2959 - bce_dice_loss: 0.2959\n",
      "Training: batch 30 begins at 22:49:38.141271\n",
      "\n",
      "Training: batch 30 ends at 22:49:38.939731\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2952 - bce_dice_loss: 0.2952\n",
      "Training: batch 31 begins at 22:49:38.945493\n",
      "\n",
      "Training: batch 31 ends at 22:49:39.761238\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2941 - bce_dice_loss: 0.2941\n",
      "Training: batch 32 begins at 22:49:39.766579\n",
      "\n",
      "Training: batch 32 ends at 22:49:40.562335\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2906 - bce_dice_loss: 0.2906\n",
      "Training: batch 33 begins at 22:49:40.567271\n",
      "\n",
      "Training: batch 33 ends at 22:49:41.364050\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2870 - bce_dice_loss: 0.2870\n",
      "Training: batch 34 begins at 22:49:41.368520\n",
      "\n",
      "Training: batch 34 ends at 22:49:42.162118\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2848 - bce_dice_loss: 0.2848\n",
      "Training: batch 35 begins at 22:49:42.169112\n",
      "\n",
      "Training: batch 35 ends at 22:49:42.968013\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2856 - bce_dice_loss: 0.2856\n",
      "Training: batch 36 begins at 22:49:42.972458\n",
      "\n",
      "Training: batch 36 ends at 22:49:43.768127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/60 [=================>............] - ETA: 18s - loss: 0.2862 - bce_dice_loss: 0.2862\n",
      "Training: batch 37 begins at 22:49:43.771529\n",
      "\n",
      "Training: batch 37 ends at 22:49:44.571430\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2835 - bce_dice_loss: 0.2835\n",
      "Training: batch 38 begins at 22:49:44.574996\n",
      "\n",
      "Training: batch 38 ends at 22:49:45.358523\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2828 - bce_dice_loss: 0.2828\n",
      "Training: batch 39 begins at 22:49:45.360995\n",
      "\n",
      "Training: batch 39 ends at 22:49:46.188281\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2838 - bce_dice_loss: 0.2838\n",
      "Training: batch 40 begins at 22:49:46.191583\n",
      "\n",
      "Training: batch 40 ends at 22:49:46.985154\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2817 - bce_dice_loss: 0.2817\n",
      "Training: batch 41 begins at 22:49:46.988415\n",
      "\n",
      "Training: batch 41 ends at 22:49:47.786033\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2776 - bce_dice_loss: 0.2776\n",
      "Training: batch 42 begins at 22:49:47.789399\n",
      "\n",
      "Training: batch 42 ends at 22:49:48.612358\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2762 - bce_dice_loss: 0.2762\n",
      "Training: batch 43 begins at 22:49:48.615600\n",
      "\n",
      "Training: batch 43 ends at 22:49:49.416479\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2761 - bce_dice_loss: 0.2761\n",
      "Training: batch 44 begins at 22:49:49.420608\n",
      "\n",
      "Training: batch 44 ends at 22:49:50.215951\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2779 - bce_dice_loss: 0.2779\n",
      "Training: batch 45 begins at 22:49:50.220314\n",
      "\n",
      "Training: batch 45 ends at 22:49:51.013748\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2820 - bce_dice_loss: 0.2820\n",
      "Training: batch 46 begins at 22:49:51.019749\n",
      "\n",
      "Training: batch 46 ends at 22:49:51.815888\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2832 - bce_dice_loss: 0.2832\n",
      "Training: batch 47 begins at 22:49:51.820060\n",
      "\n",
      "Training: batch 47 ends at 22:49:52.617260\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2838 - bce_dice_loss: 0.2838 \n",
      "Training: batch 48 begins at 22:49:52.620698\n",
      "\n",
      "Training: batch 48 ends at 22:49:53.411819\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2844 - bce_dice_loss: 0.2844\n",
      "Training: batch 49 begins at 22:49:53.415395\n",
      "\n",
      "Training: batch 49 ends at 22:49:54.210590\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2842 - bce_dice_loss: 0.2842\n",
      "Training: batch 50 begins at 22:49:54.216063\n",
      "\n",
      "Training: batch 50 ends at 22:49:55.011974\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2840 - bce_dice_loss: 0.2840\n",
      "Training: batch 51 begins at 22:49:55.022441\n",
      "\n",
      "Training: batch 51 ends at 22:49:55.843444\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2834 - bce_dice_loss: 0.2834\n",
      "Training: batch 52 begins at 22:49:55.848692\n",
      "\n",
      "Training: batch 52 ends at 22:49:56.645693\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2806 - bce_dice_loss: 0.2806\n",
      "Training: batch 53 begins at 22:49:56.651019\n",
      "\n",
      "Training: batch 53 ends at 22:49:57.449278\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2799 - bce_dice_loss: 0.2799\n",
      "Training: batch 54 begins at 22:49:57.452062\n",
      "\n",
      "Training: batch 54 ends at 22:49:58.252247\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2836 - bce_dice_loss: 0.2836\n",
      "Training: batch 55 begins at 22:49:58.255573\n",
      "\n",
      "Training: batch 55 ends at 22:49:59.067109\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2895 - bce_dice_loss: 0.2895\n",
      "Training: batch 56 begins at 22:49:59.072508\n",
      "\n",
      "Training: batch 56 ends at 22:49:59.871673\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2889 - bce_dice_loss: 0.2889\n",
      "Training: batch 57 begins at 22:49:59.876117\n",
      "\n",
      "Training: batch 57 ends at 22:50:00.676490\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2889 - bce_dice_loss: 0.2889\n",
      "Training: batch 58 begins at 22:50:00.681038\n",
      "\n",
      "Training: batch 58 ends at 22:50:01.476347\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2931 - bce_dice_loss: 0.2931\n",
      "Training: batch 59 begins at 22:50:01.482269\n",
      "\n",
      "Training: batch 59 ends at 22:50:02.291765\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2921 - bce_dice_loss: 0.2921\n",
      "Evaluating: batch 0 begins at 22:50:02.324498\n",
      "\n",
      "Evaluating: batch 0 ends at 22:50:02.590935\n",
      "\n",
      "Evaluating: batch 1 begins at 22:50:02.592366\n",
      "\n",
      "Evaluating: batch 1 ends at 22:50:02.807884\n",
      "\n",
      "Evaluating: batch 2 begins at 22:50:02.809467\n",
      "\n",
      "Evaluating: batch 2 ends at 22:50:03.028441\n",
      "\n",
      "Evaluating: batch 3 begins at 22:50:03.032903\n",
      "\n",
      "Evaluating: batch 3 ends at 22:50:03.253738\n",
      "\n",
      "Evaluating: batch 4 begins at 22:50:03.256131\n",
      "\n",
      "Evaluating: batch 4 ends at 22:50:03.476373\n",
      "\n",
      "Evaluating: batch 5 begins at 22:50:03.480909\n",
      "\n",
      "Evaluating: batch 5 ends at 22:50:03.701916\n",
      "\n",
      "Evaluating: batch 6 begins at 22:50:03.703523\n",
      "\n",
      "Evaluating: batch 6 ends at 22:50:03.928221\n",
      "\n",
      "Evaluating: batch 7 begins at 22:50:03.930095\n",
      "\n",
      "Evaluating: batch 7 ends at 22:50:04.150730\n",
      "\n",
      "Evaluating: batch 8 begins at 22:50:04.153437\n",
      "\n",
      "Evaluating: batch 8 ends at 22:50:04.368691\n",
      "\n",
      "Evaluating: batch 9 begins at 22:50:04.370655\n",
      "\n",
      "Evaluating: batch 9 ends at 22:50:04.592437\n",
      "\n",
      "Evaluating: batch 10 begins at 22:50:04.593790\n",
      "\n",
      "Evaluating: batch 10 ends at 22:50:04.813499\n",
      "\n",
      "Evaluating: batch 11 begins at 22:50:04.815918\n",
      "\n",
      "Evaluating: batch 11 ends at 22:50:05.030172\n",
      "\n",
      "Evaluating: batch 12 begins at 22:50:05.031569\n",
      "\n",
      "Evaluating: batch 12 ends at 22:50:05.255727\n",
      "\n",
      "Evaluating: batch 13 begins at 22:50:05.257107\n",
      "\n",
      "Evaluating: batch 13 ends at 22:50:05.479325\n",
      "\n",
      "Evaluating: batch 14 begins at 22:50:05.480590\n",
      "\n",
      "Evaluating: batch 14 ends at 22:50:05.710368\n",
      "\n",
      "Evaluating: batch 15 begins at 22:50:05.711706\n",
      "\n",
      "Evaluating: batch 15 ends at 22:50:05.938029\n",
      "\n",
      "Evaluating: batch 16 begins at 22:50:05.940761\n",
      "\n",
      "Evaluating: batch 16 ends at 22:50:06.157621\n",
      "\n",
      "Evaluating: batch 17 begins at 22:50:06.158873\n",
      "\n",
      "Evaluating: batch 17 ends at 22:50:06.380018\n",
      "\n",
      "Evaluating: batch 18 begins at 22:50:06.381917\n",
      "\n",
      "Evaluating: batch 18 ends at 22:50:06.602630\n",
      "\n",
      "Evaluating: batch 19 begins at 22:50:06.604788\n",
      "\n",
      "Evaluating: batch 19 ends at 22:50:06.826085\n",
      "\n",
      "Evaluating: batch 20 begins at 22:50:06.828481\n",
      "\n",
      "Evaluating: batch 20 ends at 22:50:07.046079\n",
      "\n",
      "Evaluating: batch 21 begins at 22:50:07.048796\n",
      "\n",
      "Evaluating: batch 21 ends at 22:50:07.264707\n",
      "\n",
      "Evaluating: batch 22 begins at 22:50:07.266267\n",
      "\n",
      "Evaluating: batch 22 ends at 22:50:07.490296\n",
      "\n",
      "Evaluating: batch 23 begins at 22:50:07.491989\n",
      "\n",
      "Evaluating: batch 23 ends at 22:50:07.713000\n",
      "\n",
      "Evaluating: batch 24 begins at 22:50:07.714636\n",
      "\n",
      "Evaluating: batch 24 ends at 22:50:07.937067\n",
      "\n",
      "Evaluating: batch 25 begins at 22:50:07.939370\n",
      "\n",
      "Evaluating: batch 25 ends at 22:50:08.160384\n",
      "\n",
      "Evaluating: batch 26 begins at 22:50:08.162385\n",
      "\n",
      "Evaluating: batch 26 ends at 22:50:08.385601\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29889\n",
      "60/60 [==============================] - 54s 910ms/step - loss: 0.2921 - bce_dice_loss: 0.2921 - val_loss: 0.3087 - val_bce_dice_loss: 0.3087\n",
      "Epoch 11/25\n",
      "\n",
      "Training: batch 0 begins at 22:50:08.413771\n",
      "\n",
      "Training: batch 0 ends at 22:50:09.213131\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2142 - bce_dice_loss: 0.2142\n",
      "Training: batch 1 begins at 22:50:09.220053\n",
      "\n",
      "Training: batch 1 ends at 22:50:10.022148\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.2875 - bce_dice_loss: 0.2875\n",
      "Training: batch 2 begins at 22:50:10.026410\n",
      "\n",
      "Training: batch 2 ends at 22:50:10.839861\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2742 - bce_dice_loss: 0.2742\n",
      "Training: batch 3 begins at 22:50:10.844130\n",
      "\n",
      "Training: batch 3 ends at 22:50:11.637115\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2681 - bce_dice_loss: 0.2681\n",
      "Training: batch 4 begins at 22:50:11.641392\n",
      "\n",
      "Training: batch 4 ends at 22:50:12.436982\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2959 - bce_dice_loss: 0.2959\n",
      "Training: batch 5 begins at 22:50:12.441617\n",
      "\n",
      "Training: batch 5 ends at 22:50:13.239811\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2986 - bce_dice_loss: 0.2986\n",
      "Training: batch 6 begins at 22:50:13.245241\n",
      "\n",
      "Training: batch 6 ends at 22:50:14.036545\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2814 - bce_dice_loss: 0.2814\n",
      "Training: batch 7 begins at 22:50:14.039407\n",
      "\n",
      "Training: batch 7 ends at 22:50:14.831356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2865 - bce_dice_loss: 0.2865\n",
      "Training: batch 8 begins at 22:50:14.835896\n",
      "\n",
      "Training: batch 8 ends at 22:50:15.645064\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2911 - bce_dice_loss: 0.2911\n",
      "Training: batch 9 begins at 22:50:15.647826\n",
      "\n",
      "Training: batch 9 ends at 22:50:16.443173\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2919 - bce_dice_loss: 0.2919\n",
      "Training: batch 10 begins at 22:50:16.447217\n",
      "\n",
      "Training: batch 10 ends at 22:50:17.265451\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2881 - bce_dice_loss: 0.2881\n",
      "Training: batch 11 begins at 22:50:17.272083\n",
      "\n",
      "Training: batch 11 ends at 22:50:18.067671\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.3036 - bce_dice_loss: 0.3036\n",
      "Training: batch 12 begins at 22:50:18.071291\n",
      "\n",
      "Training: batch 12 ends at 22:50:18.890490\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2960 - bce_dice_loss: 0.2960\n",
      "Training: batch 13 begins at 22:50:18.894142\n",
      "\n",
      "Training: batch 13 ends at 22:50:19.690449\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.3030 - bce_dice_loss: 0.3030\n",
      "Training: batch 14 begins at 22:50:19.693796\n",
      "\n",
      "Training: batch 14 ends at 22:50:20.480115\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2955 - bce_dice_loss: 0.2955\n",
      "Training: batch 15 begins at 22:50:20.484071\n",
      "\n",
      "Training: batch 15 ends at 22:50:21.277525\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.3098 - bce_dice_loss: 0.3098\n",
      "Training: batch 16 begins at 22:50:21.281821\n",
      "\n",
      "Training: batch 16 ends at 22:50:22.085370\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.3008 - bce_dice_loss: 0.3008\n",
      "Training: batch 17 begins at 22:50:22.088461\n",
      "\n",
      "Training: batch 17 ends at 22:50:22.885426\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.3009 - bce_dice_loss: 0.3009\n",
      "Training: batch 18 begins at 22:50:22.888774\n",
      "\n",
      "Training: batch 18 ends at 22:50:23.679425\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.3065 - bce_dice_loss: 0.3065\n",
      "Training: batch 19 begins at 22:50:23.684036\n",
      "\n",
      "Training: batch 19 ends at 22:50:24.475355\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.3005 - bce_dice_loss: 0.3005\n",
      "Training: batch 20 begins at 22:50:24.479613\n",
      "\n",
      "Training: batch 20 ends at 22:50:25.296055\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2957 - bce_dice_loss: 0.2957\n",
      "Training: batch 21 begins at 22:50:25.301158\n",
      "\n",
      "Training: batch 21 ends at 22:50:26.103218\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2939 - bce_dice_loss: 0.2939\n",
      "Training: batch 22 begins at 22:50:26.105975\n",
      "\n",
      "Training: batch 22 ends at 22:50:26.908560\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2884 - bce_dice_loss: 0.2884\n",
      "Training: batch 23 begins at 22:50:26.913331\n",
      "\n",
      "Training: batch 23 ends at 22:50:27.710166\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2923 - bce_dice_loss: 0.2923\n",
      "Training: batch 24 begins at 22:50:27.715577\n",
      "\n",
      "Training: batch 24 ends at 22:50:28.510041\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2890 - bce_dice_loss: 0.2890\n",
      "Training: batch 25 begins at 22:50:28.516332\n",
      "\n",
      "Training: batch 25 ends at 22:50:29.334968\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2834 - bce_dice_loss: 0.2834\n",
      "Training: batch 26 begins at 22:50:29.339295\n",
      "\n",
      "Training: batch 26 ends at 22:50:30.133094\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2842 - bce_dice_loss: 0.2842\n",
      "Training: batch 27 begins at 22:50:30.137453\n",
      "\n",
      "Training: batch 27 ends at 22:50:30.932982\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2831 - bce_dice_loss: 0.2831\n",
      "Training: batch 28 begins at 22:50:30.936320\n",
      "\n",
      "Training: batch 28 ends at 22:50:31.754244\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2841 - bce_dice_loss: 0.2841\n",
      "Training: batch 29 begins at 22:50:31.758401\n",
      "\n",
      "Training: batch 29 ends at 22:50:32.553886\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2856 - bce_dice_loss: 0.2856\n",
      "Training: batch 30 begins at 22:50:32.558008\n",
      "\n",
      "Training: batch 30 ends at 22:50:33.361126\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2825 - bce_dice_loss: 0.2825\n",
      "Training: batch 31 begins at 22:50:33.364748\n",
      "\n",
      "Training: batch 31 ends at 22:50:34.165736\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2967 - bce_dice_loss: 0.2967\n",
      "Training: batch 32 begins at 22:50:34.171083\n",
      "\n",
      "Training: batch 32 ends at 22:50:34.969120\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2967 - bce_dice_loss: 0.2967\n",
      "Training: batch 33 begins at 22:50:34.973698\n",
      "\n",
      "Training: batch 33 ends at 22:50:35.790189\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.3006 - bce_dice_loss: 0.3006\n",
      "Training: batch 34 begins at 22:50:35.795282\n",
      "\n",
      "Training: batch 34 ends at 22:50:36.590596\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2980 - bce_dice_loss: 0.2980\n",
      "Training: batch 35 begins at 22:50:36.593616\n",
      "\n",
      "Training: batch 35 ends at 22:50:37.392660\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2982 - bce_dice_loss: 0.2982\n",
      "Training: batch 36 begins at 22:50:37.396657\n",
      "\n",
      "Training: batch 36 ends at 22:50:38.188745\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2950 - bce_dice_loss: 0.2950\n",
      "Training: batch 37 begins at 22:50:38.193235\n",
      "\n",
      "Training: batch 37 ends at 22:50:38.988427\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2941 - bce_dice_loss: 0.2941\n",
      "Training: batch 38 begins at 22:50:38.992525\n",
      "\n",
      "Training: batch 38 ends at 22:50:39.790181\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2949 - bce_dice_loss: 0.2949\n",
      "Training: batch 39 begins at 22:50:39.793967\n",
      "\n",
      "Training: batch 39 ends at 22:50:40.591672\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2904 - bce_dice_loss: 0.2904\n",
      "Training: batch 40 begins at 22:50:40.595972\n",
      "\n",
      "Training: batch 40 ends at 22:50:41.388919\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2919 - bce_dice_loss: 0.2919\n",
      "Training: batch 41 begins at 22:50:41.394773\n",
      "\n",
      "Training: batch 41 ends at 22:50:42.194177\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2900 - bce_dice_loss: 0.2900\n",
      "Training: batch 42 begins at 22:50:42.197919\n",
      "\n",
      "Training: batch 42 ends at 22:50:42.993309\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2893 - bce_dice_loss: 0.2893\n",
      "Training: batch 43 begins at 22:50:42.996716\n",
      "\n",
      "Training: batch 43 ends at 22:50:43.791520\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2876 - bce_dice_loss: 0.2876\n",
      "Training: batch 44 begins at 22:50:43.794213\n",
      "\n",
      "Training: batch 44 ends at 22:50:44.594108\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2907 - bce_dice_loss: 0.2907\n",
      "Training: batch 45 begins at 22:50:44.597366\n",
      "\n",
      "Training: batch 45 ends at 22:50:45.398802\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2912 - bce_dice_loss: 0.2912\n",
      "Training: batch 46 begins at 22:50:45.403130\n",
      "\n",
      "Training: batch 46 ends at 22:50:46.211424\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2951 - bce_dice_loss: 0.2951\n",
      "Training: batch 47 begins at 22:50:46.214992\n",
      "\n",
      "Training: batch 47 ends at 22:50:47.017431\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2952 - bce_dice_loss: 0.2952 \n",
      "Training: batch 48 begins at 22:50:47.021890\n",
      "\n",
      "Training: batch 48 ends at 22:50:47.816365\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2938 - bce_dice_loss: 0.2938\n",
      "Training: batch 49 begins at 22:50:47.820538\n",
      "\n",
      "Training: batch 49 ends at 22:50:48.602456\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2948 - bce_dice_loss: 0.2948\n",
      "Training: batch 50 begins at 22:50:48.607866\n",
      "\n",
      "Training: batch 50 ends at 22:50:49.420514\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3002 - bce_dice_loss: 0.3002\n",
      "Training: batch 51 begins at 22:50:49.422848\n",
      "\n",
      "Training: batch 51 ends at 22:50:50.219692\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2997 - bce_dice_loss: 0.2997\n",
      "Training: batch 52 begins at 22:50:50.224046\n",
      "\n",
      "Training: batch 52 ends at 22:50:51.024041\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2973 - bce_dice_loss: 0.2973\n",
      "Training: batch 53 begins at 22:50:51.028186\n",
      "\n",
      "Training: batch 53 ends at 22:50:51.818553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2977 - bce_dice_loss: 0.2977\n",
      "Training: batch 54 begins at 22:50:51.823899\n",
      "\n",
      "Training: batch 54 ends at 22:50:52.622359\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2976 - bce_dice_loss: 0.2976\n",
      "Training: batch 55 begins at 22:50:52.627056\n",
      "\n",
      "Training: batch 55 ends at 22:50:53.421241\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2985 - bce_dice_loss: 0.2985\n",
      "Training: batch 56 begins at 22:50:53.423956\n",
      "\n",
      "Training: batch 56 ends at 22:50:54.224737\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2983 - bce_dice_loss: 0.2983\n",
      "Training: batch 57 begins at 22:50:54.228098\n",
      "\n",
      "Training: batch 57 ends at 22:50:55.043038\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2967 - bce_dice_loss: 0.2967\n",
      "Training: batch 58 begins at 22:50:55.046332\n",
      "\n",
      "Training: batch 58 ends at 22:50:55.849774\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2945 - bce_dice_loss: 0.2945\n",
      "Training: batch 59 begins at 22:50:55.854172\n",
      "\n",
      "Training: batch 59 ends at 22:50:56.639778\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2964 - bce_dice_loss: 0.2964\n",
      "Evaluating: batch 0 begins at 22:50:56.673071\n",
      "\n",
      "Evaluating: batch 0 ends at 22:50:56.941288\n",
      "\n",
      "Evaluating: batch 1 begins at 22:50:56.942560\n",
      "\n",
      "Evaluating: batch 1 ends at 22:50:57.156316\n",
      "\n",
      "Evaluating: batch 2 begins at 22:50:57.157556\n",
      "\n",
      "Evaluating: batch 2 ends at 22:50:57.376340\n",
      "\n",
      "Evaluating: batch 3 begins at 22:50:57.378096\n",
      "\n",
      "Evaluating: batch 3 ends at 22:50:57.594267\n",
      "\n",
      "Evaluating: batch 4 begins at 22:50:57.595513\n",
      "\n",
      "Evaluating: batch 4 ends at 22:50:57.818143\n",
      "\n",
      "Evaluating: batch 5 begins at 22:50:57.819469\n",
      "\n",
      "Evaluating: batch 5 ends at 22:50:58.041269\n",
      "\n",
      "Evaluating: batch 6 begins at 22:50:58.042648\n",
      "\n",
      "Evaluating: batch 6 ends at 22:50:58.262027\n",
      "\n",
      "Evaluating: batch 7 begins at 22:50:58.263938\n",
      "\n",
      "Evaluating: batch 7 ends at 22:50:58.485055\n",
      "\n",
      "Evaluating: batch 8 begins at 22:50:58.487553\n",
      "\n",
      "Evaluating: batch 8 ends at 22:50:58.711049\n",
      "\n",
      "Evaluating: batch 9 begins at 22:50:58.712378\n",
      "\n",
      "Evaluating: batch 9 ends at 22:50:58.933375\n",
      "\n",
      "Evaluating: batch 10 begins at 22:50:58.934811\n",
      "\n",
      "Evaluating: batch 10 ends at 22:50:59.156656\n",
      "\n",
      "Evaluating: batch 11 begins at 22:50:59.158008\n",
      "\n",
      "Evaluating: batch 11 ends at 22:50:59.375754\n",
      "\n",
      "Evaluating: batch 12 begins at 22:50:59.376989\n",
      "\n",
      "Evaluating: batch 12 ends at 22:50:59.595437\n",
      "\n",
      "Evaluating: batch 13 begins at 22:50:59.597791\n",
      "\n",
      "Evaluating: batch 13 ends at 22:50:59.821346\n",
      "\n",
      "Evaluating: batch 14 begins at 22:50:59.822618\n",
      "\n",
      "Evaluating: batch 14 ends at 22:51:00.042453\n",
      "\n",
      "Evaluating: batch 15 begins at 22:51:00.044107\n",
      "\n",
      "Evaluating: batch 15 ends at 22:51:00.264623\n",
      "\n",
      "Evaluating: batch 16 begins at 22:51:00.266357\n",
      "\n",
      "Evaluating: batch 16 ends at 22:51:00.485964\n",
      "\n",
      "Evaluating: batch 17 begins at 22:51:00.488092\n",
      "\n",
      "Evaluating: batch 17 ends at 22:51:00.710709\n",
      "\n",
      "Evaluating: batch 18 begins at 22:51:00.712081\n",
      "\n",
      "Evaluating: batch 18 ends at 22:51:00.932404\n",
      "\n",
      "Evaluating: batch 19 begins at 22:51:00.934492\n",
      "\n",
      "Evaluating: batch 19 ends at 22:51:01.150587\n",
      "\n",
      "Evaluating: batch 20 begins at 22:51:01.151794\n",
      "\n",
      "Evaluating: batch 20 ends at 22:51:01.375164\n",
      "\n",
      "Evaluating: batch 21 begins at 22:51:01.376504\n",
      "\n",
      "Evaluating: batch 21 ends at 22:51:01.593642\n",
      "\n",
      "Evaluating: batch 22 begins at 22:51:01.596016\n",
      "\n",
      "Evaluating: batch 22 ends at 22:51:01.815635\n",
      "\n",
      "Evaluating: batch 23 begins at 22:51:01.817835\n",
      "\n",
      "Evaluating: batch 23 ends at 22:51:02.035806\n",
      "\n",
      "Evaluating: batch 24 begins at 22:51:02.037950\n",
      "\n",
      "Evaluating: batch 24 ends at 22:51:02.259605\n",
      "\n",
      "Evaluating: batch 25 begins at 22:51:02.261117\n",
      "\n",
      "Evaluating: batch 25 ends at 22:51:02.476475\n",
      "\n",
      "Evaluating: batch 26 begins at 22:51:02.477738\n",
      "\n",
      "Evaluating: batch 26 ends at 22:51:02.694891\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29889\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.2964 - bce_dice_loss: 0.2964 - val_loss: 0.3341 - val_bce_dice_loss: 0.3341\n",
      "Epoch 12/25\n",
      "\n",
      "Training: batch 0 begins at 22:51:02.724430\n",
      "\n",
      "Training: batch 0 ends at 22:51:03.520408\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.1595 - bce_dice_loss: 0.1595\n",
      "Training: batch 1 begins at 22:51:03.526726\n",
      "\n",
      "Training: batch 1 ends at 22:51:04.353338\n",
      " 2/60 [>.............................] - ETA: 48s - loss: 0.1994 - bce_dice_loss: 0.1994\n",
      "Training: batch 2 begins at 22:51:04.358677\n",
      "\n",
      "Training: batch 2 ends at 22:51:05.142461\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2301 - bce_dice_loss: 0.2301\n",
      "Training: batch 3 begins at 22:51:05.145627\n",
      "\n",
      "Training: batch 3 ends at 22:51:05.953230\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2436 - bce_dice_loss: 0.2436\n",
      "Training: batch 4 begins at 22:51:05.959606\n",
      "\n",
      "Training: batch 4 ends at 22:51:06.749838\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2579 - bce_dice_loss: 0.2579\n",
      "Training: batch 5 begins at 22:51:06.754629\n",
      "\n",
      "Training: batch 5 ends at 22:51:07.572603\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2608 - bce_dice_loss: 0.2608\n",
      "Training: batch 6 begins at 22:51:07.576778\n",
      "\n",
      "Training: batch 6 ends at 22:51:08.367664\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2497 - bce_dice_loss: 0.2497\n",
      "Training: batch 7 begins at 22:51:08.372643\n",
      "\n",
      "Training: batch 7 ends at 22:51:09.198874\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2523 - bce_dice_loss: 0.2523\n",
      "Training: batch 8 begins at 22:51:09.203984\n",
      "\n",
      "Training: batch 8 ends at 22:51:10.002281\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2620 - bce_dice_loss: 0.2620\n",
      "Training: batch 9 begins at 22:51:10.006923\n",
      "\n",
      "Training: batch 9 ends at 22:51:10.800238\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2624 - bce_dice_loss: 0.2624\n",
      "Training: batch 10 begins at 22:51:10.804146\n",
      "\n",
      "Training: batch 10 ends at 22:51:11.599333\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2600 - bce_dice_loss: 0.2600\n",
      "Training: batch 11 begins at 22:51:11.603471\n",
      "\n",
      "Training: batch 11 ends at 22:51:12.401110\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2802 - bce_dice_loss: 0.2802\n",
      "Training: batch 12 begins at 22:51:12.405788\n",
      "\n",
      "Training: batch 12 ends at 22:51:13.203583\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2767 - bce_dice_loss: 0.2767\n",
      "Training: batch 13 begins at 22:51:13.206877\n",
      "\n",
      "Training: batch 13 ends at 22:51:14.000286\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2822 - bce_dice_loss: 0.2822\n",
      "Training: batch 14 begins at 22:51:14.005125\n",
      "\n",
      "Training: batch 14 ends at 22:51:14.802123\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2782 - bce_dice_loss: 0.2782\n",
      "Training: batch 15 begins at 22:51:14.806703\n",
      "\n",
      "Training: batch 15 ends at 22:51:15.590193\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2758 - bce_dice_loss: 0.2758\n",
      "Training: batch 16 begins at 22:51:15.594536\n",
      "\n",
      "Training: batch 16 ends at 22:51:16.403225\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2708 - bce_dice_loss: 0.2708\n",
      "Training: batch 17 begins at 22:51:16.407509\n",
      "\n",
      "Training: batch 17 ends at 22:51:17.194530\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2698 - bce_dice_loss: 0.2698\n",
      "Training: batch 18 begins at 22:51:17.197076\n",
      "\n",
      "Training: batch 18 ends at 22:51:17.990660\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2695 - bce_dice_loss: 0.2695\n",
      "Training: batch 19 begins at 22:51:17.994014\n",
      "\n",
      "Training: batch 19 ends at 22:51:18.790004\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2694 - bce_dice_loss: 0.2694\n",
      "Training: batch 20 begins at 22:51:18.794095\n",
      "\n",
      "Training: batch 20 ends at 22:51:19.590913\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2679 - bce_dice_loss: 0.2679\n",
      "Training: batch 21 begins at 22:51:19.593727\n",
      "\n",
      "Training: batch 21 ends at 22:51:20.391688\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2771 - bce_dice_loss: 0.2771\n",
      "Training: batch 22 begins at 22:51:20.395021\n",
      "\n",
      "Training: batch 22 ends at 22:51:21.210167\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2754 - bce_dice_loss: 0.2754\n",
      "Training: batch 23 begins at 22:51:21.212834\n",
      "\n",
      "Training: batch 23 ends at 22:51:22.013667\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2708 - bce_dice_loss: 0.2708\n",
      "Training: batch 24 begins at 22:51:22.018030\n",
      "\n",
      "Training: batch 24 ends at 22:51:22.827113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2857 - bce_dice_loss: 0.2857\n",
      "Training: batch 25 begins at 22:51:22.831361\n",
      "\n",
      "Training: batch 25 ends at 22:51:23.631982\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2867 - bce_dice_loss: 0.2867\n",
      "Training: batch 26 begins at 22:51:23.636819\n",
      "\n",
      "Training: batch 26 ends at 22:51:24.415685\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2904 - bce_dice_loss: 0.2904\n",
      "Training: batch 27 begins at 22:51:24.419840\n",
      "\n",
      "Training: batch 27 ends at 22:51:25.245584\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2934 - bce_dice_loss: 0.2934\n",
      "Training: batch 28 begins at 22:51:25.248611\n",
      "\n",
      "Training: batch 28 ends at 22:51:26.055559\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2902 - bce_dice_loss: 0.2902\n",
      "Training: batch 29 begins at 22:51:26.059787\n",
      "\n",
      "Training: batch 29 ends at 22:51:26.860792\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2906 - bce_dice_loss: 0.2906\n",
      "Training: batch 30 begins at 22:51:26.864233\n",
      "\n",
      "Training: batch 30 ends at 22:51:27.657646\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2859 - bce_dice_loss: 0.2859\n",
      "Training: batch 31 begins at 22:51:27.660575\n",
      "\n",
      "Training: batch 31 ends at 22:51:28.454623\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2830 - bce_dice_loss: 0.2830\n",
      "Training: batch 32 begins at 22:51:28.457101\n",
      "\n",
      "Training: batch 32 ends at 22:51:29.253064\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2875 - bce_dice_loss: 0.2875\n",
      "Training: batch 33 begins at 22:51:29.256923\n",
      "\n",
      "Training: batch 33 ends at 22:51:30.060961\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2933 - bce_dice_loss: 0.2933\n",
      "Training: batch 34 begins at 22:51:30.062233\n",
      "\n",
      "Training: batch 34 ends at 22:51:30.849020\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2932 - bce_dice_loss: 0.2932\n",
      "Training: batch 35 begins at 22:51:30.854862\n",
      "\n",
      "Training: batch 35 ends at 22:51:31.655321\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2898 - bce_dice_loss: 0.2898\n",
      "Training: batch 36 begins at 22:51:31.658466\n",
      "\n",
      "Training: batch 36 ends at 22:51:32.472504\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2871 - bce_dice_loss: 0.2871\n",
      "Training: batch 37 begins at 22:51:32.476230\n",
      "\n",
      "Training: batch 37 ends at 22:51:33.272957\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2951 - bce_dice_loss: 0.2951\n",
      "Training: batch 38 begins at 22:51:33.277365\n",
      "\n",
      "Training: batch 38 ends at 22:51:34.077613\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2907 - bce_dice_loss: 0.2907\n",
      "Training: batch 39 begins at 22:51:34.080152\n",
      "\n",
      "Training: batch 39 ends at 22:51:34.872704\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2904 - bce_dice_loss: 0.2904\n",
      "Training: batch 40 begins at 22:51:34.876974\n",
      "\n",
      "Training: batch 40 ends at 22:51:35.656930\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2880 - bce_dice_loss: 0.2880\n",
      "Training: batch 41 begins at 22:51:35.659467\n",
      "\n",
      "Training: batch 41 ends at 22:51:36.469066\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2874 - bce_dice_loss: 0.2874\n",
      "Training: batch 42 begins at 22:51:36.471917\n",
      "\n",
      "Training: batch 42 ends at 22:51:37.274564\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2873 - bce_dice_loss: 0.2873\n",
      "Training: batch 43 begins at 22:51:37.277944\n",
      "\n",
      "Training: batch 43 ends at 22:51:38.073625\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2894 - bce_dice_loss: 0.2894\n",
      "Training: batch 44 begins at 22:51:38.077858\n",
      "\n",
      "Training: batch 44 ends at 22:51:38.878874\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2902 - bce_dice_loss: 0.2902\n",
      "Training: batch 45 begins at 22:51:38.882887\n",
      "\n",
      "Training: batch 45 ends at 22:51:39.692533\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2894 - bce_dice_loss: 0.2894\n",
      "Training: batch 46 begins at 22:51:39.696847\n",
      "\n",
      "Training: batch 46 ends at 22:51:40.490262\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2887 - bce_dice_loss: 0.2887\n",
      "Training: batch 47 begins at 22:51:40.493198\n",
      "\n",
      "Training: batch 47 ends at 22:51:41.278122\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2881 - bce_dice_loss: 0.2881 \n",
      "Training: batch 48 begins at 22:51:41.281663\n",
      "\n",
      "Training: batch 48 ends at 22:51:42.072237\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2852 - bce_dice_loss: 0.2852\n",
      "Training: batch 49 begins at 22:51:42.076376\n",
      "\n",
      "Training: batch 49 ends at 22:51:42.880642\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2907 - bce_dice_loss: 0.2907\n",
      "Training: batch 50 begins at 22:51:42.885275\n",
      "\n",
      "Training: batch 50 ends at 22:51:43.689759\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2909 - bce_dice_loss: 0.2909\n",
      "Training: batch 51 begins at 22:51:43.692544\n",
      "\n",
      "Training: batch 51 ends at 22:51:44.486128\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2908 - bce_dice_loss: 0.2908\n",
      "Training: batch 52 begins at 22:51:44.490372\n",
      "\n",
      "Training: batch 52 ends at 22:51:45.284908\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2948 - bce_dice_loss: 0.2948\n",
      "Training: batch 53 begins at 22:51:45.289564\n",
      "\n",
      "Training: batch 53 ends at 22:51:46.103349\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2941 - bce_dice_loss: 0.2941\n",
      "Training: batch 54 begins at 22:51:46.107754\n",
      "\n",
      "Training: batch 54 ends at 22:51:46.906837\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2939 - bce_dice_loss: 0.2939\n",
      "Training: batch 55 begins at 22:51:46.910941\n",
      "\n",
      "Training: batch 55 ends at 22:51:47.701317\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2984 - bce_dice_loss: 0.2984\n",
      "Training: batch 56 begins at 22:51:47.706610\n",
      "\n",
      "Training: batch 56 ends at 22:51:48.495281\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2999 - bce_dice_loss: 0.2999\n",
      "Training: batch 57 begins at 22:51:48.499669\n",
      "\n",
      "Training: batch 57 ends at 22:51:49.295200\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.3012 - bce_dice_loss: 0.3012\n",
      "Training: batch 58 begins at 22:51:49.298253\n",
      "\n",
      "Training: batch 58 ends at 22:51:50.083900\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3011 - bce_dice_loss: 0.3011\n",
      "Training: batch 59 begins at 22:51:50.088367\n",
      "\n",
      "Training: batch 59 ends at 22:51:50.897889\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2991 - bce_dice_loss: 0.2991\n",
      "Evaluating: batch 0 begins at 22:51:50.928874\n",
      "\n",
      "Evaluating: batch 0 ends at 22:51:51.198130\n",
      "\n",
      "Evaluating: batch 1 begins at 22:51:51.199336\n",
      "\n",
      "Evaluating: batch 1 ends at 22:51:51.413319\n",
      "\n",
      "Evaluating: batch 2 begins at 22:51:51.414547\n",
      "\n",
      "Evaluating: batch 2 ends at 22:51:51.639986\n",
      "\n",
      "Evaluating: batch 3 begins at 22:51:51.641500\n",
      "\n",
      "Evaluating: batch 3 ends at 22:51:51.868129\n",
      "\n",
      "Evaluating: batch 4 begins at 22:51:51.870601\n",
      "\n",
      "Evaluating: batch 4 ends at 22:51:52.090061\n",
      "\n",
      "Evaluating: batch 5 begins at 22:51:52.092425\n",
      "\n",
      "Evaluating: batch 5 ends at 22:51:52.314275\n",
      "\n",
      "Evaluating: batch 6 begins at 22:51:52.316793\n",
      "\n",
      "Evaluating: batch 6 ends at 22:51:52.537588\n",
      "\n",
      "Evaluating: batch 7 begins at 22:51:52.539006\n",
      "\n",
      "Evaluating: batch 7 ends at 22:51:52.760084\n",
      "\n",
      "Evaluating: batch 8 begins at 22:51:52.761444\n",
      "\n",
      "Evaluating: batch 8 ends at 22:51:52.980805\n",
      "\n",
      "Evaluating: batch 9 begins at 22:51:52.983219\n",
      "\n",
      "Evaluating: batch 9 ends at 22:51:53.204409\n",
      "\n",
      "Evaluating: batch 10 begins at 22:51:53.207074\n",
      "\n",
      "Evaluating: batch 10 ends at 22:51:53.427213\n",
      "\n",
      "Evaluating: batch 11 begins at 22:51:53.429645\n",
      "\n",
      "Evaluating: batch 11 ends at 22:51:53.649056\n",
      "\n",
      "Evaluating: batch 12 begins at 22:51:53.650848\n",
      "\n",
      "Evaluating: batch 12 ends at 22:51:53.874346\n",
      "\n",
      "Evaluating: batch 13 begins at 22:51:53.876750\n",
      "\n",
      "Evaluating: batch 13 ends at 22:51:54.097043\n",
      "\n",
      "Evaluating: batch 14 begins at 22:51:54.099570\n",
      "\n",
      "Evaluating: batch 14 ends at 22:51:54.319499\n",
      "\n",
      "Evaluating: batch 15 begins at 22:51:54.321327\n",
      "\n",
      "Evaluating: batch 15 ends at 22:51:54.545838\n",
      "\n",
      "Evaluating: batch 16 begins at 22:51:54.546764\n",
      "\n",
      "Evaluating: batch 16 ends at 22:51:54.769382\n",
      "\n",
      "Evaluating: batch 17 begins at 22:51:54.770319\n",
      "\n",
      "Evaluating: batch 17 ends at 22:51:54.985047\n",
      "\n",
      "Evaluating: batch 18 begins at 22:51:54.986454\n",
      "\n",
      "Evaluating: batch 18 ends at 22:51:55.206483\n",
      "\n",
      "Evaluating: batch 19 begins at 22:51:55.207852\n",
      "\n",
      "Evaluating: batch 19 ends at 22:51:55.423235\n",
      "\n",
      "Evaluating: batch 20 begins at 22:51:55.424448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 20 ends at 22:51:55.642040\n",
      "\n",
      "Evaluating: batch 21 begins at 22:51:55.643788\n",
      "\n",
      "Evaluating: batch 21 ends at 22:51:55.878507\n",
      "\n",
      "Evaluating: batch 22 begins at 22:51:55.880271\n",
      "\n",
      "Evaluating: batch 22 ends at 22:51:56.101767\n",
      "\n",
      "Evaluating: batch 23 begins at 22:51:56.103159\n",
      "\n",
      "Evaluating: batch 23 ends at 22:51:56.316984\n",
      "\n",
      "Evaluating: batch 24 begins at 22:51:56.318209\n",
      "\n",
      "Evaluating: batch 24 ends at 22:51:56.534149\n",
      "\n",
      "Evaluating: batch 25 begins at 22:51:56.536416\n",
      "\n",
      "Evaluating: batch 25 ends at 22:51:56.763694\n",
      "\n",
      "Evaluating: batch 26 begins at 22:51:56.764911\n",
      "\n",
      "Evaluating: batch 26 ends at 22:51:56.984162\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29889\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 0.2991 - bce_dice_loss: 0.2991 - val_loss: 0.3067 - val_bce_dice_loss: 0.3067\n",
      "Epoch 13/25\n",
      "\n",
      "Training: batch 0 begins at 22:51:57.024157\n",
      "\n",
      "Training: batch 0 ends at 22:51:57.850015\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2983 - bce_dice_loss: 0.2983\n",
      "Training: batch 1 begins at 22:51:57.857110\n",
      "\n",
      "Training: batch 1 ends at 22:51:58.665504\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2583 - bce_dice_loss: 0.2583\n",
      "Training: batch 2 begins at 22:51:58.667972\n",
      "\n",
      "Training: batch 2 ends at 22:51:59.465242\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2231 - bce_dice_loss: 0.2231\n",
      "Training: batch 3 begins at 22:51:59.470252\n",
      "\n",
      "Training: batch 3 ends at 22:52:00.296248\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2637 - bce_dice_loss: 0.2637\n",
      "Training: batch 4 begins at 22:52:00.300335\n",
      "\n",
      "Training: batch 4 ends at 22:52:01.093302\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2372 - bce_dice_loss: 0.2372\n",
      "Training: batch 5 begins at 22:52:01.097577\n",
      "\n",
      "Training: batch 5 ends at 22:52:01.900002\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2307 - bce_dice_loss: 0.2307\n",
      "Training: batch 6 begins at 22:52:01.902677\n",
      "\n",
      "Training: batch 6 ends at 22:52:02.695590\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2353 - bce_dice_loss: 0.2353\n",
      "Training: batch 7 begins at 22:52:02.698949\n",
      "\n",
      "Training: batch 7 ends at 22:52:03.505512\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2398 - bce_dice_loss: 0.2398\n",
      "Training: batch 8 begins at 22:52:03.509908\n",
      "\n",
      "Training: batch 8 ends at 22:52:04.307612\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2476 - bce_dice_loss: 0.2476\n",
      "Training: batch 9 begins at 22:52:04.312028\n",
      "\n",
      "Training: batch 9 ends at 22:52:05.137426\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2457 - bce_dice_loss: 0.2457\n",
      "Training: batch 10 begins at 22:52:05.140894\n",
      "\n",
      "Training: batch 10 ends at 22:52:05.950442\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2407 - bce_dice_loss: 0.2407\n",
      "Training: batch 11 begins at 22:52:05.953739\n",
      "\n",
      "Training: batch 11 ends at 22:52:06.754392\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2415 - bce_dice_loss: 0.2415\n",
      "Training: batch 12 begins at 22:52:06.756994\n",
      "\n",
      "Training: batch 12 ends at 22:52:07.554796\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.2422 - bce_dice_loss: 0.2422\n",
      "Training: batch 13 begins at 22:52:07.559413\n",
      "\n",
      "Training: batch 13 ends at 22:52:08.378153\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2389 - bce_dice_loss: 0.2389\n",
      "Training: batch 14 begins at 22:52:08.382227\n",
      "\n",
      "Training: batch 14 ends at 22:52:09.170293\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2487 - bce_dice_loss: 0.2487\n",
      "Training: batch 15 begins at 22:52:09.173872\n",
      "\n",
      "Training: batch 15 ends at 22:52:09.972110\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2436 - bce_dice_loss: 0.2436\n",
      "Training: batch 16 begins at 22:52:09.976503\n",
      "\n",
      "Training: batch 16 ends at 22:52:10.780064\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2551 - bce_dice_loss: 0.2551\n",
      "Training: batch 17 begins at 22:52:10.782557\n",
      "\n",
      "Training: batch 17 ends at 22:52:11.576214\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2501 - bce_dice_loss: 0.2501\n",
      "Training: batch 18 begins at 22:52:11.578912\n",
      "\n",
      "Training: batch 18 ends at 22:52:12.372264\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2633 - bce_dice_loss: 0.2633\n",
      "Training: batch 19 begins at 22:52:12.375595\n",
      "\n",
      "Training: batch 19 ends at 22:52:13.162770\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2679 - bce_dice_loss: 0.2679\n",
      "Training: batch 20 begins at 22:52:13.167081\n",
      "\n",
      "Training: batch 20 ends at 22:52:13.975152\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2795 - bce_dice_loss: 0.2795\n",
      "Training: batch 21 begins at 22:52:13.979673\n",
      "\n",
      "Training: batch 21 ends at 22:52:14.780793\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2752 - bce_dice_loss: 0.2752\n",
      "Training: batch 22 begins at 22:52:14.785307\n",
      "\n",
      "Training: batch 22 ends at 22:52:15.569102\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2710 - bce_dice_loss: 0.2710\n",
      "Training: batch 23 begins at 22:52:15.572677\n",
      "\n",
      "Training: batch 23 ends at 22:52:16.406594\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2738 - bce_dice_loss: 0.2738\n",
      "Training: batch 24 begins at 22:52:16.409599\n",
      "\n",
      "Training: batch 24 ends at 22:52:17.205558\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2748 - bce_dice_loss: 0.2748\n",
      "Training: batch 25 begins at 22:52:17.210271\n",
      "\n",
      "Training: batch 25 ends at 22:52:18.029727\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2716 - bce_dice_loss: 0.2716\n",
      "Training: batch 26 begins at 22:52:18.033952\n",
      "\n",
      "Training: batch 26 ends at 22:52:18.838140\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2744 - bce_dice_loss: 0.2744\n",
      "Training: batch 27 begins at 22:52:18.840626\n",
      "\n",
      "Training: batch 27 ends at 22:52:19.634822\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2710 - bce_dice_loss: 0.2710\n",
      "Training: batch 28 begins at 22:52:19.637445\n",
      "\n",
      "Training: batch 28 ends at 22:52:20.431697\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.2731 - bce_dice_loss: 0.2731\n",
      "Training: batch 29 begins at 22:52:20.435068\n",
      "\n",
      "Training: batch 29 ends at 22:52:21.249300\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2738 - bce_dice_loss: 0.2738\n",
      "Training: batch 30 begins at 22:52:21.253341\n",
      "\n",
      "Training: batch 30 ends at 22:52:22.047606\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2773 - bce_dice_loss: 0.2773\n",
      "Training: batch 31 begins at 22:52:22.051680\n",
      "\n",
      "Training: batch 31 ends at 22:52:22.848677\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2812 - bce_dice_loss: 0.2812\n",
      "Training: batch 32 begins at 22:52:22.853210\n",
      "\n",
      "Training: batch 32 ends at 22:52:23.648143\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2825 - bce_dice_loss: 0.2825\n",
      "Training: batch 33 begins at 22:52:23.652158\n",
      "\n",
      "Training: batch 33 ends at 22:52:24.454140\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2813 - bce_dice_loss: 0.2813\n",
      "Training: batch 34 begins at 22:52:24.459742\n",
      "\n",
      "Training: batch 34 ends at 22:52:25.258240\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2852 - bce_dice_loss: 0.2852\n",
      "Training: batch 35 begins at 22:52:25.260891\n",
      "\n",
      "Training: batch 35 ends at 22:52:26.072313\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2849 - bce_dice_loss: 0.2849\n",
      "Training: batch 36 begins at 22:52:26.075263\n",
      "\n",
      "Training: batch 36 ends at 22:52:26.888190\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2861 - bce_dice_loss: 0.2861\n",
      "Training: batch 37 begins at 22:52:26.891590\n",
      "\n",
      "Training: batch 37 ends at 22:52:27.698165\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2844 - bce_dice_loss: 0.2844\n",
      "Training: batch 38 begins at 22:52:27.704955\n",
      "\n",
      "Training: batch 38 ends at 22:52:28.497484\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2876 - bce_dice_loss: 0.2876\n",
      "Training: batch 39 begins at 22:52:28.500005\n",
      "\n",
      "Training: batch 39 ends at 22:52:29.295750\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2885 - bce_dice_loss: 0.2885\n",
      "Training: batch 40 begins at 22:52:29.300497\n",
      "\n",
      "Training: batch 40 ends at 22:52:30.103949\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2881 - bce_dice_loss: 0.2881\n",
      "Training: batch 41 begins at 22:52:30.107995\n",
      "\n",
      "Training: batch 41 ends at 22:52:30.919041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2860 - bce_dice_loss: 0.2860\n",
      "Training: batch 42 begins at 22:52:30.923240\n",
      "\n",
      "Training: batch 42 ends at 22:52:31.800974\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2870 - bce_dice_loss: 0.2870\n",
      "Training: batch 43 begins at 22:52:31.804131\n",
      "\n",
      "Training: batch 43 ends at 22:52:32.687778\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2920 - bce_dice_loss: 0.2920\n",
      "Training: batch 44 begins at 22:52:32.692267\n",
      "\n",
      "Training: batch 44 ends at 22:52:33.561610\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2892 - bce_dice_loss: 0.2892\n",
      "Training: batch 45 begins at 22:52:33.565326\n",
      "\n",
      "Training: batch 45 ends at 22:52:34.423379\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2891 - bce_dice_loss: 0.2891\n",
      "Training: batch 46 begins at 22:52:34.425565\n",
      "\n",
      "Training: batch 46 ends at 22:52:35.226060\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2882 - bce_dice_loss: 0.2882\n",
      "Training: batch 47 begins at 22:52:35.228923\n",
      "\n",
      "Training: batch 47 ends at 22:52:36.031990\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2958 - bce_dice_loss: 0.2958 \n",
      "Training: batch 48 begins at 22:52:36.035416\n",
      "\n",
      "Training: batch 48 ends at 22:52:36.839856\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.3014 - bce_dice_loss: 0.3014\n",
      "Training: batch 49 begins at 22:52:36.844040\n",
      "\n",
      "Training: batch 49 ends at 22:52:37.646454\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.3006 - bce_dice_loss: 0.3006\n",
      "Training: batch 50 begins at 22:52:37.650805\n",
      "\n",
      "Training: batch 50 ends at 22:52:38.446635\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.3001 - bce_dice_loss: 0.3001\n",
      "Training: batch 51 begins at 22:52:38.449118\n",
      "\n",
      "Training: batch 51 ends at 22:52:39.248916\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2980 - bce_dice_loss: 0.2980\n",
      "Training: batch 52 begins at 22:52:39.252102\n",
      "\n",
      "Training: batch 52 ends at 22:52:40.052389\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2987 - bce_dice_loss: 0.2987\n",
      "Training: batch 53 begins at 22:52:40.055943\n",
      "\n",
      "Training: batch 53 ends at 22:52:40.857910\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2981 - bce_dice_loss: 0.2981\n",
      "Training: batch 54 begins at 22:52:40.862525\n",
      "\n",
      "Training: batch 54 ends at 22:52:41.647031\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2969 - bce_dice_loss: 0.2969\n",
      "Training: batch 55 begins at 22:52:41.652411\n",
      "\n",
      "Training: batch 55 ends at 22:52:42.468576\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2951 - bce_dice_loss: 0.2951\n",
      "Training: batch 56 begins at 22:52:42.472835\n",
      "\n",
      "Training: batch 56 ends at 22:52:43.273134\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2943 - bce_dice_loss: 0.2943\n",
      "Training: batch 57 begins at 22:52:43.275618\n",
      "\n",
      "Training: batch 57 ends at 22:52:44.088467\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2928 - bce_dice_loss: 0.2928\n",
      "Training: batch 58 begins at 22:52:44.092868\n",
      "\n",
      "Training: batch 58 ends at 22:52:44.890281\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2913 - bce_dice_loss: 0.2913\n",
      "Training: batch 59 begins at 22:52:44.894454\n",
      "\n",
      "Training: batch 59 ends at 22:52:45.695039\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2913 - bce_dice_loss: 0.2913\n",
      "Evaluating: batch 0 begins at 22:52:45.731892\n",
      "\n",
      "Evaluating: batch 0 ends at 22:52:46.015866\n",
      "\n",
      "Evaluating: batch 1 begins at 22:52:46.017533\n",
      "\n",
      "Evaluating: batch 1 ends at 22:52:46.232453\n",
      "\n",
      "Evaluating: batch 2 begins at 22:52:46.234215\n",
      "\n",
      "Evaluating: batch 2 ends at 22:52:46.456183\n",
      "\n",
      "Evaluating: batch 3 begins at 22:52:46.457794\n",
      "\n",
      "Evaluating: batch 3 ends at 22:52:46.676752\n",
      "\n",
      "Evaluating: batch 4 begins at 22:52:46.679441\n",
      "\n",
      "Evaluating: batch 4 ends at 22:52:46.903608\n",
      "\n",
      "Evaluating: batch 5 begins at 22:52:46.905126\n",
      "\n",
      "Evaluating: batch 5 ends at 22:52:47.125682\n",
      "\n",
      "Evaluating: batch 6 begins at 22:52:47.128045\n",
      "\n",
      "Evaluating: batch 6 ends at 22:52:47.350002\n",
      "\n",
      "Evaluating: batch 7 begins at 22:52:47.351928\n",
      "\n",
      "Evaluating: batch 7 ends at 22:52:47.571930\n",
      "\n",
      "Evaluating: batch 8 begins at 22:52:47.574782\n",
      "\n",
      "Evaluating: batch 8 ends at 22:52:47.798145\n",
      "\n",
      "Evaluating: batch 9 begins at 22:52:47.799531\n",
      "\n",
      "Evaluating: batch 9 ends at 22:52:48.018083\n",
      "\n",
      "Evaluating: batch 10 begins at 22:52:48.020439\n",
      "\n",
      "Evaluating: batch 10 ends at 22:52:48.240375\n",
      "\n",
      "Evaluating: batch 11 begins at 22:52:48.242437\n",
      "\n",
      "Evaluating: batch 11 ends at 22:52:48.467750\n",
      "\n",
      "Evaluating: batch 12 begins at 22:52:48.469173\n",
      "\n",
      "Evaluating: batch 12 ends at 22:52:48.686336\n",
      "\n",
      "Evaluating: batch 13 begins at 22:52:48.687592\n",
      "\n",
      "Evaluating: batch 13 ends at 22:52:48.910985\n",
      "\n",
      "Evaluating: batch 14 begins at 22:52:48.912684\n",
      "\n",
      "Evaluating: batch 14 ends at 22:52:49.130352\n",
      "\n",
      "Evaluating: batch 15 begins at 22:52:49.132227\n",
      "\n",
      "Evaluating: batch 15 ends at 22:52:49.353825\n",
      "\n",
      "Evaluating: batch 16 begins at 22:52:49.355217\n",
      "\n",
      "Evaluating: batch 16 ends at 22:52:49.574858\n",
      "\n",
      "Evaluating: batch 17 begins at 22:52:49.577159\n",
      "\n",
      "Evaluating: batch 17 ends at 22:52:49.802063\n",
      "\n",
      "Evaluating: batch 18 begins at 22:52:49.803479\n",
      "\n",
      "Evaluating: batch 18 ends at 22:52:50.023550\n",
      "\n",
      "Evaluating: batch 19 begins at 22:52:50.025116\n",
      "\n",
      "Evaluating: batch 19 ends at 22:52:50.240549\n",
      "\n",
      "Evaluating: batch 20 begins at 22:52:50.241791\n",
      "\n",
      "Evaluating: batch 20 ends at 22:52:50.461328\n",
      "\n",
      "Evaluating: batch 21 begins at 22:52:50.463077\n",
      "\n",
      "Evaluating: batch 21 ends at 22:52:50.680319\n",
      "\n",
      "Evaluating: batch 22 begins at 22:52:50.682166\n",
      "\n",
      "Evaluating: batch 22 ends at 22:52:50.903944\n",
      "\n",
      "Evaluating: batch 23 begins at 22:52:50.906570\n",
      "\n",
      "Evaluating: batch 23 ends at 22:52:51.122656\n",
      "\n",
      "Evaluating: batch 24 begins at 22:52:51.123898\n",
      "\n",
      "Evaluating: batch 24 ends at 22:52:51.346143\n",
      "\n",
      "Evaluating: batch 25 begins at 22:52:51.348477\n",
      "\n",
      "Evaluating: batch 25 ends at 22:52:51.567379\n",
      "\n",
      "Evaluating: batch 26 begins at 22:52:51.570110\n",
      "\n",
      "Evaluating: batch 26 ends at 22:52:51.790290\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.29889 to 0.29448, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 942ms/step - loss: 0.2913 - bce_dice_loss: 0.2913 - val_loss: 0.2945 - val_bce_dice_loss: 0.2945\n",
      "Epoch 14/25\n",
      "\n",
      "Training: batch 0 begins at 22:52:53.444666\n",
      "\n",
      "Training: batch 0 ends at 22:52:54.261016\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2518 - bce_dice_loss: 0.2518\n",
      "Training: batch 1 begins at 22:52:54.264392\n",
      "\n",
      "Training: batch 1 ends at 22:52:55.066653\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3025 - bce_dice_loss: 0.3025\n",
      "Training: batch 2 begins at 22:52:55.071115\n",
      "\n",
      "Training: batch 2 ends at 22:52:55.873795\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2840 - bce_dice_loss: 0.2840\n",
      "Training: batch 3 begins at 22:52:55.876925\n",
      "\n",
      "Training: batch 3 ends at 22:52:56.715984\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2643 - bce_dice_loss: 0.2643\n",
      "Training: batch 4 begins at 22:52:56.719666\n",
      "\n",
      "Training: batch 4 ends at 22:52:57.520270\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2690 - bce_dice_loss: 0.2690\n",
      "Training: batch 5 begins at 22:52:57.523626\n",
      "\n",
      "Training: batch 5 ends at 22:52:58.325724\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2572 - bce_dice_loss: 0.2572\n",
      "Training: batch 6 begins at 22:52:58.330380\n",
      "\n",
      "Training: batch 6 ends at 22:52:59.127354\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2374 - bce_dice_loss: 0.2374\n",
      "Training: batch 7 begins at 22:52:59.131834\n",
      "\n",
      "Training: batch 7 ends at 22:52:59.933985\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2553 - bce_dice_loss: 0.2553\n",
      "Training: batch 8 begins at 22:52:59.938372\n",
      "\n",
      "Training: batch 8 ends at 22:53:00.738210\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2535 - bce_dice_loss: 0.2535\n",
      "Training: batch 9 begins at 22:53:00.741518\n",
      "\n",
      "Training: batch 9 ends at 22:53:01.544261\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2430 - bce_dice_loss: 0.2430\n",
      "Training: batch 10 begins at 22:53:01.548110\n",
      "\n",
      "Training: batch 10 ends at 22:53:02.350466\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2472 - bce_dice_loss: 0.2472\n",
      "Training: batch 11 begins at 22:53:02.353776\n",
      "\n",
      "Training: batch 11 ends at 22:53:03.146467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2487 - bce_dice_loss: 0.2487\n",
      "Training: batch 12 begins at 22:53:03.151030\n",
      "\n",
      "Training: batch 12 ends at 22:53:03.941719\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2466 - bce_dice_loss: 0.2466\n",
      "Training: batch 13 begins at 22:53:03.944591\n",
      "\n",
      "Training: batch 13 ends at 22:53:04.739831\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2462 - bce_dice_loss: 0.2462\n",
      "Training: batch 14 begins at 22:53:04.742772\n",
      "\n",
      "Training: batch 14 ends at 22:53:05.535055\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2753 - bce_dice_loss: 0.2753\n",
      "Training: batch 15 begins at 22:53:05.538942\n",
      "\n",
      "Training: batch 15 ends at 22:53:06.366429\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2751 - bce_dice_loss: 0.2751\n",
      "Training: batch 16 begins at 22:53:06.371432\n",
      "\n",
      "Training: batch 16 ends at 22:53:07.170845\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2741 - bce_dice_loss: 0.2741\n",
      "Training: batch 17 begins at 22:53:07.175197\n",
      "\n",
      "Training: batch 17 ends at 22:53:07.978967\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2659 - bce_dice_loss: 0.2659\n",
      "Training: batch 18 begins at 22:53:07.983758\n",
      "\n",
      "Training: batch 18 ends at 22:53:08.781444\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2633 - bce_dice_loss: 0.2633\n",
      "Training: batch 19 begins at 22:53:08.786828\n",
      "\n",
      "Training: batch 19 ends at 22:53:09.614989\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2810 - bce_dice_loss: 0.2810\n",
      "Training: batch 20 begins at 22:53:09.620890\n",
      "\n",
      "Training: batch 20 ends at 22:53:10.414600\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2727 - bce_dice_loss: 0.2727\n",
      "Training: batch 21 begins at 22:53:10.419857\n",
      "\n",
      "Training: batch 21 ends at 22:53:11.219555\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2722 - bce_dice_loss: 0.2722\n",
      "Training: batch 22 begins at 22:53:11.225565\n",
      "\n",
      "Training: batch 22 ends at 22:53:12.010823\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2705 - bce_dice_loss: 0.2705\n",
      "Training: batch 23 begins at 22:53:12.015226\n",
      "\n",
      "Training: batch 23 ends at 22:53:12.810249\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2687 - bce_dice_loss: 0.2687\n",
      "Training: batch 24 begins at 22:53:12.814463\n",
      "\n",
      "Training: batch 24 ends at 22:53:13.612245\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2639 - bce_dice_loss: 0.2639\n",
      "Training: batch 25 begins at 22:53:13.616289\n",
      "\n",
      "Training: batch 25 ends at 22:53:14.408527\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2630 - bce_dice_loss: 0.2630\n",
      "Training: batch 26 begins at 22:53:14.412140\n",
      "\n",
      "Training: batch 26 ends at 22:53:15.202520\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2673 - bce_dice_loss: 0.2673\n",
      "Training: batch 27 begins at 22:53:15.206273\n",
      "\n",
      "Training: batch 27 ends at 22:53:16.008574\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2666 - bce_dice_loss: 0.2666\n",
      "Training: batch 28 begins at 22:53:16.012828\n",
      "\n",
      "Training: batch 28 ends at 22:53:16.812094\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2644 - bce_dice_loss: 0.2644\n",
      "Training: batch 29 begins at 22:53:16.817455\n",
      "\n",
      "Training: batch 29 ends at 22:53:17.619272\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2645 - bce_dice_loss: 0.2645\n",
      "Training: batch 30 begins at 22:53:17.624638\n",
      "\n",
      "Training: batch 30 ends at 22:53:18.420989\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2609 - bce_dice_loss: 0.2609\n",
      "Training: batch 31 begins at 22:53:18.424122\n",
      "\n",
      "Training: batch 31 ends at 22:53:19.244617\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2627 - bce_dice_loss: 0.2627\n",
      "Training: batch 32 begins at 22:53:19.249111\n",
      "\n",
      "Training: batch 32 ends at 22:53:20.046022\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2605 - bce_dice_loss: 0.2605\n",
      "Training: batch 33 begins at 22:53:20.049176\n",
      "\n",
      "Training: batch 33 ends at 22:53:20.847424\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2691 - bce_dice_loss: 0.2691\n",
      "Training: batch 34 begins at 22:53:20.851982\n",
      "\n",
      "Training: batch 34 ends at 22:53:21.655342\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2657 - bce_dice_loss: 0.2657\n",
      "Training: batch 35 begins at 22:53:21.659418\n",
      "\n",
      "Training: batch 35 ends at 22:53:22.463134\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2649 - bce_dice_loss: 0.2649\n",
      "Training: batch 36 begins at 22:53:22.467514\n",
      "\n",
      "Training: batch 36 ends at 22:53:23.267138\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2640 - bce_dice_loss: 0.2640\n",
      "Training: batch 37 begins at 22:53:23.270964\n",
      "\n",
      "Training: batch 37 ends at 22:53:24.071126\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2642 - bce_dice_loss: 0.2642\n",
      "Training: batch 38 begins at 22:53:24.074770\n",
      "\n",
      "Training: batch 38 ends at 22:53:24.874391\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2667 - bce_dice_loss: 0.2667\n",
      "Training: batch 39 begins at 22:53:24.878630\n",
      "\n",
      "Training: batch 39 ends at 22:53:25.664332\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2674 - bce_dice_loss: 0.2674\n",
      "Training: batch 40 begins at 22:53:25.668815\n",
      "\n",
      "Training: batch 40 ends at 22:53:26.478476\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2675 - bce_dice_loss: 0.2675\n",
      "Training: batch 41 begins at 22:53:26.481803\n",
      "\n",
      "Training: batch 41 ends at 22:53:27.277860\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2660 - bce_dice_loss: 0.2660\n",
      "Training: batch 42 begins at 22:53:27.282036\n",
      "\n",
      "Training: batch 42 ends at 22:53:28.074979\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2732 - bce_dice_loss: 0.2732\n",
      "Training: batch 43 begins at 22:53:28.079241\n",
      "\n",
      "Training: batch 43 ends at 22:53:28.905062\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2741 - bce_dice_loss: 0.2741\n",
      "Training: batch 44 begins at 22:53:28.908185\n",
      "\n",
      "Training: batch 44 ends at 22:53:29.701160\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2735 - bce_dice_loss: 0.2735\n",
      "Training: batch 45 begins at 22:53:29.707150\n",
      "\n",
      "Training: batch 45 ends at 22:53:30.488585\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2725 - bce_dice_loss: 0.2725\n",
      "Training: batch 46 begins at 22:53:30.492796\n",
      "\n",
      "Training: batch 46 ends at 22:53:31.286973\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2714 - bce_dice_loss: 0.2714\n",
      "Training: batch 47 begins at 22:53:31.291315\n",
      "\n",
      "Training: batch 47 ends at 22:53:32.109253\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2765 - bce_dice_loss: 0.2765 \n",
      "Training: batch 48 begins at 22:53:32.112867\n",
      "\n",
      "Training: batch 48 ends at 22:53:32.910483\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2756 - bce_dice_loss: 0.2756\n",
      "Training: batch 49 begins at 22:53:32.913909\n",
      "\n",
      "Training: batch 49 ends at 22:53:33.708235\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2731 - bce_dice_loss: 0.2731\n",
      "Training: batch 50 begins at 22:53:33.711540\n",
      "\n",
      "Training: batch 50 ends at 22:53:34.529744\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2726 - bce_dice_loss: 0.2726\n",
      "Training: batch 51 begins at 22:53:34.534500\n",
      "\n",
      "Training: batch 51 ends at 22:53:35.326055\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2711 - bce_dice_loss: 0.2711\n",
      "Training: batch 52 begins at 22:53:35.330496\n",
      "\n",
      "Training: batch 52 ends at 22:53:36.130975\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2692 - bce_dice_loss: 0.2692\n",
      "Training: batch 53 begins at 22:53:36.135891\n",
      "\n",
      "Training: batch 53 ends at 22:53:36.934274\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2684 - bce_dice_loss: 0.2684\n",
      "Training: batch 54 begins at 22:53:36.938579\n",
      "\n",
      "Training: batch 54 ends at 22:53:37.747783\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2705 - bce_dice_loss: 0.2705\n",
      "Training: batch 55 begins at 22:53:37.751246\n",
      "\n",
      "Training: batch 55 ends at 22:53:38.545256\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2757 - bce_dice_loss: 0.2757\n",
      "Training: batch 56 begins at 22:53:38.549696\n",
      "\n",
      "Training: batch 56 ends at 22:53:39.342175\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2748 - bce_dice_loss: 0.2748\n",
      "Training: batch 57 begins at 22:53:39.345073\n",
      "\n",
      "Training: batch 57 ends at 22:53:40.148634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/60 [============================>.] - ETA: 1s - loss: 0.2748 - bce_dice_loss: 0.2748\n",
      "Training: batch 58 begins at 22:53:40.152927\n",
      "\n",
      "Training: batch 58 ends at 22:53:40.941018\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2721 - bce_dice_loss: 0.2721\n",
      "Training: batch 59 begins at 22:53:40.943560\n",
      "\n",
      "Training: batch 59 ends at 22:53:41.739330\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2714 - bce_dice_loss: 0.2714\n",
      "Evaluating: batch 0 begins at 22:53:41.772158\n",
      "\n",
      "Evaluating: batch 0 ends at 22:53:42.043515\n",
      "\n",
      "Evaluating: batch 1 begins at 22:53:42.045428\n",
      "\n",
      "Evaluating: batch 1 ends at 22:53:42.260097\n",
      "\n",
      "Evaluating: batch 2 begins at 22:53:42.261312\n",
      "\n",
      "Evaluating: batch 2 ends at 22:53:42.483446\n",
      "\n",
      "Evaluating: batch 3 begins at 22:53:42.484615\n",
      "\n",
      "Evaluating: batch 3 ends at 22:53:42.705720\n",
      "\n",
      "Evaluating: batch 4 begins at 22:53:42.707780\n",
      "\n",
      "Evaluating: batch 4 ends at 22:53:42.925919\n",
      "\n",
      "Evaluating: batch 5 begins at 22:53:42.928270\n",
      "\n",
      "Evaluating: batch 5 ends at 22:53:43.147337\n",
      "\n",
      "Evaluating: batch 6 begins at 22:53:43.150751\n",
      "\n",
      "Evaluating: batch 6 ends at 22:53:43.373887\n",
      "\n",
      "Evaluating: batch 7 begins at 22:53:43.375619\n",
      "\n",
      "Evaluating: batch 7 ends at 22:53:43.598979\n",
      "\n",
      "Evaluating: batch 8 begins at 22:53:43.600570\n",
      "\n",
      "Evaluating: batch 8 ends at 22:53:43.823739\n",
      "\n",
      "Evaluating: batch 9 begins at 22:53:43.824972\n",
      "\n",
      "Evaluating: batch 9 ends at 22:53:44.048117\n",
      "\n",
      "Evaluating: batch 10 begins at 22:53:44.049815\n",
      "\n",
      "Evaluating: batch 10 ends at 22:53:44.272313\n",
      "\n",
      "Evaluating: batch 11 begins at 22:53:44.274201\n",
      "\n",
      "Evaluating: batch 11 ends at 22:53:44.494012\n",
      "\n",
      "Evaluating: batch 12 begins at 22:53:44.495352\n",
      "\n",
      "Evaluating: batch 12 ends at 22:53:44.716548\n",
      "\n",
      "Evaluating: batch 13 begins at 22:53:44.718966\n",
      "\n",
      "Evaluating: batch 13 ends at 22:53:44.949873\n",
      "\n",
      "Evaluating: batch 14 begins at 22:53:44.951680\n",
      "\n",
      "Evaluating: batch 14 ends at 22:53:45.172305\n",
      "\n",
      "Evaluating: batch 15 begins at 22:53:45.173974\n",
      "\n",
      "Evaluating: batch 15 ends at 22:53:45.394329\n",
      "\n",
      "Evaluating: batch 16 begins at 22:53:45.396848\n",
      "\n",
      "Evaluating: batch 16 ends at 22:53:45.621879\n",
      "\n",
      "Evaluating: batch 17 begins at 22:53:45.623317\n",
      "\n",
      "Evaluating: batch 17 ends at 22:53:45.846721\n",
      "\n",
      "Evaluating: batch 18 begins at 22:53:45.847941\n",
      "\n",
      "Evaluating: batch 18 ends at 22:53:46.080576\n",
      "\n",
      "Evaluating: batch 19 begins at 22:53:46.082108\n",
      "\n",
      "Evaluating: batch 19 ends at 22:53:46.298497\n",
      "\n",
      "Evaluating: batch 20 begins at 22:53:46.299807\n",
      "\n",
      "Evaluating: batch 20 ends at 22:53:46.519355\n",
      "\n",
      "Evaluating: batch 21 begins at 22:53:46.522157\n",
      "\n",
      "Evaluating: batch 21 ends at 22:53:46.742167\n",
      "\n",
      "Evaluating: batch 22 begins at 22:53:46.744335\n",
      "\n",
      "Evaluating: batch 22 ends at 22:53:46.962203\n",
      "\n",
      "Evaluating: batch 23 begins at 22:53:46.964493\n",
      "\n",
      "Evaluating: batch 23 ends at 22:53:47.186904\n",
      "\n",
      "Evaluating: batch 24 begins at 22:53:47.188699\n",
      "\n",
      "Evaluating: batch 24 ends at 22:53:47.407151\n",
      "\n",
      "Evaluating: batch 25 begins at 22:53:47.409682\n",
      "\n",
      "Evaluating: batch 25 ends at 22:53:47.629413\n",
      "\n",
      "Evaluating: batch 26 begins at 22:53:47.630936\n",
      "\n",
      "Evaluating: batch 26 ends at 22:53:47.850393\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.29448\n",
      "60/60 [==============================] - 54s 909ms/step - loss: 0.2714 - bce_dice_loss: 0.2714 - val_loss: 0.3012 - val_bce_dice_loss: 0.3012\n",
      "Epoch 15/25\n",
      "\n",
      "Training: batch 0 begins at 22:53:47.882166\n",
      "\n",
      "Training: batch 0 ends at 22:53:48.691016\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.1851 - bce_dice_loss: 0.1851\n",
      "Training: batch 1 begins at 22:53:48.695994\n",
      "\n",
      "Training: batch 1 ends at 22:53:49.499171\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.1685 - bce_dice_loss: 0.1685\n",
      "Training: batch 2 begins at 22:53:49.503578\n",
      "\n",
      "Training: batch 2 ends at 22:53:50.299250\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.1820 - bce_dice_loss: 0.1820\n",
      "Training: batch 3 begins at 22:53:50.303680\n",
      "\n",
      "Training: batch 3 ends at 22:53:51.093949\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.1912 - bce_dice_loss: 0.1912\n",
      "Training: batch 4 begins at 22:53:51.096800\n",
      "\n",
      "Training: batch 4 ends at 22:53:51.888138\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.1790 - bce_dice_loss: 0.1790\n",
      "Training: batch 5 begins at 22:53:51.892458\n",
      "\n",
      "Training: batch 5 ends at 22:53:52.688865\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.1890 - bce_dice_loss: 0.1890\n",
      "Training: batch 6 begins at 22:53:52.693208\n",
      "\n",
      "Training: batch 6 ends at 22:53:53.488790\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.1890 - bce_dice_loss: 0.1890\n",
      "Training: batch 7 begins at 22:53:53.492973\n",
      "\n",
      "Training: batch 7 ends at 22:53:54.289855\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2239 - bce_dice_loss: 0.2239\n",
      "Training: batch 8 begins at 22:53:54.293920\n",
      "\n",
      "Training: batch 8 ends at 22:53:55.098474\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2175 - bce_dice_loss: 0.2175\n",
      "Training: batch 9 begins at 22:53:55.102574\n",
      "\n",
      "Training: batch 9 ends at 22:53:55.902719\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2615 - bce_dice_loss: 0.2615\n",
      "Training: batch 10 begins at 22:53:55.907348\n",
      "\n",
      "Training: batch 10 ends at 22:53:56.720736\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2587 - bce_dice_loss: 0.2587\n",
      "Training: batch 11 begins at 22:53:56.723513\n",
      "\n",
      "Training: batch 11 ends at 22:53:57.519878\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2505 - bce_dice_loss: 0.2505\n",
      "Training: batch 12 begins at 22:53:57.524185\n",
      "\n",
      "Training: batch 12 ends at 22:53:58.348349\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2497 - bce_dice_loss: 0.2497\n",
      "Training: batch 13 begins at 22:53:58.351766\n",
      "\n",
      "Training: batch 13 ends at 22:53:59.147213\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2467 - bce_dice_loss: 0.2467\n",
      "Training: batch 14 begins at 22:53:59.151444\n",
      "\n",
      "Training: batch 14 ends at 22:53:59.943099\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2455 - bce_dice_loss: 0.2455\n",
      "Training: batch 15 begins at 22:53:59.945946\n",
      "\n",
      "Training: batch 15 ends at 22:54:00.742274\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2479 - bce_dice_loss: 0.2479\n",
      "Training: batch 16 begins at 22:54:00.744773\n",
      "\n",
      "Training: batch 16 ends at 22:54:01.541007\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2504 - bce_dice_loss: 0.2504\n",
      "Training: batch 17 begins at 22:54:01.544021\n",
      "\n",
      "Training: batch 17 ends at 22:54:02.338550\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2645 - bce_dice_loss: 0.2645\n",
      "Training: batch 18 begins at 22:54:02.341324\n",
      "\n",
      "Training: batch 18 ends at 22:54:03.135820\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2591 - bce_dice_loss: 0.2591\n",
      "Training: batch 19 begins at 22:54:03.140097\n",
      "\n",
      "Training: batch 19 ends at 22:54:03.937853\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2680 - bce_dice_loss: 0.2680\n",
      "Training: batch 20 begins at 22:54:03.942951\n",
      "\n",
      "Training: batch 20 ends at 22:54:04.763457\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2684 - bce_dice_loss: 0.2684\n",
      "Training: batch 21 begins at 22:54:04.768137\n",
      "\n",
      "Training: batch 21 ends at 22:54:05.563690\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2632 - bce_dice_loss: 0.2632\n",
      "Training: batch 22 begins at 22:54:05.567825\n",
      "\n",
      "Training: batch 22 ends at 22:54:06.369191\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2612 - bce_dice_loss: 0.2612\n",
      "Training: batch 23 begins at 22:54:06.373293\n",
      "\n",
      "Training: batch 23 ends at 22:54:07.177526\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2570 - bce_dice_loss: 0.2570\n",
      "Training: batch 24 begins at 22:54:07.181427\n",
      "\n",
      "Training: batch 24 ends at 22:54:07.992174\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2556 - bce_dice_loss: 0.2556\n",
      "Training: batch 25 begins at 22:54:07.993153\n",
      "\n",
      "Training: batch 25 ends at 22:54:08.784144\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2616 - bce_dice_loss: 0.2616\n",
      "Training: batch 26 begins at 22:54:08.788440\n",
      "\n",
      "Training: batch 26 ends at 22:54:09.583930\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2565 - bce_dice_loss: 0.2565\n",
      "Training: batch 27 begins at 22:54:09.588575\n",
      "\n",
      "Training: batch 27 ends at 22:54:10.386277\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2510 - bce_dice_loss: 0.2510\n",
      "Training: batch 28 begins at 22:54:10.389838\n",
      "\n",
      "Training: batch 28 ends at 22:54:11.184369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/60 [=============>................] - ETA: 24s - loss: 0.2556 - bce_dice_loss: 0.2556\n",
      "Training: batch 29 begins at 22:54:11.188474\n",
      "\n",
      "Training: batch 29 ends at 22:54:11.982083\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2591 - bce_dice_loss: 0.2591\n",
      "Training: batch 30 begins at 22:54:11.986953\n",
      "\n",
      "Training: batch 30 ends at 22:54:12.775803\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2591 - bce_dice_loss: 0.2591\n",
      "Training: batch 31 begins at 22:54:12.780705\n",
      "\n",
      "Training: batch 31 ends at 22:54:13.576979\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2589 - bce_dice_loss: 0.2589\n",
      "Training: batch 32 begins at 22:54:13.586016\n",
      "\n",
      "Training: batch 32 ends at 22:54:14.379954\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2682 - bce_dice_loss: 0.2682\n",
      "Training: batch 33 begins at 22:54:14.384958\n",
      "\n",
      "Training: batch 33 ends at 22:54:15.180155\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2689 - bce_dice_loss: 0.2689\n",
      "Training: batch 34 begins at 22:54:15.186652\n",
      "\n",
      "Training: batch 34 ends at 22:54:15.993127\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2714 - bce_dice_loss: 0.2714\n",
      "Training: batch 35 begins at 22:54:15.998071\n",
      "\n",
      "Training: batch 35 ends at 22:54:16.804847\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2678 - bce_dice_loss: 0.2678\n",
      "Training: batch 36 begins at 22:54:16.809146\n",
      "\n",
      "Training: batch 36 ends at 22:54:17.602564\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2655 - bce_dice_loss: 0.2655\n",
      "Training: batch 37 begins at 22:54:17.605846\n",
      "\n",
      "Training: batch 37 ends at 22:54:18.396870\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2661 - bce_dice_loss: 0.2661\n",
      "Training: batch 38 begins at 22:54:18.402145\n",
      "\n",
      "Training: batch 38 ends at 22:54:19.197276\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2696 - bce_dice_loss: 0.2696\n",
      "Training: batch 39 begins at 22:54:19.201223\n",
      "\n",
      "Training: batch 39 ends at 22:54:19.994052\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2700 - bce_dice_loss: 0.2700\n",
      "Training: batch 40 begins at 22:54:19.997599\n",
      "\n",
      "Training: batch 40 ends at 22:54:20.794147\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2733 - bce_dice_loss: 0.2733\n",
      "Training: batch 41 begins at 22:54:20.797264\n",
      "\n",
      "Training: batch 41 ends at 22:54:21.585324\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2691 - bce_dice_loss: 0.2691\n",
      "Training: batch 42 begins at 22:54:21.588558\n",
      "\n",
      "Training: batch 42 ends at 22:54:22.384844\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2720 - bce_dice_loss: 0.2720\n",
      "Training: batch 43 begins at 22:54:22.389068\n",
      "\n",
      "Training: batch 43 ends at 22:54:23.206924\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2705 - bce_dice_loss: 0.2705\n",
      "Training: batch 44 begins at 22:54:23.210671\n",
      "\n",
      "Training: batch 44 ends at 22:54:24.005700\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2699 - bce_dice_loss: 0.2699\n",
      "Training: batch 45 begins at 22:54:24.010067\n",
      "\n",
      "Training: batch 45 ends at 22:54:24.813993\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2713 - bce_dice_loss: 0.2713\n",
      "Training: batch 46 begins at 22:54:24.818728\n",
      "\n",
      "Training: batch 46 ends at 22:54:25.613645\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2711 - bce_dice_loss: 0.2711\n",
      "Training: batch 47 begins at 22:54:25.618889\n",
      "\n",
      "Training: batch 47 ends at 22:54:26.437831\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2715 - bce_dice_loss: 0.2715 \n",
      "Training: batch 48 begins at 22:54:26.441244\n",
      "\n",
      "Training: batch 48 ends at 22:54:27.244647\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2727 - bce_dice_loss: 0.2727\n",
      "Training: batch 49 begins at 22:54:27.252002\n",
      "\n",
      "Training: batch 49 ends at 22:54:28.044937\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2717 - bce_dice_loss: 0.2717\n",
      "Training: batch 50 begins at 22:54:28.050118\n",
      "\n",
      "Training: batch 50 ends at 22:54:28.845480\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2727 - bce_dice_loss: 0.2727\n",
      "Training: batch 51 begins at 22:54:28.852090\n",
      "\n",
      "Training: batch 51 ends at 22:54:29.639451\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2765 - bce_dice_loss: 0.2765\n",
      "Training: batch 52 begins at 22:54:29.642876\n",
      "\n",
      "Training: batch 52 ends at 22:54:30.426754\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2754 - bce_dice_loss: 0.2754\n",
      "Training: batch 53 begins at 22:54:30.431045\n",
      "\n",
      "Training: batch 53 ends at 22:54:31.229463\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2731 - bce_dice_loss: 0.2731\n",
      "Training: batch 54 begins at 22:54:31.234060\n",
      "\n",
      "Training: batch 54 ends at 22:54:32.023315\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2718 - bce_dice_loss: 0.2718\n",
      "Training: batch 55 begins at 22:54:32.027527\n",
      "\n",
      "Training: batch 55 ends at 22:54:32.837166\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2702 - bce_dice_loss: 0.2702\n",
      "Training: batch 56 begins at 22:54:32.840633\n",
      "\n",
      "Training: batch 56 ends at 22:54:33.635133\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2710 - bce_dice_loss: 0.2710\n",
      "Training: batch 57 begins at 22:54:33.639602\n",
      "\n",
      "Training: batch 57 ends at 22:54:34.434666\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2736 - bce_dice_loss: 0.2736\n",
      "Training: batch 58 begins at 22:54:34.438004\n",
      "\n",
      "Training: batch 58 ends at 22:54:35.232838\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2742 - bce_dice_loss: 0.2742\n",
      "Training: batch 59 begins at 22:54:35.237440\n",
      "\n",
      "Training: batch 59 ends at 22:54:36.045941\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2721 - bce_dice_loss: 0.2721\n",
      "Evaluating: batch 0 begins at 22:54:36.080040\n",
      "\n",
      "Evaluating: batch 0 ends at 22:54:36.357316\n",
      "\n",
      "Evaluating: batch 1 begins at 22:54:36.358923\n",
      "\n",
      "Evaluating: batch 1 ends at 22:54:36.575891\n",
      "\n",
      "Evaluating: batch 2 begins at 22:54:36.577955\n",
      "\n",
      "Evaluating: batch 2 ends at 22:54:36.802724\n",
      "\n",
      "Evaluating: batch 3 begins at 22:54:36.804797\n",
      "\n",
      "Evaluating: batch 3 ends at 22:54:37.030466\n",
      "\n",
      "Evaluating: batch 4 begins at 22:54:37.031812\n",
      "\n",
      "Evaluating: batch 4 ends at 22:54:37.255728\n",
      "\n",
      "Evaluating: batch 5 begins at 22:54:37.257973\n",
      "\n",
      "Evaluating: batch 5 ends at 22:54:37.480007\n",
      "\n",
      "Evaluating: batch 6 begins at 22:54:37.481743\n",
      "\n",
      "Evaluating: batch 6 ends at 22:54:37.706286\n",
      "\n",
      "Evaluating: batch 7 begins at 22:54:37.707671\n",
      "\n",
      "Evaluating: batch 7 ends at 22:54:37.927948\n",
      "\n",
      "Evaluating: batch 8 begins at 22:54:37.930510\n",
      "\n",
      "Evaluating: batch 8 ends at 22:54:38.150300\n",
      "\n",
      "Evaluating: batch 9 begins at 22:54:38.152757\n",
      "\n",
      "Evaluating: batch 9 ends at 22:54:38.372091\n",
      "\n",
      "Evaluating: batch 10 begins at 22:54:38.374630\n",
      "\n",
      "Evaluating: batch 10 ends at 22:54:38.597156\n",
      "\n",
      "Evaluating: batch 11 begins at 22:54:38.598879\n",
      "\n",
      "Evaluating: batch 11 ends at 22:54:38.823442\n",
      "\n",
      "Evaluating: batch 12 begins at 22:54:38.824798\n",
      "\n",
      "Evaluating: batch 12 ends at 22:54:39.043550\n",
      "\n",
      "Evaluating: batch 13 begins at 22:54:39.045933\n",
      "\n",
      "Evaluating: batch 13 ends at 22:54:39.267296\n",
      "\n",
      "Evaluating: batch 14 begins at 22:54:39.269244\n",
      "\n",
      "Evaluating: batch 14 ends at 22:54:39.487991\n",
      "\n",
      "Evaluating: batch 15 begins at 22:54:39.490575\n",
      "\n",
      "Evaluating: batch 15 ends at 22:54:39.716385\n",
      "\n",
      "Evaluating: batch 16 begins at 22:54:39.717756\n",
      "\n",
      "Evaluating: batch 16 ends at 22:54:39.937590\n",
      "\n",
      "Evaluating: batch 17 begins at 22:54:39.939569\n",
      "\n",
      "Evaluating: batch 17 ends at 22:54:40.160854\n",
      "\n",
      "Evaluating: batch 18 begins at 22:54:40.162605\n",
      "\n",
      "Evaluating: batch 18 ends at 22:54:40.383316\n",
      "\n",
      "Evaluating: batch 19 begins at 22:54:40.385951\n",
      "\n",
      "Evaluating: batch 19 ends at 22:54:40.603127\n",
      "\n",
      "Evaluating: batch 20 begins at 22:54:40.605093\n",
      "\n",
      "Evaluating: batch 20 ends at 22:54:40.825389\n",
      "\n",
      "Evaluating: batch 21 begins at 22:54:40.827548\n",
      "\n",
      "Evaluating: batch 21 ends at 22:54:41.044245\n",
      "\n",
      "Evaluating: batch 22 begins at 22:54:41.046804\n",
      "\n",
      "Evaluating: batch 22 ends at 22:54:41.265333\n",
      "\n",
      "Evaluating: batch 23 begins at 22:54:41.267515\n",
      "\n",
      "Evaluating: batch 23 ends at 22:54:41.486412\n",
      "\n",
      "Evaluating: batch 24 begins at 22:54:41.488812\n",
      "\n",
      "Evaluating: batch 24 ends at 22:54:41.709361\n",
      "\n",
      "Evaluating: batch 25 begins at 22:54:41.710633\n",
      "\n",
      "Evaluating: batch 25 ends at 22:54:41.928822\n",
      "\n",
      "Evaluating: batch 26 begins at 22:54:41.931485\n",
      "\n",
      "Evaluating: batch 26 ends at 22:54:42.151654\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.29448 to 0.28038, saving model to ./keras.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 934ms/step - loss: 0.2721 - bce_dice_loss: 0.2721 - val_loss: 0.2804 - val_bce_dice_loss: 0.2804\n",
      "Epoch 16/25\n",
      "\n",
      "Training: batch 0 begins at 22:54:43.798627\n",
      "\n",
      "Training: batch 0 ends at 22:54:44.614261\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2184 - bce_dice_loss: 0.2184\n",
      "Training: batch 1 begins at 22:54:44.622066\n",
      "\n",
      "Training: batch 1 ends at 22:54:45.431455\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.4862 - bce_dice_loss: 0.4862\n",
      "Training: batch 2 begins at 22:54:45.435944\n",
      "\n",
      "Training: batch 2 ends at 22:54:46.234822\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3811 - bce_dice_loss: 0.3811\n",
      "Training: batch 3 begins at 22:54:46.237405\n",
      "\n",
      "Training: batch 3 ends at 22:54:47.040206\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3464 - bce_dice_loss: 0.3464\n",
      "Training: batch 4 begins at 22:54:47.044551\n",
      "\n",
      "Training: batch 4 ends at 22:54:47.843784\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3331 - bce_dice_loss: 0.3331\n",
      "Training: batch 5 begins at 22:54:47.847345\n",
      "\n",
      "Training: batch 5 ends at 22:54:48.642982\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3141 - bce_dice_loss: 0.3141\n",
      "Training: batch 6 begins at 22:54:48.646886\n",
      "\n",
      "Training: batch 6 ends at 22:54:49.457798\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.3165 - bce_dice_loss: 0.3165\n",
      "Training: batch 7 begins at 22:54:49.461083\n",
      "\n",
      "Training: batch 7 ends at 22:54:50.255579\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.3028 - bce_dice_loss: 0.3028\n",
      "Training: batch 8 begins at 22:54:50.259611\n",
      "\n",
      "Training: batch 8 ends at 22:54:51.059934\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2957 - bce_dice_loss: 0.2957\n",
      "Training: batch 9 begins at 22:54:51.062997\n",
      "\n",
      "Training: batch 9 ends at 22:54:51.859686\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2881 - bce_dice_loss: 0.2881\n",
      "Training: batch 10 begins at 22:54:51.864527\n",
      "\n",
      "Training: batch 10 ends at 22:54:52.659269\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2951 - bce_dice_loss: 0.2951\n",
      "Training: batch 11 begins at 22:54:52.661876\n",
      "\n",
      "Training: batch 11 ends at 22:54:53.461208\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2931 - bce_dice_loss: 0.2931\n",
      "Training: batch 12 begins at 22:54:53.465853\n",
      "\n",
      "Training: batch 12 ends at 22:54:54.261351\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2970 - bce_dice_loss: 0.2970\n",
      "Training: batch 13 begins at 22:54:54.265686\n",
      "\n",
      "Training: batch 13 ends at 22:54:55.063806\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2871 - bce_dice_loss: 0.2871\n",
      "Training: batch 14 begins at 22:54:55.067573\n",
      "\n",
      "Training: batch 14 ends at 22:54:55.861842\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2777 - bce_dice_loss: 0.2777\n",
      "Training: batch 15 begins at 22:54:55.864930\n",
      "\n",
      "Training: batch 15 ends at 22:54:56.667325\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2713 - bce_dice_loss: 0.2713\n",
      "Training: batch 16 begins at 22:54:56.670533\n",
      "\n",
      "Training: batch 16 ends at 22:54:57.466715\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2668 - bce_dice_loss: 0.2668\n",
      "Training: batch 17 begins at 22:54:57.469390\n",
      "\n",
      "Training: batch 17 ends at 22:54:58.268787\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2606 - bce_dice_loss: 0.2606\n",
      "Training: batch 18 begins at 22:54:58.272593\n",
      "\n",
      "Training: batch 18 ends at 22:54:59.066775\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2596 - bce_dice_loss: 0.2596\n",
      "Training: batch 19 begins at 22:54:59.069274\n",
      "\n",
      "Training: batch 19 ends at 22:54:59.865131\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2749 - bce_dice_loss: 0.2749\n",
      "Training: batch 20 begins at 22:54:59.868926\n",
      "\n",
      "Training: batch 20 ends at 22:55:00.670242\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2695 - bce_dice_loss: 0.2695\n",
      "Training: batch 21 begins at 22:55:00.673484\n",
      "\n",
      "Training: batch 21 ends at 22:55:01.478745\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2676 - bce_dice_loss: 0.2676\n",
      "Training: batch 22 begins at 22:55:01.484379\n",
      "\n",
      "Training: batch 22 ends at 22:55:02.285576\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2644 - bce_dice_loss: 0.2644\n",
      "Training: batch 23 begins at 22:55:02.288177\n",
      "\n",
      "Training: batch 23 ends at 22:55:03.078313\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2614 - bce_dice_loss: 0.2614\n",
      "Training: batch 24 begins at 22:55:03.083270\n",
      "\n",
      "Training: batch 24 ends at 22:55:03.885443\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2614 - bce_dice_loss: 0.2614\n",
      "Training: batch 25 begins at 22:55:03.889077\n",
      "\n",
      "Training: batch 25 ends at 22:55:04.698326\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2692 - bce_dice_loss: 0.2692\n",
      "Training: batch 26 begins at 22:55:04.703016\n",
      "\n",
      "Training: batch 26 ends at 22:55:05.494189\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2760 - bce_dice_loss: 0.2760\n",
      "Training: batch 27 begins at 22:55:05.497200\n",
      "\n",
      "Training: batch 27 ends at 22:55:06.323499\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2792 - bce_dice_loss: 0.2792\n",
      "Training: batch 28 begins at 22:55:06.327996\n",
      "\n",
      "Training: batch 28 ends at 22:55:07.123883\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2787 - bce_dice_loss: 0.2787\n",
      "Training: batch 29 begins at 22:55:07.128023\n",
      "\n",
      "Training: batch 29 ends at 22:55:07.928199\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2798 - bce_dice_loss: 0.2798\n",
      "Training: batch 30 begins at 22:55:07.934004\n",
      "\n",
      "Training: batch 30 ends at 22:55:08.723062\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2814 - bce_dice_loss: 0.2814\n",
      "Training: batch 31 begins at 22:55:08.727298\n",
      "\n",
      "Training: batch 31 ends at 22:55:09.537132\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2793 - bce_dice_loss: 0.2793\n",
      "Training: batch 32 begins at 22:55:09.540091\n",
      "\n",
      "Training: batch 32 ends at 22:55:10.336045\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2779 - bce_dice_loss: 0.2779\n",
      "Training: batch 33 begins at 22:55:10.340319\n",
      "\n",
      "Training: batch 33 ends at 22:55:11.141896\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2737 - bce_dice_loss: 0.2737\n",
      "Training: batch 34 begins at 22:55:11.149132\n",
      "\n",
      "Training: batch 34 ends at 22:55:11.943705\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2691 - bce_dice_loss: 0.2691\n",
      "Training: batch 35 begins at 22:55:11.948544\n",
      "\n",
      "Training: batch 35 ends at 22:55:12.743328\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2709 - bce_dice_loss: 0.2709\n",
      "Training: batch 36 begins at 22:55:12.748202\n",
      "\n",
      "Training: batch 36 ends at 22:55:13.568964\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2716 - bce_dice_loss: 0.2716\n",
      "Training: batch 37 begins at 22:55:13.572445\n",
      "\n",
      "Training: batch 37 ends at 22:55:14.370262\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2702 - bce_dice_loss: 0.2702\n",
      "Training: batch 38 begins at 22:55:14.373184\n",
      "\n",
      "Training: batch 38 ends at 22:55:15.162052\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2700 - bce_dice_loss: 0.2700\n",
      "Training: batch 39 begins at 22:55:15.169181\n",
      "\n",
      "Training: batch 39 ends at 22:55:15.993105\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2676 - bce_dice_loss: 0.2676\n",
      "Training: batch 40 begins at 22:55:15.997856\n",
      "\n",
      "Training: batch 40 ends at 22:55:16.800736\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2673 - bce_dice_loss: 0.2673\n",
      "Training: batch 41 begins at 22:55:16.805097\n",
      "\n",
      "Training: batch 41 ends at 22:55:17.603762\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2689 - bce_dice_loss: 0.2689\n",
      "Training: batch 42 begins at 22:55:17.607870\n",
      "\n",
      "Training: batch 42 ends at 22:55:18.412385\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2677 - bce_dice_loss: 0.2677\n",
      "Training: batch 43 begins at 22:55:18.418141\n",
      "\n",
      "Training: batch 43 ends at 22:55:19.229675\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2687 - bce_dice_loss: 0.2687\n",
      "Training: batch 44 begins at 22:55:19.233921\n",
      "\n",
      "Training: batch 44 ends at 22:55:20.033713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2670 - bce_dice_loss: 0.2670\n",
      "Training: batch 45 begins at 22:55:20.036707\n",
      "\n",
      "Training: batch 45 ends at 22:55:20.834447\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2655 - bce_dice_loss: 0.2655\n",
      "Training: batch 46 begins at 22:55:20.837730\n",
      "\n",
      "Training: batch 46 ends at 22:55:21.630919\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2647 - bce_dice_loss: 0.2647\n",
      "Training: batch 47 begins at 22:55:21.633768\n",
      "\n",
      "Training: batch 47 ends at 22:55:22.429874\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2632 - bce_dice_loss: 0.2632 \n",
      "Training: batch 48 begins at 22:55:22.433650\n",
      "\n",
      "Training: batch 48 ends at 22:55:23.251787\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2631 - bce_dice_loss: 0.2631\n",
      "Training: batch 49 begins at 22:55:23.254719\n",
      "\n",
      "Training: batch 49 ends at 22:55:24.051625\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2624 - bce_dice_loss: 0.2624\n",
      "Training: batch 50 begins at 22:55:24.055406\n",
      "\n",
      "Training: batch 50 ends at 22:55:24.853869\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2633 - bce_dice_loss: 0.2633\n",
      "Training: batch 51 begins at 22:55:24.858584\n",
      "\n",
      "Training: batch 51 ends at 22:55:25.654518\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2620 - bce_dice_loss: 0.2620\n",
      "Training: batch 52 begins at 22:55:25.658668\n",
      "\n",
      "Training: batch 52 ends at 22:55:26.464721\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2622 - bce_dice_loss: 0.2622\n",
      "Training: batch 53 begins at 22:55:26.469002\n",
      "\n",
      "Training: batch 53 ends at 22:55:27.264994\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2666 - bce_dice_loss: 0.2666\n",
      "Training: batch 54 begins at 22:55:27.269160\n",
      "\n",
      "Training: batch 54 ends at 22:55:28.065442\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2683 - bce_dice_loss: 0.2683\n",
      "Training: batch 55 begins at 22:55:28.070004\n",
      "\n",
      "Training: batch 55 ends at 22:55:28.866382\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2706 - bce_dice_loss: 0.2706\n",
      "Training: batch 56 begins at 22:55:28.869753\n",
      "\n",
      "Training: batch 56 ends at 22:55:29.660550\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2715 - bce_dice_loss: 0.2715\n",
      "Training: batch 57 begins at 22:55:29.666198\n",
      "\n",
      "Training: batch 57 ends at 22:55:30.480737\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2686 - bce_dice_loss: 0.2686\n",
      "Training: batch 58 begins at 22:55:30.485278\n",
      "\n",
      "Training: batch 58 ends at 22:55:31.282623\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2680 - bce_dice_loss: 0.2680\n",
      "Training: batch 59 begins at 22:55:31.286863\n",
      "\n",
      "Training: batch 59 ends at 22:55:32.066936\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2661 - bce_dice_loss: 0.2661\n",
      "Evaluating: batch 0 begins at 22:55:32.096799\n",
      "\n",
      "Evaluating: batch 0 ends at 22:55:32.369849\n",
      "\n",
      "Evaluating: batch 1 begins at 22:55:32.371015\n",
      "\n",
      "Evaluating: batch 1 ends at 22:55:32.584437\n",
      "\n",
      "Evaluating: batch 2 begins at 22:55:32.585665\n",
      "\n",
      "Evaluating: batch 2 ends at 22:55:32.805498\n",
      "\n",
      "Evaluating: batch 3 begins at 22:55:32.808040\n",
      "\n",
      "Evaluating: batch 3 ends at 22:55:33.029539\n",
      "\n",
      "Evaluating: batch 4 begins at 22:55:33.030987\n",
      "\n",
      "Evaluating: batch 4 ends at 22:55:33.252724\n",
      "\n",
      "Evaluating: batch 5 begins at 22:55:33.254198\n",
      "\n",
      "Evaluating: batch 5 ends at 22:55:33.478002\n",
      "\n",
      "Evaluating: batch 6 begins at 22:55:33.482030\n",
      "\n",
      "Evaluating: batch 6 ends at 22:55:33.704069\n",
      "\n",
      "Evaluating: batch 7 begins at 22:55:33.706219\n",
      "\n",
      "Evaluating: batch 7 ends at 22:55:33.929130\n",
      "\n",
      "Evaluating: batch 8 begins at 22:55:33.934613\n",
      "\n",
      "Evaluating: batch 8 ends at 22:55:34.152839\n",
      "\n",
      "Evaluating: batch 9 begins at 22:55:34.155402\n",
      "\n",
      "Evaluating: batch 9 ends at 22:55:34.377562\n",
      "\n",
      "Evaluating: batch 10 begins at 22:55:34.379191\n",
      "\n",
      "Evaluating: batch 10 ends at 22:55:34.598228\n",
      "\n",
      "Evaluating: batch 11 begins at 22:55:34.599996\n",
      "\n",
      "Evaluating: batch 11 ends at 22:55:34.821837\n",
      "\n",
      "Evaluating: batch 12 begins at 22:55:34.823100\n",
      "\n",
      "Evaluating: batch 12 ends at 22:55:35.043805\n",
      "\n",
      "Evaluating: batch 13 begins at 22:55:35.046435\n",
      "\n",
      "Evaluating: batch 13 ends at 22:55:35.267401\n",
      "\n",
      "Evaluating: batch 14 begins at 22:55:35.268963\n",
      "\n",
      "Evaluating: batch 14 ends at 22:55:35.486304\n",
      "\n",
      "Evaluating: batch 15 begins at 22:55:35.487470\n",
      "\n",
      "Evaluating: batch 15 ends at 22:55:35.707586\n",
      "\n",
      "Evaluating: batch 16 begins at 22:55:35.710049\n",
      "\n",
      "Evaluating: batch 16 ends at 22:55:35.932804\n",
      "\n",
      "Evaluating: batch 17 begins at 22:55:35.935267\n",
      "\n",
      "Evaluating: batch 17 ends at 22:55:36.155842\n",
      "\n",
      "Evaluating: batch 18 begins at 22:55:36.158133\n",
      "\n",
      "Evaluating: batch 18 ends at 22:55:36.384495\n",
      "\n",
      "Evaluating: batch 19 begins at 22:55:36.385839\n",
      "\n",
      "Evaluating: batch 19 ends at 22:55:36.601055\n",
      "\n",
      "Evaluating: batch 20 begins at 22:55:36.602255\n",
      "\n",
      "Evaluating: batch 20 ends at 22:55:36.822192\n",
      "\n",
      "Evaluating: batch 21 begins at 22:55:36.823484\n",
      "\n",
      "Evaluating: batch 21 ends at 22:55:37.038553\n",
      "\n",
      "Evaluating: batch 22 begins at 22:55:37.040796\n",
      "\n",
      "Evaluating: batch 22 ends at 22:55:37.258342\n",
      "\n",
      "Evaluating: batch 23 begins at 22:55:37.260814\n",
      "\n",
      "Evaluating: batch 23 ends at 22:55:37.479891\n",
      "\n",
      "Evaluating: batch 24 begins at 22:55:37.481630\n",
      "\n",
      "Evaluating: batch 24 ends at 22:55:37.699730\n",
      "\n",
      "Evaluating: batch 25 begins at 22:55:37.702075\n",
      "\n",
      "Evaluating: batch 25 ends at 22:55:37.919367\n",
      "\n",
      "Evaluating: batch 26 begins at 22:55:37.920722\n",
      "\n",
      "Evaluating: batch 26 ends at 22:55:38.141541\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.28038\n",
      "60/60 [==============================] - 54s 908ms/step - loss: 0.2661 - bce_dice_loss: 0.2661 - val_loss: 0.2818 - val_bce_dice_loss: 0.2818\n",
      "Epoch 17/25\n",
      "\n",
      "Training: batch 0 begins at 22:55:38.171504\n",
      "\n",
      "Training: batch 0 ends at 22:55:38.993376\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.1824 - bce_dice_loss: 0.1824\n",
      "Training: batch 1 begins at 22:55:38.999994\n",
      "\n",
      "Training: batch 1 ends at 22:55:39.808663\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.1620 - bce_dice_loss: 0.1620\n",
      "Training: batch 2 begins at 22:55:39.816399\n",
      "\n",
      "Training: batch 2 ends at 22:55:40.611001\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.1549 - bce_dice_loss: 0.1549\n",
      "Training: batch 3 begins at 22:55:40.619229\n",
      "\n",
      "Training: batch 3 ends at 22:55:41.416295\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2370 - bce_dice_loss: 0.2370\n",
      "Training: batch 4 begins at 22:55:41.421245\n",
      "\n",
      "Training: batch 4 ends at 22:55:42.218858\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2406 - bce_dice_loss: 0.2406\n",
      "Training: batch 5 begins at 22:55:42.222118\n",
      "\n",
      "Training: batch 5 ends at 22:55:43.010087\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2360 - bce_dice_loss: 0.2360\n",
      "Training: batch 6 begins at 22:55:43.014843\n",
      "\n",
      "Training: batch 6 ends at 22:55:43.805400\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2240 - bce_dice_loss: 0.2240\n",
      "Training: batch 7 begins at 22:55:43.809939\n",
      "\n",
      "Training: batch 7 ends at 22:55:44.605927\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2555 - bce_dice_loss: 0.2555\n",
      "Training: batch 8 begins at 22:55:44.610049\n",
      "\n",
      "Training: batch 8 ends at 22:55:45.405960\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2523 - bce_dice_loss: 0.2523\n",
      "Training: batch 9 begins at 22:55:45.410364\n",
      "\n",
      "Training: batch 9 ends at 22:55:46.205120\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2458 - bce_dice_loss: 0.2458\n",
      "Training: batch 10 begins at 22:55:46.209162\n",
      "\n",
      "Training: batch 10 ends at 22:55:47.013803\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2412 - bce_dice_loss: 0.2412\n",
      "Training: batch 11 begins at 22:55:47.022043\n",
      "\n",
      "Training: batch 11 ends at 22:55:47.828296\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2381 - bce_dice_loss: 0.2381\n",
      "Training: batch 12 begins at 22:55:47.832356\n",
      "\n",
      "Training: batch 12 ends at 22:55:48.629149\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2361 - bce_dice_loss: 0.2361\n",
      "Training: batch 13 begins at 22:55:48.632459\n",
      "\n",
      "Training: batch 13 ends at 22:55:49.428539\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2326 - bce_dice_loss: 0.2326\n",
      "Training: batch 14 begins at 22:55:49.432439\n",
      "\n",
      "Training: batch 14 ends at 22:55:50.218916\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2387 - bce_dice_loss: 0.2387\n",
      "Training: batch 15 begins at 22:55:50.221969\n",
      "\n",
      "Training: batch 15 ends at 22:55:51.051209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2385 - bce_dice_loss: 0.2385\n",
      "Training: batch 16 begins at 22:55:51.054610\n",
      "\n",
      "Training: batch 16 ends at 22:55:51.837997\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2432 - bce_dice_loss: 0.2432\n",
      "Training: batch 17 begins at 22:55:51.840313\n",
      "\n",
      "Training: batch 17 ends at 22:55:52.627889\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2426 - bce_dice_loss: 0.2426\n",
      "Training: batch 18 begins at 22:55:52.632656\n",
      "\n",
      "Training: batch 18 ends at 22:55:53.453440\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2538 - bce_dice_loss: 0.2538\n",
      "Training: batch 19 begins at 22:55:53.455893\n",
      "\n",
      "Training: batch 19 ends at 22:55:54.257462\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2562 - bce_dice_loss: 0.2562\n",
      "Training: batch 20 begins at 22:55:54.261944\n",
      "\n",
      "Training: batch 20 ends at 22:55:55.064614\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2712 - bce_dice_loss: 0.2712\n",
      "Training: batch 21 begins at 22:55:55.069072\n",
      "\n",
      "Training: batch 21 ends at 22:55:55.879253\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2689 - bce_dice_loss: 0.2689\n",
      "Training: batch 22 begins at 22:55:55.882004\n",
      "\n",
      "Training: batch 22 ends at 22:55:56.690970\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2620 - bce_dice_loss: 0.2620\n",
      "Training: batch 23 begins at 22:55:56.694390\n",
      "\n",
      "Training: batch 23 ends at 22:55:57.486384\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2578 - bce_dice_loss: 0.2578\n",
      "Training: batch 24 begins at 22:55:57.489887\n",
      "\n",
      "Training: batch 24 ends at 22:55:58.318001\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2605 - bce_dice_loss: 0.2605\n",
      "Training: batch 25 begins at 22:55:58.322113\n",
      "\n",
      "Training: batch 25 ends at 22:55:59.119929\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2569 - bce_dice_loss: 0.2569\n",
      "Training: batch 26 begins at 22:55:59.125128\n",
      "\n",
      "Training: batch 26 ends at 22:55:59.940587\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2685 - bce_dice_loss: 0.2685\n",
      "Training: batch 27 begins at 22:55:59.944888\n",
      "\n",
      "Training: batch 27 ends at 22:56:00.737950\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2672 - bce_dice_loss: 0.2672\n",
      "Training: batch 28 begins at 22:56:00.742106\n",
      "\n",
      "Training: batch 28 ends at 22:56:01.541733\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2674 - bce_dice_loss: 0.2674\n",
      "Training: batch 29 begins at 22:56:01.546398\n",
      "\n",
      "Training: batch 29 ends at 22:56:02.334524\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2656 - bce_dice_loss: 0.2656\n",
      "Training: batch 30 begins at 22:56:02.337876\n",
      "\n",
      "Training: batch 30 ends at 22:56:03.156921\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2685 - bce_dice_loss: 0.2685\n",
      "Training: batch 31 begins at 22:56:03.161427\n",
      "\n",
      "Training: batch 31 ends at 22:56:03.954794\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2674 - bce_dice_loss: 0.2674\n",
      "Training: batch 32 begins at 22:56:03.958972\n",
      "\n",
      "Training: batch 32 ends at 22:56:04.755868\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2672 - bce_dice_loss: 0.2672\n",
      "Training: batch 33 begins at 22:56:04.760354\n",
      "\n",
      "Training: batch 33 ends at 22:56:05.550566\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2661 - bce_dice_loss: 0.2661\n",
      "Training: batch 34 begins at 22:56:05.553230\n",
      "\n",
      "Training: batch 34 ends at 22:56:06.352894\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2690 - bce_dice_loss: 0.2690\n",
      "Training: batch 35 begins at 22:56:06.357373\n",
      "\n",
      "Training: batch 35 ends at 22:56:07.180613\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2674 - bce_dice_loss: 0.2674\n",
      "Training: batch 36 begins at 22:56:07.186395\n",
      "\n",
      "Training: batch 36 ends at 22:56:07.983513\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2654 - bce_dice_loss: 0.2654\n",
      "Training: batch 37 begins at 22:56:07.987891\n",
      "\n",
      "Training: batch 37 ends at 22:56:08.781496\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2643 - bce_dice_loss: 0.2643\n",
      "Training: batch 38 begins at 22:56:08.784956\n",
      "\n",
      "Training: batch 38 ends at 22:56:09.570402\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2637 - bce_dice_loss: 0.2637\n",
      "Training: batch 39 begins at 22:56:09.573464\n",
      "\n",
      "Training: batch 39 ends at 22:56:10.365466\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2622 - bce_dice_loss: 0.2622\n",
      "Training: batch 40 begins at 22:56:10.370365\n",
      "\n",
      "Training: batch 40 ends at 22:56:11.163326\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2604 - bce_dice_loss: 0.2604\n",
      "Training: batch 41 begins at 22:56:11.169879\n",
      "\n",
      "Training: batch 41 ends at 22:56:11.969719\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2645 - bce_dice_loss: 0.2645\n",
      "Training: batch 42 begins at 22:56:11.972463\n",
      "\n",
      "Training: batch 42 ends at 22:56:12.763190\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2615 - bce_dice_loss: 0.2615\n",
      "Training: batch 43 begins at 22:56:12.768991\n",
      "\n",
      "Training: batch 43 ends at 22:56:13.585017\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2604 - bce_dice_loss: 0.2604\n",
      "Training: batch 44 begins at 22:56:13.589300\n",
      "\n",
      "Training: batch 44 ends at 22:56:14.381391\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2596 - bce_dice_loss: 0.2596\n",
      "Training: batch 45 begins at 22:56:14.385554\n",
      "\n",
      "Training: batch 45 ends at 22:56:15.180028\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2610 - bce_dice_loss: 0.2610\n",
      "Training: batch 46 begins at 22:56:15.183237\n",
      "\n",
      "Training: batch 46 ends at 22:56:16.007054\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2626 - bce_dice_loss: 0.2626\n",
      "Training: batch 47 begins at 22:56:16.011469\n",
      "\n",
      "Training: batch 47 ends at 22:56:16.817201\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2639 - bce_dice_loss: 0.2639 \n",
      "Training: batch 48 begins at 22:56:16.820202\n",
      "\n",
      "Training: batch 48 ends at 22:56:17.617641\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2622 - bce_dice_loss: 0.2622\n",
      "Training: batch 49 begins at 22:56:17.621253\n",
      "\n",
      "Training: batch 49 ends at 22:56:18.417326\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2600 - bce_dice_loss: 0.2600\n",
      "Training: batch 50 begins at 22:56:18.420824\n",
      "\n",
      "Training: batch 50 ends at 22:56:19.224498\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2590 - bce_dice_loss: 0.2590\n",
      "Training: batch 51 begins at 22:56:19.229045\n",
      "\n",
      "Training: batch 51 ends at 22:56:20.012111\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2565 - bce_dice_loss: 0.2565\n",
      "Training: batch 52 begins at 22:56:20.018483\n",
      "\n",
      "Training: batch 52 ends at 22:56:20.832404\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2578 - bce_dice_loss: 0.2578\n",
      "Training: batch 53 begins at 22:56:20.836732\n",
      "\n",
      "Training: batch 53 ends at 22:56:21.633801\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2578 - bce_dice_loss: 0.2578\n",
      "Training: batch 54 begins at 22:56:21.636772\n",
      "\n",
      "Training: batch 54 ends at 22:56:22.428142\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2588 - bce_dice_loss: 0.2588\n",
      "Training: batch 55 begins at 22:56:22.432464\n",
      "\n",
      "Training: batch 55 ends at 22:56:23.235041\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2598 - bce_dice_loss: 0.2598\n",
      "Training: batch 56 begins at 22:56:23.238666\n",
      "\n",
      "Training: batch 56 ends at 22:56:24.042924\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2593 - bce_dice_loss: 0.2593\n",
      "Training: batch 57 begins at 22:56:24.048753\n",
      "\n",
      "Training: batch 57 ends at 22:56:24.835622\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2596 - bce_dice_loss: 0.2596\n",
      "Training: batch 58 begins at 22:56:24.839785\n",
      "\n",
      "Training: batch 58 ends at 22:56:25.660019\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2584 - bce_dice_loss: 0.2584\n",
      "Training: batch 59 begins at 22:56:25.663652\n",
      "\n",
      "Training: batch 59 ends at 22:56:26.470472\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2594 - bce_dice_loss: 0.2594\n",
      "Evaluating: batch 0 begins at 22:56:26.505814\n",
      "\n",
      "Evaluating: batch 0 ends at 22:56:26.776583\n",
      "\n",
      "Evaluating: batch 1 begins at 22:56:26.777973\n",
      "\n",
      "Evaluating: batch 1 ends at 22:56:26.992451\n",
      "\n",
      "Evaluating: batch 2 begins at 22:56:26.993691\n",
      "\n",
      "Evaluating: batch 2 ends at 22:56:27.210588\n",
      "\n",
      "Evaluating: batch 3 begins at 22:56:27.215022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 3 ends at 22:56:27.438353\n",
      "\n",
      "Evaluating: batch 4 begins at 22:56:27.440372\n",
      "\n",
      "Evaluating: batch 4 ends at 22:56:27.660512\n",
      "\n",
      "Evaluating: batch 5 begins at 22:56:27.662544\n",
      "\n",
      "Evaluating: batch 5 ends at 22:56:27.884822\n",
      "\n",
      "Evaluating: batch 6 begins at 22:56:27.886405\n",
      "\n",
      "Evaluating: batch 6 ends at 22:56:28.107353\n",
      "\n",
      "Evaluating: batch 7 begins at 22:56:28.109495\n",
      "\n",
      "Evaluating: batch 7 ends at 22:56:28.328311\n",
      "\n",
      "Evaluating: batch 8 begins at 22:56:28.334554\n",
      "\n",
      "Evaluating: batch 8 ends at 22:56:28.557597\n",
      "\n",
      "Evaluating: batch 9 begins at 22:56:28.558956\n",
      "\n",
      "Evaluating: batch 9 ends at 22:56:28.780893\n",
      "\n",
      "Evaluating: batch 10 begins at 22:56:28.782554\n",
      "\n",
      "Evaluating: batch 10 ends at 22:56:29.003299\n",
      "\n",
      "Evaluating: batch 11 begins at 22:56:29.004903\n",
      "\n",
      "Evaluating: batch 11 ends at 22:56:29.231628\n",
      "\n",
      "Evaluating: batch 12 begins at 22:56:29.233966\n",
      "\n",
      "Evaluating: batch 12 ends at 22:56:29.461062\n",
      "\n",
      "Evaluating: batch 13 begins at 22:56:29.462831\n",
      "\n",
      "Evaluating: batch 13 ends at 22:56:29.682829\n",
      "\n",
      "Evaluating: batch 14 begins at 22:56:29.684230\n",
      "\n",
      "Evaluating: batch 14 ends at 22:56:29.910172\n",
      "\n",
      "Evaluating: batch 15 begins at 22:56:29.911489\n",
      "\n",
      "Evaluating: batch 15 ends at 22:56:30.133907\n",
      "\n",
      "Evaluating: batch 16 begins at 22:56:30.135250\n",
      "\n",
      "Evaluating: batch 16 ends at 22:56:30.354514\n",
      "\n",
      "Evaluating: batch 17 begins at 22:56:30.355786\n",
      "\n",
      "Evaluating: batch 17 ends at 22:56:30.579840\n",
      "\n",
      "Evaluating: batch 18 begins at 22:56:30.581654\n",
      "\n",
      "Evaluating: batch 18 ends at 22:56:30.802366\n",
      "\n",
      "Evaluating: batch 19 begins at 22:56:30.804536\n",
      "\n",
      "Evaluating: batch 19 ends at 22:56:31.019561\n",
      "\n",
      "Evaluating: batch 20 begins at 22:56:31.020776\n",
      "\n",
      "Evaluating: batch 20 ends at 22:56:31.239620\n",
      "\n",
      "Evaluating: batch 21 begins at 22:56:31.241956\n",
      "\n",
      "Evaluating: batch 21 ends at 22:56:31.458976\n",
      "\n",
      "Evaluating: batch 22 begins at 22:56:31.460630\n",
      "\n",
      "Evaluating: batch 22 ends at 22:56:31.682424\n",
      "\n",
      "Evaluating: batch 23 begins at 22:56:31.683866\n",
      "\n",
      "Evaluating: batch 23 ends at 22:56:31.902037\n",
      "\n",
      "Evaluating: batch 24 begins at 22:56:31.904523\n",
      "\n",
      "Evaluating: batch 24 ends at 22:56:32.122038\n",
      "\n",
      "Evaluating: batch 25 begins at 22:56:32.124531\n",
      "\n",
      "Evaluating: batch 25 ends at 22:56:32.341898\n",
      "\n",
      "Evaluating: batch 26 begins at 22:56:32.344295\n",
      "\n",
      "Evaluating: batch 26 ends at 22:56:32.560556\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.28038 to 0.26907, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 0.2594 - bce_dice_loss: 0.2594 - val_loss: 0.2691 - val_bce_dice_loss: 0.2691\n",
      "Epoch 18/25\n",
      "\n",
      "Training: batch 0 begins at 22:56:34.208146\n",
      "\n",
      "Training: batch 0 ends at 22:56:35.029405\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.3026 - bce_dice_loss: 0.3026\n",
      "Training: batch 1 begins at 22:56:35.034317\n",
      "\n",
      "Training: batch 1 ends at 22:56:35.843312\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2874 - bce_dice_loss: 0.2874\n",
      "Training: batch 2 begins at 22:56:35.848237\n",
      "\n",
      "Training: batch 2 ends at 22:56:36.646991\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3517 - bce_dice_loss: 0.3517\n",
      "Training: batch 3 begins at 22:56:36.651400\n",
      "\n",
      "Training: batch 3 ends at 22:56:37.452237\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.3115 - bce_dice_loss: 0.3115\n",
      "Training: batch 4 begins at 22:56:37.454731\n",
      "\n",
      "Training: batch 4 ends at 22:56:38.263970\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3168 - bce_dice_loss: 0.3168\n",
      "Training: batch 5 begins at 22:56:38.267180\n",
      "\n",
      "Training: batch 5 ends at 22:56:39.075566\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.3020 - bce_dice_loss: 0.3020\n",
      "Training: batch 6 begins at 22:56:39.080617\n",
      "\n",
      "Training: batch 6 ends at 22:56:39.964319\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.2832 - bce_dice_loss: 0.2832\n",
      "Training: batch 7 begins at 22:56:39.966957\n",
      "\n",
      "Training: batch 7 ends at 22:56:40.851870\n",
      " 8/60 [===>..........................] - ETA: 43s - loss: 0.2993 - bce_dice_loss: 0.2993\n",
      "Training: batch 8 begins at 22:56:40.855444\n",
      "\n",
      "Training: batch 8 ends at 22:56:41.725100\n",
      " 9/60 [===>..........................] - ETA: 42s - loss: 0.2912 - bce_dice_loss: 0.2912\n",
      "Training: batch 9 begins at 22:56:41.728564\n",
      "\n",
      "Training: batch 9 ends at 22:56:42.605823\n",
      "10/60 [====>.........................] - ETA: 42s - loss: 0.2862 - bce_dice_loss: 0.2862\n",
      "Training: batch 10 begins at 22:56:42.609496\n",
      "\n",
      "Training: batch 10 ends at 22:56:43.417940\n",
      "11/60 [====>.........................] - ETA: 41s - loss: 0.2748 - bce_dice_loss: 0.2748\n",
      "Training: batch 11 begins at 22:56:43.420833\n",
      "\n",
      "Training: batch 11 ends at 22:56:44.221956\n",
      "12/60 [=====>........................] - ETA: 40s - loss: 0.2751 - bce_dice_loss: 0.2751\n",
      "Training: batch 12 begins at 22:56:44.225357\n",
      "\n",
      "Training: batch 12 ends at 22:56:45.020590\n",
      "13/60 [=====>........................] - ETA: 39s - loss: 0.2634 - bce_dice_loss: 0.2634\n",
      "Training: batch 13 begins at 22:56:45.023264\n",
      "\n",
      "Training: batch 13 ends at 22:56:45.816855\n",
      "14/60 [======>.......................] - ETA: 38s - loss: 0.2592 - bce_dice_loss: 0.2592\n",
      "Training: batch 14 begins at 22:56:45.819579\n",
      "\n",
      "Training: batch 14 ends at 22:56:46.630549\n",
      "15/60 [======>.......................] - ETA: 37s - loss: 0.2567 - bce_dice_loss: 0.2567\n",
      "Training: batch 15 begins at 22:56:46.634063\n",
      "\n",
      "Training: batch 15 ends at 22:56:47.428479\n",
      "16/60 [=======>......................] - ETA: 36s - loss: 0.2615 - bce_dice_loss: 0.2615\n",
      "Training: batch 16 begins at 22:56:47.431913\n",
      "\n",
      "Training: batch 16 ends at 22:56:48.243001\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.2563 - bce_dice_loss: 0.2563\n",
      "Training: batch 17 begins at 22:56:48.250764\n",
      "\n",
      "Training: batch 17 ends at 22:56:49.046484\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.2514 - bce_dice_loss: 0.2514\n",
      "Training: batch 18 begins at 22:56:49.050723\n",
      "\n",
      "Training: batch 18 ends at 22:56:49.846605\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2460 - bce_dice_loss: 0.2460\n",
      "Training: batch 19 begins at 22:56:49.849075\n",
      "\n",
      "Training: batch 19 ends at 22:56:50.641455\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2462 - bce_dice_loss: 0.2462\n",
      "Training: batch 20 begins at 22:56:50.643779\n",
      "\n",
      "Training: batch 20 ends at 22:56:51.433357\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2414 - bce_dice_loss: 0.2414\n",
      "Training: batch 21 begins at 22:56:51.437726\n",
      "\n",
      "Training: batch 21 ends at 22:56:52.257476\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.2381 - bce_dice_loss: 0.2381\n",
      "Training: batch 22 begins at 22:56:52.260756\n",
      "\n",
      "Training: batch 22 ends at 22:56:53.049610\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.2360 - bce_dice_loss: 0.2360\n",
      "Training: batch 23 begins at 22:56:53.054036\n",
      "\n",
      "Training: batch 23 ends at 22:56:53.854624\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2332 - bce_dice_loss: 0.2332\n",
      "Training: batch 24 begins at 22:56:53.860818\n",
      "\n",
      "Training: batch 24 ends at 22:56:54.671548\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2376 - bce_dice_loss: 0.2376\n",
      "Training: batch 25 begins at 22:56:54.675932\n",
      "\n",
      "Training: batch 25 ends at 22:56:55.472196\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2345 - bce_dice_loss: 0.2345\n",
      "Training: batch 26 begins at 22:56:55.476531\n",
      "\n",
      "Training: batch 26 ends at 22:56:56.265894\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2316 - bce_dice_loss: 0.2316\n",
      "Training: batch 27 begins at 22:56:56.269326\n",
      "\n",
      "Training: batch 27 ends at 22:56:57.076559\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.2327 - bce_dice_loss: 0.2327\n",
      "Training: batch 28 begins at 22:56:57.086202\n",
      "\n",
      "Training: batch 28 ends at 22:56:57.890735\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.2315 - bce_dice_loss: 0.2315\n",
      "Training: batch 29 begins at 22:56:57.893306\n",
      "\n",
      "Training: batch 29 ends at 22:56:58.684638\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2288 - bce_dice_loss: 0.2288\n",
      "Training: batch 30 begins at 22:56:58.688946\n",
      "\n",
      "Training: batch 30 ends at 22:56:59.477500\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2318 - bce_dice_loss: 0.2318\n",
      "Training: batch 31 begins at 22:56:59.481806\n",
      "\n",
      "Training: batch 31 ends at 22:57:00.282027\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2285 - bce_dice_loss: 0.2285\n",
      "Training: batch 32 begins at 22:57:00.287196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 32 ends at 22:57:01.104454\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.2283 - bce_dice_loss: 0.2283\n",
      "Training: batch 33 begins at 22:57:01.109005\n",
      "\n",
      "Training: batch 33 ends at 22:57:01.915907\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.2269 - bce_dice_loss: 0.2269\n",
      "Training: batch 34 begins at 22:57:01.919754\n",
      "\n",
      "Training: batch 34 ends at 22:57:02.740474\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2230 - bce_dice_loss: 0.2230\n",
      "Training: batch 35 begins at 22:57:02.745511\n",
      "\n",
      "Training: batch 35 ends at 22:57:03.539211\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2211 - bce_dice_loss: 0.2211\n",
      "Training: batch 36 begins at 22:57:03.542883\n",
      "\n",
      "Training: batch 36 ends at 22:57:04.346538\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2216 - bce_dice_loss: 0.2216\n",
      "Training: batch 37 begins at 22:57:04.352288\n",
      "\n",
      "Training: batch 37 ends at 22:57:05.143076\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2228 - bce_dice_loss: 0.2228\n",
      "Training: batch 38 begins at 22:57:05.147615\n",
      "\n",
      "Training: batch 38 ends at 22:57:05.941193\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.2220 - bce_dice_loss: 0.2220\n",
      "Training: batch 39 begins at 22:57:05.944831\n",
      "\n",
      "Training: batch 39 ends at 22:57:06.748301\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2324 - bce_dice_loss: 0.2324\n",
      "Training: batch 40 begins at 22:57:06.752643\n",
      "\n",
      "Training: batch 40 ends at 22:57:07.559345\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2356 - bce_dice_loss: 0.2356\n",
      "Training: batch 41 begins at 22:57:07.563955\n",
      "\n",
      "Training: batch 41 ends at 22:57:08.363619\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2366 - bce_dice_loss: 0.2366\n",
      "Training: batch 42 begins at 22:57:08.367253\n",
      "\n",
      "Training: batch 42 ends at 22:57:09.159222\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2378 - bce_dice_loss: 0.2378\n",
      "Training: batch 43 begins at 22:57:09.163841\n",
      "\n",
      "Training: batch 43 ends at 22:57:09.960404\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2356 - bce_dice_loss: 0.2356\n",
      "Training: batch 44 begins at 22:57:09.964661\n",
      "\n",
      "Training: batch 44 ends at 22:57:10.757838\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2332 - bce_dice_loss: 0.2332\n",
      "Training: batch 45 begins at 22:57:10.762265\n",
      "\n",
      "Training: batch 45 ends at 22:57:11.573018\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2364 - bce_dice_loss: 0.2364\n",
      "Training: batch 46 begins at 22:57:11.576889\n",
      "\n",
      "Training: batch 46 ends at 22:57:12.368971\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2366 - bce_dice_loss: 0.2366\n",
      "Training: batch 47 begins at 22:57:12.373047\n",
      "\n",
      "Training: batch 47 ends at 22:57:13.166803\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2357 - bce_dice_loss: 0.2357 \n",
      "Training: batch 48 begins at 22:57:13.169253\n",
      "\n",
      "Training: batch 48 ends at 22:57:13.962021\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2367 - bce_dice_loss: 0.2367\n",
      "Training: batch 49 begins at 22:57:13.966595\n",
      "\n",
      "Training: batch 49 ends at 22:57:14.758260\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2397 - bce_dice_loss: 0.2397\n",
      "Training: batch 50 begins at 22:57:14.763787\n",
      "\n",
      "Training: batch 50 ends at 22:57:15.561008\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2377 - bce_dice_loss: 0.2377\n",
      "Training: batch 51 begins at 22:57:15.566157\n",
      "\n",
      "Training: batch 51 ends at 22:57:16.363642\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2369 - bce_dice_loss: 0.2369\n",
      "Training: batch 52 begins at 22:57:16.367771\n",
      "\n",
      "Training: batch 52 ends at 22:57:17.184438\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2390 - bce_dice_loss: 0.2390\n",
      "Training: batch 53 begins at 22:57:17.186997\n",
      "\n",
      "Training: batch 53 ends at 22:57:17.987862\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2435 - bce_dice_loss: 0.2435\n",
      "Training: batch 54 begins at 22:57:17.992099\n",
      "\n",
      "Training: batch 54 ends at 22:57:18.798995\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2431 - bce_dice_loss: 0.2431\n",
      "Training: batch 55 begins at 22:57:18.802590\n",
      "\n",
      "Training: batch 55 ends at 22:57:19.597755\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2425 - bce_dice_loss: 0.2425\n",
      "Training: batch 56 begins at 22:57:19.602101\n",
      "\n",
      "Training: batch 56 ends at 22:57:20.418793\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2422 - bce_dice_loss: 0.2422\n",
      "Training: batch 57 begins at 22:57:20.422997\n",
      "\n",
      "Training: batch 57 ends at 22:57:21.220766\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2416 - bce_dice_loss: 0.2416\n",
      "Training: batch 58 begins at 22:57:21.225320\n",
      "\n",
      "Training: batch 58 ends at 22:57:22.013049\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2418 - bce_dice_loss: 0.2418\n",
      "Training: batch 59 begins at 22:57:22.016884\n",
      "\n",
      "Training: batch 59 ends at 22:57:22.808277\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2416 - bce_dice_loss: 0.2416\n",
      "Evaluating: batch 0 begins at 22:57:22.840587\n",
      "\n",
      "Evaluating: batch 0 ends at 22:57:23.105142\n",
      "\n",
      "Evaluating: batch 1 begins at 22:57:23.106341\n",
      "\n",
      "Evaluating: batch 1 ends at 22:57:23.319947\n",
      "\n",
      "Evaluating: batch 2 begins at 22:57:23.321229\n",
      "\n",
      "Evaluating: batch 2 ends at 22:57:23.539855\n",
      "\n",
      "Evaluating: batch 3 begins at 22:57:23.541344\n",
      "\n",
      "Evaluating: batch 3 ends at 22:57:23.763599\n",
      "\n",
      "Evaluating: batch 4 begins at 22:57:23.766108\n",
      "\n",
      "Evaluating: batch 4 ends at 22:57:23.983155\n",
      "\n",
      "Evaluating: batch 5 begins at 22:57:23.984403\n",
      "\n",
      "Evaluating: batch 5 ends at 22:57:24.206986\n",
      "\n",
      "Evaluating: batch 6 begins at 22:57:24.209152\n",
      "\n",
      "Evaluating: batch 6 ends at 22:57:24.429108\n",
      "\n",
      "Evaluating: batch 7 begins at 22:57:24.430452\n",
      "\n",
      "Evaluating: batch 7 ends at 22:57:24.650387\n",
      "\n",
      "Evaluating: batch 8 begins at 22:57:24.652733\n",
      "\n",
      "Evaluating: batch 8 ends at 22:57:24.877788\n",
      "\n",
      "Evaluating: batch 9 begins at 22:57:24.879230\n",
      "\n",
      "Evaluating: batch 9 ends at 22:57:25.100992\n",
      "\n",
      "Evaluating: batch 10 begins at 22:57:25.103500\n",
      "\n",
      "Evaluating: batch 10 ends at 22:57:25.329925\n",
      "\n",
      "Evaluating: batch 11 begins at 22:57:25.332372\n",
      "\n",
      "Evaluating: batch 11 ends at 22:57:25.550956\n",
      "\n",
      "Evaluating: batch 12 begins at 22:57:25.552223\n",
      "\n",
      "Evaluating: batch 12 ends at 22:57:25.775334\n",
      "\n",
      "Evaluating: batch 13 begins at 22:57:25.777704\n",
      "\n",
      "Evaluating: batch 13 ends at 22:57:25.999097\n",
      "\n",
      "Evaluating: batch 14 begins at 22:57:26.001167\n",
      "\n",
      "Evaluating: batch 14 ends at 22:57:26.220570\n",
      "\n",
      "Evaluating: batch 15 begins at 22:57:26.222028\n",
      "\n",
      "Evaluating: batch 15 ends at 22:57:26.442758\n",
      "\n",
      "Evaluating: batch 16 begins at 22:57:26.448042\n",
      "\n",
      "Evaluating: batch 16 ends at 22:57:26.674289\n",
      "\n",
      "Evaluating: batch 17 begins at 22:57:26.675886\n",
      "\n",
      "Evaluating: batch 17 ends at 22:57:26.895823\n",
      "\n",
      "Evaluating: batch 18 begins at 22:57:26.898223\n",
      "\n",
      "Evaluating: batch 18 ends at 22:57:27.124961\n",
      "\n",
      "Evaluating: batch 19 begins at 22:57:27.126317\n",
      "\n",
      "Evaluating: batch 19 ends at 22:57:27.343915\n",
      "\n",
      "Evaluating: batch 20 begins at 22:57:27.345183\n",
      "\n",
      "Evaluating: batch 20 ends at 22:57:27.560605\n",
      "\n",
      "Evaluating: batch 21 begins at 22:57:27.561957\n",
      "\n",
      "Evaluating: batch 21 ends at 22:57:27.784368\n",
      "\n",
      "Evaluating: batch 22 begins at 22:57:27.786094\n",
      "\n",
      "Evaluating: batch 22 ends at 22:57:28.000927\n",
      "\n",
      "Evaluating: batch 23 begins at 22:57:28.002197\n",
      "\n",
      "Evaluating: batch 23 ends at 22:57:28.218189\n",
      "\n",
      "Evaluating: batch 24 begins at 22:57:28.219657\n",
      "\n",
      "Evaluating: batch 24 ends at 22:57:28.439556\n",
      "\n",
      "Evaluating: batch 25 begins at 22:57:28.441942\n",
      "\n",
      "Evaluating: batch 25 ends at 22:57:28.660126\n",
      "\n",
      "Evaluating: batch 26 begins at 22:57:28.665477\n",
      "\n",
      "Evaluating: batch 26 ends at 22:57:28.884564\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.26907\n",
      "60/60 [==============================] - 55s 913ms/step - loss: 0.2416 - bce_dice_loss: 0.2416 - val_loss: 0.2783 - val_bce_dice_loss: 0.2783\n",
      "Epoch 19/25\n",
      "\n",
      "Training: batch 0 begins at 22:57:28.912712\n",
      "\n",
      "Training: batch 0 ends at 22:57:29.714793\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.4710 - bce_dice_loss: 0.4710\n",
      "Training: batch 1 begins at 22:57:29.719575\n",
      "\n",
      "Training: batch 1 ends at 22:57:30.519371\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.3463 - bce_dice_loss: 0.3463\n",
      "Training: batch 2 begins at 22:57:30.523731\n",
      "\n",
      "Training: batch 2 ends at 22:57:31.318878\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2878 - bce_dice_loss: 0.2878\n",
      "Training: batch 3 begins at 22:57:31.321755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 3 ends at 22:57:32.119803\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.2923 - bce_dice_loss: 0.2923\n",
      "Training: batch 4 begins at 22:57:32.124028\n",
      "\n",
      "Training: batch 4 ends at 22:57:32.920244\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.3062 - bce_dice_loss: 0.3062\n",
      "Training: batch 5 begins at 22:57:32.924976\n",
      "\n",
      "Training: batch 5 ends at 22:57:33.721178\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2767 - bce_dice_loss: 0.2767\n",
      "Training: batch 6 begins at 22:57:33.724028\n",
      "\n",
      "Training: batch 6 ends at 22:57:34.515473\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2737 - bce_dice_loss: 0.2737\n",
      "Training: batch 7 begins at 22:57:34.518483\n",
      "\n",
      "Training: batch 7 ends at 22:57:35.313276\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2672 - bce_dice_loss: 0.2672\n",
      "Training: batch 8 begins at 22:57:35.316179\n",
      "\n",
      "Training: batch 8 ends at 22:57:36.114165\n",
      " 9/60 [===>..........................] - ETA: 40s - loss: 0.2645 - bce_dice_loss: 0.2645\n",
      "Training: batch 9 begins at 22:57:36.118103\n",
      "\n",
      "Training: batch 9 ends at 22:57:36.943790\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2517 - bce_dice_loss: 0.2517\n",
      "Training: batch 10 begins at 22:57:36.946801\n",
      "\n",
      "Training: batch 10 ends at 22:57:37.752109\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2745 - bce_dice_loss: 0.2745\n",
      "Training: batch 11 begins at 22:57:37.755655\n",
      "\n",
      "Training: batch 11 ends at 22:57:38.551347\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2796 - bce_dice_loss: 0.2796\n",
      "Training: batch 12 begins at 22:57:38.554257\n",
      "\n",
      "Training: batch 12 ends at 22:57:39.351709\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2793 - bce_dice_loss: 0.2793\n",
      "Training: batch 13 begins at 22:57:39.356348\n",
      "\n",
      "Training: batch 13 ends at 22:57:40.158970\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2711 - bce_dice_loss: 0.2711\n",
      "Training: batch 14 begins at 22:57:40.161412\n",
      "\n",
      "Training: batch 14 ends at 22:57:40.955566\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2689 - bce_dice_loss: 0.2689\n",
      "Training: batch 15 begins at 22:57:40.960996\n",
      "\n",
      "Training: batch 15 ends at 22:57:41.758502\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2664 - bce_dice_loss: 0.2664\n",
      "Training: batch 16 begins at 22:57:41.765742\n",
      "\n",
      "Training: batch 16 ends at 22:57:42.579526\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2604 - bce_dice_loss: 0.2604\n",
      "Training: batch 17 begins at 22:57:42.583587\n",
      "\n",
      "Training: batch 17 ends at 22:57:43.377642\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2825 - bce_dice_loss: 0.2825\n",
      "Training: batch 18 begins at 22:57:43.380051\n",
      "\n",
      "Training: batch 18 ends at 22:57:44.172235\n",
      "19/60 [========>.....................] - ETA: 32s - loss: 0.2812 - bce_dice_loss: 0.2812\n",
      "Training: batch 19 begins at 22:57:44.176230\n",
      "\n",
      "Training: batch 19 ends at 22:57:44.985572\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2745 - bce_dice_loss: 0.2745\n",
      "Training: batch 20 begins at 22:57:44.989704\n",
      "\n",
      "Training: batch 20 ends at 22:57:45.787176\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2708 - bce_dice_loss: 0.2708\n",
      "Training: batch 21 begins at 22:57:45.790498\n",
      "\n",
      "Training: batch 21 ends at 22:57:46.585980\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2693 - bce_dice_loss: 0.2693\n",
      "Training: batch 22 begins at 22:57:46.589640\n",
      "\n",
      "Training: batch 22 ends at 22:57:47.407842\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2668 - bce_dice_loss: 0.2668\n",
      "Training: batch 23 begins at 22:57:47.411489\n",
      "\n",
      "Training: batch 23 ends at 22:57:48.215923\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2704 - bce_dice_loss: 0.2704\n",
      "Training: batch 24 begins at 22:57:48.220233\n",
      "\n",
      "Training: batch 24 ends at 22:57:49.027588\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2672 - bce_dice_loss: 0.2672\n",
      "Training: batch 25 begins at 22:57:49.031046\n",
      "\n",
      "Training: batch 25 ends at 22:57:49.827713\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2678 - bce_dice_loss: 0.2678\n",
      "Training: batch 26 begins at 22:57:49.831980\n",
      "\n",
      "Training: batch 26 ends at 22:57:50.627531\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2712 - bce_dice_loss: 0.2712\n",
      "Training: batch 27 begins at 22:57:50.631040\n",
      "\n",
      "Training: batch 27 ends at 22:57:51.428111\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2740 - bce_dice_loss: 0.2740\n",
      "Training: batch 28 begins at 22:57:51.432926\n",
      "\n",
      "Training: batch 28 ends at 22:57:52.220890\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2707 - bce_dice_loss: 0.2707\n",
      "Training: batch 29 begins at 22:57:52.224172\n",
      "\n",
      "Training: batch 29 ends at 22:57:53.034272\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2681 - bce_dice_loss: 0.2681\n",
      "Training: batch 30 begins at 22:57:53.039380\n",
      "\n",
      "Training: batch 30 ends at 22:57:53.836849\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2645 - bce_dice_loss: 0.2645\n",
      "Training: batch 31 begins at 22:57:53.841822\n",
      "\n",
      "Training: batch 31 ends at 22:57:54.646370\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2595 - bce_dice_loss: 0.2595\n",
      "Training: batch 32 begins at 22:57:54.647563\n",
      "\n",
      "Training: batch 32 ends at 22:57:55.433658\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2575 - bce_dice_loss: 0.2575\n",
      "Training: batch 33 begins at 22:57:55.436587\n",
      "\n",
      "Training: batch 33 ends at 22:57:56.238939\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2627 - bce_dice_loss: 0.2627\n",
      "Training: batch 34 begins at 22:57:56.242601\n",
      "\n",
      "Training: batch 34 ends at 22:57:57.064403\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2602 - bce_dice_loss: 0.2602\n",
      "Training: batch 35 begins at 22:57:57.068626\n",
      "\n",
      "Training: batch 35 ends at 22:57:57.856371\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2604 - bce_dice_loss: 0.2604\n",
      "Training: batch 36 begins at 22:57:57.860635\n",
      "\n",
      "Training: batch 36 ends at 22:57:58.677363\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2601 - bce_dice_loss: 0.2601\n",
      "Training: batch 37 begins at 22:57:58.681617\n",
      "\n",
      "Training: batch 37 ends at 22:57:59.468987\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2628 - bce_dice_loss: 0.2628\n",
      "Training: batch 38 begins at 22:57:59.473510\n",
      "\n",
      "Training: batch 38 ends at 22:58:00.281086\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2610 - bce_dice_loss: 0.2610\n",
      "Training: batch 39 begins at 22:58:00.283532\n",
      "\n",
      "Training: batch 39 ends at 22:58:01.076591\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2600 - bce_dice_loss: 0.2600\n",
      "Training: batch 40 begins at 22:58:01.079898\n",
      "\n",
      "Training: batch 40 ends at 22:58:01.882728\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2586 - bce_dice_loss: 0.2586\n",
      "Training: batch 41 begins at 22:58:01.886811\n",
      "\n",
      "Training: batch 41 ends at 22:58:02.684031\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2572 - bce_dice_loss: 0.2572\n",
      "Training: batch 42 begins at 22:58:02.688087\n",
      "\n",
      "Training: batch 42 ends at 22:58:03.493717\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2574 - bce_dice_loss: 0.2574\n",
      "Training: batch 43 begins at 22:58:03.497864\n",
      "\n",
      "Training: batch 43 ends at 22:58:04.296823\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2594 - bce_dice_loss: 0.2594\n",
      "Training: batch 44 begins at 22:58:04.300309\n",
      "\n",
      "Training: batch 44 ends at 22:58:05.096726\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2639 - bce_dice_loss: 0.2639\n",
      "Training: batch 45 begins at 22:58:05.100919\n",
      "\n",
      "Training: batch 45 ends at 22:58:05.910320\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2619 - bce_dice_loss: 0.2619\n",
      "Training: batch 46 begins at 22:58:05.913366\n",
      "\n",
      "Training: batch 46 ends at 22:58:06.713314\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2607 - bce_dice_loss: 0.2607\n",
      "Training: batch 47 begins at 22:58:06.716374\n",
      "\n",
      "Training: batch 47 ends at 22:58:07.513491\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2626 - bce_dice_loss: 0.2626 \n",
      "Training: batch 48 begins at 22:58:07.518035\n",
      "\n",
      "Training: batch 48 ends at 22:58:08.309220\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2634 - bce_dice_loss: 0.2634\n",
      "Training: batch 49 begins at 22:58:08.312143\n",
      "\n",
      "Training: batch 49 ends at 22:58:09.129480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2638 - bce_dice_loss: 0.2638\n",
      "Training: batch 50 begins at 22:58:09.132831\n",
      "\n",
      "Training: batch 50 ends at 22:58:09.928536\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2641 - bce_dice_loss: 0.2641\n",
      "Training: batch 51 begins at 22:58:09.932607\n",
      "\n",
      "Training: batch 51 ends at 22:58:10.757585\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2635 - bce_dice_loss: 0.2635\n",
      "Training: batch 52 begins at 22:58:10.763704\n",
      "\n",
      "Training: batch 52 ends at 22:58:11.559877\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2625 - bce_dice_loss: 0.2625\n",
      "Training: batch 53 begins at 22:58:11.564629\n",
      "\n",
      "Training: batch 53 ends at 22:58:12.345543\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2630 - bce_dice_loss: 0.2630\n",
      "Training: batch 54 begins at 22:58:12.347869\n",
      "\n",
      "Training: batch 54 ends at 22:58:13.132127\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2630 - bce_dice_loss: 0.2630\n",
      "Training: batch 55 begins at 22:58:13.135063\n",
      "\n",
      "Training: batch 55 ends at 22:58:13.932448\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2622 - bce_dice_loss: 0.2622\n",
      "Training: batch 56 begins at 22:58:13.936615\n",
      "\n",
      "Training: batch 56 ends at 22:58:14.763448\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2600 - bce_dice_loss: 0.2600\n",
      "Training: batch 57 begins at 22:58:14.767151\n",
      "\n",
      "Training: batch 57 ends at 22:58:15.562403\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2591 - bce_dice_loss: 0.2591\n",
      "Training: batch 58 begins at 22:58:15.566150\n",
      "\n",
      "Training: batch 58 ends at 22:58:16.363182\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2583 - bce_dice_loss: 0.2583\n",
      "Training: batch 59 begins at 22:58:16.367281\n",
      "\n",
      "Training: batch 59 ends at 22:58:17.166103\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2571 - bce_dice_loss: 0.2571\n",
      "Evaluating: batch 0 begins at 22:58:17.196059\n",
      "\n",
      "Evaluating: batch 0 ends at 22:58:17.467237\n",
      "\n",
      "Evaluating: batch 1 begins at 22:58:17.468411\n",
      "\n",
      "Evaluating: batch 1 ends at 22:58:17.682133\n",
      "\n",
      "Evaluating: batch 2 begins at 22:58:17.683353\n",
      "\n",
      "Evaluating: batch 2 ends at 22:58:17.904922\n",
      "\n",
      "Evaluating: batch 3 begins at 22:58:17.906995\n",
      "\n",
      "Evaluating: batch 3 ends at 22:58:18.128228\n",
      "\n",
      "Evaluating: batch 4 begins at 22:58:18.130767\n",
      "\n",
      "Evaluating: batch 4 ends at 22:58:18.350113\n",
      "\n",
      "Evaluating: batch 5 begins at 22:58:18.352638\n",
      "\n",
      "Evaluating: batch 5 ends at 22:58:18.573558\n",
      "\n",
      "Evaluating: batch 6 begins at 22:58:18.575398\n",
      "\n",
      "Evaluating: batch 6 ends at 22:58:18.798859\n",
      "\n",
      "Evaluating: batch 7 begins at 22:58:18.800279\n",
      "\n",
      "Evaluating: batch 7 ends at 22:58:19.022835\n",
      "\n",
      "Evaluating: batch 8 begins at 22:58:19.024607\n",
      "\n",
      "Evaluating: batch 8 ends at 22:58:19.246633\n",
      "\n",
      "Evaluating: batch 9 begins at 22:58:19.248635\n",
      "\n",
      "Evaluating: batch 9 ends at 22:58:19.467956\n",
      "\n",
      "Evaluating: batch 10 begins at 22:58:19.469591\n",
      "\n",
      "Evaluating: batch 10 ends at 22:58:19.690075\n",
      "\n",
      "Evaluating: batch 11 begins at 22:58:19.692066\n",
      "\n",
      "Evaluating: batch 11 ends at 22:58:19.913529\n",
      "\n",
      "Evaluating: batch 12 begins at 22:58:19.914995\n",
      "\n",
      "Evaluating: batch 12 ends at 22:58:20.131955\n",
      "\n",
      "Evaluating: batch 13 begins at 22:58:20.134488\n",
      "\n",
      "Evaluating: batch 13 ends at 22:58:20.355693\n",
      "\n",
      "Evaluating: batch 14 begins at 22:58:20.357466\n",
      "\n",
      "Evaluating: batch 14 ends at 22:58:20.577260\n",
      "\n",
      "Evaluating: batch 15 begins at 22:58:20.579769\n",
      "\n",
      "Evaluating: batch 15 ends at 22:58:20.800151\n",
      "\n",
      "Evaluating: batch 16 begins at 22:58:20.802544\n",
      "\n",
      "Evaluating: batch 16 ends at 22:58:21.025021\n",
      "\n",
      "Evaluating: batch 17 begins at 22:58:21.026689\n",
      "\n",
      "Evaluating: batch 17 ends at 22:58:21.251561\n",
      "\n",
      "Evaluating: batch 18 begins at 22:58:21.253571\n",
      "\n",
      "Evaluating: batch 18 ends at 22:58:21.478375\n",
      "\n",
      "Evaluating: batch 19 begins at 22:58:21.479662\n",
      "\n",
      "Evaluating: batch 19 ends at 22:58:21.698299\n",
      "\n",
      "Evaluating: batch 20 begins at 22:58:21.700790\n",
      "\n",
      "Evaluating: batch 20 ends at 22:58:21.923346\n",
      "\n",
      "Evaluating: batch 21 begins at 22:58:21.925051\n",
      "\n",
      "Evaluating: batch 21 ends at 22:58:22.141480\n",
      "\n",
      "Evaluating: batch 22 begins at 22:58:22.143184\n",
      "\n",
      "Evaluating: batch 22 ends at 22:58:22.360553\n",
      "\n",
      "Evaluating: batch 23 begins at 22:58:22.361812\n",
      "\n",
      "Evaluating: batch 23 ends at 22:58:22.579506\n",
      "\n",
      "Evaluating: batch 24 begins at 22:58:22.580818\n",
      "\n",
      "Evaluating: batch 24 ends at 22:58:22.801292\n",
      "\n",
      "Evaluating: batch 25 begins at 22:58:22.803097\n",
      "\n",
      "Evaluating: batch 25 ends at 22:58:23.024270\n",
      "\n",
      "Evaluating: batch 26 begins at 22:58:23.026788\n",
      "\n",
      "Evaluating: batch 26 ends at 22:58:23.245600\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.26907\n",
      "60/60 [==============================] - 54s 908ms/step - loss: 0.2571 - bce_dice_loss: 0.2571 - val_loss: 0.2965 - val_bce_dice_loss: 0.2965\n",
      "Epoch 20/25\n",
      "\n",
      "Training: batch 0 begins at 22:58:23.274935\n",
      "\n",
      "Training: batch 0 ends at 22:58:24.093113\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2187 - bce_dice_loss: 0.2187\n",
      "Training: batch 1 begins at 22:58:24.100308\n",
      "\n",
      "Training: batch 1 ends at 22:58:24.912530\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.3543 - bce_dice_loss: 0.3543\n",
      "Training: batch 2 begins at 22:58:24.916847\n",
      "\n",
      "Training: batch 2 ends at 22:58:25.718028\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.3097 - bce_dice_loss: 0.3097\n",
      "Training: batch 3 begins at 22:58:25.720635\n",
      "\n",
      "Training: batch 3 ends at 22:58:26.513615\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2979 - bce_dice_loss: 0.2979\n",
      "Training: batch 4 begins at 22:58:26.516143\n",
      "\n",
      "Training: batch 4 ends at 22:58:27.325583\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2937 - bce_dice_loss: 0.2937\n",
      "Training: batch 5 begins at 22:58:27.329899\n",
      "\n",
      "Training: batch 5 ends at 22:58:28.148482\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2775 - bce_dice_loss: 0.2775\n",
      "Training: batch 6 begins at 22:58:28.152549\n",
      "\n",
      "Training: batch 6 ends at 22:58:28.951563\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2686 - bce_dice_loss: 0.2686\n",
      "Training: batch 7 begins at 22:58:28.955949\n",
      "\n",
      "Training: batch 7 ends at 22:58:29.750674\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2549 - bce_dice_loss: 0.2549\n",
      "Training: batch 8 begins at 22:58:29.754723\n",
      "\n",
      "Training: batch 8 ends at 22:58:30.550176\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 9 begins at 22:58:30.553200\n",
      "\n",
      "Training: batch 9 ends at 22:58:31.334951\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2515 - bce_dice_loss: 0.2515\n",
      "Training: batch 10 begins at 22:58:31.338879\n",
      "\n",
      "Training: batch 10 ends at 22:58:32.137917\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2566 - bce_dice_loss: 0.2566\n",
      "Training: batch 11 begins at 22:58:32.142807\n",
      "\n",
      "Training: batch 11 ends at 22:58:32.962666\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2532 - bce_dice_loss: 0.2532\n",
      "Training: batch 12 begins at 22:58:32.966886\n",
      "\n",
      "Training: batch 12 ends at 22:58:33.761322\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2523 - bce_dice_loss: 0.2523\n",
      "Training: batch 13 begins at 22:58:33.765519\n",
      "\n",
      "Training: batch 13 ends at 22:58:34.549984\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2516 - bce_dice_loss: 0.2516\n",
      "Training: batch 14 begins at 22:58:34.554083\n",
      "\n",
      "Training: batch 14 ends at 22:58:35.356855\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2419 - bce_dice_loss: 0.2419\n",
      "Training: batch 15 begins at 22:58:35.360606\n",
      "\n",
      "Training: batch 15 ends at 22:58:36.185036\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2406 - bce_dice_loss: 0.2406\n",
      "Training: batch 16 begins at 22:58:36.187492\n",
      "\n",
      "Training: batch 16 ends at 22:58:36.996084\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2437 - bce_dice_loss: 0.2437\n",
      "Training: batch 17 begins at 22:58:36.999484\n",
      "\n",
      "Training: batch 17 ends at 22:58:37.806157\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2430 - bce_dice_loss: 0.2430\n",
      "Training: batch 18 begins at 22:58:37.813184\n",
      "\n",
      "Training: batch 18 ends at 22:58:38.628604\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2417 - bce_dice_loss: 0.2417\n",
      "Training: batch 19 begins at 22:58:38.632265\n",
      "\n",
      "Training: batch 19 ends at 22:58:39.431001\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2392 - bce_dice_loss: 0.2392\n",
      "Training: batch 20 begins at 22:58:39.435546\n",
      "\n",
      "Training: batch 20 ends at 22:58:40.231719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2356 - bce_dice_loss: 0.2356\n",
      "Training: batch 21 begins at 22:58:40.235011\n",
      "\n",
      "Training: batch 21 ends at 22:58:41.031923\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2420 - bce_dice_loss: 0.2420\n",
      "Training: batch 22 begins at 22:58:41.034496\n",
      "\n",
      "Training: batch 22 ends at 22:58:41.828363\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2468 - bce_dice_loss: 0.2468\n",
      "Training: batch 23 begins at 22:58:41.831309\n",
      "\n",
      "Training: batch 23 ends at 22:58:42.636328\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2451 - bce_dice_loss: 0.2451\n",
      "Training: batch 24 begins at 22:58:42.640626\n",
      "\n",
      "Training: batch 24 ends at 22:58:43.437497\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2402 - bce_dice_loss: 0.2402\n",
      "Training: batch 25 begins at 22:58:43.441752\n",
      "\n",
      "Training: batch 25 ends at 22:58:44.234777\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2486 - bce_dice_loss: 0.2486\n",
      "Training: batch 26 begins at 22:58:44.237552\n",
      "\n",
      "Training: batch 26 ends at 22:58:45.032230\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2497 - bce_dice_loss: 0.2497\n",
      "Training: batch 27 begins at 22:58:45.034887\n",
      "\n",
      "Training: batch 27 ends at 22:58:45.825291\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2456 - bce_dice_loss: 0.2456\n",
      "Training: batch 28 begins at 22:58:45.828054\n",
      "\n",
      "Training: batch 28 ends at 22:58:46.621629\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2415 - bce_dice_loss: 0.2415\n",
      "Training: batch 29 begins at 22:58:46.624684\n",
      "\n",
      "Training: batch 29 ends at 22:58:47.426077\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2433 - bce_dice_loss: 0.2433\n",
      "Training: batch 30 begins at 22:58:47.428934\n",
      "\n",
      "Training: batch 30 ends at 22:58:48.241572\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2423 - bce_dice_loss: 0.2423\n",
      "Training: batch 31 begins at 22:58:48.247503\n",
      "\n",
      "Training: batch 31 ends at 22:58:49.044362\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2407 - bce_dice_loss: 0.2407\n",
      "Training: batch 32 begins at 22:58:49.048608\n",
      "\n",
      "Training: batch 32 ends at 22:58:49.846742\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2406 - bce_dice_loss: 0.2406\n",
      "Training: batch 33 begins at 22:58:49.850081\n",
      "\n",
      "Training: batch 33 ends at 22:58:50.649031\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2371 - bce_dice_loss: 0.2371\n",
      "Training: batch 34 begins at 22:58:50.653912\n",
      "\n",
      "Training: batch 34 ends at 22:58:51.458332\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2354 - bce_dice_loss: 0.2354\n",
      "Training: batch 35 begins at 22:58:51.461172\n",
      "\n",
      "Training: batch 35 ends at 22:58:52.257437\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2371 - bce_dice_loss: 0.2371\n",
      "Training: batch 36 begins at 22:58:52.261116\n",
      "\n",
      "Training: batch 36 ends at 22:58:53.057184\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2391 - bce_dice_loss: 0.2391\n",
      "Training: batch 37 begins at 22:58:53.061767\n",
      "\n",
      "Training: batch 37 ends at 22:58:53.857035\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2375 - bce_dice_loss: 0.2375\n",
      "Training: batch 38 begins at 22:58:53.859970\n",
      "\n",
      "Training: batch 38 ends at 22:58:54.666131\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2338 - bce_dice_loss: 0.2338\n",
      "Training: batch 39 begins at 22:58:54.670662\n",
      "\n",
      "Training: batch 39 ends at 22:58:55.468294\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2377 - bce_dice_loss: 0.2377\n",
      "Training: batch 40 begins at 22:58:55.472367\n",
      "\n",
      "Training: batch 40 ends at 22:58:56.290774\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2363 - bce_dice_loss: 0.2363\n",
      "Training: batch 41 begins at 22:58:56.295855\n",
      "\n",
      "Training: batch 41 ends at 22:58:57.091154\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2358 - bce_dice_loss: 0.2358\n",
      "Training: batch 42 begins at 22:58:57.094494\n",
      "\n",
      "Training: batch 42 ends at 22:58:57.891373\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2344 - bce_dice_loss: 0.2344\n",
      "Training: batch 43 begins at 22:58:57.895565\n",
      "\n",
      "Training: batch 43 ends at 22:58:58.726349\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2346 - bce_dice_loss: 0.2346\n",
      "Training: batch 44 begins at 22:58:58.729362\n",
      "\n",
      "Training: batch 44 ends at 22:58:59.524145\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2436 - bce_dice_loss: 0.2436\n",
      "Training: batch 45 begins at 22:58:59.528477\n",
      "\n",
      "Training: batch 45 ends at 22:59:00.326910\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2415 - bce_dice_loss: 0.2415\n",
      "Training: batch 46 begins at 22:59:00.330541\n",
      "\n",
      "Training: batch 46 ends at 22:59:01.122888\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2403 - bce_dice_loss: 0.2403\n",
      "Training: batch 47 begins at 22:59:01.127522\n",
      "\n",
      "Training: batch 47 ends at 22:59:01.931745\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2468 - bce_dice_loss: 0.2468 \n",
      "Training: batch 48 begins at 22:59:01.935891\n",
      "\n",
      "Training: batch 48 ends at 22:59:02.738586\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2479 - bce_dice_loss: 0.2479\n",
      "Training: batch 49 begins at 22:59:02.745606\n",
      "\n",
      "Training: batch 49 ends at 22:59:03.556841\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2449 - bce_dice_loss: 0.2449\n",
      "Training: batch 50 begins at 22:59:03.557754\n",
      "\n",
      "Training: batch 50 ends at 22:59:04.346453\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2429 - bce_dice_loss: 0.2429\n",
      "Training: batch 51 begins at 22:59:04.351659\n",
      "\n",
      "Training: batch 51 ends at 22:59:05.130746\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2441 - bce_dice_loss: 0.2441\n",
      "Training: batch 52 begins at 22:59:05.133234\n",
      "\n",
      "Training: batch 52 ends at 22:59:05.945238\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2429 - bce_dice_loss: 0.2429\n",
      "Training: batch 53 begins at 22:59:05.949070\n",
      "\n",
      "Training: batch 53 ends at 22:59:06.746810\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2453 - bce_dice_loss: 0.2453\n",
      "Training: batch 54 begins at 22:59:06.750623\n",
      "\n",
      "Training: batch 54 ends at 22:59:07.553706\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2475 - bce_dice_loss: 0.2475\n",
      "Training: batch 55 begins at 22:59:07.557572\n",
      "\n",
      "Training: batch 55 ends at 22:59:08.354757\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2456 - bce_dice_loss: 0.2456\n",
      "Training: batch 56 begins at 22:59:08.357872\n",
      "\n",
      "Training: batch 56 ends at 22:59:09.160223\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2449 - bce_dice_loss: 0.2449\n",
      "Training: batch 57 begins at 22:59:09.164390\n",
      "\n",
      "Training: batch 57 ends at 22:59:09.978140\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2441 - bce_dice_loss: 0.2441\n",
      "Training: batch 58 begins at 22:59:09.981558\n",
      "\n",
      "Training: batch 58 ends at 22:59:10.781641\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2439 - bce_dice_loss: 0.2439\n",
      "Training: batch 59 begins at 22:59:10.785124\n",
      "\n",
      "Training: batch 59 ends at 22:59:11.585272\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2449 - bce_dice_loss: 0.2449\n",
      "Evaluating: batch 0 begins at 22:59:11.617676\n",
      "\n",
      "Evaluating: batch 0 ends at 22:59:11.885128\n",
      "\n",
      "Evaluating: batch 1 begins at 22:59:11.886722\n",
      "\n",
      "Evaluating: batch 1 ends at 22:59:12.099721\n",
      "\n",
      "Evaluating: batch 2 begins at 22:59:12.100961\n",
      "\n",
      "Evaluating: batch 2 ends at 22:59:12.318560\n",
      "\n",
      "Evaluating: batch 3 begins at 22:59:12.320951\n",
      "\n",
      "Evaluating: batch 3 ends at 22:59:12.545225\n",
      "\n",
      "Evaluating: batch 4 begins at 22:59:12.546621\n",
      "\n",
      "Evaluating: batch 4 ends at 22:59:12.769714\n",
      "\n",
      "Evaluating: batch 5 begins at 22:59:12.772186\n",
      "\n",
      "Evaluating: batch 5 ends at 22:59:12.992822\n",
      "\n",
      "Evaluating: batch 6 begins at 22:59:12.994080\n",
      "\n",
      "Evaluating: batch 6 ends at 22:59:13.216478\n",
      "\n",
      "Evaluating: batch 7 begins at 22:59:13.218869\n",
      "\n",
      "Evaluating: batch 7 ends at 22:59:13.444704\n",
      "\n",
      "Evaluating: batch 8 begins at 22:59:13.446101\n",
      "\n",
      "Evaluating: batch 8 ends at 22:59:13.667615\n",
      "\n",
      "Evaluating: batch 9 begins at 22:59:13.670032\n",
      "\n",
      "Evaluating: batch 9 ends at 22:59:13.894648\n",
      "\n",
      "Evaluating: batch 10 begins at 22:59:13.896428\n",
      "\n",
      "Evaluating: batch 10 ends at 22:59:14.117941\n",
      "\n",
      "Evaluating: batch 11 begins at 22:59:14.120072\n",
      "\n",
      "Evaluating: batch 11 ends at 22:59:14.338653\n",
      "\n",
      "Evaluating: batch 12 begins at 22:59:14.340496\n",
      "\n",
      "Evaluating: batch 12 ends at 22:59:14.561360\n",
      "\n",
      "Evaluating: batch 13 begins at 22:59:14.563826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 13 ends at 22:59:14.789168\n",
      "\n",
      "Evaluating: batch 14 begins at 22:59:14.790972\n",
      "\n",
      "Evaluating: batch 14 ends at 22:59:15.016061\n",
      "\n",
      "Evaluating: batch 15 begins at 22:59:15.017469\n",
      "\n",
      "Evaluating: batch 15 ends at 22:59:15.236521\n",
      "\n",
      "Evaluating: batch 16 begins at 22:59:15.238996\n",
      "\n",
      "Evaluating: batch 16 ends at 22:59:15.459850\n",
      "\n",
      "Evaluating: batch 17 begins at 22:59:15.461050\n",
      "\n",
      "Evaluating: batch 17 ends at 22:59:15.680355\n",
      "\n",
      "Evaluating: batch 18 begins at 22:59:15.682424\n",
      "\n",
      "Evaluating: batch 18 ends at 22:59:15.907437\n",
      "\n",
      "Evaluating: batch 19 begins at 22:59:15.908667\n",
      "\n",
      "Evaluating: batch 19 ends at 22:59:16.125175\n",
      "\n",
      "Evaluating: batch 20 begins at 22:59:16.126826\n",
      "\n",
      "Evaluating: batch 20 ends at 22:59:16.346508\n",
      "\n",
      "Evaluating: batch 21 begins at 22:59:16.348253\n",
      "\n",
      "Evaluating: batch 21 ends at 22:59:16.564386\n",
      "\n",
      "Evaluating: batch 22 begins at 22:59:16.565623\n",
      "\n",
      "Evaluating: batch 22 ends at 22:59:16.784726\n",
      "\n",
      "Evaluating: batch 23 begins at 22:59:16.786151\n",
      "\n",
      "Evaluating: batch 23 ends at 22:59:17.013249\n",
      "\n",
      "Evaluating: batch 24 begins at 22:59:17.014559\n",
      "\n",
      "Evaluating: batch 24 ends at 22:59:17.234950\n",
      "\n",
      "Evaluating: batch 25 begins at 22:59:17.236261\n",
      "\n",
      "Evaluating: batch 25 ends at 22:59:17.456438\n",
      "\n",
      "Evaluating: batch 26 begins at 22:59:17.459226\n",
      "\n",
      "Evaluating: batch 26 ends at 22:59:17.677895\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.26907\n",
      "60/60 [==============================] - 54s 909ms/step - loss: 0.2449 - bce_dice_loss: 0.2449 - val_loss: 0.2738 - val_bce_dice_loss: 0.2738\n",
      "Epoch 21/25\n",
      "\n",
      "Training: batch 0 begins at 22:59:17.704686\n",
      "\n",
      "Training: batch 0 ends at 22:59:18.503977\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.2202 - bce_dice_loss: 0.2202\n",
      "Training: batch 1 begins at 22:59:18.513365\n",
      "\n",
      "Training: batch 1 ends at 22:59:19.306618\n",
      " 2/60 [>.............................] - ETA: 46s - loss: 0.2531 - bce_dice_loss: 0.2531\n",
      "Training: batch 2 begins at 22:59:19.311005\n",
      "\n",
      "Training: batch 2 ends at 22:59:20.105294\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2358 - bce_dice_loss: 0.2358\n",
      "Training: batch 3 begins at 22:59:20.110701\n",
      "\n",
      "Training: batch 3 ends at 22:59:20.905367\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.2363 - bce_dice_loss: 0.2363\n",
      "Training: batch 4 begins at 22:59:20.909314\n",
      "\n",
      "Training: batch 4 ends at 22:59:21.703299\n",
      " 5/60 [=>............................] - ETA: 43s - loss: 0.2772 - bce_dice_loss: 0.2772\n",
      "Training: batch 5 begins at 22:59:21.705863\n",
      "\n",
      "Training: batch 5 ends at 22:59:22.517258\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2533 - bce_dice_loss: 0.2533\n",
      "Training: batch 6 begins at 22:59:22.521633\n",
      "\n",
      "Training: batch 6 ends at 22:59:23.322698\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2373 - bce_dice_loss: 0.2373\n",
      "Training: batch 7 begins at 22:59:23.327245\n",
      "\n",
      "Training: batch 7 ends at 22:59:24.143613\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2224 - bce_dice_loss: 0.2224\n",
      "Training: batch 8 begins at 22:59:24.148160\n",
      "\n",
      "Training: batch 8 ends at 22:59:24.945442\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2349 - bce_dice_loss: 0.2349\n",
      "Training: batch 9 begins at 22:59:24.949585\n",
      "\n",
      "Training: batch 9 ends at 22:59:25.747398\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2446 - bce_dice_loss: 0.2446\n",
      "Training: batch 10 begins at 22:59:25.750272\n",
      "\n",
      "Training: batch 10 ends at 22:59:26.546313\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2595 - bce_dice_loss: 0.2595\n",
      "Training: batch 11 begins at 22:59:26.549022\n",
      "\n",
      "Training: batch 11 ends at 22:59:27.354167\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2581 - bce_dice_loss: 0.2581\n",
      "Training: batch 12 begins at 22:59:27.358789\n",
      "\n",
      "Training: batch 12 ends at 22:59:28.159705\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2605 - bce_dice_loss: 0.2605\n",
      "Training: batch 13 begins at 22:59:28.163851\n",
      "\n",
      "Training: batch 13 ends at 22:59:28.984726\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2536 - bce_dice_loss: 0.2536\n",
      "Training: batch 14 begins at 22:59:28.989528\n",
      "\n",
      "Training: batch 14 ends at 22:59:29.798292\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2768 - bce_dice_loss: 0.2768\n",
      "Training: batch 15 begins at 22:59:29.802682\n",
      "\n",
      "Training: batch 15 ends at 22:59:30.598904\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2719 - bce_dice_loss: 0.2719\n",
      "Training: batch 16 begins at 22:59:30.602635\n",
      "\n",
      "Training: batch 16 ends at 22:59:31.425807\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2718 - bce_dice_loss: 0.2718\n",
      "Training: batch 17 begins at 22:59:31.428394\n",
      "\n",
      "Training: batch 17 ends at 22:59:32.223368\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2647 - bce_dice_loss: 0.2647\n",
      "Training: batch 18 begins at 22:59:32.226926\n",
      "\n",
      "Training: batch 18 ends at 22:59:33.023595\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2596 - bce_dice_loss: 0.2596\n",
      "Training: batch 19 begins at 22:59:33.027793\n",
      "\n",
      "Training: batch 19 ends at 22:59:33.830958\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2531 - bce_dice_loss: 0.2531\n",
      "Training: batch 20 begins at 22:59:33.835095\n",
      "\n",
      "Training: batch 20 ends at 22:59:34.642928\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2550 - bce_dice_loss: 0.2550\n",
      "Training: batch 21 begins at 22:59:34.646010\n",
      "\n",
      "Training: batch 21 ends at 22:59:35.444789\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2569 - bce_dice_loss: 0.2569\n",
      "Training: batch 22 begins at 22:59:35.448207\n",
      "\n",
      "Training: batch 22 ends at 22:59:36.275104\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2515 - bce_dice_loss: 0.2515\n",
      "Training: batch 23 begins at 22:59:36.279428\n",
      "\n",
      "Training: batch 23 ends at 22:59:37.075741\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2505 - bce_dice_loss: 0.2505\n",
      "Training: batch 24 begins at 22:59:37.079870\n",
      "\n",
      "Training: batch 24 ends at 22:59:37.885371\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 25 begins at 22:59:37.889425\n",
      "\n",
      "Training: batch 25 ends at 22:59:38.687030\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2518 - bce_dice_loss: 0.2518\n",
      "Training: batch 26 begins at 22:59:38.692308\n",
      "\n",
      "Training: batch 26 ends at 22:59:39.484113\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2552 - bce_dice_loss: 0.2552\n",
      "Training: batch 27 begins at 22:59:39.488706\n",
      "\n",
      "Training: batch 27 ends at 22:59:40.272763\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2556 - bce_dice_loss: 0.2556\n",
      "Training: batch 28 begins at 22:59:40.277594\n",
      "\n",
      "Training: batch 28 ends at 22:59:41.075883\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2549 - bce_dice_loss: 0.2549\n",
      "Training: batch 29 begins at 22:59:41.078556\n",
      "\n",
      "Training: batch 29 ends at 22:59:41.879175\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2513 - bce_dice_loss: 0.2513\n",
      "Training: batch 30 begins at 22:59:41.881966\n",
      "\n",
      "Training: batch 30 ends at 22:59:42.674335\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2529 - bce_dice_loss: 0.2529\n",
      "Training: batch 31 begins at 22:59:42.678779\n",
      "\n",
      "Training: batch 31 ends at 22:59:43.494568\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2503 - bce_dice_loss: 0.2503\n",
      "Training: batch 32 begins at 22:59:43.497104\n",
      "\n",
      "Training: batch 32 ends at 22:59:44.295254\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2476 - bce_dice_loss: 0.2476\n",
      "Training: batch 33 begins at 22:59:44.297851\n",
      "\n",
      "Training: batch 33 ends at 22:59:45.090627\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2472 - bce_dice_loss: 0.2472\n",
      "Training: batch 34 begins at 22:59:45.093603\n",
      "\n",
      "Training: batch 34 ends at 22:59:45.897283\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2459 - bce_dice_loss: 0.2459\n",
      "Training: batch 35 begins at 22:59:45.902581\n",
      "\n",
      "Training: batch 35 ends at 22:59:46.701103\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2433 - bce_dice_loss: 0.2433\n",
      "Training: batch 36 begins at 22:59:46.706452\n",
      "\n",
      "Training: batch 36 ends at 22:59:47.513633\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2430 - bce_dice_loss: 0.2430\n",
      "Training: batch 37 begins at 22:59:47.516461\n",
      "\n",
      "Training: batch 37 ends at 22:59:48.316711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2469 - bce_dice_loss: 0.2469\n",
      "Training: batch 38 begins at 22:59:48.321127\n",
      "\n",
      "Training: batch 38 ends at 22:59:49.118814\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2535 - bce_dice_loss: 0.2535\n",
      "Training: batch 39 begins at 22:59:49.123066\n",
      "\n",
      "Training: batch 39 ends at 22:59:49.918332\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2530 - bce_dice_loss: 0.2530\n",
      "Training: batch 40 begins at 22:59:49.922647\n",
      "\n",
      "Training: batch 40 ends at 22:59:50.724160\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 41 begins at 22:59:50.726943\n",
      "\n",
      "Training: batch 41 ends at 22:59:51.517825\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2526 - bce_dice_loss: 0.2526\n",
      "Training: batch 42 begins at 22:59:51.521205\n",
      "\n",
      "Training: batch 42 ends at 22:59:52.316554\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2503 - bce_dice_loss: 0.2503\n",
      "Training: batch 43 begins at 22:59:52.320624\n",
      "\n",
      "Training: batch 43 ends at 22:59:53.119478\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2489 - bce_dice_loss: 0.2489\n",
      "Training: batch 44 begins at 22:59:53.123992\n",
      "\n",
      "Training: batch 44 ends at 22:59:53.922626\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2478 - bce_dice_loss: 0.2478\n",
      "Training: batch 45 begins at 22:59:53.927349\n",
      "\n",
      "Training: batch 45 ends at 22:59:54.740405\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2457 - bce_dice_loss: 0.2457\n",
      "Training: batch 46 begins at 22:59:54.742524\n",
      "\n",
      "Training: batch 46 ends at 22:59:55.534700\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2476 - bce_dice_loss: 0.2476\n",
      "Training: batch 47 begins at 22:59:55.539225\n",
      "\n",
      "Training: batch 47 ends at 22:59:56.334823\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2466 - bce_dice_loss: 0.2466 \n",
      "Training: batch 48 begins at 22:59:56.339434\n",
      "\n",
      "Training: batch 48 ends at 22:59:57.144363\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2449 - bce_dice_loss: 0.2449\n",
      "Training: batch 49 begins at 22:59:57.149179\n",
      "\n",
      "Training: batch 49 ends at 22:59:57.951310\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2464 - bce_dice_loss: 0.2464\n",
      "Training: batch 50 begins at 22:59:57.955991\n",
      "\n",
      "Training: batch 50 ends at 22:59:58.774460\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2454 - bce_dice_loss: 0.2454\n",
      "Training: batch 51 begins at 22:59:58.777048\n",
      "\n",
      "Training: batch 51 ends at 22:59:59.568862\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2438 - bce_dice_loss: 0.2438\n",
      "Training: batch 52 begins at 22:59:59.572473\n",
      "\n",
      "Training: batch 52 ends at 23:00:00.363858\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2429 - bce_dice_loss: 0.2429\n",
      "Training: batch 53 begins at 23:00:00.367947\n",
      "\n",
      "Training: batch 53 ends at 23:00:01.193544\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2421 - bce_dice_loss: 0.2421\n",
      "Training: batch 54 begins at 23:00:01.196113\n",
      "\n",
      "Training: batch 54 ends at 23:00:01.999039\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2395 - bce_dice_loss: 0.2395\n",
      "Training: batch 55 begins at 23:00:02.003121\n",
      "\n",
      "Training: batch 55 ends at 23:00:02.802743\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2397 - bce_dice_loss: 0.2397\n",
      "Training: batch 56 begins at 23:00:02.805424\n",
      "\n",
      "Training: batch 56 ends at 23:00:03.600474\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2383 - bce_dice_loss: 0.2383\n",
      "Training: batch 57 begins at 23:00:03.604537\n",
      "\n",
      "Training: batch 57 ends at 23:00:04.393670\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2383 - bce_dice_loss: 0.2383\n",
      "Training: batch 58 begins at 23:00:04.396104\n",
      "\n",
      "Training: batch 58 ends at 23:00:05.184066\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2371 - bce_dice_loss: 0.2371\n",
      "Training: batch 59 begins at 23:00:05.188442\n",
      "\n",
      "Training: batch 59 ends at 23:00:05.982422\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2392 - bce_dice_loss: 0.2392\n",
      "Evaluating: batch 0 begins at 23:00:06.009942\n",
      "\n",
      "Evaluating: batch 0 ends at 23:00:06.277660\n",
      "\n",
      "Evaluating: batch 1 begins at 23:00:06.278736\n",
      "\n",
      "Evaluating: batch 1 ends at 23:00:06.492840\n",
      "\n",
      "Evaluating: batch 2 begins at 23:00:06.494136\n",
      "\n",
      "Evaluating: batch 2 ends at 23:00:06.716665\n",
      "\n",
      "Evaluating: batch 3 begins at 23:00:06.718170\n",
      "\n",
      "Evaluating: batch 3 ends at 23:00:06.935294\n",
      "\n",
      "Evaluating: batch 4 begins at 23:00:06.936548\n",
      "\n",
      "Evaluating: batch 4 ends at 23:00:07.164454\n",
      "\n",
      "Evaluating: batch 5 begins at 23:00:07.165784\n",
      "\n",
      "Evaluating: batch 5 ends at 23:00:07.388696\n",
      "\n",
      "Evaluating: batch 6 begins at 23:00:07.390320\n",
      "\n",
      "Evaluating: batch 6 ends at 23:00:07.614415\n",
      "\n",
      "Evaluating: batch 7 begins at 23:00:07.615829\n",
      "\n",
      "Evaluating: batch 7 ends at 23:00:07.840382\n",
      "\n",
      "Evaluating: batch 8 begins at 23:00:07.841681\n",
      "\n",
      "Evaluating: batch 8 ends at 23:00:08.060652\n",
      "\n",
      "Evaluating: batch 9 begins at 23:00:08.062477\n",
      "\n",
      "Evaluating: batch 9 ends at 23:00:08.282730\n",
      "\n",
      "Evaluating: batch 10 begins at 23:00:08.284770\n",
      "\n",
      "Evaluating: batch 10 ends at 23:00:08.506986\n",
      "\n",
      "Evaluating: batch 11 begins at 23:00:08.508921\n",
      "\n",
      "Evaluating: batch 11 ends at 23:00:08.731191\n",
      "\n",
      "Evaluating: batch 12 begins at 23:00:08.732670\n",
      "\n",
      "Evaluating: batch 12 ends at 23:00:08.951798\n",
      "\n",
      "Evaluating: batch 13 begins at 23:00:08.953201\n",
      "\n",
      "Evaluating: batch 13 ends at 23:00:09.172905\n",
      "\n",
      "Evaluating: batch 14 begins at 23:00:09.174549\n",
      "\n",
      "Evaluating: batch 14 ends at 23:00:09.393767\n",
      "\n",
      "Evaluating: batch 15 begins at 23:00:09.396206\n",
      "\n",
      "Evaluating: batch 15 ends at 23:00:09.616962\n",
      "\n",
      "Evaluating: batch 16 begins at 23:00:09.619443\n",
      "\n",
      "Evaluating: batch 16 ends at 23:00:09.842023\n",
      "\n",
      "Evaluating: batch 17 begins at 23:00:09.843403\n",
      "\n",
      "Evaluating: batch 17 ends at 23:00:10.064785\n",
      "\n",
      "Evaluating: batch 18 begins at 23:00:10.067292\n",
      "\n",
      "Evaluating: batch 18 ends at 23:00:10.288452\n",
      "\n",
      "Evaluating: batch 19 begins at 23:00:10.290352\n",
      "\n",
      "Evaluating: batch 19 ends at 23:00:10.507326\n",
      "\n",
      "Evaluating: batch 20 begins at 23:00:10.508568\n",
      "\n",
      "Evaluating: batch 20 ends at 23:00:10.725924\n",
      "\n",
      "Evaluating: batch 21 begins at 23:00:10.727155\n",
      "\n",
      "Evaluating: batch 21 ends at 23:00:10.946842\n",
      "\n",
      "Evaluating: batch 22 begins at 23:00:10.948545\n",
      "\n",
      "Evaluating: batch 22 ends at 23:00:11.167461\n",
      "\n",
      "Evaluating: batch 23 begins at 23:00:11.169846\n",
      "\n",
      "Evaluating: batch 23 ends at 23:00:11.387869\n",
      "\n",
      "Evaluating: batch 24 begins at 23:00:11.390038\n",
      "\n",
      "Evaluating: batch 24 ends at 23:00:11.611939\n",
      "\n",
      "Evaluating: batch 25 begins at 23:00:11.613454\n",
      "\n",
      "Evaluating: batch 25 ends at 23:00:11.832359\n",
      "\n",
      "Evaluating: batch 26 begins at 23:00:11.834853\n",
      "\n",
      "Evaluating: batch 26 ends at 23:00:12.053472\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.26907\n",
      "60/60 [==============================] - 54s 908ms/step - loss: 0.2392 - bce_dice_loss: 0.2392 - val_loss: 0.2743 - val_bce_dice_loss: 0.2743\n",
      "Epoch 22/25\n",
      "\n",
      "Training: batch 0 begins at 23:00:12.088040\n",
      "\n",
      "Training: batch 0 ends at 23:00:12.902203\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.2619 - bce_dice_loss: 0.2619\n",
      "Training: batch 1 begins at 23:00:12.912236\n",
      "\n",
      "Training: batch 1 ends at 23:00:13.715740\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.1965 - bce_dice_loss: 0.1965\n",
      "Training: batch 2 begins at 23:00:13.719064\n",
      "\n",
      "Training: batch 2 ends at 23:00:14.512725\n",
      " 3/60 [>.............................] - ETA: 45s - loss: 0.2136 - bce_dice_loss: 0.2136\n",
      "Training: batch 3 begins at 23:00:14.515371\n",
      "\n",
      "Training: batch 3 ends at 23:00:15.311021\n",
      " 4/60 [=>............................] - ETA: 44s - loss: 0.2147 - bce_dice_loss: 0.2147\n",
      "Training: batch 4 begins at 23:00:15.315198\n",
      "\n",
      "Training: batch 4 ends at 23:00:16.124520\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2080 - bce_dice_loss: 0.2080\n",
      "Training: batch 5 begins at 23:00:16.128067\n",
      "\n",
      "Training: batch 5 ends at 23:00:16.925141\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.1841 - bce_dice_loss: 0.1841\n",
      "Training: batch 6 begins at 23:00:16.928725\n",
      "\n",
      "Training: batch 6 ends at 23:00:17.745807\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.1817 - bce_dice_loss: 0.1817\n",
      "Training: batch 7 begins at 23:00:17.750229\n",
      "\n",
      "Training: batch 7 ends at 23:00:18.545273\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.1779 - bce_dice_loss: 0.1779\n",
      "Training: batch 8 begins at 23:00:18.549669\n",
      "\n",
      "Training: batch 8 ends at 23:00:19.347459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.1795 - bce_dice_loss: 0.1795\n",
      "Training: batch 9 begins at 23:00:19.351539\n",
      "\n",
      "Training: batch 9 ends at 23:00:20.147202\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.1745 - bce_dice_loss: 0.1745\n",
      "Training: batch 10 begins at 23:00:20.151698\n",
      "\n",
      "Training: batch 10 ends at 23:00:20.951138\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.1847 - bce_dice_loss: 0.1847\n",
      "Training: batch 11 begins at 23:00:20.956237\n",
      "\n",
      "Training: batch 11 ends at 23:00:21.776461\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.1937 - bce_dice_loss: 0.1937\n",
      "Training: batch 12 begins at 23:00:21.781004\n",
      "\n",
      "Training: batch 12 ends at 23:00:22.586068\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.1937 - bce_dice_loss: 0.1937\n",
      "Training: batch 13 begins at 23:00:22.590833\n",
      "\n",
      "Training: batch 13 ends at 23:00:23.388928\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.1924 - bce_dice_loss: 0.1924\n",
      "Training: batch 14 begins at 23:00:23.393044\n",
      "\n",
      "Training: batch 14 ends at 23:00:24.196983\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.1951 - bce_dice_loss: 0.1951\n",
      "Training: batch 15 begins at 23:00:24.200709\n",
      "\n",
      "Training: batch 15 ends at 23:00:25.061119\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2122 - bce_dice_loss: 0.2122\n",
      "Training: batch 16 begins at 23:00:25.063330\n",
      "\n",
      "Training: batch 16 ends at 23:00:25.986116\n",
      "17/60 [=======>......................] - ETA: 35s - loss: 0.2117 - bce_dice_loss: 0.2117\n",
      "Training: batch 17 begins at 23:00:25.989740\n",
      "\n",
      "Training: batch 17 ends at 23:00:26.879588\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.2092 - bce_dice_loss: 0.2092\n",
      "Training: batch 18 begins at 23:00:26.882211\n",
      "\n",
      "Training: batch 18 ends at 23:00:27.785345\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2053 - bce_dice_loss: 0.2053\n",
      "Training: batch 19 begins at 23:00:27.789691\n",
      "\n",
      "Training: batch 19 ends at 23:00:28.594655\n",
      "20/60 [=========>....................] - ETA: 33s - loss: 0.2035 - bce_dice_loss: 0.2035\n",
      "Training: batch 20 begins at 23:00:28.598975\n",
      "\n",
      "Training: batch 20 ends at 23:00:29.390561\n",
      "21/60 [=========>....................] - ETA: 32s - loss: 0.2068 - bce_dice_loss: 0.2068\n",
      "Training: batch 21 begins at 23:00:29.393056\n",
      "\n",
      "Training: batch 21 ends at 23:00:30.188186\n",
      "22/60 [==========>...................] - ETA: 31s - loss: 0.2095 - bce_dice_loss: 0.2095\n",
      "Training: batch 22 begins at 23:00:30.191628\n",
      "\n",
      "Training: batch 22 ends at 23:00:30.987799\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.2057 - bce_dice_loss: 0.2057\n",
      "Training: batch 23 begins at 23:00:30.992880\n",
      "\n",
      "Training: batch 23 ends at 23:00:31.786584\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2105 - bce_dice_loss: 0.2105\n",
      "Training: batch 24 begins at 23:00:31.791247\n",
      "\n",
      "Training: batch 24 ends at 23:00:32.592138\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2085 - bce_dice_loss: 0.2085\n",
      "Training: batch 25 begins at 23:00:32.595129\n",
      "\n",
      "Training: batch 25 ends at 23:00:33.391778\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.2116 - bce_dice_loss: 0.2116\n",
      "Training: batch 26 begins at 23:00:33.394823\n",
      "\n",
      "Training: batch 26 ends at 23:00:34.200924\n",
      "27/60 [============>.................] - ETA: 27s - loss: 0.2138 - bce_dice_loss: 0.2138\n",
      "Training: batch 27 begins at 23:00:34.206782\n",
      "\n",
      "Training: batch 27 ends at 23:00:35.003611\n",
      "28/60 [=============>................] - ETA: 26s - loss: 0.2149 - bce_dice_loss: 0.2149\n",
      "Training: batch 28 begins at 23:00:35.009354\n",
      "\n",
      "Training: batch 28 ends at 23:00:35.803159\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.2136 - bce_dice_loss: 0.2136\n",
      "Training: batch 29 begins at 23:00:35.808080\n",
      "\n",
      "Training: batch 29 ends at 23:00:36.602126\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2129 - bce_dice_loss: 0.2129\n",
      "Training: batch 30 begins at 23:00:36.607342\n",
      "\n",
      "Training: batch 30 ends at 23:00:37.418729\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2119 - bce_dice_loss: 0.2119\n",
      "Training: batch 31 begins at 23:00:37.424941\n",
      "\n",
      "Training: batch 31 ends at 23:00:38.235753\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2151 - bce_dice_loss: 0.2151\n",
      "Training: batch 32 begins at 23:00:38.239187\n",
      "\n",
      "Training: batch 32 ends at 23:00:39.033208\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.2139 - bce_dice_loss: 0.2139\n",
      "Training: batch 33 begins at 23:00:39.037592\n",
      "\n",
      "Training: batch 33 ends at 23:00:39.849738\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.2107 - bce_dice_loss: 0.2107\n",
      "Training: batch 34 begins at 23:00:39.854009\n",
      "\n",
      "Training: batch 34 ends at 23:00:40.651550\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2095 - bce_dice_loss: 0.2095\n",
      "Training: batch 35 begins at 23:00:40.655738\n",
      "\n",
      "Training: batch 35 ends at 23:00:41.461218\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2084 - bce_dice_loss: 0.2084\n",
      "Training: batch 36 begins at 23:00:41.465658\n",
      "\n",
      "Training: batch 36 ends at 23:00:42.264425\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2088 - bce_dice_loss: 0.2088\n",
      "Training: batch 37 begins at 23:00:42.268476\n",
      "\n",
      "Training: batch 37 ends at 23:00:43.065423\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2118 - bce_dice_loss: 0.2118\n",
      "Training: batch 38 begins at 23:00:43.070000\n",
      "\n",
      "Training: batch 38 ends at 23:00:43.867101\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.2122 - bce_dice_loss: 0.2122\n",
      "Training: batch 39 begins at 23:00:43.871331\n",
      "\n",
      "Training: batch 39 ends at 23:00:44.680792\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2125 - bce_dice_loss: 0.2125\n",
      "Training: batch 40 begins at 23:00:44.685005\n",
      "\n",
      "Training: batch 40 ends at 23:00:45.481906\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2101 - bce_dice_loss: 0.2101\n",
      "Training: batch 41 begins at 23:00:45.484655\n",
      "\n",
      "Training: batch 41 ends at 23:00:46.288266\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2111 - bce_dice_loss: 0.2111\n",
      "Training: batch 42 begins at 23:00:46.291837\n",
      "\n",
      "Training: batch 42 ends at 23:00:47.118482\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2137 - bce_dice_loss: 0.2137\n",
      "Training: batch 43 begins at 23:00:47.123433\n",
      "\n",
      "Training: batch 43 ends at 23:00:47.939081\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.2132 - bce_dice_loss: 0.2132\n",
      "Training: batch 44 begins at 23:00:47.941736\n",
      "\n",
      "Training: batch 44 ends at 23:00:48.741747\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2132 - bce_dice_loss: 0.2132\n",
      "Training: batch 45 begins at 23:00:48.744817\n",
      "\n",
      "Training: batch 45 ends at 23:00:49.566854\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2128 - bce_dice_loss: 0.2128\n",
      "Training: batch 46 begins at 23:00:49.571338\n",
      "\n",
      "Training: batch 46 ends at 23:00:50.368702\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2115 - bce_dice_loss: 0.2115\n",
      "Training: batch 47 begins at 23:00:50.372444\n",
      "\n",
      "Training: batch 47 ends at 23:00:51.156953\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2102 - bce_dice_loss: 0.2102 \n",
      "Training: batch 48 begins at 23:00:51.159516\n",
      "\n",
      "Training: batch 48 ends at 23:00:51.970195\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2095 - bce_dice_loss: 0.2095\n",
      "Training: batch 49 begins at 23:00:51.974427\n",
      "\n",
      "Training: batch 49 ends at 23:00:52.774509\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2091 - bce_dice_loss: 0.2091\n",
      "Training: batch 50 begins at 23:00:52.778737\n",
      "\n",
      "Training: batch 50 ends at 23:00:53.576138\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2089 - bce_dice_loss: 0.2089\n",
      "Training: batch 51 begins at 23:00:53.578548\n",
      "\n",
      "Training: batch 51 ends at 23:00:54.389131\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2157 - bce_dice_loss: 0.2157\n",
      "Training: batch 52 begins at 23:00:54.393179\n",
      "\n",
      "Training: batch 52 ends at 23:00:55.185340\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2178 - bce_dice_loss: 0.2178\n",
      "Training: batch 53 begins at 23:00:55.189806\n",
      "\n",
      "Training: batch 53 ends at 23:00:55.978463\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2150 - bce_dice_loss: 0.2150\n",
      "Training: batch 54 begins at 23:00:55.980497\n",
      "\n",
      "Training: batch 54 ends at 23:00:56.788054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2211 - bce_dice_loss: 0.2211\n",
      "Training: batch 55 begins at 23:00:56.792940\n",
      "\n",
      "Training: batch 55 ends at 23:00:57.590127\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2257 - bce_dice_loss: 0.2257\n",
      "Training: batch 56 begins at 23:00:57.592771\n",
      "\n",
      "Training: batch 56 ends at 23:00:58.408940\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2255 - bce_dice_loss: 0.2255\n",
      "Training: batch 57 begins at 23:00:58.411866\n",
      "\n",
      "Training: batch 57 ends at 23:00:59.208483\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2242 - bce_dice_loss: 0.2242\n",
      "Training: batch 58 begins at 23:00:59.212506\n",
      "\n",
      "Training: batch 58 ends at 23:01:00.006325\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2246 - bce_dice_loss: 0.2246\n",
      "Training: batch 59 begins at 23:01:00.009321\n",
      "\n",
      "Training: batch 59 ends at 23:01:00.827657\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2268 - bce_dice_loss: 0.2268\n",
      "Evaluating: batch 0 begins at 23:01:00.857737\n",
      "\n",
      "Evaluating: batch 0 ends at 23:01:01.131479\n",
      "\n",
      "Evaluating: batch 1 begins at 23:01:01.133075\n",
      "\n",
      "Evaluating: batch 1 ends at 23:01:01.351000\n",
      "\n",
      "Evaluating: batch 2 begins at 23:01:01.352434\n",
      "\n",
      "Evaluating: batch 2 ends at 23:01:01.569499\n",
      "\n",
      "Evaluating: batch 3 begins at 23:01:01.571080\n",
      "\n",
      "Evaluating: batch 3 ends at 23:01:01.795005\n",
      "\n",
      "Evaluating: batch 4 begins at 23:01:01.796357\n",
      "\n",
      "Evaluating: batch 4 ends at 23:01:02.019584\n",
      "\n",
      "Evaluating: batch 5 begins at 23:01:02.020979\n",
      "\n",
      "Evaluating: batch 5 ends at 23:01:02.244868\n",
      "\n",
      "Evaluating: batch 6 begins at 23:01:02.246289\n",
      "\n",
      "Evaluating: batch 6 ends at 23:01:02.467754\n",
      "\n",
      "Evaluating: batch 7 begins at 23:01:02.470359\n",
      "\n",
      "Evaluating: batch 7 ends at 23:01:02.689359\n",
      "\n",
      "Evaluating: batch 8 begins at 23:01:02.691091\n",
      "\n",
      "Evaluating: batch 8 ends at 23:01:02.912921\n",
      "\n",
      "Evaluating: batch 9 begins at 23:01:02.914347\n",
      "\n",
      "Evaluating: batch 9 ends at 23:01:03.135628\n",
      "\n",
      "Evaluating: batch 10 begins at 23:01:03.137154\n",
      "\n",
      "Evaluating: batch 10 ends at 23:01:03.358711\n",
      "\n",
      "Evaluating: batch 11 begins at 23:01:03.360345\n",
      "\n",
      "Evaluating: batch 11 ends at 23:01:03.582134\n",
      "\n",
      "Evaluating: batch 12 begins at 23:01:03.583363\n",
      "\n",
      "Evaluating: batch 12 ends at 23:01:03.805665\n",
      "\n",
      "Evaluating: batch 13 begins at 23:01:03.807081\n",
      "\n",
      "Evaluating: batch 13 ends at 23:01:04.027745\n",
      "\n",
      "Evaluating: batch 14 begins at 23:01:04.030213\n",
      "\n",
      "Evaluating: batch 14 ends at 23:01:04.254673\n",
      "\n",
      "Evaluating: batch 15 begins at 23:01:04.259053\n",
      "\n",
      "Evaluating: batch 15 ends at 23:01:04.477334\n",
      "\n",
      "Evaluating: batch 16 begins at 23:01:04.479331\n",
      "\n",
      "Evaluating: batch 16 ends at 23:01:04.700070\n",
      "\n",
      "Evaluating: batch 17 begins at 23:01:04.702399\n",
      "\n",
      "Evaluating: batch 17 ends at 23:01:04.927860\n",
      "\n",
      "Evaluating: batch 18 begins at 23:01:04.929260\n",
      "\n",
      "Evaluating: batch 18 ends at 23:01:05.148089\n",
      "\n",
      "Evaluating: batch 19 begins at 23:01:05.150556\n",
      "\n",
      "Evaluating: batch 19 ends at 23:01:05.367321\n",
      "\n",
      "Evaluating: batch 20 begins at 23:01:05.368523\n",
      "\n",
      "Evaluating: batch 20 ends at 23:01:05.586607\n",
      "\n",
      "Evaluating: batch 21 begins at 23:01:05.589662\n",
      "\n",
      "Evaluating: batch 21 ends at 23:01:05.811083\n",
      "\n",
      "Evaluating: batch 22 begins at 23:01:05.812257\n",
      "\n",
      "Evaluating: batch 22 ends at 23:01:06.027416\n",
      "\n",
      "Evaluating: batch 23 begins at 23:01:06.028660\n",
      "\n",
      "Evaluating: batch 23 ends at 23:01:06.251269\n",
      "\n",
      "Evaluating: batch 24 begins at 23:01:06.252750\n",
      "\n",
      "Evaluating: batch 24 ends at 23:01:06.469311\n",
      "\n",
      "Evaluating: batch 25 begins at 23:01:06.470572\n",
      "\n",
      "Evaluating: batch 25 ends at 23:01:06.685602\n",
      "\n",
      "Evaluating: batch 26 begins at 23:01:06.687599\n",
      "\n",
      "Evaluating: batch 26 ends at 23:01:06.905389\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.26907\n",
      "60/60 [==============================] - 55s 916ms/step - loss: 0.2268 - bce_dice_loss: 0.2268 - val_loss: 0.2731 - val_bce_dice_loss: 0.2731\n",
      "Epoch 23/25\n",
      "\n",
      "Training: batch 0 begins at 23:01:06.940548\n",
      "\n",
      "Training: batch 0 ends at 23:01:07.740305\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.1529 - bce_dice_loss: 0.1529\n",
      "Training: batch 1 begins at 23:01:07.744593\n",
      "\n",
      "Training: batch 1 ends at 23:01:08.554251\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.1615 - bce_dice_loss: 0.1615\n",
      "Training: batch 2 begins at 23:01:08.556824\n",
      "\n",
      "Training: batch 2 ends at 23:01:09.359817\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.1715 - bce_dice_loss: 0.1715\n",
      "Training: batch 3 begins at 23:01:09.364053\n",
      "\n",
      "Training: batch 3 ends at 23:01:10.160142\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.2103 - bce_dice_loss: 0.2103\n",
      "Training: batch 4 begins at 23:01:10.164353\n",
      "\n",
      "Training: batch 4 ends at 23:01:10.967994\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.2405 - bce_dice_loss: 0.2405\n",
      "Training: batch 5 begins at 23:01:10.973293\n",
      "\n",
      "Training: batch 5 ends at 23:01:11.767975\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.2254 - bce_dice_loss: 0.2254\n",
      "Training: batch 6 begins at 23:01:11.772207\n",
      "\n",
      "Training: batch 6 ends at 23:01:12.569135\n",
      " 7/60 [==>...........................] - ETA: 42s - loss: 0.2140 - bce_dice_loss: 0.2140\n",
      "Training: batch 7 begins at 23:01:12.573021\n",
      "\n",
      "Training: batch 7 ends at 23:01:13.393971\n",
      " 8/60 [===>..........................] - ETA: 41s - loss: 0.2175 - bce_dice_loss: 0.2175\n",
      "Training: batch 8 begins at 23:01:13.398294\n",
      "\n",
      "Training: batch 8 ends at 23:01:14.194020\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2259 - bce_dice_loss: 0.2259\n",
      "Training: batch 9 begins at 23:01:14.198125\n",
      "\n",
      "Training: batch 9 ends at 23:01:14.991062\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2455 - bce_dice_loss: 0.2455\n",
      "Training: batch 10 begins at 23:01:14.994393\n",
      "\n",
      "Training: batch 10 ends at 23:01:15.784802\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2478 - bce_dice_loss: 0.2478\n",
      "Training: batch 11 begins at 23:01:15.788604\n",
      "\n",
      "Training: batch 11 ends at 23:01:16.573530\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2444 - bce_dice_loss: 0.2444\n",
      "Training: batch 12 begins at 23:01:16.576291\n",
      "\n",
      "Training: batch 12 ends at 23:01:17.382969\n",
      "13/60 [=====>........................] - ETA: 37s - loss: 0.2402 - bce_dice_loss: 0.2402\n",
      "Training: batch 13 begins at 23:01:17.388328\n",
      "\n",
      "Training: batch 13 ends at 23:01:18.186778\n",
      "14/60 [======>.......................] - ETA: 36s - loss: 0.2700 - bce_dice_loss: 0.2700\n",
      "Training: batch 14 begins at 23:01:18.190137\n",
      "\n",
      "Training: batch 14 ends at 23:01:18.978671\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2618 - bce_dice_loss: 0.2618\n",
      "Training: batch 15 begins at 23:01:18.981877\n",
      "\n",
      "Training: batch 15 ends at 23:01:19.780593\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2562 - bce_dice_loss: 0.2562\n",
      "Training: batch 16 begins at 23:01:19.784939\n",
      "\n",
      "Training: batch 16 ends at 23:01:20.606485\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2553 - bce_dice_loss: 0.2553\n",
      "Training: batch 17 begins at 23:01:20.609898\n",
      "\n",
      "Training: batch 17 ends at 23:01:21.406766\n",
      "18/60 [========>.....................] - ETA: 33s - loss: 0.2544 - bce_dice_loss: 0.2544\n",
      "Training: batch 18 begins at 23:01:21.410175\n",
      "\n",
      "Training: batch 18 ends at 23:01:22.235934\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2636 - bce_dice_loss: 0.2636\n",
      "Training: batch 19 begins at 23:01:22.239566\n",
      "\n",
      "Training: batch 19 ends at 23:01:23.035062\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2584 - bce_dice_loss: 0.2584\n",
      "Training: batch 20 begins at 23:01:23.039087\n",
      "\n",
      "Training: batch 20 ends at 23:01:23.838729\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2615 - bce_dice_loss: 0.2615\n",
      "Training: batch 21 begins at 23:01:23.842941\n",
      "\n",
      "Training: batch 21 ends at 23:01:24.633836\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2574 - bce_dice_loss: 0.2574\n",
      "Training: batch 22 begins at 23:01:24.638663\n",
      "\n",
      "Training: batch 22 ends at 23:01:25.465442\n",
      "23/60 [==========>...................] - ETA: 29s - loss: 0.2534 - bce_dice_loss: 0.2534\n",
      "Training: batch 23 begins at 23:01:25.469426\n",
      "\n",
      "Training: batch 23 ends at 23:01:26.266268\n",
      "24/60 [===========>..................] - ETA: 28s - loss: 0.2555 - bce_dice_loss: 0.2555\n",
      "Training: batch 24 begins at 23:01:26.271067\n",
      "\n",
      "Training: batch 24 ends at 23:01:27.065460\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 25 begins at 23:01:27.069544\n",
      "\n",
      "Training: batch 25 ends at 23:01:27.885901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/60 [============>.................] - ETA: 27s - loss: 0.2635 - bce_dice_loss: 0.2635\n",
      "Training: batch 26 begins at 23:01:27.893309\n",
      "\n",
      "Training: batch 26 ends at 23:01:28.691498\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.2611 - bce_dice_loss: 0.2611\n",
      "Training: batch 27 begins at 23:01:28.694671\n",
      "\n",
      "Training: batch 27 ends at 23:01:29.492187\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.2575 - bce_dice_loss: 0.2575\n",
      "Training: batch 28 begins at 23:01:29.495360\n",
      "\n",
      "Training: batch 28 ends at 23:01:30.290502\n",
      "29/60 [=============>................] - ETA: 24s - loss: 0.2527 - bce_dice_loss: 0.2527\n",
      "Training: batch 29 begins at 23:01:30.293208\n",
      "\n",
      "Training: batch 29 ends at 23:01:31.097948\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2577 - bce_dice_loss: 0.2577\n",
      "Training: batch 30 begins at 23:01:31.102071\n",
      "\n",
      "Training: batch 30 ends at 23:01:31.890871\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2564 - bce_dice_loss: 0.2564\n",
      "Training: batch 31 begins at 23:01:31.895321\n",
      "\n",
      "Training: batch 31 ends at 23:01:32.721024\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2546 - bce_dice_loss: 0.2546\n",
      "Training: batch 32 begins at 23:01:32.726218\n",
      "\n",
      "Training: batch 32 ends at 23:01:33.521575\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2545 - bce_dice_loss: 0.2545\n",
      "Training: batch 33 begins at 23:01:33.525657\n",
      "\n",
      "Training: batch 33 ends at 23:01:34.345637\n",
      "34/60 [================>.............] - ETA: 20s - loss: 0.2522 - bce_dice_loss: 0.2522\n",
      "Training: batch 34 begins at 23:01:34.350001\n",
      "\n",
      "Training: batch 34 ends at 23:01:35.151522\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 35 begins at 23:01:35.155949\n",
      "\n",
      "Training: batch 35 ends at 23:01:35.949588\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2497 - bce_dice_loss: 0.2497\n",
      "Training: batch 36 begins at 23:01:35.954023\n",
      "\n",
      "Training: batch 36 ends at 23:01:36.746159\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2511 - bce_dice_loss: 0.2511\n",
      "Training: batch 37 begins at 23:01:36.749062\n",
      "\n",
      "Training: batch 37 ends at 23:01:37.548124\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2469 - bce_dice_loss: 0.2469\n",
      "Training: batch 38 begins at 23:01:37.552779\n",
      "\n",
      "Training: batch 38 ends at 23:01:38.359383\n",
      "39/60 [==================>...........] - ETA: 16s - loss: 0.2484 - bce_dice_loss: 0.2484\n",
      "Training: batch 39 begins at 23:01:38.362328\n",
      "\n",
      "Training: batch 39 ends at 23:01:39.159193\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2502 - bce_dice_loss: 0.2502\n",
      "Training: batch 40 begins at 23:01:39.162588\n",
      "\n",
      "Training: batch 40 ends at 23:01:39.961496\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2481 - bce_dice_loss: 0.2481\n",
      "Training: batch 41 begins at 23:01:39.963952\n",
      "\n",
      "Training: batch 41 ends at 23:01:40.772000\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2468 - bce_dice_loss: 0.2468\n",
      "Training: batch 42 begins at 23:01:40.776328\n",
      "\n",
      "Training: batch 42 ends at 23:01:41.576445\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2476 - bce_dice_loss: 0.2476\n",
      "Training: batch 43 begins at 23:01:41.580834\n",
      "\n",
      "Training: batch 43 ends at 23:01:42.376473\n",
      "44/60 [=====================>........] - ETA: 12s - loss: 0.2450 - bce_dice_loss: 0.2450\n",
      "Training: batch 44 begins at 23:01:42.380766\n",
      "\n",
      "Training: batch 44 ends at 23:01:43.206471\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2432 - bce_dice_loss: 0.2432\n",
      "Training: batch 45 begins at 23:01:43.210474\n",
      "\n",
      "Training: batch 45 ends at 23:01:43.999533\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2425 - bce_dice_loss: 0.2425\n",
      "Training: batch 46 begins at 23:01:44.003244\n",
      "\n",
      "Training: batch 46 ends at 23:01:44.789812\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2399 - bce_dice_loss: 0.2399\n",
      "Training: batch 47 begins at 23:01:44.793907\n",
      "\n",
      "Training: batch 47 ends at 23:01:45.612779\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2376 - bce_dice_loss: 0.2376 \n",
      "Training: batch 48 begins at 23:01:45.617594\n",
      "\n",
      "Training: batch 48 ends at 23:01:46.411820\n",
      "49/60 [=======================>......] - ETA: 8s - loss: 0.2355 - bce_dice_loss: 0.2355\n",
      "Training: batch 49 begins at 23:01:46.415390\n",
      "\n",
      "Training: batch 49 ends at 23:01:47.210696\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2353 - bce_dice_loss: 0.2353\n",
      "Training: batch 50 begins at 23:01:47.215004\n",
      "\n",
      "Training: batch 50 ends at 23:01:48.042755\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2339 - bce_dice_loss: 0.2339\n",
      "Training: batch 51 begins at 23:01:48.047153\n",
      "\n",
      "Training: batch 51 ends at 23:01:48.842489\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2337 - bce_dice_loss: 0.2337\n",
      "Training: batch 52 begins at 23:01:48.846012\n",
      "\n",
      "Training: batch 52 ends at 23:01:49.661255\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2331 - bce_dice_loss: 0.2331\n",
      "Training: batch 53 begins at 23:01:49.664818\n",
      "\n",
      "Training: batch 53 ends at 23:01:50.459405\n",
      "54/60 [==========================>...] - ETA: 4s - loss: 0.2326 - bce_dice_loss: 0.2326\n",
      "Training: batch 54 begins at 23:01:50.461946\n",
      "\n",
      "Training: batch 54 ends at 23:01:51.248771\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2353 - bce_dice_loss: 0.2353\n",
      "Training: batch 55 begins at 23:01:51.253446\n",
      "\n",
      "Training: batch 55 ends at 23:01:52.055179\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2328 - bce_dice_loss: 0.2328\n",
      "Training: batch 56 begins at 23:01:52.058503\n",
      "\n",
      "Training: batch 56 ends at 23:01:52.841128\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2302 - bce_dice_loss: 0.2302\n",
      "Training: batch 57 begins at 23:01:52.844111\n",
      "\n",
      "Training: batch 57 ends at 23:01:53.638315\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2289 - bce_dice_loss: 0.2289\n",
      "Training: batch 58 begins at 23:01:53.642045\n",
      "\n",
      "Training: batch 58 ends at 23:01:54.437247\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2302 - bce_dice_loss: 0.2302\n",
      "Training: batch 59 begins at 23:01:54.440081\n",
      "\n",
      "Training: batch 59 ends at 23:01:55.276450\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2290 - bce_dice_loss: 0.2290\n",
      "Evaluating: batch 0 begins at 23:01:55.308146\n",
      "\n",
      "Evaluating: batch 0 ends at 23:01:55.576184\n",
      "\n",
      "Evaluating: batch 1 begins at 23:01:55.577718\n",
      "\n",
      "Evaluating: batch 1 ends at 23:01:55.794577\n",
      "\n",
      "Evaluating: batch 2 begins at 23:01:55.796717\n",
      "\n",
      "Evaluating: batch 2 ends at 23:01:56.024151\n",
      "\n",
      "Evaluating: batch 3 begins at 23:01:56.025548\n",
      "\n",
      "Evaluating: batch 3 ends at 23:01:56.244669\n",
      "\n",
      "Evaluating: batch 4 begins at 23:01:56.247323\n",
      "\n",
      "Evaluating: batch 4 ends at 23:01:56.468923\n",
      "\n",
      "Evaluating: batch 5 begins at 23:01:56.477976\n",
      "\n",
      "Evaluating: batch 5 ends at 23:01:56.704399\n",
      "\n",
      "Evaluating: batch 6 begins at 23:01:56.706448\n",
      "\n",
      "Evaluating: batch 6 ends at 23:01:56.928692\n",
      "\n",
      "Evaluating: batch 7 begins at 23:01:56.930507\n",
      "\n",
      "Evaluating: batch 7 ends at 23:01:57.151422\n",
      "\n",
      "Evaluating: batch 8 begins at 23:01:57.152675\n",
      "\n",
      "Evaluating: batch 8 ends at 23:01:57.377727\n",
      "\n",
      "Evaluating: batch 9 begins at 23:01:57.379106\n",
      "\n",
      "Evaluating: batch 9 ends at 23:01:57.609230\n",
      "\n",
      "Evaluating: batch 10 begins at 23:01:57.610587\n",
      "\n",
      "Evaluating: batch 10 ends at 23:01:57.833229\n",
      "\n",
      "Evaluating: batch 11 begins at 23:01:57.835230\n",
      "\n",
      "Evaluating: batch 11 ends at 23:01:58.056930\n",
      "\n",
      "Evaluating: batch 12 begins at 23:01:58.059805\n",
      "\n",
      "Evaluating: batch 12 ends at 23:01:58.278015\n",
      "\n",
      "Evaluating: batch 13 begins at 23:01:58.280336\n",
      "\n",
      "Evaluating: batch 13 ends at 23:01:58.501183\n",
      "\n",
      "Evaluating: batch 14 begins at 23:01:58.503305\n",
      "\n",
      "Evaluating: batch 14 ends at 23:01:58.729391\n",
      "\n",
      "Evaluating: batch 15 begins at 23:01:58.731040\n",
      "\n",
      "Evaluating: batch 15 ends at 23:01:58.952923\n",
      "\n",
      "Evaluating: batch 16 begins at 23:01:58.955402\n",
      "\n",
      "Evaluating: batch 16 ends at 23:01:59.175258\n",
      "\n",
      "Evaluating: batch 17 begins at 23:01:59.177710\n",
      "\n",
      "Evaluating: batch 17 ends at 23:01:59.400633\n",
      "\n",
      "Evaluating: batch 18 begins at 23:01:59.402273\n",
      "\n",
      "Evaluating: batch 18 ends at 23:01:59.623430\n",
      "\n",
      "Evaluating: batch 19 begins at 23:01:59.625821\n",
      "\n",
      "Evaluating: batch 19 ends at 23:01:59.846481\n",
      "\n",
      "Evaluating: batch 20 begins at 23:01:59.848950\n",
      "\n",
      "Evaluating: batch 20 ends at 23:02:00.069610\n",
      "\n",
      "Evaluating: batch 21 begins at 23:02:00.070936\n",
      "\n",
      "Evaluating: batch 21 ends at 23:02:00.292107\n",
      "\n",
      "Evaluating: batch 22 begins at 23:02:00.294262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: batch 22 ends at 23:02:00.510950\n",
      "\n",
      "Evaluating: batch 23 begins at 23:02:00.512629\n",
      "\n",
      "Evaluating: batch 23 ends at 23:02:00.733714\n",
      "\n",
      "Evaluating: batch 24 begins at 23:02:00.735465\n",
      "\n",
      "Evaluating: batch 24 ends at 23:02:00.953516\n",
      "\n",
      "Evaluating: batch 25 begins at 23:02:00.954981\n",
      "\n",
      "Evaluating: batch 25 ends at 23:02:01.171779\n",
      "\n",
      "Evaluating: batch 26 begins at 23:02:01.174147\n",
      "\n",
      "Evaluating: batch 26 ends at 23:02:01.395912\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.26907 to 0.25974, saving model to ./keras.model\n",
      "INFO:tensorflow:Assets written to: ./keras.model/assets\n",
      "60/60 [==============================] - 56s 937ms/step - loss: 0.2290 - bce_dice_loss: 0.2290 - val_loss: 0.2597 - val_bce_dice_loss: 0.2597\n",
      "Epoch 24/25\n",
      "\n",
      "Training: batch 0 begins at 23:02:03.052415\n",
      "\n",
      "Training: batch 0 ends at 23:02:03.871486\n",
      " 1/60 [..............................] - ETA: 48s - loss: 0.1393 - bce_dice_loss: 0.1393\n",
      "Training: batch 1 begins at 23:02:03.876726\n",
      "\n",
      "Training: batch 1 ends at 23:02:04.692028\n",
      " 2/60 [>.............................] - ETA: 47s - loss: 0.2085 - bce_dice_loss: 0.2085\n",
      "Training: batch 2 begins at 23:02:04.694928\n",
      "\n",
      "Training: batch 2 ends at 23:02:05.490120\n",
      " 3/60 [>.............................] - ETA: 46s - loss: 0.2020 - bce_dice_loss: 0.2020\n",
      "Training: batch 3 begins at 23:02:05.494531\n",
      "\n",
      "Training: batch 3 ends at 23:02:06.293648\n",
      " 4/60 [=>............................] - ETA: 45s - loss: 0.1821 - bce_dice_loss: 0.1821\n",
      "Training: batch 4 begins at 23:02:06.297593\n",
      "\n",
      "Training: batch 4 ends at 23:02:07.094449\n",
      " 5/60 [=>............................] - ETA: 44s - loss: 0.1807 - bce_dice_loss: 0.1807\n",
      "Training: batch 5 begins at 23:02:07.099141\n",
      "\n",
      "Training: batch 5 ends at 23:02:07.941111\n",
      " 6/60 [==>...........................] - ETA: 43s - loss: 0.1743 - bce_dice_loss: 0.1743\n",
      "Training: batch 6 begins at 23:02:07.943592\n",
      "\n",
      "Training: batch 6 ends at 23:02:08.741357\n",
      " 7/60 [==>...........................] - ETA: 43s - loss: 0.1868 - bce_dice_loss: 0.1868\n",
      "Training: batch 7 begins at 23:02:08.744705\n",
      "\n",
      "Training: batch 7 ends at 23:02:09.541494\n",
      " 8/60 [===>..........................] - ETA: 42s - loss: 0.2175 - bce_dice_loss: 0.2175\n",
      "Training: batch 8 begins at 23:02:09.545873\n",
      "\n",
      "Training: batch 8 ends at 23:02:10.341523\n",
      " 9/60 [===>..........................] - ETA: 41s - loss: 0.2159 - bce_dice_loss: 0.2159\n",
      "Training: batch 9 begins at 23:02:10.346158\n",
      "\n",
      "Training: batch 9 ends at 23:02:11.173293\n",
      "10/60 [====>.........................] - ETA: 40s - loss: 0.2015 - bce_dice_loss: 0.2015\n",
      "Training: batch 10 begins at 23:02:11.176169\n",
      "\n",
      "Training: batch 10 ends at 23:02:11.975020\n",
      "11/60 [====>.........................] - ETA: 39s - loss: 0.2105 - bce_dice_loss: 0.2105\n",
      "Training: batch 11 begins at 23:02:11.979325\n",
      "\n",
      "Training: batch 11 ends at 23:02:12.781588\n",
      "12/60 [=====>........................] - ETA: 38s - loss: 0.2058 - bce_dice_loss: 0.2058\n",
      "Training: batch 12 begins at 23:02:12.784153\n",
      "\n",
      "Training: batch 12 ends at 23:02:13.611217\n",
      "13/60 [=====>........................] - ETA: 38s - loss: 0.2135 - bce_dice_loss: 0.2135\n",
      "Training: batch 13 begins at 23:02:13.614155\n",
      "\n",
      "Training: batch 13 ends at 23:02:14.412228\n",
      "14/60 [======>.......................] - ETA: 37s - loss: 0.2081 - bce_dice_loss: 0.2081\n",
      "Training: batch 14 begins at 23:02:14.415010\n",
      "\n",
      "Training: batch 14 ends at 23:02:15.210135\n",
      "15/60 [======>.......................] - ETA: 36s - loss: 0.2054 - bce_dice_loss: 0.2054\n",
      "Training: batch 15 begins at 23:02:15.212597\n",
      "\n",
      "Training: batch 15 ends at 23:02:16.042053\n",
      "16/60 [=======>......................] - ETA: 35s - loss: 0.2036 - bce_dice_loss: 0.2036\n",
      "Training: batch 16 begins at 23:02:16.044674\n",
      "\n",
      "Training: batch 16 ends at 23:02:16.842616\n",
      "17/60 [=======>......................] - ETA: 34s - loss: 0.2076 - bce_dice_loss: 0.2076\n",
      "Training: batch 17 begins at 23:02:16.846707\n",
      "\n",
      "Training: batch 17 ends at 23:02:17.652630\n",
      "18/60 [========>.....................] - ETA: 34s - loss: 0.2060 - bce_dice_loss: 0.2060\n",
      "Training: batch 18 begins at 23:02:17.657263\n",
      "\n",
      "Training: batch 18 ends at 23:02:18.472804\n",
      "19/60 [========>.....................] - ETA: 33s - loss: 0.2095 - bce_dice_loss: 0.2095\n",
      "Training: batch 19 begins at 23:02:18.475560\n",
      "\n",
      "Training: batch 19 ends at 23:02:19.275880\n",
      "20/60 [=========>....................] - ETA: 32s - loss: 0.2037 - bce_dice_loss: 0.2037\n",
      "Training: batch 20 begins at 23:02:19.280320\n",
      "\n",
      "Training: batch 20 ends at 23:02:20.076441\n",
      "21/60 [=========>....................] - ETA: 31s - loss: 0.2091 - bce_dice_loss: 0.2091\n",
      "Training: batch 21 begins at 23:02:20.080878\n",
      "\n",
      "Training: batch 21 ends at 23:02:20.877899\n",
      "22/60 [==========>...................] - ETA: 30s - loss: 0.2067 - bce_dice_loss: 0.2067\n",
      "Training: batch 22 begins at 23:02:20.882195\n",
      "\n",
      "Training: batch 22 ends at 23:02:21.711938\n",
      "23/60 [==========>...................] - ETA: 30s - loss: 0.2063 - bce_dice_loss: 0.2063\n",
      "Training: batch 23 begins at 23:02:21.714287\n",
      "\n",
      "Training: batch 23 ends at 23:02:22.507167\n",
      "24/60 [===========>..................] - ETA: 29s - loss: 0.2019 - bce_dice_loss: 0.2019\n",
      "Training: batch 24 begins at 23:02:22.511272\n",
      "\n",
      "Training: batch 24 ends at 23:02:23.335052\n",
      "25/60 [===========>..................] - ETA: 28s - loss: 0.1968 - bce_dice_loss: 0.1968\n",
      "Training: batch 25 begins at 23:02:23.340900\n",
      "\n",
      "Training: batch 25 ends at 23:02:24.137702\n",
      "26/60 [============>.................] - ETA: 27s - loss: 0.1961 - bce_dice_loss: 0.1961\n",
      "Training: batch 26 begins at 23:02:24.141254\n",
      "\n",
      "Training: batch 26 ends at 23:02:24.948085\n",
      "27/60 [============>.................] - ETA: 26s - loss: 0.1943 - bce_dice_loss: 0.1943\n",
      "Training: batch 27 begins at 23:02:24.952081\n",
      "\n",
      "Training: batch 27 ends at 23:02:25.749542\n",
      "28/60 [=============>................] - ETA: 25s - loss: 0.1930 - bce_dice_loss: 0.1930\n",
      "Training: batch 28 begins at 23:02:25.755317\n",
      "\n",
      "Training: batch 28 ends at 23:02:26.556891\n",
      "29/60 [=============>................] - ETA: 25s - loss: 0.1960 - bce_dice_loss: 0.1960\n",
      "Training: batch 29 begins at 23:02:26.559306\n",
      "\n",
      "Training: batch 29 ends at 23:02:27.358146\n",
      "30/60 [==============>...............] - ETA: 24s - loss: 0.2092 - bce_dice_loss: 0.2092\n",
      "Training: batch 30 begins at 23:02:27.362178\n",
      "\n",
      "Training: batch 30 ends at 23:02:28.174153\n",
      "31/60 [==============>...............] - ETA: 23s - loss: 0.2112 - bce_dice_loss: 0.2112\n",
      "Training: batch 31 begins at 23:02:28.178552\n",
      "\n",
      "Training: batch 31 ends at 23:02:28.978132\n",
      "32/60 [===============>..............] - ETA: 22s - loss: 0.2121 - bce_dice_loss: 0.2121\n",
      "Training: batch 32 begins at 23:02:28.982532\n",
      "\n",
      "Training: batch 32 ends at 23:02:29.783346\n",
      "33/60 [===============>..............] - ETA: 21s - loss: 0.2170 - bce_dice_loss: 0.2170\n",
      "Training: batch 33 begins at 23:02:29.787367\n",
      "\n",
      "Training: batch 33 ends at 23:02:30.591091\n",
      "34/60 [================>.............] - ETA: 21s - loss: 0.2159 - bce_dice_loss: 0.2159\n",
      "Training: batch 34 begins at 23:02:30.594708\n",
      "\n",
      "Training: batch 34 ends at 23:02:31.406281\n",
      "35/60 [================>.............] - ETA: 20s - loss: 0.2148 - bce_dice_loss: 0.2148\n",
      "Training: batch 35 begins at 23:02:31.410450\n",
      "\n",
      "Training: batch 35 ends at 23:02:32.207760\n",
      "36/60 [=================>............] - ETA: 19s - loss: 0.2140 - bce_dice_loss: 0.2140\n",
      "Training: batch 36 begins at 23:02:32.210716\n",
      "\n",
      "Training: batch 36 ends at 23:02:33.014982\n",
      "37/60 [=================>............] - ETA: 18s - loss: 0.2136 - bce_dice_loss: 0.2136\n",
      "Training: batch 37 begins at 23:02:33.017976\n",
      "\n",
      "Training: batch 37 ends at 23:02:33.843064\n",
      "38/60 [==================>...........] - ETA: 17s - loss: 0.2143 - bce_dice_loss: 0.2143\n",
      "Training: batch 38 begins at 23:02:33.847347\n",
      "\n",
      "Training: batch 38 ends at 23:02:34.656036\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.2140 - bce_dice_loss: 0.2140\n",
      "Training: batch 39 begins at 23:02:34.659223\n",
      "\n",
      "Training: batch 39 ends at 23:02:35.460948\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2134 - bce_dice_loss: 0.2134\n",
      "Training: batch 40 begins at 23:02:35.464979\n",
      "\n",
      "Training: batch 40 ends at 23:02:36.307336\n",
      "41/60 [===================>..........] - ETA: 15s - loss: 0.2136 - bce_dice_loss: 0.2136\n",
      "Training: batch 41 begins at 23:02:36.309713\n",
      "\n",
      "Training: batch 41 ends at 23:02:37.116257\n",
      "42/60 [====================>.........] - ETA: 14s - loss: 0.2128 - bce_dice_loss: 0.2128\n",
      "Training: batch 42 begins at 23:02:37.119977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 42 ends at 23:02:37.930770\n",
      "43/60 [====================>.........] - ETA: 13s - loss: 0.2114 - bce_dice_loss: 0.2114\n",
      "Training: batch 43 begins at 23:02:37.934159\n",
      "\n",
      "Training: batch 43 ends at 23:02:38.939045\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.2123 - bce_dice_loss: 0.2123\n",
      "Training: batch 44 begins at 23:02:38.942103\n",
      "\n",
      "Training: batch 44 ends at 23:02:39.864489\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2160 - bce_dice_loss: 0.2160\n",
      "Training: batch 45 begins at 23:02:39.867974\n",
      "\n",
      "Training: batch 45 ends at 23:02:40.879768\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2206 - bce_dice_loss: 0.2206\n",
      "Training: batch 46 begins at 23:02:40.884960\n",
      "\n",
      "Training: batch 46 ends at 23:02:41.831109\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2212 - bce_dice_loss: 0.2212\n",
      "Training: batch 47 begins at 23:02:41.834376\n",
      "\n",
      "Training: batch 47 ends at 23:02:42.907745\n",
      "48/60 [=======================>......] - ETA: 9s - loss: 0.2200 - bce_dice_loss: 0.2200 \n",
      "Training: batch 48 begins at 23:02:42.910349\n",
      "\n",
      "Training: batch 48 ends at 23:02:43.921320\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.2198 - bce_dice_loss: 0.2198\n",
      "Training: batch 49 begins at 23:02:43.923870\n",
      "\n",
      "Training: batch 49 ends at 23:02:44.903915\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2202 - bce_dice_loss: 0.2202\n",
      "Training: batch 50 begins at 23:02:44.906747\n",
      "\n",
      "Training: batch 50 ends at 23:02:45.813336\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2212 - bce_dice_loss: 0.2212\n",
      "Training: batch 51 begins at 23:02:45.816600\n",
      "\n",
      "Training: batch 51 ends at 23:02:46.688377\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2226 - bce_dice_loss: 0.2226\n",
      "Training: batch 52 begins at 23:02:46.691559\n",
      "\n",
      "Training: batch 52 ends at 23:02:47.503093\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2236 - bce_dice_loss: 0.2236\n",
      "Training: batch 53 begins at 23:02:47.506777\n",
      "\n",
      "Training: batch 53 ends at 23:02:48.310059\n",
      "54/60 [==========================>...] - ETA: 5s - loss: 0.2234 - bce_dice_loss: 0.2234\n",
      "Training: batch 54 begins at 23:02:48.313643\n",
      "\n",
      "Training: batch 54 ends at 23:02:49.105996\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2220 - bce_dice_loss: 0.2220\n",
      "Training: batch 55 begins at 23:02:49.109532\n",
      "\n",
      "Training: batch 55 ends at 23:02:49.953903\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2213 - bce_dice_loss: 0.2213\n",
      "Training: batch 56 begins at 23:02:49.957001\n",
      "\n",
      "Training: batch 56 ends at 23:02:50.976388\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2202 - bce_dice_loss: 0.2202\n",
      "Training: batch 57 begins at 23:02:50.979086\n",
      "\n",
      "Training: batch 57 ends at 23:02:51.825568\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2233 - bce_dice_loss: 0.2233\n",
      "Training: batch 58 begins at 23:02:51.828123\n",
      "\n",
      "Training: batch 58 ends at 23:02:52.626440\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2221 - bce_dice_loss: 0.2221\n",
      "Training: batch 59 begins at 23:02:52.630754\n",
      "\n",
      "Training: batch 59 ends at 23:02:53.440661\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2230 - bce_dice_loss: 0.2230\n",
      "Evaluating: batch 0 begins at 23:02:53.468239\n",
      "\n",
      "Evaluating: batch 0 ends at 23:02:53.739651\n",
      "\n",
      "Evaluating: batch 1 begins at 23:02:53.741135\n",
      "\n",
      "Evaluating: batch 1 ends at 23:02:53.955532\n",
      "\n",
      "Evaluating: batch 2 begins at 23:02:53.956725\n",
      "\n",
      "Evaluating: batch 2 ends at 23:02:54.174986\n",
      "\n",
      "Evaluating: batch 3 begins at 23:02:54.177411\n",
      "\n",
      "Evaluating: batch 3 ends at 23:02:54.400202\n",
      "\n",
      "Evaluating: batch 4 begins at 23:02:54.404859\n",
      "\n",
      "Evaluating: batch 4 ends at 23:02:54.623955\n",
      "\n",
      "Evaluating: batch 5 begins at 23:02:54.625413\n",
      "\n",
      "Evaluating: batch 5 ends at 23:02:54.848717\n",
      "\n",
      "Evaluating: batch 6 begins at 23:02:54.850212\n",
      "\n",
      "Evaluating: batch 6 ends at 23:02:55.067077\n",
      "\n",
      "Evaluating: batch 7 begins at 23:02:55.071488\n",
      "\n",
      "Evaluating: batch 7 ends at 23:02:55.290295\n",
      "\n",
      "Evaluating: batch 8 begins at 23:02:55.292166\n",
      "\n",
      "Evaluating: batch 8 ends at 23:02:55.511211\n",
      "\n",
      "Evaluating: batch 9 begins at 23:02:55.513607\n",
      "\n",
      "Evaluating: batch 9 ends at 23:02:55.739859\n",
      "\n",
      "Evaluating: batch 10 begins at 23:02:55.741249\n",
      "\n",
      "Evaluating: batch 10 ends at 23:02:55.961412\n",
      "\n",
      "Evaluating: batch 11 begins at 23:02:55.962976\n",
      "\n",
      "Evaluating: batch 11 ends at 23:02:56.181760\n",
      "\n",
      "Evaluating: batch 12 begins at 23:02:56.187000\n",
      "\n",
      "Evaluating: batch 12 ends at 23:02:56.411237\n",
      "\n",
      "Evaluating: batch 13 begins at 23:02:56.412620\n",
      "\n",
      "Evaluating: batch 13 ends at 23:02:56.633029\n",
      "\n",
      "Evaluating: batch 14 begins at 23:02:56.634579\n",
      "\n",
      "Evaluating: batch 14 ends at 23:02:56.857665\n",
      "\n",
      "Evaluating: batch 15 begins at 23:02:56.859020\n",
      "\n",
      "Evaluating: batch 15 ends at 23:02:57.077296\n",
      "\n",
      "Evaluating: batch 16 begins at 23:02:57.078570\n",
      "\n",
      "Evaluating: batch 16 ends at 23:02:57.297440\n",
      "\n",
      "Evaluating: batch 17 begins at 23:02:57.299775\n",
      "\n",
      "Evaluating: batch 17 ends at 23:02:57.524334\n",
      "\n",
      "Evaluating: batch 18 begins at 23:02:57.525676\n",
      "\n",
      "Evaluating: batch 18 ends at 23:02:57.752998\n",
      "\n",
      "Evaluating: batch 19 begins at 23:02:57.754335\n",
      "\n",
      "Evaluating: batch 19 ends at 23:02:57.970832\n",
      "\n",
      "Evaluating: batch 20 begins at 23:02:57.972140\n",
      "\n",
      "Evaluating: batch 20 ends at 23:02:58.191170\n",
      "\n",
      "Evaluating: batch 21 begins at 23:02:58.192358\n",
      "\n",
      "Evaluating: batch 21 ends at 23:02:58.407418\n",
      "\n",
      "Evaluating: batch 22 begins at 23:02:58.408650\n",
      "\n",
      "Evaluating: batch 22 ends at 23:02:58.624275\n",
      "\n",
      "Evaluating: batch 23 begins at 23:02:58.626123\n",
      "\n",
      "Evaluating: batch 23 ends at 23:02:58.854204\n",
      "\n",
      "Evaluating: batch 24 begins at 23:02:58.856382\n",
      "\n",
      "Evaluating: batch 24 ends at 23:02:59.075639\n",
      "\n",
      "Evaluating: batch 25 begins at 23:02:59.076912\n",
      "\n",
      "Evaluating: batch 25 ends at 23:02:59.298498\n",
      "\n",
      "Evaluating: batch 26 begins at 23:02:59.302092\n",
      "\n",
      "Evaluating: batch 26 ends at 23:02:59.532105\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.25974\n",
      "60/60 [==============================] - 57s 944ms/step - loss: 0.2230 - bce_dice_loss: 0.2230 - val_loss: 0.2734 - val_bce_dice_loss: 0.2734\n",
      "Epoch 25/25\n",
      "\n",
      "Training: batch 0 begins at 23:02:59.561005\n",
      "\n",
      "Training: batch 0 ends at 23:03:00.369032\n",
      " 1/60 [..............................] - ETA: 47s - loss: 0.1642 - bce_dice_loss: 0.1642\n",
      "Training: batch 1 begins at 23:03:00.371494\n",
      "\n",
      "Training: batch 1 ends at 23:03:01.474142\n",
      " 2/60 [>.............................] - ETA: 1:04 - loss: 0.1857 - bce_dice_loss: 0.1857\n",
      "Training: batch 2 begins at 23:03:01.476760\n",
      "\n",
      "Training: batch 2 ends at 23:03:02.348784\n",
      " 3/60 [>.............................] - ETA: 56s - loss: 0.1791 - bce_dice_loss: 0.1791 \n",
      "Training: batch 3 begins at 23:03:02.353113\n",
      "\n",
      "Training: batch 3 ends at 23:03:03.277332\n",
      " 4/60 [=>............................] - ETA: 54s - loss: 0.1741 - bce_dice_loss: 0.1741\n",
      "Training: batch 4 begins at 23:03:03.280107\n",
      "\n",
      "Training: batch 4 ends at 23:03:04.150283\n",
      " 5/60 [=>............................] - ETA: 51s - loss: 0.1629 - bce_dice_loss: 0.1629\n",
      "Training: batch 5 begins at 23:03:04.154670\n",
      "\n",
      "Training: batch 5 ends at 23:03:04.981017\n",
      " 6/60 [==>...........................] - ETA: 49s - loss: 0.1697 - bce_dice_loss: 0.1697\n",
      "Training: batch 6 begins at 23:03:04.984397\n",
      "\n",
      "Training: batch 6 ends at 23:03:05.827938\n",
      " 7/60 [==>...........................] - ETA: 48s - loss: 0.1750 - bce_dice_loss: 0.1750\n",
      "Training: batch 7 begins at 23:03:05.830494\n",
      "\n",
      "Training: batch 7 ends at 23:03:06.651645\n",
      " 8/60 [===>..........................] - ETA: 46s - loss: 0.1856 - bce_dice_loss: 0.1856\n",
      "Training: batch 8 begins at 23:03:06.655902\n",
      "\n",
      "Training: batch 8 ends at 23:03:07.529846\n",
      " 9/60 [===>..........................] - ETA: 45s - loss: 0.1954 - bce_dice_loss: 0.1954\n",
      "Training: batch 9 begins at 23:03:07.535089\n",
      "\n",
      "Training: batch 9 ends at 23:03:08.384567\n",
      "10/60 [====>.........................] - ETA: 44s - loss: 0.1919 - bce_dice_loss: 0.1919\n",
      "Training: batch 10 begins at 23:03:08.387622\n",
      "\n",
      "Training: batch 10 ends at 23:03:09.204477\n",
      "11/60 [====>.........................] - ETA: 43s - loss: 0.1879 - bce_dice_loss: 0.1879\n",
      "Training: batch 11 begins at 23:03:09.208163\n",
      "\n",
      "Training: batch 11 ends at 23:03:10.018124\n",
      "12/60 [=====>........................] - ETA: 42s - loss: 0.1829 - bce_dice_loss: 0.1829\n",
      "Training: batch 12 begins at 23:03:10.020527\n",
      "\n",
      "Training: batch 12 ends at 23:03:10.858600\n",
      "13/60 [=====>........................] - ETA: 41s - loss: 0.1855 - bce_dice_loss: 0.1855\n",
      "Training: batch 13 begins at 23:03:10.862917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: batch 13 ends at 23:03:11.691920\n",
      "14/60 [======>.......................] - ETA: 40s - loss: 0.1857 - bce_dice_loss: 0.1857\n",
      "Training: batch 14 begins at 23:03:11.696336\n",
      "\n",
      "Training: batch 14 ends at 23:03:12.515929\n",
      "15/60 [======>.......................] - ETA: 39s - loss: 0.1858 - bce_dice_loss: 0.1858\n",
      "Training: batch 15 begins at 23:03:12.518965\n",
      "\n",
      "Training: batch 15 ends at 23:03:13.356689\n",
      "16/60 [=======>......................] - ETA: 38s - loss: 0.1837 - bce_dice_loss: 0.1837\n",
      "Training: batch 16 begins at 23:03:13.360364\n",
      "\n",
      "Training: batch 16 ends at 23:03:14.182771\n",
      "17/60 [=======>......................] - ETA: 37s - loss: 0.1824 - bce_dice_loss: 0.1824\n",
      "Training: batch 17 begins at 23:03:14.186123\n",
      "\n",
      "Training: batch 17 ends at 23:03:15.044666\n",
      "18/60 [========>.....................] - ETA: 36s - loss: 0.1995 - bce_dice_loss: 0.1995\n",
      "Training: batch 18 begins at 23:03:15.049184\n",
      "\n",
      "Training: batch 18 ends at 23:03:15.839357\n",
      "19/60 [========>.....................] - ETA: 35s - loss: 0.1972 - bce_dice_loss: 0.1972\n",
      "Training: batch 19 begins at 23:03:15.842559\n",
      "\n",
      "Training: batch 19 ends at 23:03:16.668854\n",
      "20/60 [=========>....................] - ETA: 34s - loss: 0.1981 - bce_dice_loss: 0.1981\n",
      "Training: batch 20 begins at 23:03:16.673464\n",
      "\n",
      "Training: batch 20 ends at 23:03:17.537679\n",
      "21/60 [=========>....................] - ETA: 33s - loss: 0.1957 - bce_dice_loss: 0.1957\n",
      "Training: batch 21 begins at 23:03:17.540478\n",
      "\n",
      "Training: batch 21 ends at 23:03:18.426285\n",
      "22/60 [==========>...................] - ETA: 32s - loss: 0.1917 - bce_dice_loss: 0.1917\n",
      "Training: batch 22 begins at 23:03:18.429990\n",
      "\n",
      "Training: batch 22 ends at 23:03:19.256139\n",
      "23/60 [==========>...................] - ETA: 31s - loss: 0.1878 - bce_dice_loss: 0.1878\n",
      "Training: batch 23 begins at 23:03:19.260252\n",
      "\n",
      "Training: batch 23 ends at 23:03:20.089962\n",
      "24/60 [===========>..................] - ETA: 30s - loss: 0.1965 - bce_dice_loss: 0.1965\n",
      "Training: batch 24 begins at 23:03:20.092595\n",
      "\n",
      "Training: batch 24 ends at 23:03:20.893920\n",
      "25/60 [===========>..................] - ETA: 29s - loss: 0.1971 - bce_dice_loss: 0.1971\n",
      "Training: batch 25 begins at 23:03:20.897884\n",
      "\n",
      "Training: batch 25 ends at 23:03:21.743070\n",
      "26/60 [============>.................] - ETA: 29s - loss: 0.1969 - bce_dice_loss: 0.1969\n",
      "Training: batch 26 begins at 23:03:21.746010\n",
      "\n",
      "Training: batch 26 ends at 23:03:22.590696\n",
      "27/60 [============>.................] - ETA: 28s - loss: 0.1952 - bce_dice_loss: 0.1952\n",
      "Training: batch 27 begins at 23:03:22.591624\n",
      "\n",
      "Training: batch 27 ends at 23:03:23.404298\n",
      "28/60 [=============>................] - ETA: 27s - loss: 0.1991 - bce_dice_loss: 0.1991\n",
      "Training: batch 28 begins at 23:03:23.406907\n",
      "\n",
      "Training: batch 28 ends at 23:03:24.247211\n",
      "29/60 [=============>................] - ETA: 26s - loss: 0.1969 - bce_dice_loss: 0.1969\n",
      "Training: batch 29 begins at 23:03:24.251406\n",
      "\n",
      "Training: batch 29 ends at 23:03:25.063248\n",
      "30/60 [==============>...............] - ETA: 25s - loss: 0.1960 - bce_dice_loss: 0.1960\n",
      "Training: batch 30 begins at 23:03:25.066277\n",
      "\n",
      "Training: batch 30 ends at 23:03:25.901784\n",
      "31/60 [==============>...............] - ETA: 24s - loss: 0.1989 - bce_dice_loss: 0.1989\n",
      "Training: batch 31 begins at 23:03:25.905040\n",
      "\n",
      "Training: batch 31 ends at 23:03:26.711942\n",
      "32/60 [===============>..............] - ETA: 23s - loss: 0.2012 - bce_dice_loss: 0.2012\n",
      "Training: batch 32 begins at 23:03:26.715200\n",
      "\n",
      "Training: batch 32 ends at 23:03:27.557206\n",
      "33/60 [===============>..............] - ETA: 22s - loss: 0.2000 - bce_dice_loss: 0.2000\n",
      "Training: batch 33 begins at 23:03:27.560388\n",
      "\n",
      "Training: batch 33 ends at 23:03:28.373933\n",
      "34/60 [================>.............] - ETA: 22s - loss: 0.2003 - bce_dice_loss: 0.2003\n",
      "Training: batch 34 begins at 23:03:28.380868\n",
      "\n",
      "Training: batch 34 ends at 23:03:29.203088\n",
      "35/60 [================>.............] - ETA: 21s - loss: 0.1994 - bce_dice_loss: 0.1994\n",
      "Training: batch 35 begins at 23:03:29.207729\n",
      "\n",
      "Training: batch 35 ends at 23:03:30.018910\n",
      "36/60 [=================>............] - ETA: 20s - loss: 0.1986 - bce_dice_loss: 0.1986\n",
      "Training: batch 36 begins at 23:03:30.022191\n",
      "\n",
      "Training: batch 36 ends at 23:03:30.843864\n",
      "37/60 [=================>............] - ETA: 19s - loss: 0.1977 - bce_dice_loss: 0.1977\n",
      "Training: batch 37 begins at 23:03:30.847080\n",
      "\n",
      "Training: batch 37 ends at 23:03:31.663305\n",
      "38/60 [==================>...........] - ETA: 18s - loss: 0.2041 - bce_dice_loss: 0.2041\n",
      "Training: batch 38 begins at 23:03:31.668345\n",
      "\n",
      "Training: batch 38 ends at 23:03:32.473446\n",
      "39/60 [==================>...........] - ETA: 17s - loss: 0.2062 - bce_dice_loss: 0.2062\n",
      "Training: batch 39 begins at 23:03:32.476253\n",
      "\n",
      "Training: batch 39 ends at 23:03:33.311993\n",
      "40/60 [===================>..........] - ETA: 16s - loss: 0.2033 - bce_dice_loss: 0.2033\n",
      "Training: batch 40 begins at 23:03:33.315757\n",
      "\n",
      "Training: batch 40 ends at 23:03:34.135752\n",
      "41/60 [===================>..........] - ETA: 16s - loss: 0.2011 - bce_dice_loss: 0.2011\n",
      "Training: batch 41 begins at 23:03:34.139279\n",
      "\n",
      "Training: batch 41 ends at 23:03:34.963567\n",
      "42/60 [====================>.........] - ETA: 15s - loss: 0.2018 - bce_dice_loss: 0.2018\n",
      "Training: batch 42 begins at 23:03:34.968613\n",
      "\n",
      "Training: batch 42 ends at 23:03:35.781439\n",
      "43/60 [====================>.........] - ETA: 14s - loss: 0.2038 - bce_dice_loss: 0.2038\n",
      "Training: batch 43 begins at 23:03:35.784685\n",
      "\n",
      "Training: batch 43 ends at 23:03:36.580302\n",
      "44/60 [=====================>........] - ETA: 13s - loss: 0.2043 - bce_dice_loss: 0.2043\n",
      "Training: batch 44 begins at 23:03:36.584732\n",
      "\n",
      "Training: batch 44 ends at 23:03:37.405822\n",
      "45/60 [=====================>........] - ETA: 12s - loss: 0.2076 - bce_dice_loss: 0.2076\n",
      "Training: batch 45 begins at 23:03:37.408451\n",
      "\n",
      "Training: batch 45 ends at 23:03:38.209569\n",
      "46/60 [======================>.......] - ETA: 11s - loss: 0.2102 - bce_dice_loss: 0.2102\n",
      "Training: batch 46 begins at 23:03:38.212583\n",
      "\n",
      "Training: batch 46 ends at 23:03:39.046715\n",
      "47/60 [======================>.......] - ETA: 10s - loss: 0.2107 - bce_dice_loss: 0.2107\n",
      "Training: batch 47 begins at 23:03:39.049910\n",
      "\n",
      "Training: batch 47 ends at 23:03:39.987576\n",
      "48/60 [=======================>......] - ETA: 10s - loss: 0.2112 - bce_dice_loss: 0.2112\n",
      "Training: batch 48 begins at 23:03:39.990830\n",
      "\n",
      "Training: batch 48 ends at 23:03:40.916255\n",
      "49/60 [=======================>......] - ETA: 9s - loss: 0.2135 - bce_dice_loss: 0.2135 \n",
      "Training: batch 49 begins at 23:03:40.919604\n",
      "\n",
      "Training: batch 49 ends at 23:03:41.730977\n",
      "50/60 [========================>.....] - ETA: 8s - loss: 0.2134 - bce_dice_loss: 0.2134\n",
      "Training: batch 50 begins at 23:03:41.735439\n",
      "\n",
      "Training: batch 50 ends at 23:03:42.572033\n",
      "51/60 [========================>.....] - ETA: 7s - loss: 0.2124 - bce_dice_loss: 0.2124\n",
      "Training: batch 51 begins at 23:03:42.575394\n",
      "\n",
      "Training: batch 51 ends at 23:03:43.395218\n",
      "52/60 [=========================>....] - ETA: 6s - loss: 0.2112 - bce_dice_loss: 0.2112\n",
      "Training: batch 52 begins at 23:03:43.401461\n",
      "\n",
      "Training: batch 52 ends at 23:03:44.197396\n",
      "53/60 [=========================>....] - ETA: 5s - loss: 0.2182 - bce_dice_loss: 0.2182\n",
      "Training: batch 53 begins at 23:03:44.200895\n",
      "\n",
      "Training: batch 53 ends at 23:03:45.016966\n",
      "54/60 [==========================>...] - ETA: 5s - loss: 0.2198 - bce_dice_loss: 0.2198\n",
      "Training: batch 54 begins at 23:03:45.021021\n",
      "\n",
      "Training: batch 54 ends at 23:03:45.843188\n",
      "55/60 [==========================>...] - ETA: 4s - loss: 0.2193 - bce_dice_loss: 0.2193\n",
      "Training: batch 55 begins at 23:03:45.847868\n",
      "\n",
      "Training: batch 55 ends at 23:03:46.658743\n",
      "56/60 [===========================>..] - ETA: 3s - loss: 0.2191 - bce_dice_loss: 0.2191\n",
      "Training: batch 56 begins at 23:03:46.663194\n",
      "\n",
      "Training: batch 56 ends at 23:03:47.485801\n",
      "57/60 [===========================>..] - ETA: 2s - loss: 0.2204 - bce_dice_loss: 0.2204\n",
      "Training: batch 57 begins at 23:03:47.488662\n",
      "\n",
      "Training: batch 57 ends at 23:03:48.311499\n",
      "58/60 [============================>.] - ETA: 1s - loss: 0.2214 - bce_dice_loss: 0.2214\n",
      "Training: batch 58 begins at 23:03:48.316873\n",
      "\n",
      "Training: batch 58 ends at 23:03:49.123605\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2200 - bce_dice_loss: 0.2200\n",
      "Training: batch 59 begins at 23:03:49.127512\n",
      "\n",
      "Training: batch 59 ends at 23:03:49.974017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - ETA: 0s - loss: 0.2204 - bce_dice_loss: 0.2204\n",
      "Evaluating: batch 0 begins at 23:03:50.014075\n",
      "\n",
      "Evaluating: batch 0 ends at 23:03:50.321746\n",
      "\n",
      "Evaluating: batch 1 begins at 23:03:50.323522\n",
      "\n",
      "Evaluating: batch 1 ends at 23:03:50.542720\n",
      "\n",
      "Evaluating: batch 2 begins at 23:03:50.545289\n",
      "\n",
      "Evaluating: batch 2 ends at 23:03:50.772781\n",
      "\n",
      "Evaluating: batch 3 begins at 23:03:50.774216\n",
      "\n",
      "Evaluating: batch 3 ends at 23:03:51.005364\n",
      "\n",
      "Evaluating: batch 4 begins at 23:03:51.007789\n",
      "\n",
      "Evaluating: batch 4 ends at 23:03:51.229559\n",
      "\n",
      "Evaluating: batch 5 begins at 23:03:51.230973\n",
      "\n",
      "Evaluating: batch 5 ends at 23:03:51.466509\n",
      "\n",
      "Evaluating: batch 6 begins at 23:03:51.467939\n",
      "\n",
      "Evaluating: batch 6 ends at 23:03:51.723298\n",
      "\n",
      "Evaluating: batch 7 begins at 23:03:51.725450\n",
      "\n",
      "Evaluating: batch 7 ends at 23:03:51.948033\n",
      "\n",
      "Evaluating: batch 8 begins at 23:03:51.951889\n",
      "\n",
      "Evaluating: batch 8 ends at 23:03:52.191678\n",
      "\n",
      "Evaluating: batch 9 begins at 23:03:52.193731\n",
      "\n",
      "Evaluating: batch 9 ends at 23:03:52.421365\n",
      "\n",
      "Evaluating: batch 10 begins at 23:03:52.422655\n",
      "\n",
      "Evaluating: batch 10 ends at 23:03:52.653506\n",
      "\n",
      "Evaluating: batch 11 begins at 23:03:52.655017\n",
      "\n",
      "Evaluating: batch 11 ends at 23:03:52.880061\n",
      "\n",
      "Evaluating: batch 12 begins at 23:03:52.881527\n",
      "\n",
      "Evaluating: batch 12 ends at 23:03:53.109288\n",
      "\n",
      "Evaluating: batch 13 begins at 23:03:53.110538\n",
      "\n",
      "Evaluating: batch 13 ends at 23:03:53.341865\n",
      "\n",
      "Evaluating: batch 14 begins at 23:03:53.343129\n",
      "\n",
      "Evaluating: batch 14 ends at 23:03:53.587963\n",
      "\n",
      "Evaluating: batch 15 begins at 23:03:53.590024\n",
      "\n",
      "Evaluating: batch 15 ends at 23:03:53.813708\n",
      "\n",
      "Evaluating: batch 16 begins at 23:03:53.815137\n",
      "\n",
      "Evaluating: batch 16 ends at 23:03:54.037094\n",
      "\n",
      "Evaluating: batch 17 begins at 23:03:54.038511\n",
      "\n",
      "Evaluating: batch 17 ends at 23:03:54.267235\n",
      "\n",
      "Evaluating: batch 18 begins at 23:03:54.269388\n",
      "\n",
      "Evaluating: batch 18 ends at 23:03:54.490320\n",
      "\n",
      "Evaluating: batch 19 begins at 23:03:54.491424\n",
      "\n",
      "Evaluating: batch 19 ends at 23:03:54.728663\n",
      "\n",
      "Evaluating: batch 20 begins at 23:03:54.729649\n",
      "\n",
      "Evaluating: batch 20 ends at 23:03:54.947422\n",
      "\n",
      "Evaluating: batch 21 begins at 23:03:54.949166\n",
      "\n",
      "Evaluating: batch 21 ends at 23:03:55.174270\n",
      "\n",
      "Evaluating: batch 22 begins at 23:03:55.175580\n",
      "\n",
      "Evaluating: batch 22 ends at 23:03:55.412590\n",
      "\n",
      "Evaluating: batch 23 begins at 23:03:55.414614\n",
      "\n",
      "Evaluating: batch 23 ends at 23:03:55.644132\n",
      "\n",
      "Evaluating: batch 24 begins at 23:03:55.645470\n",
      "\n",
      "Evaluating: batch 24 ends at 23:03:55.865852\n",
      "\n",
      "Evaluating: batch 25 begins at 23:03:55.867311\n",
      "\n",
      "Evaluating: batch 25 ends at 23:03:56.085040\n",
      "\n",
      "Evaluating: batch 26 begins at 23:03:56.086476\n",
      "\n",
      "Evaluating: batch 26 ends at 23:03:56.309121\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.25974\n",
      "60/60 [==============================] - 57s 949ms/step - loss: 0.2204 - bce_dice_loss: 0.2204 - val_loss: 0.2648 - val_bce_dice_loss: 0.2648\n",
      "Train hist recorded for batch -  2\n",
      "Saving the model\n",
      "Saved the model at saved_models/training_25/batch_2/\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.metrics import Metric\n",
    "\n",
    "# Training configs\n",
    "EPOCHS = 25 #25-100\n",
    "LEARNING_RATE = 0.0005 #3e-14\n",
    "TRAINING_SAVE_LOC = \"saved_models/training_25/\"\n",
    "\n",
    "model = None\n",
    "scaler = MinMaxScaler()\n",
    "train_hist = {}\n",
    "import random\n",
    "\n",
    "for b,img_names in enumerate(img_names_batches):\n",
    "#     if len(img_names) > 90:\n",
    "#         EPOCHS = 30\n",
    "#         print(\"EPOCHS set to \", EPOCHS)\n",
    "#     elif len(img_names) < 70:\n",
    "#         EPOCHS = 5\n",
    "#         print(\"EPOCHS set to \", EPOCHS)\n",
    "#     else:\n",
    "#         EPOCHS = 20\n",
    "#         print(\"EPOCHS set to \", EPOCHS)\n",
    "    \n",
    "    # Setting the model\n",
    "    if model is None:\n",
    "        print(\"No model set. Creating new model.\")\n",
    "        model = get_unet()\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss= bce_dice_loss, metrics=[bce_dice_loss]) #loss =tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "        INITIAL_TRAIN = False\n",
    "        print(\"UNet model created from scratch.\")\n",
    "    # else:\n",
    "    #      model = tf.keras.models.load_model(TRAINING_SAVE_LOC + \"batch_\" + str(b - 1) + \"/\", custom_objects={\"CustomModel\": mean_iou})\n",
    "    #      print(\"UNet model loaded from \", TRAINING_SAVE_LOC + \"batch_\" + str(b - 1) + \"/\") \n",
    "    # if model is None:\n",
    "    #          print(\"!!! MODEL NOT SET !!!\")\n",
    "    #          break\n",
    "              \n",
    "    # Preparing the data\n",
    "    x = []\n",
    "    y = []\n",
    "    print(\"Reading train data\")\n",
    "    for i in img_names:\n",
    "        x.append(np.array(PIL.Image.open(train_x_loc + i + \".jpg\").resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST))/255.)\n",
    "        y.append(to_categorical(np.array(PIL.Image.open(train_y_loc + i + \".png\").resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST)), N_CLASSES))\n",
    "    x = np.array(x)\n",
    "    #x_mean = x.mean() #centering pixels\n",
    "    #x = x - x_mean #centering pixels\n",
    "    y = np.array(y)\n",
    "    #flip = random.randint(0, 1)\n",
    "    #if flip:\n",
    "    x = np.append(x, [np.fliplr(i) for i in x], axis=0) #data augmentation (flipping)\n",
    "    y = np.append(y, [np.fliplr(i) for i in y], axis=0) #data augmentation (flipping)\n",
    "\n",
    "    # Reserve samples for validation\n",
    "    val_samples = 54\n",
    "    x_train = x[:-val_samples]\n",
    "    print(\"x train ---- \", len(x_train))\n",
    "    y_train = y[:-val_samples]\n",
    "    x_val = x[-val_samples:]\n",
    "    print(\"x val ---- \", len(x_val))\n",
    "    y_val = y[-val_samples:]\n",
    "    del x,y\n",
    "    print(\"Train and Validation data created\")\n",
    "    \n",
    "    # Training the model\n",
    "    model_history = model.fit(x_train, y_train, batch_size=2, epochs=EPOCHS, validation_data=(x_val, y_val), shuffle=True, callbacks=[MyCustomCallback(),tensorboard,model_checkpoint])\n",
    "    train_hist[b] = model_history\n",
    "    print(\"Train hist recorded for batch - \",b)\n",
    "    #del x_train,y_train,x_val,y_val\n",
    "    \n",
    "    # Saving the model\n",
    "    print(\"Saving the model\")\n",
    "    BATCH_LOC = \"batch_\" + str(b) + \"/\"\n",
    "    CURRENT_SAVE_LOC = TRAINING_SAVE_LOC + BATCH_LOC\n",
    "    #model.save(CURRENT_SAVE_LOC)\n",
    "    #model.save(CURRENT_SAVE_LOC,save_format='h5')\n",
    "    print(\"Saved the model at\", CURRENT_SAVE_LOC)\n",
    "\n",
    "    #0.645 com early stopping\n",
    "    #0.66 sem early stopping\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBuYEyZrzMsB"
   },
   "source": [
    "training_11 is the best train acc as of now but kaggle acc - 0.60\n",
    "training_13 is the best kaggle acc with 0.62\n",
    "training_12 - 261 - sftm kaggle acc was still 0.60\n",
    "training_15 - reduced epoch - kaggle 0.64\n",
    "training_16 - reduced epoch still - BEST kaggle 0.67\n",
    "training_19 - diff approach - 3 batches of 87 each - kaggle 0.66\n",
    "t_20 - 87-batch - model on 2 batch seems to perform good - kaggle 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkzKAjsRzMsB"
   },
   "source": [
    "setttings for training_15:\n",
    "    if len(img_names) > 90:\n",
    "        EPOCHS = 35\n",
    "        print(\" EPOCHS set to \", EPOCHS)\n",
    "    elif len(img_names) < 70:\n",
    "        EPOCHS = 5\n",
    "        print(\" EPOCHS set to \", EPOCHS)\n",
    "    else:\n",
    "        EPOCHS = 20\n",
    "        print(\" EPOCHS set to \", EPOCHS)\n",
    "    >>>>>>> and <<<<<<<<\n",
    "    90 - train; 10 - val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC9lBIY7zMsB"
   },
   "source": [
    "settings for training_16: \n",
    "    if len(img_names) > 90:\n",
    "        EPOCHS = 25\n",
    "        print(\"EPOCHS set to \", EPOCHS)\n",
    "    elif len(img_names) < 70:\n",
    "        EPOCHS = 5\n",
    "        print(\"EPOCHS set to \", EPOCHS)\n",
    "    else:\n",
    "        EPOCHS = 20\n",
    "        print(\"EPOCHS set to \", EPOCHS)\n",
    "        >>>>>>> and <<<<<<<\n",
    "    80 - train; 20 - val\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-PfpGbRzMsB"
   },
   "source": [
    "settings for training_19:\n",
    "epochs = 20 >>>>>> and <<<<<< 70 - train; 17 - val\n",
    "\n",
    "settings for t_20:\n",
    "epochs = 20 and t/v = 60/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/training_25/assets\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SAVE_LOC = \"../saved_models/training_25/\"\n",
    "model.save(TRAINING_SAVE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.4059067964553833,\n",
       "  1.0307209491729736,\n",
       "  0.8801981806755066,\n",
       "  0.7873104214668274,\n",
       "  0.7639487385749817,\n",
       "  0.735186755657196,\n",
       "  0.7018197178840637,\n",
       "  0.6453587412834167,\n",
       "  0.645464301109314,\n",
       "  0.6293708682060242,\n",
       "  0.6005591750144958,\n",
       "  0.5849204063415527,\n",
       "  0.5728281736373901,\n",
       "  0.577838659286499,\n",
       "  0.5718914866447449,\n",
       "  0.5625578165054321,\n",
       "  0.5432363152503967,\n",
       "  0.5887104868888855,\n",
       "  0.5750364065170288,\n",
       "  0.5466738939285278,\n",
       "  0.549603283405304,\n",
       "  0.5459474921226501,\n",
       "  0.5307615399360657,\n",
       "  0.551544189453125,\n",
       "  0.5254135727882385],\n",
       " 'bce_dice_loss': [1.4059067964553833,\n",
       "  1.0307209491729736,\n",
       "  0.8801981806755066,\n",
       "  0.7873104214668274,\n",
       "  0.7639487385749817,\n",
       "  0.735186755657196,\n",
       "  0.7018197178840637,\n",
       "  0.6453587412834167,\n",
       "  0.645464301109314,\n",
       "  0.6293708682060242,\n",
       "  0.6005591750144958,\n",
       "  0.5849204063415527,\n",
       "  0.5728281736373901,\n",
       "  0.577838659286499,\n",
       "  0.5718914866447449,\n",
       "  0.5625578165054321,\n",
       "  0.5432363152503967,\n",
       "  0.5887104868888855,\n",
       "  0.5750364065170288,\n",
       "  0.5466738939285278,\n",
       "  0.549603283405304,\n",
       "  0.5459474921226501,\n",
       "  0.5307615399360657,\n",
       "  0.551544189453125,\n",
       "  0.5254135727882385],\n",
       " 'val_loss': [1.195053219795227,\n",
       "  0.9412115812301636,\n",
       "  0.8287210464477539,\n",
       "  0.7840392589569092,\n",
       "  0.7772467732429504,\n",
       "  0.7064434885978699,\n",
       "  0.6471844911575317,\n",
       "  0.613293468952179,\n",
       "  0.6677700281143188,\n",
       "  0.6917762160301208,\n",
       "  0.6218538880348206,\n",
       "  0.5852115750312805,\n",
       "  0.5701692700386047,\n",
       "  0.5631206631660461,\n",
       "  0.5447548031806946,\n",
       "  0.5554596781730652,\n",
       "  0.546464741230011,\n",
       "  0.5985831618309021,\n",
       "  0.5647032260894775,\n",
       "  0.5557414293289185,\n",
       "  0.5569612979888916,\n",
       "  0.5344721078872681,\n",
       "  0.5735964775085449,\n",
       "  0.5495742559432983,\n",
       "  0.5208629369735718],\n",
       " 'val_bce_dice_loss': [1.195053219795227,\n",
       "  0.9412115812301636,\n",
       "  0.8287210464477539,\n",
       "  0.7840392589569092,\n",
       "  0.7772467732429504,\n",
       "  0.7064434885978699,\n",
       "  0.6471844911575317,\n",
       "  0.613293468952179,\n",
       "  0.6677700281143188,\n",
       "  0.6917762160301208,\n",
       "  0.6218538880348206,\n",
       "  0.5852115750312805,\n",
       "  0.5701692700386047,\n",
       "  0.5631206631660461,\n",
       "  0.5447548031806946,\n",
       "  0.5554596781730652,\n",
       "  0.546464741230011,\n",
       "  0.5985831618309021,\n",
       "  0.5647032260894775,\n",
       "  0.5557414293289185,\n",
       "  0.5569612979888916,\n",
       "  0.5344721078872681,\n",
       "  0.5735964775085449,\n",
       "  0.5495742559432983,\n",
       "  0.5208629369735718]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hist[0].history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b6qC3bWzMsB"
   },
   "source": [
    "## Visualize Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "bPnVeKlMzMsC",
    "outputId": "96d839e8-8b55-49f9-c9fb-8bc7ea98569e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualise_train_hist(hist_dict, metric=\"acc\"):\n",
    "    if metric == \"loss\":\n",
    "        y_lbl = \"Loss\"\n",
    "        titl = \"Loss\"\n",
    "        \n",
    "        train_met = \"bce_dice_loss\"\n",
    "        train_lbl = \"Train Loss\"\n",
    "        \n",
    "        val_met = \"bce_dice_loss\"\n",
    "        val_lbl = \"Val Loss\"\n",
    "    else:\n",
    "        y_lbl = \"Accuracy\"\n",
    "        titl = \"Accuracy\"\n",
    "        \n",
    "        train_met = \"accuracy\"\n",
    "        train_lbl = \"Train Acc\"\n",
    "        \n",
    "        val_met = \"val_accuracy\"\n",
    "        val_lbl = \"Val Acc\"\n",
    "        \n",
    "        \n",
    "    batch_0_hist = hist_dict[0]\n",
    "    batch_1_hist = hist_dict[1]\n",
    "    batch_2_hist = hist_dict[2]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    \n",
    "    axs[0].plot(batch_0_hist.epoch,batch_0_hist.history[val_met], color=\"red\", label = val_lbl)\n",
    "    axs[0].plot(batch_0_hist.epoch,batch_0_hist.history[train_met], color=\"blue\", label = train_lbl)\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel(y_lbl)\n",
    "    axs[0].set_title(titl + ' for Batch 1')\n",
    "    axs[0].legend(loc='upper left')\n",
    "    \n",
    "    axs[1].plot(batch_1_hist.epoch,batch_1_hist.history[val_met], color=\"red\", label = val_lbl)\n",
    "    axs[1].plot(batch_1_hist.epoch,batch_1_hist.history[train_met], color=\"blue\", label = train_lbl)\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_title(titl + ' for Batch 2')\n",
    "    axs[1].legend(loc='upper left')\n",
    "    \n",
    "    axs[2].plot(batch_2_hist.epoch,batch_2_hist.history[val_met], color=\"red\", label = \"Val Acc\")\n",
    "    axs[2].plot(batch_2_hist.epoch,batch_2_hist.history[train_met], color=\"blue\", label = train_lbl)\n",
    "    axs[2].set_xlabel('Epochs')\n",
    "    axs[2].set_title(titl + ' for Batch 3')\n",
    "    axs[2].legend(loc='upper left')\n",
    "\n",
    "visualise_train_hist(train_hist, metric=\"loss\") # , metric=\"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2SiTrjf3mHd",
    "outputId": "13103b87-1d1c-4885-f10f-24da588c09db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 21:49:34.565162: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_val, y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y_/ff7_m0c146ddrr_mctd4vpkh0000gn/T/ipykernel_66079/3151121285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hist' is not defined"
     ]
    }
   ],
   "source": [
    "train_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvPrMRdvzMsC"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3EuFYJHzMsC",
    "outputId": "dfd00093-759e-4a0f-f215-4bf3e5e34bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1659982b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUdmEsQ0nGMb",
    "outputId": "da785c32-2f77-4c56-8fc1-3c5ea759fb05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4dbf1024d0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_weights('./keras.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBzRK_Re3tM6",
    "outputId": "ea245f79-2ae1-4cf7-8174-e99a97ca0cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.2607 - accuracy: 0.8005 - 171ms/epoch - 171ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_val, y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag65eW5BzMsC"
   },
   "source": [
    "### Test data\n",
    "\n",
    "Setting the path to test images. `test_y_loc` is the location where the predicted outputs should be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0QaBJQJwzMsD"
   },
   "outputs": [],
   "source": [
    "test_x_loc = \"../../data/test_images/\"\n",
    "test_y_loc = \"../../data/test_preds/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMhdPeTFzMsD"
   },
   "source": [
    "Extract the names of the test images and sort them and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2hhLHht1zMsD"
   },
   "outputs": [],
   "source": [
    "test_img_names = sorted([s[:-4] for s in os.listdir(test_x_loc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pQsVA5PzMsD",
    "outputId": "0b043ca3-72a5-44d2-f018-0c11a108930a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images =  112\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of test images = \", len(test_img_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddz6rvH-zMsD"
   },
   "source": [
    "### Ready the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7CfTwbczMsD"
   },
   "source": [
    "If `USE_CURRENT_MODEL` is False, then provide the location of the saved model you want to use in `SAVED_MODEL_LOC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SAVE_LOC = \"../saved_models/training_24/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5yCDOI6xzMsD"
   },
   "outputs": [],
   "source": [
    "USE_CURRENT_MODEL = False #False\n",
    "SAVED_MODEL_LOC = TRAINING_SAVE_LOC # \"saved_models/training_11/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../saved_models/training_24/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVED_MODEL_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QBDjYnH2zMsE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 21:50:51.669272: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-14 21:50:51.669436: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model loaded from  ../saved_models/training_24/\n"
     ]
    }
   ],
   "source": [
    "if not USE_CURRENT_MODEL:\n",
    "    model = tf.keras.models.load_model(SAVED_MODEL_LOC,custom_objects={'bce_dice_loss':bce_dice_loss})\n",
    "    print(\"Model loaded from \", SAVED_MODEL_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SszEKkDzMsE"
   },
   "source": [
    "### Predict the output and save the mask as .png file\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDZPADpJzMsE",
    "outputId": "20cbbf06-5f71-47af-e0ba-f6e22d931be6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for 112 images\n",
      "Predicting for img 10170\n",
      "1/1 [==============================] - 0s 191ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 23:28:01.370183: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for img 10171\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 10184\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 10566\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 10808\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicting for img 10812\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicting for img 6413\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicting for img 6424\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6427\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6450\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6455\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6457\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6459\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6462\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6463\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6465\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6466\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicting for img 6476\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6481\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6492\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicting for img 6499\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6504\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6546\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6550\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6560\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6569\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6574\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6608\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6644\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6648\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6654\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6656\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6657\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6662\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6677\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6701\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6710\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6717\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6795\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6804\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6854\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6856\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6901\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6908\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6933\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6963\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6974\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6982\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 6994\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 6996\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7009\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7193\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicting for img 7235\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7236\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7241\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7251\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7257\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7263\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7265\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7270\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7272\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7300\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7304\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7308\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7311\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7323\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7330\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7356\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7366\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7412\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7415\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7422\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7427\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7429\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7438\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7457\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7466\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7481\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7485\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7489\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7521\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 7541\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7556\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicting for img 7560\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7580\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7582\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7586\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7593\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7597\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7673\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicting for img 7719\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7721\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 7829\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicting for img 8361\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8516\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8526\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8774\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8786\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8833\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8906\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8947\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 8962\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9006\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9070\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9079\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9084\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9087\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9101\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9110\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9723\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicting for img 9795\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Creating masks for 112 predictions\n",
      "Image saved:  ../../data/test_preds/10170.png\n",
      "Image saved:  ../../data/test_preds/10171.png\n",
      "Image saved:  ../../data/test_preds/10184.png\n",
      "Image saved:  ../../data/test_preds/10566.png\n",
      "Image saved:  ../../data/test_preds/10808.png\n",
      "Image saved:  ../../data/test_preds/10812.png\n",
      "Image saved:  ../../data/test_preds/6413.png\n",
      "Image saved:  ../../data/test_preds/6424.png\n",
      "Image saved:  ../../data/test_preds/6427.png\n",
      "Image saved:  ../../data/test_preds/6450.png\n",
      "Image saved:  ../../data/test_preds/6455.png\n",
      "Image saved:  ../../data/test_preds/6457.png\n",
      "Image saved:  ../../data/test_preds/6459.png\n",
      "Image saved:  ../../data/test_preds/6462.png\n",
      "Image saved:  ../../data/test_preds/6463.png\n",
      "Image saved:  ../../data/test_preds/6465.png\n",
      "Image saved:  ../../data/test_preds/6466.png\n",
      "Image saved:  ../../data/test_preds/6476.png\n",
      "Image saved:  ../../data/test_preds/6481.png\n",
      "Image saved:  ../../data/test_preds/6492.png\n",
      "Image saved:  ../../data/test_preds/6499.png\n",
      "Image saved:  ../../data/test_preds/6504.png\n",
      "Image saved:  ../../data/test_preds/6546.png\n",
      "Image saved:  ../../data/test_preds/6550.png\n",
      "Image saved:  ../../data/test_preds/6560.png\n",
      "Image saved:  ../../data/test_preds/6569.png\n",
      "Image saved:  ../../data/test_preds/6574.png\n",
      "Image saved:  ../../data/test_preds/6608.png\n",
      "Image saved:  ../../data/test_preds/6644.png\n",
      "Image saved:  ../../data/test_preds/6648.png\n",
      "Image saved:  ../../data/test_preds/6654.png\n",
      "Image saved:  ../../data/test_preds/6656.png\n",
      "Image saved:  ../../data/test_preds/6657.png\n",
      "Image saved:  ../../data/test_preds/6662.png\n",
      "Image saved:  ../../data/test_preds/6677.png\n",
      "Image saved:  ../../data/test_preds/6701.png\n",
      "Image saved:  ../../data/test_preds/6710.png\n",
      "Image saved:  ../../data/test_preds/6717.png\n",
      "Image saved:  ../../data/test_preds/6795.png\n",
      "Image saved:  ../../data/test_preds/6804.png\n",
      "Image saved:  ../../data/test_preds/6854.png\n",
      "Image saved:  ../../data/test_preds/6856.png\n",
      "Image saved:  ../../data/test_preds/6901.png\n",
      "Image saved:  ../../data/test_preds/6908.png\n",
      "Image saved:  ../../data/test_preds/6933.png\n",
      "Image saved:  ../../data/test_preds/6963.png\n",
      "Image saved:  ../../data/test_preds/6974.png\n",
      "Image saved:  ../../data/test_preds/6982.png\n",
      "Image saved:  ../../data/test_preds/6994.png\n",
      "Image saved:  ../../data/test_preds/6996.png\n",
      "Image saved:  ../../data/test_preds/7009.png\n",
      "Image saved:  ../../data/test_preds/7193.png\n",
      "Image saved:  ../../data/test_preds/7235.png\n",
      "Image saved:  ../../data/test_preds/7236.png\n",
      "Image saved:  ../../data/test_preds/7241.png\n",
      "Image saved:  ../../data/test_preds/7251.png\n",
      "Image saved:  ../../data/test_preds/7257.png\n",
      "Image saved:  ../../data/test_preds/7263.png\n",
      "Image saved:  ../../data/test_preds/7265.png\n",
      "Image saved:  ../../data/test_preds/7270.png\n",
      "Image saved:  ../../data/test_preds/7272.png\n",
      "Image saved:  ../../data/test_preds/7300.png\n",
      "Image saved:  ../../data/test_preds/7304.png\n",
      "Image saved:  ../../data/test_preds/7308.png\n",
      "Image saved:  ../../data/test_preds/7311.png\n",
      "Image saved:  ../../data/test_preds/7323.png\n",
      "Image saved:  ../../data/test_preds/7330.png\n",
      "Image saved:  ../../data/test_preds/7356.png\n",
      "Image saved:  ../../data/test_preds/7366.png\n",
      "Image saved:  ../../data/test_preds/7412.png\n",
      "Image saved:  ../../data/test_preds/7415.png\n",
      "Image saved:  ../../data/test_preds/7422.png\n",
      "Image saved:  ../../data/test_preds/7427.png\n",
      "Image saved:  ../../data/test_preds/7429.png\n",
      "Image saved:  ../../data/test_preds/7438.png\n",
      "Image saved:  ../../data/test_preds/7457.png\n",
      "Image saved:  ../../data/test_preds/7466.png\n",
      "Image saved:  ../../data/test_preds/7481.png\n",
      "Image saved:  ../../data/test_preds/7485.png\n",
      "Image saved:  ../../data/test_preds/7489.png\n",
      "Image saved:  ../../data/test_preds/7521.png\n",
      "Image saved:  ../../data/test_preds/7541.png\n",
      "Image saved:  ../../data/test_preds/7556.png\n",
      "Image saved:  ../../data/test_preds/7560.png\n",
      "Image saved:  ../../data/test_preds/7580.png\n",
      "Image saved:  ../../data/test_preds/7582.png\n",
      "Image saved:  ../../data/test_preds/7586.png\n",
      "Image saved:  ../../data/test_preds/7593.png\n",
      "Image saved:  ../../data/test_preds/7597.png\n",
      "Image saved:  ../../data/test_preds/7673.png\n",
      "Image saved:  ../../data/test_preds/7719.png\n",
      "Image saved:  ../../data/test_preds/7721.png\n",
      "Image saved:  ../../data/test_preds/7829.png\n",
      "Image saved:  ../../data/test_preds/8361.png\n",
      "Image saved:  ../../data/test_preds/8516.png\n",
      "Image saved:  ../../data/test_preds/8526.png\n",
      "Image saved:  ../../data/test_preds/8774.png\n",
      "Image saved:  ../../data/test_preds/8786.png\n",
      "Image saved:  ../../data/test_preds/8833.png\n",
      "Image saved:  ../../data/test_preds/8906.png\n",
      "Image saved:  ../../data/test_preds/8947.png\n",
      "Image saved:  ../../data/test_preds/8962.png\n",
      "Image saved:  ../../data/test_preds/9006.png\n",
      "Image saved:  ../../data/test_preds/9070.png\n",
      "Image saved:  ../../data/test_preds/9079.png\n",
      "Image saved:  ../../data/test_preds/9084.png\n",
      "Image saved:  ../../data/test_preds/9087.png\n",
      "Image saved:  ../../data/test_preds/9101.png\n",
      "Image saved:  ../../data/test_preds/9106.png\n",
      "Image saved:  ../../data/test_preds/9110.png\n",
      "Image saved:  ../../data/test_preds/9723.png\n",
      "Image saved:  ../../data/test_preds/9795.png\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(image_loc, test_img_names):\n",
    "    y_test = {}\n",
    "    print(\"Predicting for {} images\".format(len(test_img_names)))\n",
    "    for i in test_img_names:\n",
    "        print(\"Predicting for img\",i)\n",
    "        x_test = np.array(PIL.Image.open(image_loc + i + \".jpg\").resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST))/255 #\n",
    "        x_test = x_test.reshape((1,INPUT_SIZE,INPUT_SIZE,3))\n",
    "        #x_test_mean = x_test.mean()\n",
    "        #x_test = x_test - x_test_mean\n",
    "        y_test[i] = model.predict(x_test, verbose=1)\n",
    "        del x_test\n",
    "    return y_test\n",
    "\n",
    "def create_mask(image_dict):\n",
    "    y_test_mask = {}\n",
    "    print(\"Creating masks for {} predictions\".format(len(image_dict)))\n",
    "    for name, img in image_dict.items():\n",
    "        pred_mask = tf.argmax(img, axis=3)\n",
    "        pred_mask= np.uint8(pred_mask[0].numpy())\n",
    "        y_test_mask[name] = pred_mask\n",
    "    return y_test_mask\n",
    "        \n",
    "def save_mask(mask_dict):\n",
    "    for name, mask in mask_dict.items():\n",
    "        im = PIL.Image.fromarray(mask)\n",
    "        im.save(test_y_loc + name + '.png')\n",
    "        print(\"Image saved: \", test_y_loc + name + '.png')\n",
    "\n",
    "preds = get_predictions(test_x_loc,test_img_names)\n",
    "masks = create_mask(preds)\n",
    "save_mask(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-OOCl90zMsE"
   },
   "source": [
    "### Saving the output for kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMgBQq7VzMsF",
    "outputId": "60774ec2-fd7b-4efe-8dfb-e6280b8fc595",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 0/112\n",
      "Saving 1/112\n",
      "Saving 2/112\n",
      "Saving 3/112\n",
      "Saving 4/112\n",
      "Saving 5/112\n",
      "Saving 6/112\n",
      "Saving 7/112\n",
      "Saving 8/112\n",
      "Saving 9/112\n",
      "Saving 10/112\n",
      "Saving 11/112\n",
      "Saving 12/112\n",
      "Saving 13/112\n",
      "Saving 14/112\n",
      "Saving 15/112\n",
      "Saving 16/112\n",
      "Saving 17/112\n",
      "Saving 18/112\n",
      "Saving 19/112\n",
      "Saving 20/112\n",
      "Saving 21/112\n",
      "Saving 22/112\n",
      "Saving 23/112\n",
      "Saving 24/112\n",
      "Saving 25/112\n",
      "Saving 26/112\n",
      "Saving 27/112\n",
      "Saving 28/112\n",
      "Saving 29/112\n",
      "Saving 30/112\n",
      "Saving 31/112\n",
      "Saving 32/112\n",
      "Saving 33/112\n",
      "Saving 34/112\n",
      "Saving 35/112\n",
      "Saving 36/112\n",
      "Saving 37/112\n",
      "Saving 38/112\n",
      "Saving 39/112\n",
      "Saving 40/112\n",
      "Saving 41/112\n",
      "Saving 42/112\n",
      "Saving 43/112\n",
      "Saving 44/112\n",
      "Saving 45/112\n",
      "Saving 46/112\n",
      "Saving 47/112\n",
      "Saving 48/112\n",
      "Saving 49/112\n",
      "Saving 50/112\n",
      "Saving 51/112\n",
      "Saving 52/112\n",
      "Saving 53/112\n",
      "Saving 54/112\n",
      "Saving 55/112\n",
      "Saving 56/112\n",
      "Saving 57/112\n",
      "Saving 58/112\n",
      "Saving 59/112\n",
      "Saving 60/112\n",
      "Saving 61/112\n",
      "Saving 62/112\n",
      "Saving 63/112\n",
      "Saving 64/112\n",
      "Saving 65/112\n",
      "Saving 66/112\n",
      "Saving 67/112\n",
      "Saving 68/112\n",
      "Saving 69/112\n",
      "Saving 70/112\n",
      "Saving 71/112\n",
      "Saving 72/112\n",
      "Saving 73/112\n",
      "Saving 74/112\n",
      "Saving 75/112\n",
      "Saving 76/112\n",
      "Saving 77/112\n",
      "Saving 78/112\n",
      "Saving 79/112\n",
      "Saving 80/112\n",
      "Saving 81/112\n",
      "Saving 82/112\n",
      "Saving 83/112\n",
      "Saving 84/112\n",
      "Saving 85/112\n",
      "Saving 86/112\n",
      "Saving 87/112\n",
      "Saving 88/112\n",
      "Saving 89/112\n",
      "Saving 90/112\n",
      "Saving 91/112\n",
      "Saving 92/112\n",
      "Saving 93/112\n",
      "Saving 94/112\n",
      "Saving 95/112\n",
      "Saving 96/112\n",
      "Saving 97/112\n",
      "Saving 98/112\n",
      "Saving 99/112\n",
      "Saving 100/112\n",
      "Saving 101/112\n",
      "Saving 102/112\n",
      "Saving 103/112\n",
      "Saving 104/112\n",
      "Saving 105/112\n",
      "Saving 106/112\n",
      "Saving 107/112\n",
      "Saving 108/112\n",
      "Saving 109/112\n",
      "Saving 110/112\n",
      "Saving 111/112\n"
     ]
    }
   ],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def create_rles(test_pred_dir):\n",
    "    \"\"\"Used for Kaggle submission: predicts and encode all test images\"\"\"\n",
    "    N = len([i for i in list(os.listdir(test_pred_dir)) if i[-3:] == \"png\"])\n",
    "    with open('../submissions/submission_file.csv', 'w') as f:\n",
    "        f.write('ImageClassId,rle_mask\\n')\n",
    "        for index, i in enumerate(os.listdir(test_pred_dir)):\n",
    "            if i[-3:] == \"png\":\n",
    "                print('Saving {}/{}'.format(index, N))\n",
    "                mask = PIL.Image.open(test_pred_dir + i)\n",
    "                mask = mask.resize((1024, 1024), resample= PIL.Image.NEAREST)\n",
    "                mask = np.array(mask)\n",
    "\n",
    "                for x in range(1, 25):\n",
    "                    enc = rle_encode(mask == x)\n",
    "                    f.write(f\"{i.split('_')[0]}_{x},{enc}\\n\")\n",
    "\n",
    "create_rles(test_y_loc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esfcza8SzMsF"
   },
   "source": [
    "## Making a kaggle submission\n",
    "\n",
    "Make sure you have installed the kaggle package and completed all the steps listed here : https://medium.com/@jeff.daniel77/accessing-the-kaggle-com-api-with-jupyter-notebook-on-windows-d6f330bc6953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Gg1MDR2zMsF",
    "outputId": "588912b2-0866-4426-c129-199a6a8f7d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
      "    self.config_file, self.config_dir))\n",
      "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c fdl21-fdl-dsba -f submissions/submission_file.csv -m \"batch 87 change t20 m0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-kHStYnzMsF"
   },
   "source": [
    "## Visualise the output of the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "yiK1QtY2zMsF",
    "outputId": "d781fda1-0f21-4612-d821-10ea93490fe7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for 1 images\n",
      "Predicting for img 6456\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating masks for 1 predictions\n",
      "dict_keys(['6456'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualise_test(image_name):\n",
    "\n",
    "    img1 = np.array(PIL.Image.open(test_x_loc + image_name +'.jpg').resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST))/255\n",
    "    img2 = mpimg.imread(test_y_loc + image_name +'.png')\n",
    "    # mpimg.imread(test_x_loc + '6413.jpg')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    axs[0].imshow(img1)\n",
    "    axs[0].set_title('Processed Img')\n",
    "    axs[1].imshow(img2)\n",
    "    axs[1].set_title('Predicted img')\n",
    "    \n",
    "def visualise_train(image_name):\n",
    "\n",
    "    img1 = np.array(PIL.Image.open(train_x_loc + image_name +'.jpg').resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST))/255\n",
    "    #img1_mean = img1.mean()\n",
    "    #img1 = img1 - img1_mean\n",
    "    img2 = np.array(PIL.Image.open(train_y_loc + image_name + \".png\").resize((INPUT_SIZE,INPUT_SIZE),resample=PIL.Image.NEAREST))\n",
    "    # mpimg.imread(train_y_loc + image_name +'.png')\n",
    "    # mpimg.imread(test_x_loc + '6413.jpg')\n",
    "\n",
    "    img_name = [image_name]\n",
    "    pred = get_predictions(train_x_loc,img_name)\n",
    "    mask = create_mask(pred)\n",
    "    print(mask.keys())\n",
    "    img3 = mask[image_name]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 15))\n",
    "    axs[0].imshow(img1)\n",
    "    axs[0].set_title('Processed Img')\n",
    "    axs[1].imshow(img2)\n",
    "    axs[1].set_title('True Mask')\n",
    "    axs[2].imshow(img3)\n",
    "    axs[2].set_title('Predicted Mask')\n",
    "    \n",
    "assert(model is not None)\n",
    "# visualise_test('') # provide only image names from 'test_images'\n",
    "visualise_train('6456') # provide only image names from 'train_images'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "harvey_img_seg_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
