{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d840d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "# from utils import *\n",
    "import albumentations as album\n",
    "#import extractors\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8973c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /Users/rishikeshavanrengarajan/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8426436a6b394a28a7a761bb4e60229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/95.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to be run only one time\n",
    "ENCODER = 'resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = 27\n",
    "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d','softmax' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.PSPNet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=27, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c67726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_image_size_open(path, size=(640, 640)):\n",
    "    img = Image.open(path)\n",
    "    side = max(img.size)  # Get the longest side of the image\n",
    "    mask = Image.new('RGB', (side, side), (0, 0, 0))  # Create a square canvas\n",
    "    mask.paste(img, (0, 0))  # Paste the original image on the left top of the canvas\n",
    "    mask = mask.resize(size)  # Resize the new image to a uniform size\n",
    "    return mask\n",
    "\n",
    "def keep_mask_size_open(path, size=(640, 640)):\n",
    "    img = Image.open(path)\n",
    "    side = max(img.size)  # Get the longest side of the image\n",
    "    mask = Image.new('L', (side, side), 0)  # Create a square canvas\n",
    "    mask.paste(img, (0, 0))  # Paste the original image on the left top of the canvas\n",
    "    mask = mask.resize(size)  # Resize the new image to a uniform size\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6669a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(image,n_classes):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    x = F.one_hot(image,n_classes)\n",
    "    return x\n",
    " \n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d528ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        album.PadIfNeeded(min_height=640, min_width=640, always_apply=True, border_mode=0),\n",
    "        album.OneOf([album.HorizontalFlip(p=1),album.VerticalFlip(p=1),album.RandomRotate90(p=1),],p=0.5)\n",
    "#         album.ShiftScaleRotate(scale_limit=0.5,rotate_limit=0,shift_limit=0.1,p=0.5,border_mode=0),\n",
    "#         album.GridDistortion(p=0.5)\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=640, min_width=640, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2,0,1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = [\n",
    "        album.Lambda(image=preprocessing_fn),\n",
    "        album.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cda0116",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y_/ff7_m0c146ddrr_mctd4vpkh0000gn/T/ipykernel_42596/177386211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78fa2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "class BackgroundDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Stanford Background Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        df (str): DataFrame containing images / labels paths\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,path, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.path = path\n",
    "        print(path)\n",
    "        self.name = os.listdir(os.path.join(path, '../data/train_masks_copy'))\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # read images and masks\n",
    "        mask_name = self.name[idx]\n",
    "        mask_path = os.path.join(self.path,'../data/train_masks_copy',mask_name)\n",
    "        img_path = os.path.join(self.path,'../data/train_images_copy',mask_name.replace('png','jpg'))\n",
    "\n",
    "        \n",
    "#         image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "#         mask = cv2.cvtColor(cv2.imread(mask_path),0)\n",
    "        image = keep_image_size_open(img_path)\n",
    "        mask = keep_mask_size_open(mask_path)\n",
    "        \n",
    "        image = np.asarray(image).astype('int64')\n",
    "        mask = np.asarray(mask).astype('int64')\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "        # one-hot-encode the mask  \n",
    "        mask = torch.from_numpy(mask).to(torch.int64)\n",
    "        mask = one_hot_encode(mask,27)\n",
    "        \n",
    "        \n",
    "         #Augmentation\n",
    "        mask = np.asarray(mask).astype('int64')\n",
    "        sample = self.augmentation(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "\n",
    "        \n",
    "         # preprocessing applied only on numpy array image\n",
    "        sample = self.preprocessing(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "            \n",
    "        return image,mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a7d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(3, 640, 640) (27, 640, 640)\n",
      "float32 float32\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = BackgroundDataset('',augmentation=get_training_augmentation(),preprocessing=get_preprocessing(preprocessing_fn))\n",
    "    check_image = data[100][0] # checking for the random 100th image\n",
    "    check_mask = data[10|0][1]\n",
    "    print(check_image.shape,check_mask.shape)\n",
    "    print(check_image.dtype,check_mask.dtype)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ab99191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p  = os.path.join('train_images',os.listdir('train_images')[11])\n",
    "# ip = Image.open(p)\n",
    "# ipa = np.asarray(ip)\n",
    "# ic = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n",
    "# tp=np.transpose(ic,(2,1,0))\n",
    "# tp.shape\n",
    "# tt = torch.from_numpy(ipa).transpose(2,0)\n",
    "# tt.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48676020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bs = 8\n",
    "nw = 0\n",
    "# Splitting into Train and Val\n",
    "full_dataset = BackgroundDataset('',augmentation=get_training_augmentation(),preprocessing=get_preprocessing(preprocessing_fn))\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size   = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Creating  data_loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs,num_workers=nw,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs,num_workers=nw,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a585a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 640, 640]) torch.Size([8, 27, 640, 640])\n",
      "torch.float32 torch.float32\n",
      "240 32\n"
     ]
    }
   ],
   "source": [
    "it, lt = next(iter(train_loader))\n",
    "print(it.shape,lt.shape)\n",
    "print(it.dtype,lt.dtype)\n",
    "\n",
    "print(len(train_loader)*bs,len(val_loader)*bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ef0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = False\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('best_model_pspnet.pth'):\n",
    "    model = torch.load('best_model_pspnet.pth', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e309d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44b02c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(val_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, 'best_model_pspnet.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "18730e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'best_model_pspnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a06e2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PSPNet' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y_/ff7_m0c146ddrr_mctd4vpkh0000gn/T/ipykernel_42596/3184710788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_model_pspnet.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;31m# mypy isn't aware that \"_metadata\" exists in state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PSPNet' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "PATH = 'best_model_pspnet.pth'\n",
    "device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347db22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
